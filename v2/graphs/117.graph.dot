digraph G {
"r9y9/gantts" -> "leimao/Voice-Converter-CycleGAN"
"r9y9/gantts" -> "k2kobayashi/sprocket"
"r9y9/gantts" -> "r9y9/nnmnkwii"
"r9y9/gantts" -> "liusongxiang/StarGAN-Voice-Conversion"
"r9y9/gantts" -> "yanggeng1995/GAN-TTS"
"r9y9/gantts" -> "CSTR-Edinburgh/merlin"
"r9y9/gantts" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"r9y9/gantts" -> "r9y9/wavenet_vocoder"
"r9y9/gantts" -> "r9y9/pysptk"
"r9y9/gantts" -> "NVIDIA/mellotron"
"r9y9/gantts" -> "andabi/parallel-wavenet-vocoder"
"r9y9/gantts" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/gantts" -> "jjery2243542/voice_conversion"
"r9y9/gantts" -> "ksw0306/FloWaveNet"
"r9y9/gantts" -> "r9y9/deepvoice3_pytorch"
"xcmyz/FastSpeech" -> "ming024/FastSpeech2"
"xcmyz/FastSpeech" -> "kan-bayashi/ParallelWaveGAN"
"xcmyz/FastSpeech" -> "soobinseo/Transformer-TTS"
"xcmyz/FastSpeech" -> "NVIDIA/mellotron"
"xcmyz/FastSpeech" -> "wenet-e2e/speech-synthesis-paper"
"xcmyz/FastSpeech" -> "syang1993/gst-tacotron"
"xcmyz/FastSpeech" -> "xiph/LPCNet"
"xcmyz/FastSpeech" -> "seungwonpark/melgan"
"xcmyz/FastSpeech" -> "jaywalnut310/glow-tts"
"xcmyz/FastSpeech" -> "KinglittleQ/GST-Tacotron"
"xcmyz/FastSpeech" -> "descriptinc/melgan-neurips"
"xcmyz/FastSpeech" -> "speechio/chinese_text_normalization" ["e"=1]
"xcmyz/FastSpeech" -> "NVIDIA/waveglow"
"xcmyz/FastSpeech" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"xcmyz/FastSpeech" -> "fatchord/WaveRNN"
"facebookresearch/music-translation" -> "sumuzhao/CycleGAN-Music-Style-Transfer"
"facebookresearch/music-translation" -> "joansj/blow"
"facebookresearch/music-translation" -> "ksw0306/FloWaveNet"
"facebookresearch/music-translation" -> "NVIDIA/nv-wavenet"
"facebookresearch/music-translation" -> "descriptinc/melgan-neurips"
"facebookresearch/music-translation" -> "marcoppasini/MelGAN-VC"
"facebookresearch/music-translation" -> "jason9693/MusicTransformer-tensorflow2.0" ["e"=1]
"facebookresearch/music-translation" -> "gabolsgabs/DALI" ["e"=1]
"facebookresearch/music-translation" -> "mairaksi/PiENet"
"facebookresearch/music-translation" -> "NVIDIA/mellotron"
"facebookresearch/music-translation" -> "DmitryUlyanov/neural-style-audio-tf"
"facebookresearch/music-translation" -> "ksw0306/ClariNet"
"facebookresearch/music-translation" -> "huangsicong/TimbreTron"
"facebookresearch/music-translation" -> "MTG/WGANSing" ["e"=1]
"facebookresearch/music-translation" -> "jordipons/music-audio-tagging-at-scale-models" ["e"=1]
"pndurette/gTTS" -> "nateshmbhat/pyttsx3" ["e"=1]
"pndurette/gTTS" -> "Uberi/speech_recognition" ["e"=1]
"pndurette/gTTS" -> "RapidWareTech/pyttsx"
"pndurette/gTTS" -> "ssut/py-googletrans" ["e"=1]
"pndurette/gTTS" -> "mozilla/TTS"
"pndurette/gTTS" -> "jiaaro/pydub" ["e"=1]
"pndurette/gTTS" -> "readbeyond/aeneas"
"pndurette/gTTS" -> "TaylorSMarks/playsound" ["e"=1]
"pndurette/gTTS" -> "marytts/marytts"
"pndurette/gTTS" -> "buriburisuri/speech-to-text-wavenet"
"pndurette/gTTS" -> "rany2/edge-tts" ["e"=1]
"pndurette/gTTS" -> "goldsmith/Wikipedia" ["e"=1]
"pndurette/gTTS" -> "r9y9/deepvoice3_pytorch"
"pndurette/gTTS" -> "desbma/GoogleSpeech"
"pndurette/gTTS" -> "Kyubyong/tacotron"
"quadrismegistus/prosodic" -> "quadrismegistus/poesy"
"quadrismegistus/prosodic" -> "Helsinki-NLP/prosody"
"quadrismegistus/prosodic" -> "manexagirrezabal/zeuscansion"
"quadrismegistus/prosodic" -> "quadrismegistus/litlab-poetry"
"quadrismegistus/prosodic" -> "hyperreality/Poetry-Tools"
"quadrismegistus/prosodic" -> "a-coles/deep-rhyme-detection"
"quadrismegistus/prosodic" -> "jproft/Scandroid"
"mozilla/TTS" -> "coqui-ai/TTS" ["e"=1]
"mozilla/TTS" -> "TensorSpeech/TensorFlowTTS"
"mozilla/TTS" -> "NVIDIA/tacotron2"
"mozilla/TTS" -> "mozilla/DeepSpeech" ["e"=1]
"mozilla/TTS" -> "keithito/tacotron"
"mozilla/TTS" -> "fatchord/WaveRNN"
"mozilla/TTS" -> "espnet/espnet" ["e"=1]
"mozilla/TTS" -> "Rayhane-mamah/Tacotron-2"
"mozilla/TTS" -> "neonbjb/tortoise-tts" ["e"=1]
"mozilla/TTS" -> "r9y9/wavenet_vocoder"
"mozilla/TTS" -> "ming024/FastSpeech2"
"mozilla/TTS" -> "jaywalnut310/vits" ["e"=1]
"mozilla/TTS" -> "NVIDIA/waveglow"
"mozilla/TTS" -> "espeak-ng/espeak-ng" ["e"=1]
"mozilla/TTS" -> "r9y9/deepvoice3_pytorch"
"niquejoe/Classification-of-Depression-on-Social-Media-Using-Text-Mining" -> "isrugeek/depression_detection"
"niquejoe/Classification-of-Depression-on-Social-Media-Using-Text-Mining" -> "halolimat/Social-media-Depression-Detector"
"niquejoe/Classification-of-Depression-on-Social-Media-Using-Text-Mining" -> "AshwanthRamji/Depression-Sentiment-Analysis-with-Twitter-Data"
"KinglittleQ/GST-Tacotron" -> "syang1993/gst-tacotron"
"KinglittleQ/GST-Tacotron" -> "jinhan/tacotron2-gst"
"KinglittleQ/GST-Tacotron" -> "jinhan/tacotron2-vae"
"KinglittleQ/GST-Tacotron" -> "rishikksh20/vae_tacotron2"
"KinglittleQ/GST-Tacotron" -> "NVIDIA/mellotron"
"KinglittleQ/GST-Tacotron" -> "xcmyz/FastSpeech"
"KinglittleQ/GST-Tacotron" -> "Wendison/VQMIVC"
"KinglittleQ/GST-Tacotron" -> "nii-yamagishilab/multi-speaker-tacotron"
"KinglittleQ/GST-Tacotron" -> "keonlee9420/Parallel-Tacotron2"
"KinglittleQ/GST-Tacotron" -> "yanggeng1995/vae_tacotron"
"KinglittleQ/GST-Tacotron" -> "rishikksh20/gmvae_tacotron"
"KinglittleQ/GST-Tacotron" -> "ide8/tacotron2"
"KinglittleQ/GST-Tacotron" -> "jjery2243542/adaptive_voice_conversion"
"KinglittleQ/GST-Tacotron" -> "kan-bayashi/ParallelWaveGAN"
"KinglittleQ/GST-Tacotron" -> "keonlee9420/STYLER" ["e"=1]
"jjery2243542/adaptive_voice_conversion" -> "jjery2243542/voice_conversion"
"jjery2243542/adaptive_voice_conversion" -> "auspicious3000/autovc"
"jjery2243542/adaptive_voice_conversion" -> "jxzhanggg/nonparaSeq2seqVC_code"
"jjery2243542/adaptive_voice_conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"jjery2243542/adaptive_voice_conversion" -> "auspicious3000/SpeechSplit"
"jjery2243542/adaptive_voice_conversion" -> "yistLin/FragmentVC"
"jjery2243542/adaptive_voice_conversion" -> "KimythAnly/AGAIN-VC"
"jjery2243542/adaptive_voice_conversion" -> "bshall/ZeroSpeech"
"jjery2243542/adaptive_voice_conversion" -> "Wendison/VQMIVC"
"jjery2243542/adaptive_voice_conversion" -> "liusongxiang/ppg-vc"
"jjery2243542/adaptive_voice_conversion" -> "k2kobayashi/sprocket"
"jjery2243542/adaptive_voice_conversion" -> "cyhuang-tw/AdaIN-VC"
"jjery2243542/adaptive_voice_conversion" -> "ericwudayi/SkipVQVC"
"jjery2243542/adaptive_voice_conversion" -> "descriptinc/melgan-neurips"
"jjery2243542/adaptive_voice_conversion" -> "kan-bayashi/ParallelWaveGAN"
"Hiroshiba/realtime-yukarin" -> "Hiroshiba/yukarin"
"Hiroshiba/realtime-yukarin" -> "Hiroshiba/become-yukarin"
"Hiroshiba/realtime-yukarin" -> "k2kobayashi/sprocket"
"Hiroshiba/realtime-yukarin" -> "k2kobayashi/crank"
"Hiroshiba/realtime-yukarin" -> "mmorise/kiritan_singing"
"OpenVoiceOS/ovos-buildroot" -> "JarbasHiveMind/ZZZ-HiveMind-core"
"OpenVoiceOS/ovos-buildroot" -> "MycroftAI/mycroft-gui"
"OpenVoiceOS/ovos-buildroot" -> "OpenVoiceOS/ovos-personal-backend"
"OpenVoiceOS/ovos-buildroot" -> "ChanceNCounter/awesome-mycroft-community"
"OpenVoiceOS/ovos-buildroot" -> "OpenVoiceOS/ovos-core"
"OpenVoiceOS/ovos-buildroot" -> "alexisdiaz008/react-mycroft-gui"
"OpenVoiceOS/ovos-buildroot" -> "OpenVoiceOS/raspOVOS"
"OpenVoiceOS/ovos-buildroot" -> "OpenVoiceOS/ovos-installer"
"OpenVoiceOS/ovos-buildroot" -> "LinusSkucas/communications-skill"
"OpenVoiceOS/ovos-buildroot" -> "OpenVoiceOS/ovos-docker"
"OpenVoiceOS/ovos-buildroot" -> "MycroftAI/Precise-Community-Data"
"OpenVoiceOS/ovos-buildroot" -> "gras64/learning-skill"
"OpenVoiceOS/ovos-buildroot" -> "pcwii/mesh-skill"
"OpenVoiceOS/ovos-buildroot" -> "OpenVoiceOS/ovos-skill-fallback-chatgpt"
"OpenVoiceOS/ovos-buildroot" -> "el-tocino/localcroft"
"MattShannon/mcd" -> "SamuelBroughton/Mel-Cepstral-Distortion"
"MattShannon/mcd" -> "ttslr/python-MCD"
"MattShannon/mcd" -> "thuhcsi/VAENAR-TTS"
"NVIDIA/waveglow" -> "NVIDIA/tacotron2"
"NVIDIA/waveglow" -> "r9y9/wavenet_vocoder"
"NVIDIA/waveglow" -> "fatchord/WaveRNN"
"NVIDIA/waveglow" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/waveglow" -> "xiph/LPCNet"
"NVIDIA/waveglow" -> "Rayhane-mamah/Tacotron-2"
"NVIDIA/waveglow" -> "descriptinc/melgan-neurips"
"NVIDIA/waveglow" -> "NVIDIA/mellotron"
"NVIDIA/waveglow" -> "xcmyz/FastSpeech"
"NVIDIA/waveglow" -> "jik876/hifi-gan"
"NVIDIA/waveglow" -> "keithito/tacotron"
"NVIDIA/waveglow" -> "seungwonpark/melgan"
"NVIDIA/waveglow" -> "NVIDIA/nv-wavenet"
"NVIDIA/waveglow" -> "ksw0306/FloWaveNet"
"NVIDIA/waveglow" -> "NVIDIA/flowtron"
"Kyubyong/dc_tts" -> "Kyubyong/tacotron"
"Kyubyong/dc_tts" -> "Kyubyong/deepvoice3"
"Kyubyong/dc_tts" -> "keithito/tacotron"
"Kyubyong/dc_tts" -> "Rayhane-mamah/Tacotron-2"
"Kyubyong/dc_tts" -> "r9y9/wavenet_vocoder"
"Kyubyong/dc_tts" -> "fatchord/WaveRNN"
"Kyubyong/dc_tts" -> "syang1993/gst-tacotron"
"Kyubyong/dc_tts" -> "r9y9/deepvoice3_pytorch"
"Kyubyong/dc_tts" -> "CSTR-Edinburgh/merlin"
"Kyubyong/dc_tts" -> "soobinseo/Transformer-TTS"
"Kyubyong/dc_tts" -> "xcmyz/FastSpeech"
"Kyubyong/dc_tts" -> "Kyubyong/speaker_adapted_tts"
"Kyubyong/dc_tts" -> "NVIDIA/waveglow"
"Kyubyong/dc_tts" -> "xiph/LPCNet"
"Kyubyong/dc_tts" -> "NVIDIA/tacotron2"
"RapidWareTech/pyttsx" -> "parente/espeakbox"
"Rayhane-mamah/Tacotron-2" -> "keithito/tacotron"
"Rayhane-mamah/Tacotron-2" -> "r9y9/wavenet_vocoder"
"Rayhane-mamah/Tacotron-2" -> "NVIDIA/tacotron2"
"Rayhane-mamah/Tacotron-2" -> "fatchord/WaveRNN"
"Rayhane-mamah/Tacotron-2" -> "NVIDIA/waveglow"
"Rayhane-mamah/Tacotron-2" -> "Kyubyong/tacotron"
"Rayhane-mamah/Tacotron-2" -> "xiph/LPCNet"
"Rayhane-mamah/Tacotron-2" -> "r9y9/deepvoice3_pytorch"
"Rayhane-mamah/Tacotron-2" -> "syang1993/gst-tacotron"
"Rayhane-mamah/Tacotron-2" -> "kan-bayashi/ParallelWaveGAN"
"Rayhane-mamah/Tacotron-2" -> "xcmyz/FastSpeech"
"Rayhane-mamah/Tacotron-2" -> "TensorSpeech/TensorFlowTTS"
"Rayhane-mamah/Tacotron-2" -> "CSTR-Edinburgh/merlin"
"Rayhane-mamah/Tacotron-2" -> "descriptinc/melgan-neurips"
"Rayhane-mamah/Tacotron-2" -> "jik876/hifi-gan"
"r9y9/wavenet_vocoder" -> "Rayhane-mamah/Tacotron-2"
"r9y9/wavenet_vocoder" -> "fatchord/WaveRNN"
"r9y9/wavenet_vocoder" -> "NVIDIA/waveglow"
"r9y9/wavenet_vocoder" -> "keithito/tacotron"
"r9y9/wavenet_vocoder" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/wavenet_vocoder" -> "r9y9/deepvoice3_pytorch"
"r9y9/wavenet_vocoder" -> "descriptinc/melgan-neurips"
"r9y9/wavenet_vocoder" -> "NVIDIA/tacotron2"
"r9y9/wavenet_vocoder" -> "xiph/LPCNet"
"r9y9/wavenet_vocoder" -> "ibab/tensorflow-wavenet"
"r9y9/wavenet_vocoder" -> "CSTR-Edinburgh/merlin"
"r9y9/wavenet_vocoder" -> "tomlepaine/fast-wavenet"
"r9y9/wavenet_vocoder" -> "NVIDIA/nv-wavenet"
"r9y9/wavenet_vocoder" -> "Kyubyong/tacotron"
"r9y9/wavenet_vocoder" -> "kan-bayashi/PytorchWaveNetVocoder"
"opensourceteams/google-sdk-speech-to-text" -> "Renovamen/Speech-and-Text"
"NVIDIA/tacotron2" -> "NVIDIA/waveglow"
"NVIDIA/tacotron2" -> "Rayhane-mamah/Tacotron-2"
"NVIDIA/tacotron2" -> "keithito/tacotron"
"NVIDIA/tacotron2" -> "r9y9/wavenet_vocoder"
"NVIDIA/tacotron2" -> "jik876/hifi-gan"
"NVIDIA/tacotron2" -> "fatchord/WaveRNN"
"NVIDIA/tacotron2" -> "ming024/FastSpeech2"
"NVIDIA/tacotron2" -> "espnet/espnet" ["e"=1]
"NVIDIA/tacotron2" -> "r9y9/deepvoice3_pytorch"
"NVIDIA/tacotron2" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/tacotron2" -> "mozilla/TTS"
"NVIDIA/tacotron2" -> "TensorSpeech/TensorFlowTTS"
"NVIDIA/tacotron2" -> "jaywalnut310/vits" ["e"=1]
"NVIDIA/tacotron2" -> "xcmyz/FastSpeech"
"NVIDIA/tacotron2" -> "NVIDIA/mellotron"
"kylebgorman/textgrid" -> "hbuschme/TextGridTools"
"kylebgorman/textgrid" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"kylebgorman/textgrid" -> "timmahrt/praatIO"
"kylebgorman/textgrid" -> "YannickJadoul/Parselmouth"
"kylebgorman/textgrid" -> "r9y9/pysptk"
"MycroftAI/mycroft-gui" -> "alexisdiaz008/react-mycroft-gui"
"MycroftAI/mycroft-gui" -> "OpenVoiceOS/ovos-buildroot"
"begeekmyfriend/tacotron" -> "begeekmyfriend/Tacotron-2"
"begeekmyfriend/tacotron" -> "Jackiexiao/MTTS"
"begeekmyfriend/tacotron" -> "atomicoo/tacotron2-mandarin"
"begeekmyfriend/tacotron" -> "syang1993/gst-tacotron"
"begeekmyfriend/tacotron" -> "keithito/tacotron"
"begeekmyfriend/tacotron" -> "JasonWei512/Tacotron-2-Chinese"
"begeekmyfriend/tacotron" -> "Kyubyong/tacotron"
"begeekmyfriend/tacotron" -> "Rayhane-mamah/Tacotron-2"
"begeekmyfriend/tacotron" -> "CSTR-Edinburgh/merlin"
"begeekmyfriend/tacotron" -> "Kyubyong/deepvoice3"
"begeekmyfriend/tacotron" -> "xcmyz/FastSpeech"
"begeekmyfriend/tacotron" -> "rishikksh20/vae_tacotron2"
"begeekmyfriend/tacotron" -> "zuoxiang95/tacotron-1"
"open-speech/speech-aligner" -> "Kyubyong/g2pC"
"open-speech/speech-aligner" -> "aishell-foundation/DaCiDian" ["e"=1]
"open-speech/speech-aligner" -> "KuangDD/phkit"
"open-speech/speech-aligner" -> "Jackiexiao/MTTS"
"open-speech/speech-aligner" -> "kakaobrain/g2pm"
"open-speech/speech-aligner" -> "speechio/chinese_text_normalization" ["e"=1]
"open-speech/speech-aligner" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"open-speech/speech-aligner" -> "KuangDD/zhvoice"
"open-speech/speech-aligner" -> "open-speech/cn-text-normalizer"
"open-speech/speech-aligner" -> "thuhcsi/Crystal"
"open-speech/speech-aligner" -> "npuichigo/waveglow"
"open-speech/speech-aligner" -> "XiaoMi/kaldi-onnx" ["e"=1]
"open-speech/speech-aligner" -> "xcmyz/FastSpeech"
"open-speech/speech-aligner" -> "speechio/BigCiDian" ["e"=1]
"open-speech/speech-aligner" -> "thu-spmi/CAT" ["e"=1]
"andabi/deep-voice-conversion" -> "buriburisuri/speech-to-text-wavenet"
"andabi/deep-voice-conversion" -> "r9y9/deepvoice3_pytorch"
"andabi/deep-voice-conversion" -> "r9y9/wavenet_vocoder"
"andabi/deep-voice-conversion" -> "MrNothing/AI-Blocks" ["e"=1]
"andabi/deep-voice-conversion" -> "k2kobayashi/sprocket"
"andabi/deep-voice-conversion" -> "Kyubyong/tacotron"
"andabi/deep-voice-conversion" -> "keithito/tacotron"
"andabi/deep-voice-conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"andabi/deep-voice-conversion" -> "Rayhane-mamah/Tacotron-2"
"andabi/deep-voice-conversion" -> "ibab/tensorflow-wavenet"
"andabi/deep-voice-conversion" -> "NVIDIA/waveglow"
"andabi/deep-voice-conversion" -> "yunjey/stargan" ["e"=1]
"andabi/deep-voice-conversion" -> "oarriaga/face_classification" ["e"=1]
"andabi/deep-voice-conversion" -> "auspicious3000/autovc"
"andabi/deep-voice-conversion" -> "junyanz/iGAN" ["e"=1]
"liusongxiang/StarGAN-Voice-Conversion" -> "hujinsen/pytorch-StarGAN-VC"
"liusongxiang/StarGAN-Voice-Conversion" -> "leimao/Voice-Converter-CycleGAN"
"liusongxiang/StarGAN-Voice-Conversion" -> "hujinsen/StarGAN-Voice-Conversion"
"liusongxiang/StarGAN-Voice-Conversion" -> "auspicious3000/autovc"
"liusongxiang/StarGAN-Voice-Conversion" -> "k2kobayashi/sprocket"
"liusongxiang/StarGAN-Voice-Conversion" -> "jjery2243542/adaptive_voice_conversion"
"liusongxiang/StarGAN-Voice-Conversion" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"liusongxiang/StarGAN-Voice-Conversion" -> "jjery2243542/voice_conversion"
"liusongxiang/StarGAN-Voice-Conversion" -> "jxzhanggg/nonparaSeq2seqVC_code"
"liusongxiang/StarGAN-Voice-Conversion" -> "joansj/blow"
"liusongxiang/StarGAN-Voice-Conversion" -> "auspicious3000/SpeechSplit"
"liusongxiang/StarGAN-Voice-Conversion" -> "liusongxiang/ppg-vc"
"liusongxiang/StarGAN-Voice-Conversion" -> "jackaduma/CycleGAN-VC2"
"liusongxiang/StarGAN-Voice-Conversion" -> "r9y9/gantts"
"liusongxiang/StarGAN-Voice-Conversion" -> "bshall/ZeroSpeech"
"hujinsen/pytorch-StarGAN-VC" -> "liusongxiang/StarGAN-Voice-Conversion"
"hujinsen/pytorch-StarGAN-VC" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"hujinsen/pytorch-StarGAN-VC" -> "hujinsen/StarGAN-Voice-Conversion"
"hujinsen/pytorch-StarGAN-VC" -> "jjery2243542/adaptive_voice_conversion"
"hujinsen/pytorch-StarGAN-VC" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"hujinsen/pytorch-StarGAN-VC" -> "joansj/blow"
"hujinsen/pytorch-StarGAN-VC" -> "jjery2243542/voice_conversion"
"hujinsen/pytorch-StarGAN-VC" -> "leimao/Voice-Converter-CycleGAN"
"hujinsen/pytorch-StarGAN-VC" -> "marcoppasini/MelGAN-VC"
"hujinsen/pytorch-StarGAN-VC" -> "Oscarshu0719/pytorch-StarGAN-VC2"
"hujinsen/pytorch-StarGAN-VC" -> "ericwudayi/SkipVQVC"
"hujinsen/pytorch-StarGAN-VC" -> "bigpon/vcc20_baseline_cyclevae"
"hujinsen/pytorch-StarGAN-VC" -> "auspicious3000/autovc"
"hujinsen/pytorch-StarGAN-VC" -> "k2kobayashi/sprocket"
"hujinsen/pytorch-StarGAN-VC" -> "jxzhanggg/nonparaSeq2seqVC_code"
"auspicious3000/autovc" -> "auspicious3000/SpeechSplit"
"auspicious3000/autovc" -> "jjery2243542/adaptive_voice_conversion"
"auspicious3000/autovc" -> "liusongxiang/StarGAN-Voice-Conversion"
"auspicious3000/autovc" -> "auspicious3000/AutoPST"
"auspicious3000/autovc" -> "kan-bayashi/ParallelWaveGAN"
"auspicious3000/autovc" -> "Wendison/VQMIVC"
"auspicious3000/autovc" -> "jxzhanggg/nonparaSeq2seqVC_code"
"auspicious3000/autovc" -> "leimao/Voice-Converter-CycleGAN"
"auspicious3000/autovc" -> "descriptinc/melgan-neurips"
"auspicious3000/autovc" -> "jik876/hifi-gan"
"auspicious3000/autovc" -> "bshall/ZeroSpeech"
"auspicious3000/autovc" -> "NVIDIA/mellotron"
"auspicious3000/autovc" -> "liusongxiang/ppg-vc"
"auspicious3000/autovc" -> "jackaduma/CycleGAN-VC2"
"auspicious3000/autovc" -> "jjery2243542/voice_conversion"
"joansj/blow" -> "ericwudayi/SkipVQVC"
"joansj/blow" -> "jxzhanggg/nonparaSeq2seqVC_code"
"joansj/blow" -> "jaywalnut310/waveglow-vqvae"
"MycroftAI/mimic2" -> "MycroftAI/mimic-recording-studio"
"MycroftAI/mimic2" -> "MycroftAI/mimic1"
"MycroftAI/mimic2" -> "keithito/tacotron"
"MycroftAI/mimic2" -> "MycroftAI/selene-backend"
"MycroftAI/mimic2" -> "google/voice-builder"
"MycroftAI/mimic2" -> "MycroftAI/adapt"
"MycroftAI/mimic2" -> "Rayhane-mamah/Tacotron-2"
"MycroftAI/mimic2" -> "MycroftAI/mycroft-gui"
"MycroftAI/mimic2" -> "Kyubyong/dc_tts"
"MycroftAI/mimic2" -> "thorstenMueller/Thorsten-Voice"
"MycroftAI/mimic2" -> "NVIDIA/flowtron"
"MycroftAI/mimic2" -> "r9y9/deepvoice3_pytorch"
"soobinseo/Transformer-TTS" -> "xcmyz/FastSpeech"
"soobinseo/Transformer-TTS" -> "NVIDIA/mellotron"
"soobinseo/Transformer-TTS" -> "kan-bayashi/ParallelWaveGAN"
"soobinseo/Transformer-TTS" -> "seungwonpark/melgan"
"soobinseo/Transformer-TTS" -> "jaywalnut310/glow-tts"
"soobinseo/Transformer-TTS" -> "descriptinc/melgan-neurips"
"soobinseo/Transformer-TTS" -> "spring-media/TransformerTTS"
"soobinseo/Transformer-TTS" -> "wenet-e2e/speech-synthesis-paper"
"soobinseo/Transformer-TTS" -> "ksw0306/ClariNet"
"soobinseo/Transformer-TTS" -> "yanggeng1995/GAN-TTS"
"soobinseo/Transformer-TTS" -> "ksw0306/FloWaveNet"
"soobinseo/Transformer-TTS" -> "rishikksh20/VocGAN"
"soobinseo/Transformer-TTS" -> "syang1993/gst-tacotron"
"soobinseo/Transformer-TTS" -> "KinglittleQ/GST-Tacotron"
"soobinseo/Transformer-TTS" -> "jik876/hifi-gan"
"fatchord/WaveRNN" -> "r9y9/wavenet_vocoder"
"fatchord/WaveRNN" -> "xiph/LPCNet"
"fatchord/WaveRNN" -> "Rayhane-mamah/Tacotron-2"
"fatchord/WaveRNN" -> "NVIDIA/waveglow"
"fatchord/WaveRNN" -> "kan-bayashi/ParallelWaveGAN"
"fatchord/WaveRNN" -> "keithito/tacotron"
"fatchord/WaveRNN" -> "xcmyz/FastSpeech"
"fatchord/WaveRNN" -> "NVIDIA/tacotron2"
"fatchord/WaveRNN" -> "jik876/hifi-gan"
"fatchord/WaveRNN" -> "NVIDIA/mellotron"
"fatchord/WaveRNN" -> "descriptinc/melgan-neurips"
"fatchord/WaveRNN" -> "TensorSpeech/TensorFlowTTS"
"fatchord/WaveRNN" -> "CSTR-Edinburgh/merlin"
"fatchord/WaveRNN" -> "r9y9/deepvoice3_pytorch"
"fatchord/WaveRNN" -> "auspicious3000/autovc"
"tiberiu44/TTS-Cube" -> "bfs18/nsynth_wavenet"
"tiberiu44/TTS-Cube" -> "mkotha/WaveRNN"
"tiberiu44/TTS-Cube" -> "h-meru/Tacotron-WaveRNN"
"tiberiu44/TTS-Cube" -> "geneing/WaveRNN-Pytorch"
"tiberiu44/TTS-Cube" -> "npuichigo/waveglow"
"tiberiu44/TTS-Cube" -> "erogol/FFTNet"
"tiberiu44/TTS-Cube" -> "G-Wang/WaveRNN-Pytorch"
"tiberiu44/TTS-Cube" -> "ksw0306/ClariNet"
"tiberiu44/TTS-Cube" -> "azraelkuan/parallel_wavenet_vocoder"
"tiberiu44/TTS-Cube" -> "syang1993/FFTNet"
"tiberiu44/TTS-Cube" -> "kan-bayashi/PytorchWaveNetVocoder"
"tiberiu44/TTS-Cube" -> "begeekmyfriend/Tacotron-2"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "Sharad24/Neural-Voice-Cloning-with-Few-Samples"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "r9y9/deepvoice3_pytorch"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "liusongxiang/StarGAN-Voice-Conversion"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "NVIDIA/mellotron"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "seungwonpark/melgan"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "IEEE-NITK/Neural-Voice-Cloning"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "nii-yamagishilab/multi-speaker-tacotron"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "Tomiinek/Multilingual_Text_to_Speech"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "syang1993/gst-tacotron"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "r9y9/gantts"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "vlomme/Multi-Tacotron-Voice-Cloning"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "jxzhanggg/nonparaSeq2seqVC_code"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "descriptinc/melgan-neurips"
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" -> "KinglittleQ/GST-Tacotron"
"nii-yamagishilab/self-attention-tacotron" -> "nii-yamagishilab/tacotron2"
"nii-yamagishilab/self-attention-tacotron" -> "yanggeng1995/vae_tacotron"
"numediart/MBROLA" -> "numediart/MBROLA-voices"
"numediart/MBROLA" -> "numediart/MBROLATOR"
"numediart/MBROLA" -> "hadware/voxpopuli"
"chrisdonahue/wavegan" -> "descriptinc/melgan-neurips"
"chrisdonahue/wavegan" -> "kan-bayashi/ParallelWaveGAN"
"chrisdonahue/wavegan" -> "mostafaelaraby/wavegan-pytorch"
"chrisdonahue/wavegan" -> "r9y9/wavenet_vocoder"
"chrisdonahue/wavegan" -> "magenta/ddsp" ["e"=1]
"chrisdonahue/wavegan" -> "NVIDIA/waveglow"
"chrisdonahue/wavegan" -> "xiph/LPCNet"
"chrisdonahue/wavegan" -> "chaeyoung-lee/cwavegan"
"chrisdonahue/wavegan" -> "santi-pdp/segan" ["e"=1]
"chrisdonahue/wavegan" -> "f90/Wave-U-Net" ["e"=1]
"chrisdonahue/wavegan" -> "NVIDIA/nv-wavenet"
"chrisdonahue/wavegan" -> "acids-ircam/RAVE" ["e"=1]
"chrisdonahue/wavegan" -> "fatchord/WaveRNN"
"chrisdonahue/wavegan" -> "vincentherrmann/pytorch-wavenet"
"chrisdonahue/wavegan" -> "seungwonpark/melgan"
"HaiFengZeng/clari_wavenet_vocoder" -> "dhgrs/chainer-ClariNet"
"nii-yamagishilab/tacotron2" -> "nii-yamagishilab/self-attention-tacotron"
"facebookresearch/SING" -> "tuan3w/cnn_vocoder"
"facebookresearch/SING" -> "mkotha/WaveRNN"
"xiph/LPCNet" -> "kan-bayashi/ParallelWaveGAN"
"xiph/LPCNet" -> "fatchord/WaveRNN"
"xiph/LPCNet" -> "xcmyz/FastSpeech"
"xiph/LPCNet" -> "descriptinc/melgan-neurips"
"xiph/LPCNet" -> "mmorise/World"
"xiph/LPCNet" -> "NVIDIA/waveglow"
"xiph/LPCNet" -> "jik876/hifi-gan"
"xiph/LPCNet" -> "syang1993/gst-tacotron"
"xiph/LPCNet" -> "seungwonpark/melgan"
"xiph/LPCNet" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"xiph/LPCNet" -> "NVIDIA/mellotron"
"xiph/LPCNet" -> "aliutkus/speechmetrics" ["e"=1]
"xiph/LPCNet" -> "Rayhane-mamah/Tacotron-2"
"xiph/LPCNet" -> "CSTR-Edinburgh/merlin"
"xiph/LPCNet" -> "r9y9/wavenet_vocoder"
"npuichigo/waveglow" -> "azraelkuan/parallel_wavenet_vocoder"
"npuichigo/waveglow" -> "ksw0306/WaveVAE"
"npuichigo/waveglow" -> "nii-yamagishilab/TSNetVocoder"
"npuichigo/waveglow" -> "npuichigo/voicenet"
"npuichigo/waveglow" -> "ksw0306/FloWaveNet"
"npuichigo/waveglow" -> "ksw0306/ClariNet"
"npuichigo/waveglow" -> "mkotha/WaveRNN"
"npuichigo/waveglow" -> "tiberiu44/TTS-Cube"
"npuichigo/waveglow" -> "Kyubyong/expressive_tacotron"
"npuichigo/waveglow" -> "h-meru/Tacotron-WaveRNN"
"npuichigo/waveglow" -> "kan-bayashi/PytorchWaveNetVocoder"
"npuichigo/waveglow" -> "HaiFengZeng/clari_wavenet_vocoder"
"npuichigo/waveglow" -> "bfs18/nsynth_wavenet"
"ksw0306/ClariNet" -> "ksw0306/FloWaveNet"
"ksw0306/ClariNet" -> "azraelkuan/parallel_wavenet_vocoder"
"ksw0306/ClariNet" -> "npuichigo/waveglow"
"ksw0306/ClariNet" -> "mkotha/WaveRNN"
"ksw0306/ClariNet" -> "andabi/parallel-wavenet-vocoder"
"ksw0306/ClariNet" -> "ksw0306/WaveVAE"
"ksw0306/ClariNet" -> "geneing/WaveRNN-Pytorch"
"ksw0306/ClariNet" -> "bshall/UniversalVocoding"
"ksw0306/ClariNet" -> "nii-yamagishilab/TSNetVocoder"
"ksw0306/ClariNet" -> "tianrengao/SqueezeWave"
"ksw0306/ClariNet" -> "bfs18/nsynth_wavenet"
"ksw0306/ClariNet" -> "kan-bayashi/PytorchWaveNetVocoder"
"ksw0306/ClariNet" -> "soobinseo/Transformer-TTS"
"ksw0306/ClariNet" -> "tiberiu44/TTS-Cube"
"ksw0306/ClariNet" -> "Jackiexiao/MTTS"
"cnlinxi/style-token_tacotron2" -> "yanggeng1995/vae_tacotron"
"hrbigelow/ae-wavenet" -> "swasun/VQ-VAE-Speech"
"hrbigelow/ae-wavenet" -> "DongyaoZhu/VQ-VAE-WaveNet"
"hrbigelow/ae-wavenet" -> "mkotha/WaveRNN"
"mkotha/WaveRNN" -> "kan-bayashi/PytorchWaveNetVocoder"
"mkotha/WaveRNN" -> "h-meru/Tacotron-WaveRNN"
"mkotha/WaveRNN" -> "ksw0306/ClariNet"
"mkotha/WaveRNN" -> "nii-yamagishilab/Extended_VQVAE" ["e"=1]
"mkotha/WaveRNN" -> "geneing/WaveRNN-Pytorch"
"mkotha/WaveRNN" -> "npuichigo/waveglow"
"mkotha/WaveRNN" -> "G-Wang/WaveRNN-Pytorch"
"mkotha/WaveRNN" -> "tiberiu44/TTS-Cube"
"mkotha/WaveRNN" -> "hrbigelow/ae-wavenet"
"mkotha/WaveRNN" -> "azraelkuan/parallel_wavenet_vocoder"
"mkotha/WaveRNN" -> "alokprasad/LPCTron"
"mkotha/WaveRNN" -> "nii-yamagishilab/multi-speaker-tacotron"
"mkotha/WaveRNN" -> "bshall/UniversalVocoding"
"mkotha/WaveRNN" -> "ksw0306/WaveVAE"
"mkotha/WaveRNN" -> "k2kobayashi/crank"
"jinhan/tacotron2-gst" -> "keonlee9420/Robust_Fine_Grained_Prosody_Control"
"jinhan/tacotron2-gst" -> "CODEJIN/GST_Tacotron"
"ksw0306/WaveVAE" -> "npuichigo/waveglow"
"ksw0306/WaveVAE" -> "jaywalnut310/waveglow-vqvae"
"ksw0306/WaveVAE" -> "h-meru/Tacotron-WaveRNN"
"s3prl/s3prl" -> "kan-bayashi/ParallelWaveGAN"
"s3prl/s3prl" -> "speechbrain/speechbrain" ["e"=1]
"s3prl/s3prl" -> "k2-fsa/k2" ["e"=1]
"s3prl/s3prl" -> "espnet/espnet" ["e"=1]
"s3prl/s3prl" -> "wenet-e2e/wenet" ["e"=1]
"s3prl/s3prl" -> "jik876/hifi-gan"
"s3prl/s3prl" -> "ga642381/speech-trident" ["e"=1]
"s3prl/s3prl" -> "lhotse-speech/lhotse" ["e"=1]
"s3prl/s3prl" -> "wenet-e2e/speech-synthesis-paper"
"s3prl/s3prl" -> "asteroid-team/asteroid" ["e"=1]
"s3prl/s3prl" -> "facebookresearch/WavAugment" ["e"=1]
"s3prl/s3prl" -> "mravanelli/pytorch-kaldi" ["e"=1]
"s3prl/s3prl" -> "facebookresearch/encodec" ["e"=1]
"s3prl/s3prl" -> "TencentGameMate/chinese_speech_pretrain" ["e"=1]
"s3prl/s3prl" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"syang1993/gst-tacotron" -> "KinglittleQ/GST-Tacotron"
"syang1993/gst-tacotron" -> "Kyubyong/expressive_tacotron"
"syang1993/gst-tacotron" -> "rishikksh20/vae_tacotron2"
"syang1993/gst-tacotron" -> "NVIDIA/mellotron"
"syang1993/gst-tacotron" -> "xcmyz/FastSpeech"
"syang1993/gst-tacotron" -> "xiph/LPCNet"
"syang1993/gst-tacotron" -> "yanggeng1995/vae_tacotron"
"syang1993/gst-tacotron" -> "nii-yamagishilab/multi-speaker-tacotron"
"syang1993/gst-tacotron" -> "kan-bayashi/PytorchWaveNetVocoder"
"syang1993/gst-tacotron" -> "Jackiexiao/MTTS"
"syang1993/gst-tacotron" -> "thuhcsi/VAENAR-TTS"
"syang1993/gst-tacotron" -> "jinhan/tacotron2-vae"
"syang1993/gst-tacotron" -> "seungwonpark/melgan"
"syang1993/gst-tacotron" -> "npuichigo/waveglow"
"syang1993/gst-tacotron" -> "ksw0306/FloWaveNet"
"coqui-ai/open-speech-corpora" -> "jim-schwoebel/voice_datasets"
"coqui-ai/open-speech-corpora" -> "wenet-e2e/speech-synthesis-paper"
"coqui-ai/open-speech-corpora" -> "s3prl/s3prl"
"coqui-ai/open-speech-corpora" -> "lhotse-speech/lhotse" ["e"=1]
"coqui-ai/open-speech-corpora" -> "kan-bayashi/ParallelWaveGAN"
"coqui-ai/open-speech-corpora" -> "SpeechColab/GigaSpeech" ["e"=1]
"coqui-ai/open-speech-corpora" -> "bootphon/phonemizer"
"coqui-ai/open-speech-corpora" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"coqui-ai/open-speech-corpora" -> "k2-fsa/k2" ["e"=1]
"coqui-ai/open-speech-corpora" -> "aliutkus/speechmetrics" ["e"=1]
"coqui-ai/open-speech-corpora" -> "freewym/espresso" ["e"=1]
"coqui-ai/open-speech-corpora" -> "syhw/wer_are_we" ["e"=1]
"coqui-ai/open-speech-corpora" -> "coqui-ai/TTS-papers"
"coqui-ai/open-speech-corpora" -> "Kyubyong/css10"
"coqui-ai/open-speech-corpora" -> "facebookresearch/WavAugment" ["e"=1]
"google/voice-builder" -> "google/language-resources"
"google/voice-builder" -> "MycroftAI/mimic2"
"google/voice-builder" -> "sotelo/parrot"
"google/voice-builder" -> "CSTR-Edinburgh/merlin"
"google/voice-builder" -> "google/sparrowhawk"
"google/voice-builder" -> "nii-yamagishilab/multi-speaker-tacotron"
"google/voice-builder" -> "synesthesiam/opentts" ["e"=1]
"google/voice-builder" -> "tiberiu44/TTS-Cube"
"google/voice-builder" -> "marytts/marytts"
"google/voice-builder" -> "syang1993/gst-tacotron"
"google/voice-builder" -> "festvox/festvox"
"google/voice-builder" -> "mozilla/TTS"
"shibing624/parrots" -> "Renovamen/Speech-and-Text"
"shibing624/parrots" -> "xxbb1234021/speech_recognition" ["e"=1]
"shibing624/parrots" -> "CynthiaSuwi/ASR-for-Chinese-Pipeline"
"shibing624/parrots" -> "ranchlai/mandarin-tts"
"shibing624/parrots" -> "liangstein/Chinese-speech-to-text"
"shibing624/parrots" -> "kuangdd/ttskit"
"shibing624/parrots" -> "Z-yq/TensorflowTTS"
"shibing624/parrots" -> "yeyupiaoling/MASR" ["e"=1]
"shibing624/parrots" -> "atomicoo/FCH-TTS"
"shibing624/parrots" -> "Z-yq/TensorflowASR" ["e"=1]
"shibing624/parrots" -> "xdcesc/my_ch_speech_recognition" ["e"=1]
"shibing624/parrots" -> "yeyupiaoling/PPASR" ["e"=1]
"shibing624/parrots" -> "atomicoo/tacotron2-mandarin"
"shibing624/parrots" -> "binzhouchn/masr" ["e"=1]
"shibing624/parrots" -> "yeyupiaoling/PaddlePaddle-DeepSpeech" ["e"=1]
"bfs18/nsynth_wavenet" -> "andabi/parallel-wavenet-vocoder"
"bfs18/nsynth_wavenet" -> "kensun0/Parallel-Wavenet"
"bfs18/nsynth_wavenet" -> "azraelkuan/parallel_wavenet_vocoder"
"bfs18/nsynth_wavenet" -> "zhf459/P_wavenet_vocoder"
"festvox/festival" -> "festvox/festvox"
"festvox/festival" -> "festvox/flite"
"festvox/festival" -> "festvox/speech_tools"
"festvox/festival" -> "festvox/datasets-CMU_Wilderness"
"festvox/festival" -> "espeak-ng/espeak-ng" ["e"=1]
"festvox/festival" -> "MycroftAI/mimic1"
"Hiroshiba/become-yukarin" -> "Hiroshiba/realtime-yukarin"
"Hiroshiba/become-yukarin" -> "Hiroshiba/yukarin"
"Hiroshiba/become-yukarin" -> "k2kobayashi/sprocket"
"Hiroshiba/become-yukarin" -> "isletennos/MMVC_Trainer" ["e"=1]
"Hiroshiba/become-yukarin" -> "mmorise/World"
"Hiroshiba/become-yukarin" -> "pstuvwx/Deep_VoiceChanger"
"Hiroshiba/become-yukarin" -> "mmorise/kiritan_singing"
"Hiroshiba/become-yukarin" -> "r9y9/nnmnkwii"
"Hiroshiba/become-yukarin" -> "isletennos/MMVC_Client" ["e"=1]
"Hiroshiba/become-yukarin" -> "omegasisters/homepage" ["e"=1]
"Hiroshiba/become-yukarin" -> "yui540/satella.io" ["e"=1]
"Hiroshiba/become-yukarin" -> "VOICEVOX/voicevox" ["e"=1]
"Hiroshiba/become-yukarin" -> "hidefuku/AnimeEffects" ["e"=1]
"Hiroshiba/become-yukarin" -> "ksasao/Gochiusearch" ["e"=1]
"BogiHsu/Tacotron2-PyTorch" -> "ttaoREtw/Tacotron-pytorch"
"BogiHsu/Tacotron2-PyTorch" -> "BogiHsu/WG-WaveNet"
"BogiHsu/Tacotron2-PyTorch" -> "cyhuang-tw/AdaIN-VC"
"BogiHsu/Tacotron2-PyTorch" -> "andi611/ZeroSpeech-TTS-without-T"
"BogiHsu/Tacotron2-PyTorch" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"praat/praat" -> "YannickJadoul/Parselmouth"
"praat/praat" -> "timmahrt/praatIO"
"praat/praat" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"praat/praat" -> "mmorise/World"
"praat/praat" -> "kylebgorman/textgrid"
"praat/praat" -> "coqui-ai/open-speech-corpora"
"praat/praat" -> "bootphon/phonemizer"
"praat/praat" -> "marl/crepe" ["e"=1]
"praat/praat" -> "stylerw/styler_praat_scripts"
"praat/praat" -> "covarep/covarep"
"praat/praat" -> "strob/gentle"
"praat/praat" -> "pettarin/forced-alignment-tools"
"praat/praat" -> "jik876/hifi-gan"
"praat/praat" -> "xinjli/allosaurus"
"praat/praat" -> "feelins/Praat_Scripts"
"covarep/covarep" -> "jckane/Voice_Analysis_Toolkit"
"covarep/covarep" -> "adbailey1/DepAudioNet_reproduction"
"covarep/covarep" -> "A2Zadeh/CMU-MultimodalSDK" ["e"=1]
"covarep/covarep" -> "AudioVisualEmotionChallenge/AVEC2019"
"covarep/covarep" -> "google/REAPER"
"covarep/covarep" -> "audeering/opensmile"
"covarep/covarep" -> "kykiefer/depression-detect"
"covarep/covarep" -> "ImperialCollegeLondon/sap-voicebox" ["e"=1]
"covarep/covarep" -> "thuiar/Cross-Modal-BERT" ["e"=1]
"covarep/covarep" -> "naxingyu/opensmile"
"covarep/covarep" -> "mmorise/World"
"covarep/covarep" -> "audeering/opensmile-python"
"covarep/covarep" -> "pranaymanocha/PerceptualAudio" ["e"=1]
"covarep/covarep" -> "jcvasquezc/DisVoice"
"covarep/covarep" -> "r9y9/pysptk"
"Kyubyong/expressive_tacotron" -> "syang1993/gst-tacotron"
"Kyubyong/expressive_tacotron" -> "syang1993/FFTNet"
"Kyubyong/expressive_tacotron" -> "npuichigo/waveglow"
"festvox/flite" -> "festvox/festival"
"festvox/flite" -> "festvox/festvox"
"festvox/flite" -> "numediart/MBROLA"
"festvox/flite" -> "MycroftAI/mimic1"
"festvox/flite" -> "espeak-ng/espeak-ng" ["e"=1]
"festvox/flite" -> "happyalu/Flite-TTS-Engine-for-Android"
"festvox/flite" -> "dmort27/epitran"
"festvox/flite" -> "festvox/datasets-CMU_Wilderness"
"festvox/flite" -> "huakunyang/SummerTTS" ["e"=1]
"festvox/flite" -> "Kyubyong/g2p"
"festvox/flite" -> "festvox/bard"
"festvox/flite" -> "festvox/speech_tools"
"festvox/flite" -> "AdolfVonKleist/Phonetisaurus"
"festvox/flite" -> "numediart/MBROLA-voices"
"festvox/flite" -> "CSTR-Edinburgh/merlin"
"Kyubyong/deepvoice3" -> "Kyubyong/tacotron"
"Kyubyong/deepvoice3" -> "Kyubyong/dc_tts"
"Kyubyong/deepvoice3" -> "israelg99/deepvoice"
"Kyubyong/deepvoice3" -> "r9y9/deepvoice3_pytorch"
"Kyubyong/deepvoice3" -> "syang1993/gst-tacotron"
"Kyubyong/deepvoice3" -> "riverphoenix/tacotron2"
"Kyubyong/deepvoice3" -> "azraelkuan/parallel_wavenet_vocoder"
"Kyubyong/deepvoice3" -> "keithito/tacotron"
"Kyubyong/deepvoice3" -> "ttsunion/Deep-Expression"
"Kyubyong/deepvoice3" -> "begeekmyfriend/tacotron"
"Kyubyong/deepvoice3" -> "ksw0306/ClariNet"
"Kyubyong/deepvoice3" -> "andabi/parallel-wavenet-vocoder"
"Kyubyong/deepvoice3" -> "r9y9/wavenet_vocoder"
"Kyubyong/deepvoice3" -> "mkotha/WaveRNN"
"Kyubyong/deepvoice3" -> "Kyubyong/expressive_tacotron"
"r9y9/deepvoice3_pytorch" -> "r9y9/wavenet_vocoder"
"r9y9/deepvoice3_pytorch" -> "keithito/tacotron"
"r9y9/deepvoice3_pytorch" -> "Rayhane-mamah/Tacotron-2"
"r9y9/deepvoice3_pytorch" -> "NVIDIA/waveglow"
"r9y9/deepvoice3_pytorch" -> "fatchord/WaveRNN"
"r9y9/deepvoice3_pytorch" -> "NVIDIA/tacotron2"
"r9y9/deepvoice3_pytorch" -> "Kyubyong/deepvoice3"
"r9y9/deepvoice3_pytorch" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"r9y9/deepvoice3_pytorch" -> "xcmyz/FastSpeech"
"r9y9/deepvoice3_pytorch" -> "Kyubyong/tacotron"
"r9y9/deepvoice3_pytorch" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/deepvoice3_pytorch" -> "CSTR-Edinburgh/merlin"
"r9y9/deepvoice3_pytorch" -> "xiph/LPCNet"
"r9y9/deepvoice3_pytorch" -> "andabi/deep-voice-conversion"
"r9y9/deepvoice3_pytorch" -> "r9y9/gantts"
"mazzzystar/WaveGAN-pytorch" -> "auroracramer/wavegan"
"mazzzystar/WaveGAN-pytorch" -> "mostafaelaraby/wavegan-pytorch"
"leimao/Voice-Converter-CycleGAN" -> "liusongxiang/StarGAN-Voice-Conversion"
"leimao/Voice-Converter-CycleGAN" -> "hujinsen/StarGAN-Voice-Conversion"
"leimao/Voice-Converter-CycleGAN" -> "r9y9/gantts"
"leimao/Voice-Converter-CycleGAN" -> "auspicious3000/autovc"
"leimao/Voice-Converter-CycleGAN" -> "k2kobayashi/sprocket"
"leimao/Voice-Converter-CycleGAN" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"leimao/Voice-Converter-CycleGAN" -> "mazzzystar/randomCNN-voice-transfer"
"leimao/Voice-Converter-CycleGAN" -> "jackaduma/CycleGAN-VC2"
"leimao/Voice-Converter-CycleGAN" -> "jjery2243542/voice_conversion"
"leimao/Voice-Converter-CycleGAN" -> "jjery2243542/adaptive_voice_conversion"
"leimao/Voice-Converter-CycleGAN" -> "jxzhanggg/nonparaSeq2seqVC_code"
"leimao/Voice-Converter-CycleGAN" -> "pritishyuvraj/Voice-Conversion-GAN"
"leimao/Voice-Converter-CycleGAN" -> "bshall/ZeroSpeech"
"leimao/Voice-Converter-CycleGAN" -> "JeremyCCHsu/vae-npvc"
"leimao/Voice-Converter-CycleGAN" -> "hujinsen/pytorch-StarGAN-VC"
"Renovamen/Speech-and-Text" -> "opensourceteams/google-sdk-speech-to-text"
"Renovamen/Speech-and-Text" -> "shibing624/parrots"
"Renovamen/Speech-and-Text" -> "liangstein/Chinese-speech-to-text"
"Renovamen/Speech-and-Text" -> "Zws-China/VoiceToWords"
"Renovamen/Speech-and-Text" -> "weineng-zhou/text2voice"
"Renovamen/Speech-and-Text" -> "wulee510505/Text2Speach"
"mazzzystar/randomCNN-voice-transfer" -> "leimao/Voice-Converter-CycleGAN"
"mazzzystar/randomCNN-voice-transfer" -> "auspicious3000/autovc"
"mazzzystar/randomCNN-voice-transfer" -> "liusongxiang/StarGAN-Voice-Conversion"
"mazzzystar/randomCNN-voice-transfer" -> "marcoppasini/MelGAN-VC"
"mazzzystar/randomCNN-voice-transfer" -> "hujinsen/StarGAN-Voice-Conversion"
"mazzzystar/randomCNN-voice-transfer" -> "k2kobayashi/sprocket"
"mazzzystar/randomCNN-voice-transfer" -> "JeremyCCHsu/vae-npvc"
"mazzzystar/randomCNN-voice-transfer" -> "hujinsen/pytorch-StarGAN-VC"
"mazzzystar/randomCNN-voice-transfer" -> "pritishyuvraj/Voice-Conversion-GAN"
"mazzzystar/randomCNN-voice-transfer" -> "alishdipani/Neural-Style-Transfer-Audio"
"mazzzystar/randomCNN-voice-transfer" -> "jackaduma/CycleGAN-VC2"
"mazzzystar/randomCNN-voice-transfer" -> "jjery2243542/adaptive_voice_conversion"
"mazzzystar/randomCNN-voice-transfer" -> "jjery2243542/voice_conversion"
"mazzzystar/randomCNN-voice-transfer" -> "auspicious3000/SpeechSplit"
"mazzzystar/randomCNN-voice-transfer" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"NVIDIA/nv-wavenet" -> "NVIDIA/waveglow"
"NVIDIA/nv-wavenet" -> "xiph/LPCNet"
"NVIDIA/nv-wavenet" -> "r9y9/wavenet_vocoder"
"NVIDIA/nv-wavenet" -> "ksw0306/FloWaveNet"
"NVIDIA/nv-wavenet" -> "kan-bayashi/PytorchWaveNetVocoder"
"NVIDIA/nv-wavenet" -> "tomlepaine/fast-wavenet"
"NVIDIA/nv-wavenet" -> "NVIDIA/mellotron"
"NVIDIA/nv-wavenet" -> "soobinseo/Transformer-TTS"
"NVIDIA/nv-wavenet" -> "descriptinc/melgan-neurips"
"NVIDIA/nv-wavenet" -> "fatchord/WaveRNN"
"NVIDIA/nv-wavenet" -> "tiberiu44/TTS-Cube"
"NVIDIA/nv-wavenet" -> "azraelkuan/parallel_wavenet_vocoder"
"NVIDIA/nv-wavenet" -> "Rayhane-mamah/Tacotron-2"
"NVIDIA/nv-wavenet" -> "mkotha/WaveRNN"
"NVIDIA/nv-wavenet" -> "NVIDIA/tacotron2"
"jim-schwoebel/voice_datasets" -> "coqui-ai/open-speech-corpora"
"jim-schwoebel/voice_datasets" -> "s3prl/s3prl"
"jim-schwoebel/voice_datasets" -> "aliutkus/speechmetrics" ["e"=1]
"jim-schwoebel/voice_datasets" -> "asteroid-team/asteroid" ["e"=1]
"jim-schwoebel/voice_datasets" -> "jik876/hifi-gan"
"jim-schwoebel/voice_datasets" -> "wenet-e2e/speech-synthesis-paper"
"jim-schwoebel/voice_datasets" -> "microsoft/DNS-Challenge" ["e"=1]
"jim-schwoebel/voice_datasets" -> "iver56/audiomentations" ["e"=1]
"jim-schwoebel/voice_datasets" -> "facebookresearch/denoiser" ["e"=1]
"jim-schwoebel/voice_datasets" -> "gabrielmittag/NISQA" ["e"=1]
"jim-schwoebel/voice_datasets" -> "kan-bayashi/ParallelWaveGAN"
"jim-schwoebel/voice_datasets" -> "nanahou/Awesome-Speech-Enhancement" ["e"=1]
"jim-schwoebel/voice_datasets" -> "microsoft/MS-SNSD" ["e"=1]
"jim-schwoebel/voice_datasets" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"jim-schwoebel/voice_datasets" -> "Yuan-ManX/ai-audio-datasets" ["e"=1]
"JasonWei512/Tacotron-2-Chinese" -> "JasonWei512/wavenet_vocoder"
"JasonWei512/Tacotron-2-Chinese" -> "foamliu/Tacotron2-Mandarin"
"JasonWei512/Tacotron-2-Chinese" -> "lturing/tacotronv2_wavernn_chinese"
"JasonWei512/Tacotron-2-Chinese" -> "atomicoo/tacotron2-mandarin"
"JasonWei512/Tacotron-2-Chinese" -> "Jackiexiao/MTTS"
"JasonWei512/Tacotron-2-Chinese" -> "begeekmyfriend/tacotron"
"JasonWei512/Tacotron-2-Chinese" -> "KuangDD/zhrtvc"
"JasonWei512/Tacotron-2-Chinese" -> "KuangDD/zhvoice"
"JasonWei512/Tacotron-2-Chinese" -> "iwater/Real-Time-Voice-Cloning-Chinese"
"JasonWei512/Tacotron-2-Chinese" -> "MachineLP/TensorFlowTTS_chinese"
"JasonWei512/Tacotron-2-Chinese" -> "syang1993/gst-tacotron"
"JasonWei512/Tacotron-2-Chinese" -> "begeekmyfriend/Tacotron-2"
"JasonWei512/Tacotron-2-Chinese" -> "aidreamwin/TTS-Clone-Chinese"
"JasonWei512/Tacotron-2-Chinese" -> "bbepoch/CuteChineseTTS"
"JasonWei512/Tacotron-2-Chinese" -> "NVIDIA/mellotron"
"alokprasad/LPCTron" -> "MlWoo/LPCNet"
"alokprasad/LPCTron" -> "alokprasad/lpctron-tts-cpp"
"alokprasad/LPCTron" -> "h-meru/Tacotron-WaveRNN"
"alokprasad/LPCTron" -> "geneing/WaveRNN-Pytorch"
"llSourcell/Neural_Network_Voices" -> "baidu-research/deep-voice"
"Shahabks/my-voice-analysis" -> "Shahabks/myprosody"
"Shahabks/my-voice-analysis" -> "jcvasquezc/DisVoice"
"Shahabks/my-voice-analysis" -> "drfeinberg/PraatScripts"
"Shahabks/my-voice-analysis" -> "YannickJadoul/Parselmouth"
"Shahabks/my-voice-analysis" -> "Shahabks/Speechat"
"Shahabks/my-voice-analysis" -> "timmahrt/praatIO"
"Shahabks/my-voice-analysis" -> "audeering/opensmile-python"
"end2you/end2you" -> "tzirakis/Multimodal-Emotion-Recognition" ["e"=1]
"end2you/end2you" -> "auDeep/auDeep"
"end2you/end2you" -> "openXBOW/openXBOW"
"end2you/end2you" -> "DeepSpectrum/DeepSpectrum"
"begeekmyfriend/Tacotron-2" -> "begeekmyfriend/tacotron"
"begeekmyfriend/Tacotron-2" -> "Jackiexiao/MTTS"
"begeekmyfriend/Tacotron-2" -> "begeekmyfriend/tacotron2"
"begeekmyfriend/Tacotron-2" -> "h-meru/Tacotron-WaveRNN"
"begeekmyfriend/Tacotron-2" -> "azraelkuan/parallel_wavenet_vocoder"
"begeekmyfriend/Tacotron-2" -> "tiberiu44/TTS-Cube"
"begeekmyfriend/Tacotron-2" -> "begeekmyfriend/WaveRNN"
"phoible/dev" -> "unicode-cookbook/cookbook"
"phoible/dev" -> "dmort27/panphon"
"phoible/dev" -> "mlml/autovot"
"phoible/dev" -> "dmort27/allovera"
"jaekookang/p2fa_py3" -> "prosodylab/Prosodylab-Aligner"
"prosodylab/Prosodylab-Aligner" -> "pettarin/forced-alignment-tools"
"prosodylab/Prosodylab-Aligner" -> "jaekookang/p2fa_py3"
"prosodylab/Prosodylab-Aligner" -> "JoFrhwld/FAVE"
"prosodylab/Prosodylab-Aligner" -> "hbuschme/TextGridTools"
"prosodylab/Prosodylab-Aligner" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"prosodylab/Prosodylab-Aligner" -> "strob/gentle"
"prosodylab/Prosodylab-Aligner" -> "prosodylab/prosodylab.dictionaries"
"prosodylab/Prosodylab-Aligner" -> "dopefishh/praatalign"
"prosodylab/Prosodylab-Aligner" -> "mmcauliffe/Conch-sounds"
"prosodylab/Prosodylab-Aligner" -> "open-speech/speech-aligner"
"prosodylab/Prosodylab-Aligner" -> "Helsinki-NLP/prosody"
"prosodylab/Prosodylab-Aligner" -> "tbright17/kaldi-dnn-ali-gop" ["e"=1]
"prosodylab/Prosodylab-Aligner" -> "kylebgorman/textgrid"
"prosodylab/Prosodylab-Aligner" -> "r9y9/pysptk"
"prosodylab/Prosodylab-Aligner" -> "nassosoassos/sail_align"
"Helsinki-NLP/prosody" -> "asuni/wavelet_prosody_toolkit"
"Helsinki-NLP/prosody" -> "BoragoCode/AttentionBasedProsodyPrediction"
"Helsinki-NLP/prosody" -> "Daisyqk/Automatic-Prosody-Annotation"
"Helsinki-NLP/prosody" -> "Jackiexiao/MTTS"
"Helsinki-NLP/prosody" -> "nii-yamagishilab/multi-speaker-tacotron"
"Helsinki-NLP/prosody" -> "kakaobrain/g2pm"
"Helsinki-NLP/prosody" -> "Riroaki/Chinese-Rhythm-Predictor"
"Helsinki-NLP/prosody" -> "Zeqiang-Lai/Prosody_Prediction"
"Helsinki-NLP/prosody" -> "quadrismegistus/prosodic"
"Helsinki-NLP/prosody" -> "syang1993/gst-tacotron"
"bshall/ZeroSpeech" -> "bshall/VectorQuantizedCPC"
"bshall/ZeroSpeech" -> "hhguo/EA-SVC"
"bshall/ZeroSpeech" -> "Wendison/VQMIVC"
"bshall/ZeroSpeech" -> "bigpon/vcc20_baseline_cyclevae"
"bshall/ZeroSpeech" -> "ericwudayi/SkipVQVC"
"bshall/ZeroSpeech" -> "jxzhanggg/nonparaSeq2seqVC_code"
"bshall/ZeroSpeech" -> "jjery2243542/adaptive_voice_conversion"
"bshall/ZeroSpeech" -> "swasun/VQ-VAE-Speech"
"bshall/ZeroSpeech" -> "thuhcsi/VAENAR-TTS"
"bshall/ZeroSpeech" -> "k2kobayashi/crank"
"bshall/ZeroSpeech" -> "liusongxiang/ppg-vc"
"bshall/ZeroSpeech" -> "guanlongzhao/fac-via-ppg"
"bshall/ZeroSpeech" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"bshall/ZeroSpeech" -> "KimythAnly/AGAIN-VC"
"bshall/ZeroSpeech" -> "bshall/UniversalVocoding"
"wnhsu/SpeechVAE" -> "edchengg/generative_model_speech"
"wnhsu/FactorizedHierarchicalVAE" -> "wnhsu/ScalableFHVAE"
"wnhsu/FactorizedHierarchicalVAE" -> "wnhsu/SpeechVAE"
"wnhsu/FactorizedHierarchicalVAE" -> "hrbigelow/ae-wavenet"
"wnhsu/FactorizedHierarchicalVAE" -> "jjery2243542/voice_conversion"
"MycroftAI/mimic-recording-studio" -> "MycroftAI/mimic2"
"MycroftAI/mimic-recording-studio" -> "thorstenMueller/Thorsten-Voice"
"MycroftAI/mimic-recording-studio" -> "LearnedVector/A-Hackers-AI-Voice-Assistant" ["e"=1]
"MycroftAI/mimic-recording-studio" -> "getalp/ALFFA_PUBLIC"
"L0SG/relational-rnn-pytorch" -> "ksw0306/FloWaveNet"
"ttaoREtw/Tacotron-pytorch" -> "BogiHsu/Tacotron2-PyTorch"
"ttaoREtw/Tacotron-pytorch" -> "BogiHsu/Voice-Conversion"
"ttaoREtw/Tacotron-pytorch" -> "andi611/ZeroSpeech-TTS-without-T"
"erogol/WaveRNN" -> "yoyolicoris/pytorch_FFTNet"
"erogol/WaveRNN" -> "h-meru/Tacotron-WaveRNN"
"Jackiexiao/MTTS" -> "kakaobrain/g2pm"
"Jackiexiao/MTTS" -> "thuhcsi/Crystal"
"Jackiexiao/MTTS" -> "begeekmyfriend/Tacotron-2"
"Jackiexiao/MTTS" -> "begeekmyfriend/tacotron"
"Jackiexiao/MTTS" -> "speechio/chinese_text_normalization" ["e"=1]
"Jackiexiao/MTTS" -> "syang1993/gst-tacotron"
"Jackiexiao/MTTS" -> "CSTR-Edinburgh/merlin"
"Jackiexiao/MTTS" -> "Helsinki-NLP/prosody"
"Jackiexiao/MTTS" -> "xiph/LPCNet"
"Jackiexiao/MTTS" -> "ksw0306/ClariNet"
"Jackiexiao/MTTS" -> "BoragoCode/AttentionBasedProsodyPrediction"
"Jackiexiao/MTTS" -> "xcmyz/FastSpeech"
"Jackiexiao/MTTS" -> "open-speech/speech-aligner"
"Jackiexiao/MTTS" -> "ivanvovk/durian-pytorch"
"Jackiexiao/MTTS" -> "lturing/tacotronv2_wavernn_chinese"
"facebookarchive/loop" -> "CSTR-Edinburgh/merlin"
"facebookarchive/loop" -> "Kyubyong/tacotron"
"facebookarchive/loop" -> "r9y9/deepvoice3_pytorch"
"facebookarchive/loop" -> "sotelo/parrot"
"facebookarchive/loop" -> "syang1993/gst-tacotron"
"facebookarchive/loop" -> "mmorise/World"
"facebookarchive/loop" -> "tomlepaine/fast-wavenet"
"facebookarchive/loop" -> "ksw0306/FloWaveNet"
"facebookarchive/loop" -> "r9y9/wavenet_vocoder"
"facebookarchive/loop" -> "keithito/tacotron"
"facebookarchive/loop" -> "kan-bayashi/PytorchWaveNetVocoder"
"facebookarchive/loop" -> "Kyubyong/deepvoice3"
"facebookarchive/loop" -> "xiph/LPCNet"
"facebookarchive/loop" -> "Jackiexiao/MTTS"
"facebookarchive/loop" -> "r9y9/gantts"
"festvox/datasets-CMU_Wilderness" -> "xinjli/transphone"
"festvox/datasets-CMU_Wilderness" -> "uiuc-sst/g2ps"
"festvox/datasets-CMU_Wilderness" -> "xinjli/ucla-phonetic-corpus"
"festvox/datasets-CMU_Wilderness" -> "xinjli/allosaurus"
"festvox/datasets-CMU_Wilderness" -> "AdolfVonKleist/Phonetisaurus"
"festvox/datasets-CMU_Wilderness" -> "dmort27/epitran"
"festvox/datasets-CMU_Wilderness" -> "nii-yamagishilab/multi-speaker-tacotron"
"FieldDB/Praat-Scripts" -> "lennes/spect"
"isrugeek/depression_detection" -> "chanjunweimy/FYP_Submission"
"RicherMans/text_based_depression" -> "adbailey1/DepAudioNet_reproduction"
"OpenVoiceOS/ovos-personal-backend" -> "ChanceNCounter/awesome-mycroft-community"
"pstuvwx/Deep_VoiceChanger" -> "Hiroshiba/yukarin"
"pstuvwx/Deep_VoiceChanger" -> "Hiroshiba/realtime-yukarin"
"pstuvwx/Deep_VoiceChanger" -> "Hiroshiba/become-yukarin"
"pstuvwx/Deep_VoiceChanger" -> "leimao/Voice-Converter-CycleGAN"
"pstuvwx/Deep_VoiceChanger" -> "k2kobayashi/sprocket"
"Kyubyong/g2p" -> "bootphon/phonemizer"
"Kyubyong/g2p" -> "cmusphinx/g2p-seq2seq" ["e"=1]
"Kyubyong/g2p" -> "kakaobrain/g2pm"
"Kyubyong/g2p" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"Kyubyong/g2p" -> "jaywalnut310/glow-tts"
"Kyubyong/g2p" -> "xcmyz/FastSpeech"
"Kyubyong/g2p" -> "xiph/LPCNet"
"Kyubyong/g2p" -> "jik876/hifi-gan"
"Kyubyong/g2p" -> "AdolfVonKleist/Phonetisaurus"
"Kyubyong/g2p" -> "kan-bayashi/ParallelWaveGAN"
"Kyubyong/g2p" -> "speechio/chinese_text_normalization" ["e"=1]
"Kyubyong/g2p" -> "wenet-e2e/speech-synthesis-paper"
"Kyubyong/g2p" -> "ming024/FastSpeech2"
"Kyubyong/g2p" -> "aliutkus/speechmetrics" ["e"=1]
"Kyubyong/g2p" -> "syang1993/gst-tacotron"
"MycroftAI/selene-backend" -> "MycroftAI/selene-ui"
"MycroftAI/selene-backend" -> "MycroftAI/personal-backend"
"hujinsen/StarGAN-Voice-Conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"hujinsen/StarGAN-Voice-Conversion" -> "leimao/Voice-Converter-CycleGAN"
"hujinsen/StarGAN-Voice-Conversion" -> "hujinsen/pytorch-StarGAN-VC"
"hujinsen/StarGAN-Voice-Conversion" -> "jjery2243542/voice_conversion"
"hujinsen/StarGAN-Voice-Conversion" -> "jjery2243542/adaptive_voice_conversion"
"hujinsen/StarGAN-Voice-Conversion" -> "ryokamoi/ppg_vc"
"hujinsen/StarGAN-Voice-Conversion" -> "JeremyCCHsu/vae-npvc"
"hujinsen/StarGAN-Voice-Conversion" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"hujinsen/StarGAN-Voice-Conversion" -> "mazzzystar/randomCNN-voice-transfer"
"hujinsen/StarGAN-Voice-Conversion" -> "k2kobayashi/sprocket"
"hujinsen/StarGAN-Voice-Conversion" -> "jxzhanggg/nonparaSeq2seqVC_code"
"hujinsen/StarGAN-Voice-Conversion" -> "auspicious3000/autovc"
"hujinsen/StarGAN-Voice-Conversion" -> "k2kobayashi/crank"
"hujinsen/StarGAN-Voice-Conversion" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"hujinsen/StarGAN-Voice-Conversion" -> "pritishyuvraj/Voice-Conversion-GAN"
"HidekiKawahara/legacy_STRAIGHT" -> "shuaijiang/STRAIGHT"
"HidekiKawahara/legacy_STRAIGHT" -> "kan-bayashi/PytorchWaveNetVocoder"
"jjery2243542/voice_conversion" -> "jjery2243542/adaptive_voice_conversion"
"jjery2243542/voice_conversion" -> "liusongxiang/StarGAN-Voice-Conversion"
"jjery2243542/voice_conversion" -> "andi611/ZeroSpeech-TTS-without-T"
"jjery2243542/voice_conversion" -> "JeremyCCHsu/vae-npvc"
"jjery2243542/voice_conversion" -> "joansj/blow"
"jjery2243542/voice_conversion" -> "hujinsen/StarGAN-Voice-Conversion"
"jjery2243542/voice_conversion" -> "ericwudayi/SkipVQVC"
"jjery2243542/voice_conversion" -> "auspicious3000/autovc"
"jjery2243542/voice_conversion" -> "bigpon/vcc20_baseline_cyclevae"
"jjery2243542/voice_conversion" -> "leimao/Voice-Converter-CycleGAN"
"jjery2243542/voice_conversion" -> "acetylSv/GST-tacotron" ["e"=1]
"jjery2243542/voice_conversion" -> "k2kobayashi/sprocket"
"jjery2243542/voice_conversion" -> "BogiHsu/Voice-Conversion"
"jjery2243542/voice_conversion" -> "hujinsen/pytorch-StarGAN-VC"
"jjery2243542/voice_conversion" -> "bshall/ZeroSpeech"
"ksw0306/FloWaveNet" -> "ksw0306/ClariNet"
"ksw0306/FloWaveNet" -> "npuichigo/waveglow"
"ksw0306/FloWaveNet" -> "azraelkuan/parallel_wavenet_vocoder"
"ksw0306/FloWaveNet" -> "andabi/parallel-wavenet-vocoder"
"ksw0306/FloWaveNet" -> "kan-bayashi/PytorchWaveNetVocoder"
"ksw0306/FloWaveNet" -> "syang1993/gst-tacotron"
"ksw0306/FloWaveNet" -> "L0SG/WaveFlow"
"ksw0306/FloWaveNet" -> "mkotha/WaveRNN"
"ksw0306/FloWaveNet" -> "xiph/LPCNet"
"ksw0306/FloWaveNet" -> "NVIDIA/waveglow"
"ksw0306/FloWaveNet" -> "NVIDIA/nv-wavenet"
"ksw0306/FloWaveNet" -> "soobinseo/Transformer-TTS"
"ksw0306/FloWaveNet" -> "tianrengao/SqueezeWave"
"ksw0306/FloWaveNet" -> "descriptinc/melgan-neurips"
"ksw0306/FloWaveNet" -> "NVIDIA/mellotron"
"viritaromero/Detecting-Depression-in-Tweets" -> "peijoy/DetectDepressionInTwitterPosts"
"viritaromero/Detecting-Depression-in-Tweets" -> "AshwanthRamji/Depression-Sentiment-Analysis-with-Twitter-Data"
"viritaromero/Detecting-Depression-in-Tweets" -> "swcwang/depression-detection"
"swasun/VQ-VAE-Speech" -> "hrbigelow/ae-wavenet"
"swasun/VQ-VAE-Speech" -> "bshall/ZeroSpeech"
"swasun/VQ-VAE-Speech" -> "DongyaoZhu/VQ-VAE-WaveNet"
"swasun/VQ-VAE-Speech" -> "JeremyCCHsu/vqvae-speech"
"swasun/VQ-VAE-Speech" -> "mkotha/WaveRNN"
"swasun/VQ-VAE-Speech" -> "ksw0306/WaveVAE"
"swasun/VQ-VAE-Speech" -> "bshall/VectorQuantizedCPC"
"swasun/VQ-VAE-Speech" -> "yjlolo/vae-audio" ["e"=1]
"swasun/VQ-VAE-Speech" -> "joansj/blow"
"swasun/VQ-VAE-Speech" -> "syang1993/gst-tacotron"
"swasun/VQ-VAE-Speech" -> "facebookresearch/speech-resynthesis" ["e"=1]
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "sumuzhao/CycleGAN-Music-Style-Transfer-Refactorization"
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "cifkao/groove2groove"
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "facebookresearch/music-translation"
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "brunnergino/MIDI-VAE" ["e"=1]
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "salu133445/lakh-pianoroll-dataset"
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "YatingMusic/MuseMorphose" ["e"=1]
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "music-x-lab/POP909-Dataset" ["e"=1]
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "jason9693/MusicTransformer-pytorch" ["e"=1]
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "umbrellabeach/music-generation-with-DL" ["e"=1]
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "ChienYuLu/Play-As-You-Like-Timbre-Enhanced-Multi-modal-Music-Style-Transfer" ["e"=1]
"sumuzhao/CycleGAN-Music-Style-Transfer" -> "huangsicong/TimbreTron"
"twidddj/tf-wavenet_vocoder" -> "azraelkuan/tensorflow_wavenet_vocoder"
"Kyubyong/css10" -> "NVIDIA/mellotron"
"Kyubyong/css10" -> "Tomiinek/Multilingual_Text_to_Speech"
"Kyubyong/css10" -> "ivanvovk/WaveGrad"
"Kyubyong/css10" -> "KinglittleQ/GST-Tacotron"
"Kyubyong/css10" -> "jaywalnut310/glow-tts"
"Kyubyong/css10" -> "rhasspy/gruut"
"Kyubyong/css10" -> "seungwonpark/melgan"
"Kyubyong/css10" -> "Kyubyong/g2p"
"Kyubyong/css10" -> "syang1993/gst-tacotron"
"Kyubyong/css10" -> "yanggeng1995/GAN-TTS"
"Kyubyong/css10" -> "wenet-e2e/speech-synthesis-paper"
"Kyubyong/css10" -> "maum-ai/phaseaug" ["e"=1]
"Kyubyong/css10" -> "rishikksh20/VocGAN"
"Kyubyong/css10" -> "coqui-ai/open-speech-corpora"
"Kyubyong/css10" -> "keonlee9420/StyleSpeech" ["e"=1]
"Hiroshiba/yukarin" -> "Hiroshiba/realtime-yukarin"
"Hiroshiba/yukarin" -> "Hiroshiba/become-yukarin"
"andabi/parallel-wavenet-vocoder" -> "bfs18/nsynth_wavenet"
"andabi/parallel-wavenet-vocoder" -> "azraelkuan/parallel_wavenet_vocoder"
"andabi/parallel-wavenet-vocoder" -> "kensun0/Parallel-Wavenet"
"andabi/parallel-wavenet-vocoder" -> "zhf459/P_wavenet_vocoder"
"andabi/parallel-wavenet-vocoder" -> "ksw0306/ClariNet"
"andabi/parallel-wavenet-vocoder" -> "ksw0306/FloWaveNet"
"andabi/parallel-wavenet-vocoder" -> "h-meru/Tacotron-WaveRNN"
"andabi/parallel-wavenet-vocoder" -> "G-Wang/WaveRNN-Pytorch"
"azraelkuan/parallel_wavenet_vocoder" -> "andabi/parallel-wavenet-vocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "bfs18/nsynth_wavenet"
"azraelkuan/parallel_wavenet_vocoder" -> "ksw0306/ClariNet"
"azraelkuan/parallel_wavenet_vocoder" -> "npuichigo/waveglow"
"azraelkuan/parallel_wavenet_vocoder" -> "kan-bayashi/PytorchWaveNetVocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "A-Jacobson/tacotron2"
"azraelkuan/parallel_wavenet_vocoder" -> "geneing/WaveRNN-Pytorch"
"azraelkuan/parallel_wavenet_vocoder" -> "ksw0306/FloWaveNet"
"azraelkuan/parallel_wavenet_vocoder" -> "nii-yamagishilab/TSNetVocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "azraelkuan/tensorflow_wavenet_vocoder"
"azraelkuan/parallel_wavenet_vocoder" -> "m-toman/tacorn"
"azraelkuan/parallel_wavenet_vocoder" -> "h-meru/Tacotron-WaveRNN"
"azraelkuan/parallel_wavenet_vocoder" -> "syang1993/FFTNet"
"azraelkuan/parallel_wavenet_vocoder" -> "ksw0306/WaveVAE"
"CUNY-CL/wikipron" -> "dmort27/epitran"
"CUNY-CL/wikipron" -> "dmort27/panphon"
"CUNY-CL/wikipron" -> "AdolfVonKleist/Phonetisaurus"
"CUNY-CL/wikipron" -> "spring-media/DeepPhonemizer"
"CUNY-CL/wikipron" -> "uiuc-sst/g2ps"
"CUNY-CL/wikipron" -> "roedoejet/g2p"
"CUNY-CL/wikipron" -> "lumaku/ctc-segmentation" ["e"=1]
"CUNY-CL/wikipron" -> "xinjli/allosaurus"
"CUNY-CL/wikipron" -> "lingjzhu/charsiu"
"CUNY-CL/wikipron" -> "lingjzhu/CharsiuG2P" ["e"=1]
"CUNY-CL/wikipron" -> "festvox/datasets-CMU_Wilderness"
"CUNY-CL/wikipron" -> "bootphon/phonemizer"
"CUNY-CL/wikipron" -> "phoible/dev"
"CUNY-CL/wikipron" -> "open-dict-data/ipa-dict"
"MycroftAI/personal-backend" -> "MycroftAI/selene-backend"
"riverphoenix/tacotron2" -> "selap91/Tacotron2"
"riverphoenix/tacotron2" -> "zuoxiang95/tacotron-1"
"riverphoenix/tacotron2" -> "A-Jacobson/tacotron2"
"dhgrs/chainer-VQ-VAE" -> "dhgrs/chainer-ClariNet"
"dhgrs/chainer-ClariNet" -> "HaiFengZeng/clari_wavenet_vocoder"
"dhgrs/chainer-ClariNet" -> "geneing/parallel_wavenet_vocoder"
"Shahabks/myprosody" -> "Shahabks/my-voice-analysis"
"Shahabks/myprosody" -> "jcvasquezc/DisVoice"
"Shahabks/myprosody" -> "YannickJadoul/Parselmouth"
"Shahabks/myprosody" -> "drfeinberg/PraatScripts"
"Shahabks/myprosody" -> "Shahabks/Speechat"
"Shahabks/myprosody" -> "timmahrt/ProMo"
"Kyubyong/speaker_adapted_tts" -> "Kyubyong/expressive_tacotron"
"r9y9/tacotron_pytorch" -> "syang1993/gst-tacotron"
"r9y9/tacotron_pytorch" -> "soobinseo/Transformer-TTS"
"r9y9/tacotron_pytorch" -> "NVIDIA/mellotron"
"r9y9/tacotron_pytorch" -> "jjery2243542/adaptive_voice_conversion"
"r9y9/tacotron_pytorch" -> "BogiHsu/Tacotron2-PyTorch"
"r9y9/tacotron_pytorch" -> "thuhcsi/tacotron"
"r9y9/tacotron_pytorch" -> "keithito/tacotron"
"r9y9/tacotron_pytorch" -> "nii-yamagishilab/multi-speaker-tacotron"
"pritishyuvraj/Voice-Conversion-GAN" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"pritishyuvraj/Voice-Conversion-GAN" -> "leimao/Voice-Converter-CycleGAN"
"Kyubyong/g2pC" -> "kakaobrain/g2pm"
"Kyubyong/g2pC" -> "BoragoCode/AttentionBasedProsodyPrediction"
"Kyubyong/g2pC" -> "open-speech/speech-aligner"
"Kyubyong/g2pC" -> "speechio/chinese_text_normalization" ["e"=1]
"Kyubyong/g2pC" -> "thuhcsi/Crystal"
"Kyubyong/g2pC" -> "Yablon/auorange"
"Kyubyong/g2pC" -> "speechio/BigCiDian" ["e"=1]
"Kyubyong/g2pC" -> "XierHacker/Model_Fusion_Based_Prosody_Prediction"
"Kyubyong/g2pC" -> "KuangDD/phkit"
"Kyubyong/g2pC" -> "hjzin/PolyphoneDisambiguation" ["e"=1]
"mostafaelaraby/wavegan-pytorch" -> "mazzzystar/WaveGAN-pytorch"
"mostafaelaraby/wavegan-pytorch" -> "auroracramer/wavegan"
"atomicoo/tacotron2-mandarin" -> "foamliu/Tacotron2-Mandarin"
"atomicoo/tacotron2-mandarin" -> "atomicoo/Tacotron2-PyTorch"
"atomicoo/tacotron2-mandarin" -> "ArwenFeng/tacotron_mandarin"
"atomicoo/tacotron2-mandarin" -> "lturing/tacotronv2_wavernn_chinese"
"atomicoo/tacotron2-mandarin" -> "atomicoo/FCH-TTS"
"atomicoo/tacotron2-mandarin" -> "wqt2019/tacotron-2_melgan"
"atomicoo/tacotron2-mandarin" -> "JasonWei512/Tacotron-2-Chinese"
"atomicoo/tacotron2-mandarin" -> "wqt2019/tacotron-2_wavernn"
"atomicoo/tacotron2-mandarin" -> "begeekmyfriend/tacotron"
"atomicoo/tacotron2-mandarin" -> "awesome-archive/tacotron_cn"
"talhanai/redbud-tree-depression" -> "RicherMans/text_based_depression"
"talhanai/redbud-tree-depression" -> "kykiefer/depression-detect"
"talhanai/redbud-tree-depression" -> "linlemn/DepressionDectection"
"talhanai/redbud-tree-depression" -> "adbailey1/DepAudioNet_reproduction"
"DeepSpectrum/DeepSpectrum" -> "auDeep/auDeep"
"DeepSpectrum/DeepSpectrum" -> "openXBOW/openXBOW"
"DeepSpectrum/DeepSpectrum" -> "AudioVisualEmotionChallenge/AVEC2019"
"DeepSpectrum/DeepSpectrum" -> "end2you/end2you"
"DeepSpectrum/DeepSpectrum" -> "AudioVisualEmotionChallenge/AVEC2018"
"tuan3w/cnn_vocoder" -> "azraelkuan/FFTNet"
"drfeinberg/PraatScripts" -> "drfeinberg/Parselmouth-Guides"
"drfeinberg/PraatScripts" -> "YannickJadoul/Parselmouth"
"drfeinberg/PraatScripts" -> "Voice-Lab/VoiceLab"
"drfeinberg/PraatScripts" -> "timmahrt/praatIO"
"drfeinberg/PraatScripts" -> "Shahabks/myprosody"
"drfeinberg/PraatScripts" -> "jcvasquezc/DisVoice"
"JoFrhwld/FAVE" -> "voicesauce/opensauce-python"
"JoFrhwld/FAVE" -> "prosodylab/Prosodylab-Aligner"
"Sharad24/Neural-Voice-Cloning-with-Few-Samples" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"Sharad24/Neural-Voice-Cloning-with-Few-Samples" -> "IEEE-NITK/Neural-Voice-Cloning"
"Sharad24/Neural-Voice-Cloning-with-Few-Samples" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"CoEDL/elpis" -> "persephone-tools/persephone"
"CoEDL/elpis" -> "onset/lameta"
"CoEDL/elpis" -> "ReadAlongs/Studio"
"CoEDL/elpis" -> "lgessler/glam"
"jinhan/tacotron2-vae" -> "rishikksh20/vae_tacotron2"
"jinhan/tacotron2-vae" -> "yanggeng1995/vae_tacotron"
"jinhan/tacotron2-vae" -> "jinhan/tacotron2-gst"
"jinhan/tacotron2-vae" -> "KinglittleQ/GST-Tacotron"
"jinhan/tacotron2-vae" -> "nii-yamagishilab/multi-speaker-tacotron"
"jinhan/tacotron2-vae" -> "ide8/tacotron2"
"jinhan/tacotron2-vae" -> "jxzhanggg/nonparaSeq2seqVC_code"
"jinhan/tacotron2-vae" -> "keonlee9420/StyleSpeech" ["e"=1]
"MycroftAI/selene-ui" -> "MycroftAI/selene-backend"
"ardaillon/FCN-f0" -> "mairaksi/PiENet"
"mairaksi/PiENet" -> "ardaillon/FCN-f0"
"bshall/UniversalVocoding" -> "bshall/Tacotron"
"bshall/UniversalVocoding" -> "geneing/WaveRNN-Pytorch"
"bshall/UniversalVocoding" -> "yistLin/universal-vocoder"
"bshall/UniversalVocoding" -> "tianrengao/SqueezeWave"
"bshall/UniversalVocoding" -> "ksw0306/WaveVAE"
"bshall/UniversalVocoding" -> "G-Wang/WaveRNN-Pytorch"
"bshall/UniversalVocoding" -> "hhguo/EA-SVC"
"bshall/UniversalVocoding" -> "ivanvovk/WaveGrad"
"bshall/UniversalVocoding" -> "bshall/ZeroSpeech"
"bshall/UniversalVocoding" -> "ksw0306/ClariNet"
"bshall/UniversalVocoding" -> "mkotha/WaveRNN"
"bshall/UniversalVocoding" -> "LeoniusChen/Attentions-in-Tacotron"
"festvox/festvox" -> "festvox/speech_tools"
"festvox/festvox" -> "festvox/festival"
"hbuschme/TextGridTools" -> "kirbyj/praatsauce"
"hbuschme/TextGridTools" -> "kylebgorman/textgrid"
"BoragoCode/AttentionBasedProsodyPrediction" -> "Zeqiang-Lai/Prosody_Prediction"
"BoragoCode/AttentionBasedProsodyPrediction" -> "Liu-Feng-deeplearning/TTS-frontend"
"BoragoCode/AttentionBasedProsodyPrediction" -> "XierHacker/Model_Fusion_Based_Prosody_Prediction"
"yanggeng1995/vae_tacotron" -> "rishikksh20/vae_tacotron2"
"yanggeng1995/vae_tacotron" -> "rishikksh20/gmvae_tacotron"
"yanggeng1995/vae_tacotron" -> "rishikksh20/Phone-Level-Mixture-Density-Network-for-TTS"
"r9y9/nnmnkwii_gallery" -> "r9y9/nnmnkwii"
"ajinkyaT/CNN_Intent_Classification" -> "deepmipt/intent_classifier"
"halolimat/Social-media-Depression-Detector" -> "isrugeek/depression_detection"
"halolimat/Social-media-Depression-Detector" -> "niquejoe/Classification-of-Depression-on-Social-Media-Using-Text-Mining"
"halolimat/Social-media-Depression-Detector" -> "kykiefer/depression-detect"
"roedoejet/g2p" -> "nrc-cnrc/gramble"
"roedoejet/g2p" -> "ReadAlongs/Studio"
"roedoejet/g2p" -> "EveryVoiceTTS/EveryVoice"
"tuanad121/Python-WORLD" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"cognibit/Text-Normalization-Demo" -> "google-research-datasets/TextNormalizationCoveringGrammars"
"mozilla/DSAlign" -> "lumaku/ctc-segmentation" ["e"=1]
"mozilla/DSAlign" -> "pettarin/forced-alignment-tools"
"mozilla/DSAlign" -> "mlcommons/peoples-speech"
"kensun0/Parallel-Wavenet" -> "bfs18/nsynth_wavenet"
"kensun0/Parallel-Wavenet" -> "zhf459/P_wavenet_vocoder"
"kensun0/Parallel-Wavenet" -> "andabi/parallel-wavenet-vocoder"
"Jackustc/Question-Level-Feature-Extraction-on-DAIC-WOZ-dataset" -> "AliceOTHMANI/EmoAudioNet"
"Jackustc/Question-Level-Feature-Extraction-on-DAIC-WOZ-dataset" -> "chuyuanli/MTL4Depr"
"Jackustc/Question-Level-Feature-Extraction-on-DAIC-WOZ-dataset" -> "CMDC-corpus/CMDC-Baseline"
"Jackustc/Question-Level-Feature-Extraction-on-DAIC-WOZ-dataset" -> "adbailey1/daic_woz_process"
"soobinseo/Tacotron-pytorch" -> "soobinseo/bytenet_masked"
"soobinseo/Tacotron-pytorch" -> "ksw0306/ClariNet"
"soobinseo/Tacotron-pytorch" -> "G-Wang/WaveRNN-Pytorch"
"KuangDD/aukit" -> "KuangDD/phkit"
"KuangDD/aukit" -> "KuangDD/zhvoice"
"voicesauce/opensauce-python" -> "kirbyj/praatsauce"
"kan-bayashi/PytorchWaveNetVocoder" -> "azraelkuan/parallel_wavenet_vocoder"
"kan-bayashi/PytorchWaveNetVocoder" -> "mkotha/WaveRNN"
"kan-bayashi/PytorchWaveNetVocoder" -> "syang1993/gst-tacotron"
"kan-bayashi/PytorchWaveNetVocoder" -> "npuichigo/waveglow"
"kan-bayashi/PytorchWaveNetVocoder" -> "ksw0306/FloWaveNet"
"kan-bayashi/PytorchWaveNetVocoder" -> "bfs18/nsynth_wavenet"
"kan-bayashi/PytorchWaveNetVocoder" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"kan-bayashi/PytorchWaveNetVocoder" -> "ksw0306/ClariNet"
"kan-bayashi/PytorchWaveNetVocoder" -> "r9y9/nnmnkwii"
"kan-bayashi/PytorchWaveNetVocoder" -> "k2kobayashi/sprocket"
"kan-bayashi/PytorchWaveNetVocoder" -> "andabi/parallel-wavenet-vocoder"
"kan-bayashi/PytorchWaveNetVocoder" -> "Kyubyong/expressive_tacotron"
"kan-bayashi/PytorchWaveNetVocoder" -> "xiph/LPCNet"
"kan-bayashi/PytorchWaveNetVocoder" -> "NVIDIA/nv-wavenet"
"kan-bayashi/PytorchWaveNetVocoder" -> "r9y9/wavenet_vocoder"
"JeremyCCHsu/vqvae-speech" -> "Kyubyong/vq-vae"
"JeremyCCHsu/vqvae-speech" -> "JeremyCCHsu/vae-npvc"
"JeremyCCHsu/vqvae-speech" -> "swasun/VQ-VAE-Speech"
"asuni/wavelet_prosody_toolkit" -> "Helsinki-NLP/prosody"
"atomicoo/chn_text_norm" -> "candlewill/CNTN"
"AudioVisualEmotionChallenge/AVEC2018" -> "AudioVisualEmotionChallenge/AVEC2019"
"guanlongzhao/fac-via-ppg" -> "guanlongzhao/ppg-gmm"
"guanlongzhao/fac-via-ppg" -> "cjerry1243/TransferLearning-CLVC"
"guanlongzhao/fac-via-ppg" -> "ryokamoi/ppg_vc"
"guanlongzhao/fac-via-ppg" -> "jxzhanggg/nonparaSeq2seqVC_code"
"guanlongzhao/fac-via-ppg" -> "hhguo/EA-SVC"
"guanlongzhao/fac-via-ppg" -> "BridgetteSong/ExpressiveTacotron" ["e"=1]
"numediart/MBROLA-voices" -> "numediart/MBROLA"
"numediart/MBROLA-voices" -> "numediart/MBROLATOR"
"awesome-archive/tacotron_cn" -> "peak1995/tacotron-chinese"
"geneing/WaveRNN-Pytorch" -> "G-Wang/WaveRNN-Pytorch"
"geneing/WaveRNN-Pytorch" -> "bshall/UniversalVocoding"
"geneing/WaveRNN-Pytorch" -> "azraelkuan/parallel_wavenet_vocoder"
"geneing/WaveRNN-Pytorch" -> "h-meru/Tacotron-WaveRNN"
"geneing/WaveRNN-Pytorch" -> "alokprasad/LPCTron"
"MlWoo/LPCNet" -> "alokprasad/LPCTron"
"MlWoo/LPCNet" -> "h-meru/Tacotron-WaveRNN"
"rishikksh20/vae_tacotron2" -> "yanggeng1995/vae_tacotron"
"rishikksh20/vae_tacotron2" -> "jinhan/tacotron2-vae"
"rishikksh20/vae_tacotron2" -> "rishikksh20/gmvae_tacotron"
"ArwenFeng/tacotron_mandarin" -> "karamarieliu/gst_tacotron2_wavenet"
"JarbasHiveMind/ZZZ-HiveMind-core" -> "OpenVoiceOS/ovos-buildroot"
"JarbasHiveMind/ZZZ-HiveMind-core" -> "JarbasHiveMind/HiveMind-voice-sat"
"JarbasHiveMind/ZZZ-HiveMind-core" -> "alexisdiaz008/react-mycroft-gui"
"JarbasHiveMind/ZZZ-HiveMind-core" -> "OpenVoiceOS/ovos-personal-backend"
"vsimkus/vae-voice-conversion" -> "Suhee05/Zerospeech2019"
"vsimkus/vae-voice-conversion" -> "ericwudayi/SkipVQVC"
"hon9g/Text-to-Color" -> "zamlz/dlcampjeju2018-I2A-cube"
"chaeyoung-lee/cwavegan" -> "jwkanggist/tpu-tutorial-experiment"
"chaeyoung-lee/cwavegan" -> "mazzzystar/WaveGAN-pytorch"
"chaeyoung-lee/cwavegan" -> "zamlz/dlcampjeju2018-I2A-cube"
"Cortexelus/dadabots_sampleRNN" -> "ZVK/sampleRNN_ICLR2017"
"auroracramer/wavegan" -> "mazzzystar/WaveGAN-pytorch"
"alishdipani/Neural-Style-Transfer-Audio" -> "inzva/Audio-Style-Transfer"
"dophist/DaCiDian-Develop" -> "naxingyu/kaldi-gadgets"
"ttsunion/Deep-Expression" -> "syang1993/FFTNet"
"Kyubyong/cross_vc" -> "ryokamoi/ppg_vc"
"unilight/cdvae-vc" -> "acetylSv/non-parallel-rhythm-flexible-VC"
"tifgan/stftGAN" -> "andimarafioti/tifresi"
"tifgan/stftGAN" -> "kastnerkyle/representation_mixing"
"fatchord/FFTNet" -> "azraelkuan/FFTNet"
"fatchord/FFTNet" -> "erogol/FFTNet"
"AudioVisualEmotionChallenge/AVEC2019" -> "LouisYZK/dds-avec2019"
"AudioVisualEmotionChallenge/AVEC2019" -> "ihp-lab/Avec2019_DDS"
"AudioVisualEmotionChallenge/AVEC2019" -> "AudioVisualEmotionChallenge/AVEC2018"
"AudioVisualEmotionChallenge/AVEC2019" -> "adbailey1/daic_woz_process"
"AudioVisualEmotionChallenge/AVEC2019" -> "adbailey1/DepAudioNet_reproduction"
"AudioVisualEmotionChallenge/AVEC2019" -> "Jackustc/Question-Level-Feature-Extraction-on-DAIC-WOZ-dataset"
"AudioVisualEmotionChallenge/AVEC2019" -> "PingCheng-Wei/DepressionEstimation"
"onejiin/CycleGAN-VC2" -> "cjerry1243/TransferLearning-CLVC"
"onejiin/CycleGAN-VC2" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"MycroftAI/Precise-Community-Data" -> "gras64/learning-skill"
"zamlz/dlcampjeju2018-I2A-cube" -> "jwkanggist/tpu-tutorial-experiment"
"G-Wang/WaveRNN-Pytorch" -> "geneing/WaveRNN-Pytorch"
"G-Wang/WaveRNN-Pytorch" -> "h-meru/Tacotron-WaveRNN"
"G-Wang/WaveRNN-Pytorch" -> "tuan3w/cnn_vocoder"
"mlml/autovot" -> "MLSpeech/Dr.VOT"
"h-meru/Tacotron-WaveRNN" -> "m-toman/tacorn"
"h-meru/Tacotron-WaveRNN" -> "G-Wang/WaveRNN-Pytorch"
"h-meru/Tacotron-WaveRNN" -> "MlWoo/LPCNet"
"bottlecapper/EmoMUNIT" -> "bottlecapper/EmoCycleGAN"
"bottlecapper/EmoMUNIT" -> "ktho22/vctts"
"CynthiaSuwi/ASR-for-Chinese-Pipeline" -> "liangstein/Chinese-speech-to-text"
"CynthiaSuwi/ASR-for-Chinese-Pipeline" -> "EliasCai/speech_recognition_ctc" ["e"=1]
"prosegrinder/python-cmudict" -> "prosegrinder/python-syllables"
"open-speech/cn-text-normalizer" -> "LingLing-Speech/mandarin-tts-frontend-python"
"IPS-LMU/EMU-webApp" -> "IPS-LMU/emuR"
"XierHacker/Model_Fusion_Based_Prosody_Prediction" -> "xcmyz/DurIAN"
"rishikksh20/gmvae_tacotron" -> "yanggeng1995/vae_tacotron"
"rishikksh20/gmvae_tacotron" -> "Yangyangii/TPGST-Tacotron"
"festvox/speech_tools" -> "festvox/festvox"
"zhf459/P_wavenet_vocoder" -> "maozhiqiang/wavernn"
"LinusSkucas/communications-skill" -> "gras64/learning-skill"
"erogol/FFTNet" -> "syang1993/FFTNet"
"syang1993/FFTNet" -> "erogol/FFTNet"
"syang1993/FFTNet" -> "kastnerkyle/representation_mixing"
"HimangiM/Depression_Detection" -> "derong97/automatic-depression-detector"
"shauryr/google_text_normalization" -> "rwsproat/text-normalization-data"
"peijoy/DetectDepressionInTwitterPosts" -> "viritaromero/Detecting-Depression-in-Tweets"
"peijoy/DetectDepressionInTwitterPosts" -> "AshwanthRamji/Depression-Sentiment-Analysis-with-Twitter-Data"
"german-asr/kaldi-german" -> "bmilde/german-asr-lm-tools"
"ReadAlongs/Studio" -> "ReadAlongs/Studio-Web"
"ReadAlongs/Studio" -> "roedoejet/convertextract"
"ReadAlongs/Studio" -> "nrc-cnrc/gramble"
"gras64/learning-skill" -> "LinusSkucas/communications-skill"
"bottlecapper/EmoCycleGAN" -> "ktho22/vctts"
"AIIX/youtube-skill" -> "pcwii/mesh-skill"
"IPS-LMU/wrassp" -> "IPS-LMU/emuR"
"HappyBall/tacotron" -> "peak1995/tacotron-chinese"
"HappyBall/tacotron" -> "awesome-archive/tacotron_cn"
"peak1995/tacotron-chinese" -> "awesome-archive/tacotron_cn"
"MachineJeff/Chinese_Polyphone_Disambiguation" -> "npujcong/Chinese_PSP"
"maozhiqiang/tacotron_cn" -> "maozhiqiang/wavernn"
"resemble-ai/Resemblyzer" -> "kan-bayashi/ParallelWaveGAN"
"resemble-ai/Resemblyzer" -> "jik876/hifi-gan"
"resemble-ai/Resemblyzer" -> "auspicious3000/autovc"
"resemble-ai/Resemblyzer" -> "wq2012/awesome-diarization" ["e"=1]
"resemble-ai/Resemblyzer" -> "s3prl/s3prl"
"resemble-ai/Resemblyzer" -> "ming024/FastSpeech2"
"resemble-ai/Resemblyzer" -> "aliutkus/speechmetrics" ["e"=1]
"resemble-ai/Resemblyzer" -> "wenet-e2e/speech-synthesis-paper"
"resemble-ai/Resemblyzer" -> "espnet/espnet" ["e"=1]
"resemble-ai/Resemblyzer" -> "Rayhane-mamah/Tacotron-2"
"resemble-ai/Resemblyzer" -> "auspicious3000/SpeechSplit"
"resemble-ai/Resemblyzer" -> "r9y9/wavenet_vocoder"
"resemble-ai/Resemblyzer" -> "fatchord/WaveRNN"
"resemble-ai/Resemblyzer" -> "descriptinc/melgan-neurips"
"resemble-ai/Resemblyzer" -> "bootphon/phonemizer"
"NVIDIA/mellotron" -> "seungwonpark/melgan"
"NVIDIA/mellotron" -> "NVIDIA/flowtron"
"NVIDIA/mellotron" -> "jaywalnut310/glow-tts"
"NVIDIA/mellotron" -> "xcmyz/FastSpeech"
"NVIDIA/mellotron" -> "syang1993/gst-tacotron"
"NVIDIA/mellotron" -> "KinglittleQ/GST-Tacotron"
"NVIDIA/mellotron" -> "nii-yamagishilab/multi-speaker-tacotron"
"NVIDIA/mellotron" -> "auspicious3000/SpeechSplit"
"NVIDIA/mellotron" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/mellotron" -> "xiph/LPCNet"
"NVIDIA/mellotron" -> "descriptinc/melgan-neurips"
"NVIDIA/mellotron" -> "NVIDIA/waveglow"
"NVIDIA/mellotron" -> "Tomiinek/Multilingual_Text_to_Speech"
"NVIDIA/mellotron" -> "soobinseo/Transformer-TTS"
"NVIDIA/mellotron" -> "Kyubyong/css10"
"auspicious3000/SpeechSplit" -> "auspicious3000/autovc"
"auspicious3000/SpeechSplit" -> "auspicious3000/AutoPST"
"auspicious3000/SpeechSplit" -> "Wendison/VQMIVC"
"auspicious3000/SpeechSplit" -> "jjery2243542/adaptive_voice_conversion"
"auspicious3000/SpeechSplit" -> "NVIDIA/mellotron"
"auspicious3000/SpeechSplit" -> "biggytruck/SpeechSplit2" ["e"=1]
"auspicious3000/SpeechSplit" -> "descriptinc/melgan-neurips"
"auspicious3000/SpeechSplit" -> "auspicious3000/contentvec" ["e"=1]
"auspicious3000/SpeechSplit" -> "jxzhanggg/nonparaSeq2seqVC_code"
"auspicious3000/SpeechSplit" -> "kan-bayashi/ParallelWaveGAN"
"auspicious3000/SpeechSplit" -> "maum-ai/cotatron"
"auspicious3000/SpeechSplit" -> "liusongxiang/ppg-vc"
"auspicious3000/SpeechSplit" -> "liusongxiang/StarGAN-Voice-Conversion"
"auspicious3000/SpeechSplit" -> "wenet-e2e/speech-synthesis-paper"
"auspicious3000/SpeechSplit" -> "jaywalnut310/glow-tts"
"ming024/FastSpeech2" -> "jik876/hifi-gan"
"ming024/FastSpeech2" -> "xcmyz/FastSpeech"
"ming024/FastSpeech2" -> "kan-bayashi/ParallelWaveGAN"
"ming024/FastSpeech2" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"ming024/FastSpeech2" -> "wenet-e2e/speech-synthesis-paper"
"ming024/FastSpeech2" -> "jaywalnut310/glow-tts"
"ming024/FastSpeech2" -> "lifeiteng/vall-e" ["e"=1]
"ming024/FastSpeech2" -> "TensorSpeech/TensorFlowTTS"
"ming024/FastSpeech2" -> "microsoft/NeuralSpeech" ["e"=1]
"ming024/FastSpeech2" -> "lucidrains/naturalspeech2-pytorch" ["e"=1]
"ming024/FastSpeech2" -> "shivammehta25/Matcha-TTS" ["e"=1]
"ming024/FastSpeech2" -> "NVIDIA/tacotron2"
"ming024/FastSpeech2" -> "jaywalnut310/vits" ["e"=1]
"ming024/FastSpeech2" -> "s3prl/s3prl"
"ming024/FastSpeech2" -> "NVIDIA/BigVGAN" ["e"=1]
"Tomiinek/Multilingual_Text_to_Speech" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"Tomiinek/Multilingual_Text_to_Speech" -> "nii-yamagishilab/multi-speaker-tacotron"
"Tomiinek/Multilingual_Text_to_Speech" -> "jaywalnut310/glow-tts"
"Tomiinek/Multilingual_Text_to_Speech" -> "NVIDIA/mellotron"
"Tomiinek/Multilingual_Text_to_Speech" -> "xcmyz/FastSpeech"
"Tomiinek/Multilingual_Text_to_Speech" -> "Kyubyong/css10"
"Tomiinek/Multilingual_Text_to_Speech" -> "wenet-e2e/speech-synthesis-paper"
"Tomiinek/Multilingual_Text_to_Speech" -> "kan-bayashi/ParallelWaveGAN"
"Tomiinek/Multilingual_Text_to_Speech" -> "jik876/hifi-gan"
"Tomiinek/Multilingual_Text_to_Speech" -> "auspicious3000/SpeechSplit"
"Tomiinek/Multilingual_Text_to_Speech" -> "ivanvovk/WaveGrad"
"Tomiinek/Multilingual_Text_to_Speech" -> "rishikksh20/VocGAN"
"Tomiinek/Multilingual_Text_to_Speech" -> "syang1993/gst-tacotron"
"Tomiinek/Multilingual_Text_to_Speech" -> "ming024/FastSpeech2"
"Tomiinek/Multilingual_Text_to_Speech" -> "LEEYOONHYUNG/BVAE-TTS"
"TensorSpeech/TensorFlowTTS" -> "ming024/FastSpeech2"
"TensorSpeech/TensorFlowTTS" -> "kan-bayashi/ParallelWaveGAN"
"TensorSpeech/TensorFlowTTS" -> "TensorSpeech/TensorFlowASR" ["e"=1]
"TensorSpeech/TensorFlowTTS" -> "Rayhane-mamah/Tacotron-2"
"TensorSpeech/TensorFlowTTS" -> "fatchord/WaveRNN"
"TensorSpeech/TensorFlowTTS" -> "jik876/hifi-gan"
"TensorSpeech/TensorFlowTTS" -> "mozilla/TTS"
"TensorSpeech/TensorFlowTTS" -> "espnet/espnet" ["e"=1]
"TensorSpeech/TensorFlowTTS" -> "xcmyz/FastSpeech"
"TensorSpeech/TensorFlowTTS" -> "keithito/tacotron"
"TensorSpeech/TensorFlowTTS" -> "NVIDIA/tacotron2"
"TensorSpeech/TensorFlowTTS" -> "wenet-e2e/speech-synthesis-paper"
"TensorSpeech/TensorFlowTTS" -> "spring-media/TransformerTTS"
"TensorSpeech/TensorFlowTTS" -> "xiph/LPCNet"
"TensorSpeech/TensorFlowTTS" -> "r9y9/wavenet_vocoder"
"spring-media/TransformerTTS" -> "soobinseo/Transformer-TTS"
"spring-media/TransformerTTS" -> "as-ideas/ForwardTacotron"
"spring-media/TransformerTTS" -> "xcmyz/FastSpeech"
"spring-media/TransformerTTS" -> "ming024/FastSpeech2"
"spring-media/TransformerTTS" -> "NVIDIA/mellotron"
"spring-media/TransformerTTS" -> "jaywalnut310/glow-tts"
"spring-media/TransformerTTS" -> "jik876/hifi-gan"
"spring-media/TransformerTTS" -> "TensorSpeech/TensorFlowTTS"
"spring-media/TransformerTTS" -> "seungwonpark/melgan"
"spring-media/TransformerTTS" -> "Tomiinek/Multilingual_Text_to_Speech"
"spring-media/TransformerTTS" -> "kan-bayashi/ParallelWaveGAN"
"spring-media/TransformerTTS" -> "NVIDIA/flowtron"
"spring-media/TransformerTTS" -> "wenet-e2e/speech-synthesis-paper"
"spring-media/TransformerTTS" -> "rishikksh20/VocGAN"
"spring-media/TransformerTTS" -> "fatchord/WaveRNN"
"rishikksh20/VocGAN" -> "ivanvovk/WaveGrad"
"rishikksh20/VocGAN" -> "keonlee9420/Parallel-Tacotron2"
"rishikksh20/VocGAN" -> "rishikksh20/FastSpeech2"
"rishikksh20/VocGAN" -> "rishikksh20/AdaSpeech"
"rishikksh20/VocGAN" -> "seungwonpark/melgan"
"rishikksh20/VocGAN" -> "thuhcsi/VAENAR-TTS"
"rishikksh20/VocGAN" -> "ivanvovk/durian-pytorch"
"rishikksh20/VocGAN" -> "rishikksh20/TFGAN"
"rishikksh20/VocGAN" -> "yanggeng1995/GAN-TTS"
"rishikksh20/VocGAN" -> "HGU-DLLAB/Korean-FastSpeech2-Pytorch" ["e"=1]
"rishikksh20/VocGAN" -> "LEEYOONHYUNG/BVAE-TTS"
"rishikksh20/VocGAN" -> "tencent-ailab/bddm" ["e"=1]
"rishikksh20/VocGAN" -> "zceng/LVCNet"
"rishikksh20/VocGAN" -> "dipjyoti92/SC-WaveRNN"
"rishikksh20/VocGAN" -> "facebookresearch/speech-resynthesis" ["e"=1]
"marcoppasini/MelGAN-VC" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"marcoppasini/MelGAN-VC" -> "jxzhanggg/nonparaSeq2seqVC_code"
"marcoppasini/MelGAN-VC" -> "JeremyCCHsu/vae-npvc"
"marcoppasini/MelGAN-VC" -> "jjery2243542/adaptive_voice_conversion"
"marcoppasini/MelGAN-VC" -> "ebadawy/voice_conversion"
"marcoppasini/MelGAN-VC" -> "k2kobayashi/crank"
"marcoppasini/MelGAN-VC" -> "liusongxiang/StarGAN-Voice-Conversion"
"marcoppasini/MelGAN-VC" -> "mazzzystar/randomCNN-voice-transfer"
"marcoppasini/MelGAN-VC" -> "hujinsen/pytorch-StarGAN-VC"
"marcoppasini/MelGAN-VC" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"marcoppasini/MelGAN-VC" -> "himajin2045/voice-conversion"
"marcoppasini/MelGAN-VC" -> "guanlongzhao/fac-via-ppg"
"marcoppasini/MelGAN-VC" -> "leimao/Voice-Converter-CycleGAN"
"lmnt-com/diffwave" -> "ivanvovk/WaveGrad"
"lmnt-com/diffwave" -> "lmnt-com/wavegrad"
"lmnt-com/diffwave" -> "NVIDIA/BigVGAN" ["e"=1]
"lmnt-com/diffwave" -> "kan-bayashi/ParallelWaveGAN"
"lmnt-com/diffwave" -> "jik876/hifi-gan"
"lmnt-com/diffwave" -> "huawei-noah/Speech-Backbones" ["e"=1]
"lmnt-com/diffwave" -> "neillu23/CDiffuSE" ["e"=1]
"lmnt-com/diffwave" -> "Rongjiehuang/FastDiff" ["e"=1]
"lmnt-com/diffwave" -> "Rongjiehuang/ProDiff" ["e"=1]
"lmnt-com/diffwave" -> "sp-uhh/sgmse" ["e"=1]
"lmnt-com/diffwave" -> "facebookresearch/vocoder-benchmark" ["e"=1]
"lmnt-com/diffwave" -> "archinetai/audio-diffusion-pytorch" ["e"=1]
"lmnt-com/diffwave" -> "wenet-e2e/speech-synthesis-paper"
"lmnt-com/diffwave" -> "philsyn/DiffWave-Vocoder"
"lmnt-com/diffwave" -> "descriptinc/descript-audio-codec" ["e"=1]
"r4victor/syncabook" -> "r4victor/afaligner"
"r4victor/syncabook" -> "r4victor/synclibrivox"
"r4victor/syncabook" -> "readbeyond/aeneas"
"r4victor/syncabook" -> "smoores-dev/storyteller"
"r4victor/syncabook" -> "kanjieater/SubPlz" ["e"=1]
"thorstenMueller/Thorsten-Voice" -> "rhasspy/gruut"
"thorstenMueller/Thorsten-Voice" -> "AASHISHAG/deepspeech-german" ["e"=1]
"thorstenMueller/Thorsten-Voice" -> "MycroftAI/mimic-recording-studio"
"thorstenMueller/Thorsten-Voice" -> "coqui-ai/open-speech-corpora"
"thorstenMueller/Thorsten-Voice" -> "coqui-ai/TTS-papers"
"thorstenMueller/Thorsten-Voice" -> "german-asr/megs"
"thorstenMueller/Thorsten-Voice" -> "repodiac/german_transliterate"
"thorstenMueller/Thorsten-Voice" -> "Tomiinek/Multilingual_Text_to_Speech"
"thorstenMueller/Thorsten-Voice" -> "uhh-lt/kaldi-tuda-de" ["e"=1]
"thorstenMueller/Thorsten-Voice" -> "monatis/german-tts"
"thorstenMueller/Thorsten-Voice" -> "spring-media/DeepPhonemizer"
"thorstenMueller/Thorsten-Voice" -> "Kyubyong/css10"
"thorstenMueller/Thorsten-Voice" -> "coqui-ai/TTS-recipes"
"thorstenMueller/Thorsten-Voice" -> "AdolfVonKleist/Phonetisaurus"
"thorstenMueller/Thorsten-Voice" -> "adbar/German-NLP" ["e"=1]
"NVIDIA/flowtron" -> "NVIDIA/mellotron"
"NVIDIA/flowtron" -> "jaywalnut310/glow-tts"
"NVIDIA/flowtron" -> "NVIDIA/waveglow"
"NVIDIA/flowtron" -> "NVIDIA/radtts" ["e"=1]
"NVIDIA/flowtron" -> "wenet-e2e/speech-synthesis-paper"
"NVIDIA/flowtron" -> "xcmyz/FastSpeech"
"NVIDIA/flowtron" -> "nii-yamagishilab/multi-speaker-tacotron"
"NVIDIA/flowtron" -> "seungwonpark/melgan"
"NVIDIA/flowtron" -> "kan-bayashi/ParallelWaveGAN"
"NVIDIA/flowtron" -> "Tomiinek/Multilingual_Text_to_Speech"
"NVIDIA/flowtron" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"NVIDIA/flowtron" -> "as-ideas/ForwardTacotron"
"NVIDIA/flowtron" -> "maum-ai/cotatron"
"NVIDIA/flowtron" -> "jik876/hifi-gan"
"NVIDIA/flowtron" -> "ivanvovk/WaveGrad"
"kakaobrain/g2pm" -> "Kyubyong/g2pC"
"kakaobrain/g2pm" -> "GitYCC/g2pW" ["e"=1]
"kakaobrain/g2pm" -> "speechio/chinese_text_normalization" ["e"=1]
"kakaobrain/g2pm" -> "Jackiexiao/MTTS"
"kakaobrain/g2pm" -> "thuhcsi/Crystal"
"kakaobrain/g2pm" -> "Kyubyong/g2p"
"kakaobrain/g2pm" -> "KuangDD/phkit"
"kakaobrain/g2pm" -> "speechio/BigCiDian" ["e"=1]
"kakaobrain/g2pm" -> "xcmyz/FastVocoder"
"kakaobrain/g2pm" -> "xcmyz/FastSpeech"
"kakaobrain/g2pm" -> "BoragoCode/AttentionBasedProsodyPrediction"
"kakaobrain/g2pm" -> "thuhcsi/VAENAR-TTS"
"kakaobrain/g2pm" -> "Helsinki-NLP/prosody"
"kakaobrain/g2pm" -> "open-speech/cn-text-normalizer"
"kakaobrain/g2pm" -> "syang1993/gst-tacotron"
"jxzhanggg/nonparaSeq2seqVC_code" -> "k2kobayashi/crank"
"jxzhanggg/nonparaSeq2seqVC_code" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"jxzhanggg/nonparaSeq2seqVC_code" -> "liusongxiang/ppg-vc"
"jxzhanggg/nonparaSeq2seqVC_code" -> "KunZhou9646/seq2seq-EVC"
"jxzhanggg/nonparaSeq2seqVC_code" -> "yistLin/FragmentVC"
"jxzhanggg/nonparaSeq2seqVC_code" -> "Wendison/VQMIVC"
"jxzhanggg/nonparaSeq2seqVC_code" -> "jjery2243542/adaptive_voice_conversion"
"jxzhanggg/nonparaSeq2seqVC_code" -> "dipjyoti92/SC-WaveRNN"
"jxzhanggg/nonparaSeq2seqVC_code" -> "joansj/blow"
"jxzhanggg/nonparaSeq2seqVC_code" -> "bigpon/vcc20_baseline_cyclevae"
"jxzhanggg/nonparaSeq2seqVC_code" -> "patrickltobing/cyclevae-vc-neuralvoco"
"jxzhanggg/nonparaSeq2seqVC_code" -> "jinhan/tacotron2-vae"
"jxzhanggg/nonparaSeq2seqVC_code" -> "JeremyCCHsu/vae-npvc"
"jxzhanggg/nonparaSeq2seqVC_code" -> "guanlongzhao/fac-via-ppg"
"jxzhanggg/nonparaSeq2seqVC_code" -> "bshall/ZeroSpeech"
"maum-ai/cotatron" -> "maum-ai/assem-vc" ["e"=1]
"maum-ai/cotatron" -> "nii-yamagishilab/multi-speaker-tacotron"
"maum-ai/cotatron" -> "dipjyoti92/SC-WaveRNN"
"maum-ai/cotatron" -> "LEEYOONHYUNG/BVAE-TTS"
"maum-ai/cotatron" -> "k2kobayashi/crank"
"patrickltobing/cyclevae-vc-neuralvoco" -> "k2kobayashi/crank"
"wenet-e2e/speech-synthesis-paper" -> "tts-tutorial/survey"
"wenet-e2e/speech-synthesis-paper" -> "NVIDIA/BigVGAN" ["e"=1]
"wenet-e2e/speech-synthesis-paper" -> "kan-bayashi/ParallelWaveGAN"
"wenet-e2e/speech-synthesis-paper" -> "xcmyz/FastSpeech"
"wenet-e2e/speech-synthesis-paper" -> "jaywalnut310/glow-tts"
"wenet-e2e/speech-synthesis-paper" -> "jik876/hifi-gan"
"wenet-e2e/speech-synthesis-paper" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers" ["e"=1]
"wenet-e2e/speech-synthesis-paper" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"wenet-e2e/speech-synthesis-paper" -> "ddlBoJack/Speech-Resources" ["e"=1]
"wenet-e2e/speech-synthesis-paper" -> "ming024/FastSpeech2"
"wenet-e2e/speech-synthesis-paper" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"wenet-e2e/speech-synthesis-paper" -> "microsoft/NeuralSpeech" ["e"=1]
"wenet-e2e/speech-synthesis-paper" -> "huawei-noah/Speech-Backbones" ["e"=1]
"wenet-e2e/speech-synthesis-paper" -> "yangdongchao/AcademiCodec" ["e"=1]
"wenet-e2e/speech-synthesis-paper" -> "p0p4k/vits2_pytorch" ["e"=1]
"AdolfVonKleist/Phonetisaurus" -> "sequitur-g2p/sequitur-g2p"
"AdolfVonKleist/Phonetisaurus" -> "cmusphinx/g2p-seq2seq" ["e"=1]
"AdolfVonKleist/Phonetisaurus" -> "uiuc-sst/g2ps"
"AdolfVonKleist/Phonetisaurus" -> "Kyubyong/g2p"
"AdolfVonKleist/Phonetisaurus" -> "bootphon/phonemizer"
"AdolfVonKleist/Phonetisaurus" -> "dmort27/panphon"
"AdolfVonKleist/Phonetisaurus" -> "HawkAaron/warp-transducer" ["e"=1]
"AdolfVonKleist/Phonetisaurus" -> "lingjzhu/charsiu"
"AdolfVonKleist/Phonetisaurus" -> "google/sparrowhawk"
"AdolfVonKleist/Phonetisaurus" -> "CUNY-CL/wikipron"
"AdolfVonKleist/Phonetisaurus" -> "cmusphinx/cmudict"
"AdolfVonKleist/Phonetisaurus" -> "lingjzhu/CharsiuG2P" ["e"=1]
"AdolfVonKleist/Phonetisaurus" -> "k2-fsa/k2" ["e"=1]
"AdolfVonKleist/Phonetisaurus" -> "coqui-ai/open-speech-corpora"
"AdolfVonKleist/Phonetisaurus" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"timmahrt/praatIO" -> "timmahrt/ProMo"
"timmahrt/praatIO" -> "YannickJadoul/Parselmouth"
"timmahrt/praatIO" -> "drfeinberg/PraatScripts"
"timmahrt/praatIO" -> "jcvasquezc/DisVoice"
"timmahrt/praatIO" -> "kylebgorman/textgrid"
"timmahrt/praatIO" -> "lingjzhu/charsiu"
"timmahrt/praatIO" -> "mlml/autovot"
"timmahrt/praatIO" -> "hbuschme/TextGridTools"
"timmahrt/praatIO" -> "r9y9/pysptk"
"timmahrt/praatIO" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"timmahrt/praatIO" -> "praat/praat"
"timmahrt/praatIO" -> "prosodylab/Prosodylab-Aligner"
"timmahrt/praatIO" -> "HidekiKawahara/legacy_STRAIGHT"
"timmahrt/praatIO" -> "stylerw/styler_praat_scripts"
"timmahrt/praatIO" -> "bbTomas/rPraat"
"xinjli/allosaurus" -> "xinjli/transphone"
"xinjli/allosaurus" -> "dmort27/panphon"
"xinjli/allosaurus" -> "dmort27/epitran"
"xinjli/allosaurus" -> "festvox/datasets-CMU_Wilderness"
"xinjli/allosaurus" -> "bootphon/phonemizer"
"xinjli/allosaurus" -> "CUNY-CL/wikipron"
"xinjli/allosaurus" -> "lingjzhu/charsiu"
"xinjli/allosaurus" -> "dmort27/allovera"
"xinjli/allosaurus" -> "AdolfVonKleist/Phonetisaurus"
"xinjli/allosaurus" -> "YannickJadoul/Parselmouth"
"xinjli/allosaurus" -> "k2-fsa/k2" ["e"=1]
"xinjli/allosaurus" -> "s3prl/s3prl"
"xinjli/allosaurus" -> "asuni/wavelet_prosody_toolkit"
"xinjli/allosaurus" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"xinjli/allosaurus" -> "Kyubyong/g2p"
"alexisdiaz008/react-mycroft-gui" -> "LinusSkucas/communications-skill"
"alexisdiaz008/react-mycroft-gui" -> "gras64/learning-skill"
"L0SG/WaveFlow" -> "Deepest-Project/AlignTTS"
"L0SG/WaveFlow" -> "tianrengao/SqueezeWave"
"L0SG/WaveFlow" -> "L0SG/NanoFlow"
"descriptinc/melgan-neurips" -> "seungwonpark/melgan"
"descriptinc/melgan-neurips" -> "kan-bayashi/ParallelWaveGAN"
"descriptinc/melgan-neurips" -> "jik876/hifi-gan"
"descriptinc/melgan-neurips" -> "xiph/LPCNet"
"descriptinc/melgan-neurips" -> "NVIDIA/waveglow"
"descriptinc/melgan-neurips" -> "r9y9/wavenet_vocoder"
"descriptinc/melgan-neurips" -> "NVIDIA/mellotron"
"descriptinc/melgan-neurips" -> "auspicious3000/SpeechSplit"
"descriptinc/melgan-neurips" -> "xcmyz/FastSpeech"
"descriptinc/melgan-neurips" -> "auspicious3000/autovc"
"descriptinc/melgan-neurips" -> "aliutkus/speechmetrics" ["e"=1]
"descriptinc/melgan-neurips" -> "soobinseo/Transformer-TTS"
"descriptinc/melgan-neurips" -> "jjery2243542/adaptive_voice_conversion"
"descriptinc/melgan-neurips" -> "yanggeng1995/GAN-TTS"
"descriptinc/melgan-neurips" -> "syang1993/gst-tacotron"
"kan-bayashi/ParallelWaveGAN" -> "descriptinc/melgan-neurips"
"kan-bayashi/ParallelWaveGAN" -> "jik876/hifi-gan"
"kan-bayashi/ParallelWaveGAN" -> "seungwonpark/melgan"
"kan-bayashi/ParallelWaveGAN" -> "xiph/LPCNet"
"kan-bayashi/ParallelWaveGAN" -> "xcmyz/FastSpeech"
"kan-bayashi/ParallelWaveGAN" -> "ming024/FastSpeech2"
"kan-bayashi/ParallelWaveGAN" -> "wenet-e2e/speech-synthesis-paper"
"kan-bayashi/ParallelWaveGAN" -> "r9y9/wavenet_vocoder"
"kan-bayashi/ParallelWaveGAN" -> "jaywalnut310/glow-tts"
"kan-bayashi/ParallelWaveGAN" -> "fatchord/WaveRNN"
"kan-bayashi/ParallelWaveGAN" -> "NVIDIA/waveglow"
"kan-bayashi/ParallelWaveGAN" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"kan-bayashi/ParallelWaveGAN" -> "auspicious3000/autovc"
"kan-bayashi/ParallelWaveGAN" -> "s3prl/s3prl"
"kan-bayashi/ParallelWaveGAN" -> "NVIDIA/BigVGAN" ["e"=1]
"tianrengao/SqueezeWave" -> "janvainer/speedyspeech"
"tianrengao/SqueezeWave" -> "L0SG/WaveFlow"
"tianrengao/SqueezeWave" -> "nii-yamagishilab/multi-speaker-tacotron"
"tianrengao/SqueezeWave" -> "seungwonpark/melgan"
"tianrengao/SqueezeWave" -> "bshall/UniversalVocoding"
"tianrengao/SqueezeWave" -> "alokprasad/LPCTron"
"tianrengao/SqueezeWave" -> "jaywalnut310/glow-tts"
"tianrengao/SqueezeWave" -> "jxzhanggg/nonparaSeq2seqVC_code"
"tianrengao/SqueezeWave" -> "joansj/blow"
"tianrengao/SqueezeWave" -> "ivanvovk/WaveGrad"
"tianrengao/SqueezeWave" -> "NVIDIA/mellotron"
"tianrengao/SqueezeWave" -> "Yablon/auorange"
"tianrengao/SqueezeWave" -> "ksw0306/ClariNet"
"tianrengao/SqueezeWave" -> "LEEYOONHYUNG/BVAE-TTS"
"tianrengao/SqueezeWave" -> "G-Wang/WaveRNN-Pytorch"
"PaddlePaddle/Parakeet" -> "kan-bayashi/ParallelWaveGAN"
"PaddlePaddle/Parakeet" -> "ivanvovk/WaveGrad"
"PaddlePaddle/Parakeet" -> "xcmyz/FastSpeech"
"PaddlePaddle/Parakeet" -> "tianrengao/SqueezeWave"
"PaddlePaddle/Parakeet" -> "L0SG/WaveFlow"
"PaddlePaddle/Parakeet" -> "jaywalnut310/glow-tts"
"PaddlePaddle/Parakeet" -> "xiph/LPCNet"
"PaddlePaddle/Parakeet" -> "janvainer/speedyspeech"
"PaddlePaddle/Parakeet" -> "seungwonpark/melgan"
"PaddlePaddle/Parakeet" -> "nii-yamagishilab/multi-speaker-tacotron"
"PaddlePaddle/Parakeet" -> "syang1993/gst-tacotron"
"PaddlePaddle/Parakeet" -> "soobinseo/Transformer-TTS"
"PaddlePaddle/Parakeet" -> "Tomiinek/Multilingual_Text_to_Speech"
"PaddlePaddle/Parakeet" -> "LEEYOONHYUNG/BVAE-TTS"
"PaddlePaddle/Parakeet" -> "Kyubyong/g2p"
"jaywalnut310/glow-tts" -> "NVIDIA/mellotron"
"jaywalnut310/glow-tts" -> "jik876/hifi-gan"
"jaywalnut310/glow-tts" -> "kan-bayashi/ParallelWaveGAN"
"jaywalnut310/glow-tts" -> "wenet-e2e/speech-synthesis-paper"
"jaywalnut310/glow-tts" -> "seungwonpark/melgan"
"jaywalnut310/glow-tts" -> "xcmyz/FastSpeech"
"jaywalnut310/glow-tts" -> "tts-tutorial/survey"
"jaywalnut310/glow-tts" -> "NVIDIA/flowtron"
"jaywalnut310/glow-tts" -> "thuhcsi/VAENAR-TTS"
"jaywalnut310/glow-tts" -> "LEEYOONHYUNG/BVAE-TTS"
"jaywalnut310/glow-tts" -> "ivanvovk/WaveGrad"
"jaywalnut310/glow-tts" -> "NVIDIA/BigVGAN" ["e"=1]
"jaywalnut310/glow-tts" -> "Tomiinek/Multilingual_Text_to_Speech"
"jaywalnut310/glow-tts" -> "ming024/FastSpeech2"
"jaywalnut310/glow-tts" -> "tianrengao/SqueezeWave"
"ivanvovk/WaveGrad" -> "rishikksh20/VocGAN"
"ivanvovk/WaveGrad" -> "lmnt-com/wavegrad"
"ivanvovk/WaveGrad" -> "lmnt-com/diffwave"
"ivanvovk/WaveGrad" -> "thuhcsi/VAENAR-TTS"
"ivanvovk/WaveGrad" -> "keonlee9420/Parallel-Tacotron2"
"ivanvovk/WaveGrad" -> "jaywalnut310/glow-tts"
"ivanvovk/WaveGrad" -> "Wendison/VQMIVC"
"ivanvovk/WaveGrad" -> "huawei-noah/Speech-Backbones" ["e"=1]
"ivanvovk/WaveGrad" -> "maum-ai/wavegrad2" ["e"=1]
"ivanvovk/WaveGrad" -> "janvainer/speedyspeech"
"ivanvovk/WaveGrad" -> "LEEYOONHYUNG/BVAE-TTS"
"ivanvovk/WaveGrad" -> "zceng/LVCNet"
"ivanvovk/WaveGrad" -> "maum-ai/univnet" ["e"=1]
"ivanvovk/WaveGrad" -> "bshall/UniversalVocoding"
"ivanvovk/WaveGrad" -> "facebookresearch/WavAugment" ["e"=1]
"BogiHsu/WG-WaveNet" -> "grtzsohalf/buy_vs_rent_and_invest"
"sukesh167/Depression-Detection-in-speech" -> "chanjunweimy/FYP_Submission"
"sukesh167/Depression-Detection-in-speech" -> "Akshat2430/Detecting-Depression-From-Speech-Signals"
"sukesh167/Depression-Detection-in-speech" -> "derong97/automatic-depression-detector"
"sukesh167/Depression-Detection-in-speech" -> "adbailey1/daic_woz_process"
"sukesh167/Depression-Detection-in-speech" -> "adbailey1/DepAudioNet_reproduction"
"sukesh167/Depression-Detection-in-speech" -> "ihp-lab/Avec2019_DDS"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "hariharitha21/Detection-of-Anxiety-and-Depression"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "chanjunweimy/FYP_Submission"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "Fancy-Block/ICASSP2022-Depression"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "linlemn/DepressionDectection"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "sahasourav17/Student-Anxiety-and-Depression-Prediction"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "HimangiM/Depression_Detection"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "sukesh167/Depression-Detection-in-speech"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "LouisYZK/dds-avec2019"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "speechandlanguageprocessing/ICASSP2022-Depression"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "cosmaadrian/time-enriched-multimodal-depression-detection"
"notmanan/Depression-Detection-Through-Multi-Modal-Data" -> "AudioVisualEmotionChallenge/AVEC2019"
"coqui-ai/TTS-papers" -> "wenet-e2e/speech-synthesis-paper"
"coqui-ai/TTS-papers" -> "kan-bayashi/ParallelWaveGAN"
"coqui-ai/TTS-papers" -> "coqui-ai/open-speech-corpora"
"coqui-ai/TTS-papers" -> "tts-tutorial/survey"
"coqui-ai/TTS-papers" -> "jaywalnut310/glow-tts"
"coqui-ai/TTS-papers" -> "jik876/hifi-gan"
"coqui-ai/TTS-papers" -> "ming024/FastSpeech2"
"coqui-ai/TTS-papers" -> "gemelo-ai/vocos" ["e"=1]
"coqui-ai/TTS-papers" -> "bootphon/phonemizer"
"coqui-ai/TTS-papers" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"coqui-ai/TTS-papers" -> "huawei-noah/Speech-Backbones" ["e"=1]
"coqui-ai/TTS-papers" -> "rishikksh20/FastSpeech2"
"coqui-ai/TTS-papers" -> "janvainer/speedyspeech"
"coqui-ai/TTS-papers" -> "NVIDIA/flowtron"
"coqui-ai/TTS-papers" -> "maum-ai/cotatron"
"KuangDD/zhrtvc" -> "KuangDD/zhvoice"
"KuangDD/zhrtvc" -> "lturing/tacotronv2_wavernn_chinese"
"KuangDD/zhrtvc" -> "JasonWei512/Tacotron-2-Chinese"
"KuangDD/zhrtvc" -> "aidreamwin/TTS-Clone-Chinese"
"KuangDD/zhrtvc" -> "KuangDD/aukit"
"KuangDD/zhrtvc" -> "jackaduma/CycleGAN-VC2"
"KuangDD/zhrtvc" -> "athena-team/athena" ["e"=1]
"KuangDD/zhrtvc" -> "KuangDD/phkit"
"KuangDD/zhrtvc" -> "xcmyz/FastSpeech"
"KuangDD/zhrtvc" -> "TensorSpeech/TensorFlowTTS"
"KuangDD/zhrtvc" -> "kan-bayashi/ParallelWaveGAN"
"KuangDD/zhrtvc" -> "speechio/chinese_text_normalization" ["e"=1]
"KuangDD/zhrtvc" -> "liusongxiang/StarGAN-Voice-Conversion"
"KuangDD/zhrtvc" -> "fatchord/WaveRNN"
"KuangDD/zhrtvc" -> "syang1993/gst-tacotron"
"thuhcsi/Crystal" -> "npuichigo/voicenet"
"thuhcsi/Crystal" -> "Jackiexiao/MTTS"
"thuhcsi/Crystal" -> "BoragoCode/AttentionBasedProsodyPrediction"
"thuhcsi/Crystal" -> "kakaobrain/g2pm"
"thuhcsi/Crystal" -> "thuhcsi/Crystal.TTVS"
"thuhcsi/Crystal" -> "thuhcsi/FlatTN"
"thuhcsi/Crystal" -> "Kyubyong/g2pC"
"thuhcsi/Crystal" -> "Liu-Feng-deeplearning/TTS-frontend"
"thuhcsi/Crystal" -> "xcmyz/FastVocoder"
"thuhcsi/Crystal" -> "speechio/chinese_text_normalization" ["e"=1]
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "Tomiinek/Multilingual_Text_to_Speech"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "keonlee9420/Parallel-Tacotron2"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "NVIDIA/mellotron"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "rishikksh20/FastSpeech2"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "nii-yamagishilab/multi-speaker-tacotron"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "liusongxiang/ppg-vc"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "Wendison/VQMIVC"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "jjery2243542/adaptive_voice_conversion"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "Sharad24/Neural-Voice-Cloning-with-Few-Samples"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "MTG/WGANSing" ["e"=1]
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "rishikksh20/VocGAN"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "rishikksh20/AdaSpeech"
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" -> "maum-ai/cotatron"
"Alexir/CMUdict" -> "cmusphinx/cmudict"
"NeonGeckoCom/NeonCore" -> "OpenVoiceOS/ovos-core"
"NeonGeckoCom/NeonCore" -> "OpenVoiceOS/ovos-installer"
"seungwonpark/melgan" -> "descriptinc/melgan-neurips"
"seungwonpark/melgan" -> "kan-bayashi/ParallelWaveGAN"
"seungwonpark/melgan" -> "NVIDIA/mellotron"
"seungwonpark/melgan" -> "xcmyz/FastSpeech"
"seungwonpark/melgan" -> "jaywalnut310/glow-tts"
"seungwonpark/melgan" -> "yanggeng1995/GAN-TTS"
"seungwonpark/melgan" -> "tianrengao/SqueezeWave"
"seungwonpark/melgan" -> "xiph/LPCNet"
"seungwonpark/melgan" -> "rishikksh20/VocGAN"
"seungwonpark/melgan" -> "soobinseo/Transformer-TTS"
"seungwonpark/melgan" -> "syang1993/gst-tacotron"
"seungwonpark/melgan" -> "NVIDIA/waveglow"
"seungwonpark/melgan" -> "as-ideas/ForwardTacotron"
"seungwonpark/melgan" -> "wenet-e2e/speech-synthesis-paper"
"seungwonpark/melgan" -> "jjery2243542/adaptive_voice_conversion"
"KuangDD/phkit" -> "KuangDD/aukit"
"X-CCS/mandarin_tacotron_GL" -> "npujcong/Chinese_PSP"
"cmusphinx/cmudict" -> "cmusphinx/g2p-seq2seq" ["e"=1]
"cmusphinx/cmudict" -> "AdolfVonKleist/Phonetisaurus"
"cmusphinx/cmudict" -> "Kyubyong/g2p"
"cmusphinx/cmudict" -> "cmusphinx/cmudict-tools" ["e"=1]
"cmusphinx/cmudict" -> "Alexir/CMUdict"
"cmusphinx/cmudict" -> "prosegrinder/python-cmudict"
"cmusphinx/cmudict" -> "open-dict-data/ipa-dict"
"cmusphinx/cmudict" -> "cmusphinx/sphinxbase" ["e"=1]
"cmusphinx/cmudict" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"cmusphinx/cmudict" -> "Kyubyong/pron_dictionaries"
"cmusphinx/cmudict" -> "aparrish/pronouncingpy"
"cmusphinx/cmudict" -> "sequitur-g2p/sequitur-g2p"
"cmusphinx/cmudict" -> "mphilli/English-to-IPA"
"cmusphinx/cmudict" -> "Jackiexiao/MTTS"
"cmusphinx/cmudict" -> "kakaobrain/g2pm"
"k2kobayashi/crank" -> "jxzhanggg/nonparaSeq2seqVC_code"
"k2kobayashi/crank" -> "bigpon/vcc20_baseline_cyclevae"
"k2kobayashi/crank" -> "patrickltobing/cyclevae-vc-neuralvoco"
"k2kobayashi/crank" -> "ericwudayi/SkipVQVC"
"k2kobayashi/crank" -> "yistLin/FragmentVC"
"k2kobayashi/crank" -> "bshall/VectorQuantizedCPC"
"k2kobayashi/crank" -> "Wendison/VQMIVC"
"cifkao/groove2groove" -> "YatingMusic/MuseMorphose" ["e"=1]
"cifkao/groove2groove" -> "cifkao/ss-vq-vae"
"cifkao/groove2groove" -> "sumuzhao/CycleGAN-Music-Style-Transfer"
"cifkao/groove2groove" -> "sumuzhao/CycleGAN-Music-Style-Transfer-Refactorization"
"janvainer/speedyspeech" -> "tianrengao/SqueezeWave"
"janvainer/speedyspeech" -> "ivanvovk/WaveGrad"
"janvainer/speedyspeech" -> "rishikksh20/AdaSpeech"
"janvainer/speedyspeech" -> "LEEYOONHYUNG/BVAE-TTS"
"janvainer/speedyspeech" -> "keonlee9420/StyleSpeech" ["e"=1]
"janvainer/speedyspeech" -> "maum-ai/cotatron"
"janvainer/speedyspeech" -> "rishikksh20/VocGAN"
"janvainer/speedyspeech" -> "lmnt-com/wavegrad"
"janvainer/speedyspeech" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"janvainer/speedyspeech" -> "jaywalnut310/glow-tts"
"janvainer/speedyspeech" -> "nii-yamagishilab/multi-speaker-tacotron"
"janvainer/speedyspeech" -> "descriptinc/cargan" ["e"=1]
"janvainer/speedyspeech" -> "rendchevi/nix-tts" ["e"=1]
"google/REAPER" -> "r9y9/pyreaper"
"google/REAPER" -> "dgaspari/pyrapt"
"google/REAPER" -> "mmorise/World"
"google/REAPER" -> "YannickJadoul/Parselmouth"
"google/REAPER" -> "r9y9/pysptk"
"google/REAPER" -> "CSTR-Edinburgh/merlin"
"google/REAPER" -> "HidekiKawahara/legacy_STRAIGHT"
"google/REAPER" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"google/REAPER" -> "google/sparrowhawk"
"google/REAPER" -> "xiph/LPCNet"
"google/REAPER" -> "ronggong/pypYIN"
"google/REAPER" -> "CSTR-Edinburgh/magphase"
"google/REAPER" -> "SJTMusicTeam/Muskits" ["e"=1]
"google/REAPER" -> "sp-nitech/SPTK" ["e"=1]
"google/REAPER" -> "r9y9/SPTK"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "nsu-ai/russian_g2p" ["e"=1]
"vlomme/Multi-Tacotron-Voice-Cloning" -> "SforAiDl/Neural-Voice-Cloning-With-Few-Samples"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "snakers4/open_stt" ["e"=1]
"vlomme/Multi-Tacotron-Voice-Cloning" -> "Tomiinek/Multilingual_Text_to_Speech"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "Sharad24/Neural-Voice-Cloning-with-Few-Samples"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "NVIDIA/flowtron"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "nii-yamagishilab/multi-speaker-tacotron"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "rhasspy/gruut"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "keonlee9420/Parallel-Tacotron2"
"vlomme/Multi-Tacotron-Voice-Cloning" -> "sovaai/sova-tts" ["e"=1]
"resemble-ai/MelNet" -> "Deepest-Project/MelNet"
"resemble-ai/MelNet" -> "geneing/WaveRNN-Pytorch"
"resemble-ai/MelNet" -> "bshall/UniversalVocoding"
"resemble-ai/MelNet" -> "keonlee9420/StyleSpeech" ["e"=1]
"IMLHF/Real-Time-Voice-Cloning" -> "IEEE-NITK/Neural-Voice-Cloning"
"swcwang/depression-detection" -> "rahulsharma-rks/DepressionDetection"
"swcwang/depression-detection" -> "viritaromero/Detecting-Depression-in-Tweets"
"jackaduma/CycleGAN-VC2" -> "jackaduma/CycleGAN-VC3"
"jackaduma/CycleGAN-VC2" -> "GANtastic3/MaskCycleGAN-VC"
"jackaduma/CycleGAN-VC2" -> "liusongxiang/StarGAN-Voice-Conversion"
"jackaduma/CycleGAN-VC2" -> "auspicious3000/autovc"
"jackaduma/CycleGAN-VC2" -> "leimao/Voice-Converter-CycleGAN"
"jackaduma/CycleGAN-VC2" -> "jjery2243542/adaptive_voice_conversion"
"jackaduma/CycleGAN-VC2" -> "KuangDD/zhrtvc"
"jackaduma/CycleGAN-VC2" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"jackaduma/CycleGAN-VC2" -> "lturing/tacotronv2_wavernn_chinese"
"jackaduma/CycleGAN-VC2" -> "yl4579/StarGANv2-VC"
"jackaduma/CycleGAN-VC2" -> "jxzhanggg/nonparaSeq2seqVC_code"
"jackaduma/CycleGAN-VC2" -> "auspicious3000/SpeechSplit"
"jackaduma/CycleGAN-VC2" -> "hujinsen/pytorch-StarGAN-VC"
"jackaduma/CycleGAN-VC2" -> "liusongxiang/ppg-vc"
"jackaduma/CycleGAN-VC2" -> "bshall/ZeroSpeech"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/controllable_evc_code"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/seq2seq-EVC"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "KunZhou9646/Emovox"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "CZ26/CycleTransGAN-EVC"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "ktho22/vctts"
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" -> "jxzhanggg/nonparaSeq2seqVC_code"
"lturing/tacotronv2_wavernn_chinese" -> "KuangDD/zhrtvc"
"lturing/tacotronv2_wavernn_chinese" -> "JasonWei512/Tacotron-2-Chinese"
"lturing/tacotronv2_wavernn_chinese" -> "ranchlai/mandarin-tts"
"lturing/tacotronv2_wavernn_chinese" -> "atomicoo/tacotron2-mandarin"
"lturing/tacotronv2_wavernn_chinese" -> "KuangDD/zhvoice"
"lturing/tacotronv2_wavernn_chinese" -> "syang1993/gst-tacotron"
"lturing/tacotronv2_wavernn_chinese" -> "thuhcsi/Crystal"
"lturing/tacotronv2_wavernn_chinese" -> "Jackiexiao/MTTS"
"lturing/tacotronv2_wavernn_chinese" -> "atomicoo/FCH-TTS"
"lturing/tacotronv2_wavernn_chinese" -> "xcmyz/FastSpeech"
"lturing/tacotronv2_wavernn_chinese" -> "speechio/chinese_text_normalization" ["e"=1]
"lturing/tacotronv2_wavernn_chinese" -> "kakaobrain/g2pm"
"lturing/tacotronv2_wavernn_chinese" -> "ivanvovk/durian-pytorch"
"lturing/tacotronv2_wavernn_chinese" -> "jaywalnut310/glow-tts"
"lturing/tacotronv2_wavernn_chinese" -> "jackaduma/CycleGAN-VC2"
"Voice-Lab/VoiceLab" -> "drfeinberg/PraatScripts"
"Voice-Lab/VoiceLab" -> "kirbyj/praatsauce"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "KunZhou9646/Emovox"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "KunZhou9646/seq2seq-EVC"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "CZ26/CycleTransGAN-EVC"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "glam-imperial/EmotionalConversionStarGAN"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "KunZhou9646/controllable_evc_code"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "ktho22/vctts"
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" -> "jxzhanggg/nonparaSeq2seqVC_code"
"bshall/VectorQuantizedCPC" -> "bshall/ZeroSpeech"
"bshall/VectorQuantizedCPC" -> "Wendison/VQMIVC"
"bshall/VectorQuantizedCPC" -> "nii-yamagishilab/Extended_VQVAE" ["e"=1]
"bshall/VectorQuantizedCPC" -> "k2kobayashi/crank"
"bshall/VectorQuantizedCPC" -> "LEEYOONHYUNG/BVAE-TTS"
"adbailey1/daic_woz_process" -> "adbailey1/DepAudioNet_reproduction"
"adbailey1/daic_woz_process" -> "kingformatty/NUSD"
"adbailey1/daic_woz_process" -> "PingCheng-Wei/DepressionEstimation"
"adbailey1/daic_woz_process" -> "linlemn/DepressionDectection"
"adbailey1/daic_woz_process" -> "speechandlanguageprocessing/ICASSP2022-Depression"
"adbailey1/daic_woz_process" -> "derong97/automatic-depression-detector"
"adbailey1/daic_woz_process" -> "HimangiM/Depression_Detection"
"ide8/tacotron2" -> "jinhan/tacotron2-vae"
"ide8/tacotron2" -> "noetits/ICE-Talk"
"ide8/tacotron2" -> "KinglittleQ/GST-Tacotron"
"yanggeng1995/GAN-TTS" -> "thuhcsi/VAENAR-TTS"
"yanggeng1995/GAN-TTS" -> "seungwonpark/melgan"
"yanggeng1995/GAN-TTS" -> "rishikksh20/VocGAN"
"yanggeng1995/GAN-TTS" -> "yanggeng1995/EATS"
"yanggeng1995/GAN-TTS" -> "mbinkowski/DeepSpeechDistances"
"yanggeng1995/GAN-TTS" -> "rishikksh20/vae_tacotron2"
"yanggeng1995/GAN-TTS" -> "ivanvovk/durian-pytorch"
"yanggeng1995/GAN-TTS" -> "ivanvovk/WaveGrad"
"yanggeng1995/GAN-TTS" -> "descriptinc/melgan-neurips"
"yanggeng1995/GAN-TTS" -> "soobinseo/Transformer-TTS"
"yanggeng1995/GAN-TTS" -> "bshall/UniversalVocoding"
"yanggeng1995/GAN-TTS" -> "r9y9/gantts"
"yanggeng1995/GAN-TTS" -> "jaywalnut310/glow-tts"
"yanggeng1995/GAN-TTS" -> "kan-bayashi/ParallelWaveGAN"
"mmorise/kiritan_singing" -> "mmorise/itako_singing"
"as-ideas/ForwardTacotron" -> "xcmyz/FastSpeech"
"as-ideas/ForwardTacotron" -> "jaywalnut310/glow-tts"
"as-ideas/ForwardTacotron" -> "seungwonpark/melgan"
"as-ideas/ForwardTacotron" -> "spring-media/TransformerTTS"
"as-ideas/ForwardTacotron" -> "maum-ai/cotatron"
"as-ideas/ForwardTacotron" -> "NVIDIA/mellotron"
"as-ideas/ForwardTacotron" -> "tianrengao/SqueezeWave"
"as-ideas/ForwardTacotron" -> "NVIDIA/flowtron"
"as-ideas/ForwardTacotron" -> "Tomiinek/Multilingual_Text_to_Speech"
"as-ideas/ForwardTacotron" -> "syang1993/gst-tacotron"
"as-ideas/ForwardTacotron" -> "kan-bayashi/ParallelWaveGAN"
"as-ideas/ForwardTacotron" -> "ivanvovk/WaveGrad"
"as-ideas/ForwardTacotron" -> "wenet-e2e/speech-synthesis-paper"
"as-ideas/ForwardTacotron" -> "rishikksh20/VocGAN"
"as-ideas/ForwardTacotron" -> "nii-yamagishilab/multi-speaker-tacotron"
"yistLin/universal-vocoder" -> "yistLin/FragmentVC"
"yistLin/universal-vocoder" -> "yistLin/human-evaluation"
"yistLin/universal-vocoder" -> "grtzsohalf/buy_vs_rent_and_invest"
"yistLin/universal-vocoder" -> "howard1337/S2VC"
"yistLin/universal-vocoder" -> "cyhuang-tw/AutoVC"
"mbinkowski/DeepSpeechDistances" -> "yanggeng1995/GAN-TTS"
"r4victor/afaligner" -> "r4victor/syncabook"
"r4victor/afaligner" -> "r4victor/synclibrivox"
"foamliu/Tacotron2-Mandarin" -> "atomicoo/tacotron2-mandarin"
"foamliu/Tacotron2-Mandarin" -> "JasonWei512/Tacotron-2-Chinese"
"foamliu/Tacotron2-Mandarin" -> "JasonWei512/wavenet_vocoder"
"TaiChunYen/Pytorch-CycleGAN-VC2" -> "onejiin/CycleGAN-VC2"
"TaiChunYen/Pytorch-CycleGAN-VC2" -> "ariacat3366/pytorch-StarGAN-VC2-implementation"
"TaiChunYen/Pytorch-CycleGAN-VC2" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"aidreamwin/TTS-Clone-Chinese" -> "KuangDD/zhrtvc"
"aidreamwin/TTS-Clone-Chinese" -> "JasonWei512/Tacotron-2-Chinese"
"aidreamwin/TTS-Clone-Chinese" -> "MachineLP/TensorFlowTTS_chinese"
"aidreamwin/TTS-Clone-Chinese" -> "jackaduma/CycleGAN-VC2"
"aidreamwin/TTS-Clone-Chinese" -> "ranchlai/mandarin-tts"
"aidreamwin/TTS-Clone-Chinese" -> "Jackiexiao/zhtts"
"wqt2019/tacotron-2_melgan" -> "wqt2019/tacotron-2_wavernn"
"aidenwang9867/Weibo-User-Depression-Detection-Dataset" -> "cyZhu98/Depression_Baseline"
"aidenwang9867/Weibo-User-Depression-Detection-Dataset" -> "ethan-nicholas-tsai/SWDD"
"aidenwang9867/Weibo-User-Depression-Detection-Dataset" -> "speechandlanguageprocessing/ICASSP2022-Depression"
"aidenwang9867/Weibo-User-Depression-Detection-Dataset" -> "cosmaadrian/time-enriched-multimodal-depression-detection"
"aidenwang9867/Weibo-User-Depression-Detection-Dataset" -> "sunlightsgy/MDDL"
"nii-yamagishilab/multi-speaker-tacotron" -> "maum-ai/cotatron"
"nii-yamagishilab/multi-speaker-tacotron" -> "jinhan/tacotron2-vae"
"nii-yamagishilab/multi-speaker-tacotron" -> "jefflai108/pytorch-kaldi-neural-speaker-embeddings" ["e"=1]
"nii-yamagishilab/multi-speaker-tacotron" -> "Tomiinek/Multilingual_Text_to_Speech"
"nii-yamagishilab/multi-speaker-tacotron" -> "rishikksh20/vae_tacotron2"
"nii-yamagishilab/multi-speaker-tacotron" -> "NVIDIA/mellotron"
"nii-yamagishilab/multi-speaker-tacotron" -> "CODEJIN/multi_speaker_tts"
"nii-yamagishilab/multi-speaker-tacotron" -> "tianrengao/SqueezeWave"
"nii-yamagishilab/multi-speaker-tacotron" -> "keonlee9420/Parallel-Tacotron2"
"nii-yamagishilab/multi-speaker-tacotron" -> "syang1993/gst-tacotron"
"nii-yamagishilab/multi-speaker-tacotron" -> "dipjyoti92/SC-WaveRNN"
"nii-yamagishilab/multi-speaker-tacotron" -> "mkotha/WaveRNN"
"nii-yamagishilab/multi-speaker-tacotron" -> "keonlee9420/StyleSpeech" ["e"=1]
"nii-yamagishilab/multi-speaker-tacotron" -> "k2kobayashi/crank"
"nii-yamagishilab/multi-speaker-tacotron" -> "KinglittleQ/GST-Tacotron"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "hujinsen/pytorch-StarGAN-VC"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "bigpon/vcc20_baseline_cyclevae"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "TaiChunYen/Pytorch-CycleGAN-VC2"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "liusongxiang/StarGAN-Voice-Conversion"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "Oscarshu0719/pytorch-StarGAN-VC2"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "SamuelBroughton/StarGAN-Voice-Conversion"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "k2kobayashi/crank"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "MingjieChen/LowResourceVC"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "dipjyoti92/StarGAN-Voice-Conversion-2"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "bshall/ZeroSpeech"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "jackaduma/CycleGAN-VC3"
"SamuelBroughton/StarGAN-Voice-Conversion-2" -> "marcoppasini/MelGAN-VC"
"joongbo/tta" -> "hwanheelee1993/MFMA"
"joongbo/tta" -> "YunahJang/IterCQR"
"joongbo/tta" -> "klee972/exec-filter"
"joongbo/tta" -> "DongryeolLee96/AskCQ"
"joongbo/tta" -> "hwanheelee1993/UMIC"
"joongbo/tta" -> "hwanheelee1993/KPQA"
"joongbo/tta" -> "minbeomkim/CriticControl"
"joongbo/tta" -> "hyukhunkoh-ai/multi_view_zero_shot_open_intent_induction"
"joongbo/tta" -> "LEEYOONHYUNG/BVAE-TTS"
"joongbo/tta" -> "jiwoohong93/ai_dep" ["e"=1]
"glam-imperial/EmotionalConversionStarGAN" -> "KunZhou9646/Emovox"
"glam-imperial/EmotionalConversionStarGAN" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"glam-imperial/EmotionalConversionStarGAN" -> "KunZhou9646/seq2seq-EVC"
"glam-imperial/EmotionalConversionStarGAN" -> "KunZhou9646/controllable_evc_code"
"glam-imperial/EmotionalConversionStarGAN" -> "CZ26/CycleTransGAN-EVC"
"glam-imperial/EmotionalConversionStarGAN" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"glam-imperial/EmotionalConversionStarGAN" -> "ktho22/vctts"
"ktho22/vctts" -> "bottlecapper/EmoCycleGAN"
"bigpon/vcc20_baseline_cyclevae" -> "k2kobayashi/crank"
"bigpon/vcc20_baseline_cyclevae" -> "tarepan/VoiceConversionLab"
"bigpon/vcc20_baseline_cyclevae" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"bigpon/vcc20_baseline_cyclevae" -> "jxzhanggg/nonparaSeq2seqVC_code"
"bigpon/vcc20_baseline_cyclevae" -> "patrickltobing/cyclevae-vc"
"bigpon/vcc20_baseline_cyclevae" -> "bshall/ZeroSpeech"
"xcmyz/FastSpeech2" -> "LingLing-Speech/mandarin-tts-frontend-python"
"KuangDD/zhvoice" -> "KuangDD/zhrtvc"
"KuangDD/zhvoice" -> "KuangDD/phkit"
"KuangDD/zhvoice" -> "speechio/chinese_text_normalization" ["e"=1]
"KuangDD/zhvoice" -> "lturing/tacotronv2_wavernn_chinese"
"KuangDD/zhvoice" -> "KuangDD/aukit"
"KuangDD/zhvoice" -> "maum-ai/cotatron"
"KuangDD/zhvoice" -> "open-speech/speech-aligner"
"KuangDD/zhvoice" -> "kakaobrain/g2pm"
"KuangDD/zhvoice" -> "Helsinki-NLP/prosody"
"KuangDD/zhvoice" -> "thuhcsi/Crystal"
"KuangDD/zhvoice" -> "JasonWei512/Tacotron-2-Chinese"
"KuangDD/zhvoice" -> "cywang97/StreamingTransformer" ["e"=1]
"KuangDD/zhvoice" -> "speechio/BigCiDian" ["e"=1]
"KuangDD/zhvoice" -> "Kyubyong/g2pC"
"KuangDD/zhvoice" -> "xcmyz/FastSpeech"
"hhguo/EA-SVC" -> "SongRongLee/mir-svc"
"hhguo/EA-SVC" -> "liusongxiang/ppg-vc"
"Liu-Feng-deeplearning/TTS-frontend" -> "BoragoCode/AttentionBasedProsodyPrediction"
"dmort27/allovera" -> "xinjli/ucla-phonetic-corpus"
"dmort27/allovera" -> "xinjli/phonepiece"
"lmnt-com/wavegrad" -> "ivanvovk/WaveGrad"
"lmnt-com/wavegrad" -> "lmnt-com/diffwave"
"lmnt-com/wavegrad" -> "nii-yamagishilab/multi-speaker-tacotron"
"lmnt-com/wavegrad" -> "rishikksh20/VocGAN"
"lmnt-com/wavegrad" -> "janvainer/speedyspeech"
"lmnt-com/wavegrad" -> "yerfor/SyntaSpeech" ["e"=1]
"lmnt-com/wavegrad" -> "yanggeng1995/FB-MelGAN"
"santiagobarreda/FastTrack" -> "ChristopherCarignan/formant-optimization"
"ivanvovk/durian-pytorch" -> "Deepest-Project/AlignTTS"
"ivanvovk/durian-pytorch" -> "yanggeng1995/Multi-band-WaveRNN"
"ivanvovk/durian-pytorch" -> "entn-at/DurIAN-1"
"ivanvovk/durian-pytorch" -> "thuhcsi/VAENAR-TTS"
"ivanvovk/durian-pytorch" -> "liusongxiang/efficient_tts"
"Yablon/auorange" -> "zzw922cn/LPC_for_TTS"
"ericwudayi/SkipVQVC" -> "KimythAnly/AGAIN-VC"
"ericwudayi/SkipVQVC" -> "vsimkus/vae-voice-conversion"
"ericwudayi/SkipVQVC" -> "k2kobayashi/crank"
"ericwudayi/SkipVQVC" -> "joansj/blow"
"stylerw/styler_praat_scripts" -> "lennes/spect"
"stylerw/styler_praat_scripts" -> "stylerw/usingpraat"
"stylerw/styler_praat_scripts" -> "santiagobarreda/FastTrack"
"stylerw/styler_praat_scripts" -> "mlml/autovot"
"DongyaoZhu/VQ-VAE-WaveNet" -> "hrbigelow/ae-wavenet"
"ZDisket/TensorVox" -> "tulasiram58827/TTS_TFLite"
"Deepest-Project/MelNet" -> "resemble-ai/MelNet"
"Deepest-Project/MelNet" -> "seungwonpark/melgan"
"Deepest-Project/MelNet" -> "YuvalBecker/MelNet"
"Deepest-Project/MelNet" -> "jaeyeun97/MelNet"
"Deepest-Project/MelNet" -> "maum-ai/cotatron"
"Deepest-Project/MelNet" -> "thuhcsi/VAENAR-TTS"
"ttslr/python-MCD" -> "SamuelBroughton/Mel-Cepstral-Distortion"
"SamuelBroughton/Mel-Cepstral-Distortion" -> "MattShannon/mcd"
"SamuelBroughton/Mel-Cepstral-Distortion" -> "SamuelBroughton/StarGAN-Voice-Conversion"
"SamuelBroughton/Mel-Cepstral-Distortion" -> "ttslr/python-MCD"
"HelloChatterbox/wikipedia_for_humans" -> "pcwii/mesh-skill"
"Deepest-Project/AlignTTS" -> "LEEYOONHYUNG/BVAE-TTS"
"german-asr/megs" -> "german-asr/kaldi-german"
"yanggeng1995/EATS" -> "yanggeng1995/FB-MelGAN"
"yanggeng1995/EATS" -> "liusongxiang/efficient_tts"
"yanggeng1995/EATS" -> "keonlee9420/StyleSpeech" ["e"=1]
"rishikksh20/FastSpeech2" -> "rishikksh20/VocGAN"
"rishikksh20/FastSpeech2" -> "ivanvovk/durian-pytorch"
"rishikksh20/FastSpeech2" -> "ga642381/FastSpeech2"
"rishikksh20/FastSpeech2" -> "keonlee9420/Expressive-FastSpeech2" ["e"=1]
"rishikksh20/FastSpeech2" -> "deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning"
"rishikksh20/FastSpeech2" -> "rishikksh20/TFGAN"
"rishikksh20/FastSpeech2" -> "rishikksh20/AdaSpeech"
"bmilde/german-asr-lm-tools" -> "german-asr/kaldi-german"
"Riroaki/Chinese-Rhythm-Predictor" -> "npujcong/Chinese_PSP"
"linlemn/DepressionDectection" -> "derong97/automatic-depression-detector"
"npuichigo/grpc_gateway_demo" -> "npuichigo/ttsflow"
"npuichigo/grpc_gateway_demo" -> "npuichigo/voicenet"
"timmahrt/pysle" -> "Madoshakalaka/English-IPA"
"begeekmyfriend/tacotron2" -> "begeekmyfriend/WaveRNN"
"begeekmyfriend/tacotron2" -> "begeekmyfriend/Tacotron-2"
"LingLing-Speech/mandarin-tts-frontend-python" -> "xcmyz/FastSpeech2"
"pohanchi/huggingface_albert" -> "grtzsohalf/buy_vs_rent_and_invest"
"Zeqiang-Lai/Prosody_Prediction" -> "kichanll/Chinese_Prosody_Predict"
"Zeqiang-Lai/Prosody_Prediction" -> "npujcong/Chinese_PSP"
"Zeqiang-Lai/Prosody_Prediction" -> "BoragoCode/AttentionBasedProsodyPrediction"
"wqt2019/tacotron-2_wavernn" -> "wqt2019/tacotron-2_melgan"
"yistLin/human-evaluation" -> "yistLin/universal-vocoder"
"IPS-LMU/emuR" -> "IPS-LMU/wrassp"
"pcwii/mesh-skill" -> "ChanceNCounter/awesome-mycroft-community"
"jik876/hifi-gan" -> "kan-bayashi/ParallelWaveGAN"
"jik876/hifi-gan" -> "ming024/FastSpeech2"
"jik876/hifi-gan" -> "NVIDIA/BigVGAN" ["e"=1]
"jik876/hifi-gan" -> "jaywalnut310/glow-tts"
"jik876/hifi-gan" -> "descriptinc/melgan-neurips"
"jik876/hifi-gan" -> "descriptinc/descript-audio-codec" ["e"=1]
"jik876/hifi-gan" -> "wenet-e2e/speech-synthesis-paper"
"jik876/hifi-gan" -> "gemelo-ai/vocos" ["e"=1]
"jik876/hifi-gan" -> "facebookresearch/encodec" ["e"=1]
"jik876/hifi-gan" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"jik876/hifi-gan" -> "xiph/LPCNet"
"jik876/hifi-gan" -> "lifeiteng/vall-e" ["e"=1]
"jik876/hifi-gan" -> "yangdongchao/AcademiCodec" ["e"=1]
"jik876/hifi-gan" -> "s3prl/s3prl"
"jik876/hifi-gan" -> "lucidrains/audiolm-pytorch" ["e"=1]
"readbeyond/aeneas" -> "pettarin/forced-alignment-tools"
"readbeyond/aeneas" -> "strob/gentle"
"readbeyond/aeneas" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"readbeyond/aeneas" -> "r4victor/syncabook"
"readbeyond/aeneas" -> "prosodylab/Prosodylab-Aligner"
"readbeyond/aeneas" -> "sillsdev/aeneas-installer"
"readbeyond/aeneas" -> "bootphon/phonemizer"
"readbeyond/aeneas" -> "mozilla/DSAlign"
"readbeyond/aeneas" -> "wiseman/py-webrtcvad" ["e"=1]
"readbeyond/aeneas" -> "NVIDIA/waveglow"
"readbeyond/aeneas" -> "r9y9/wavenet_vocoder"
"readbeyond/aeneas" -> "r4victor/afaligner"
"readbeyond/aeneas" -> "Rayhane-mamah/Tacotron-2"
"readbeyond/aeneas" -> "fatchord/WaveRNN"
"readbeyond/aeneas" -> "buriburisuri/speech-to-text-wavenet"
"JeffC0628/awesome-voice-conversion" -> "hayeong0/Diff-HierVC" ["e"=1]
"JeffC0628/awesome-voice-conversion" -> "cyhuang-tw/AdaIN-VC"
"tulasiram58827/TTS_TFLite" -> "xcmyz/FastVocoder"
"ebadawy/voice_conversion" -> "RussellSB/tt-vae-gan"
"ebadawy/voice_conversion" -> "BrightGu/SingleVC"
"marytts/marytts" -> "synesthesiam/opentts" ["e"=1]
"marytts/marytts" -> "espeak-ng/espeak-ng" ["e"=1]
"marytts/marytts" -> "CSTR-Edinburgh/merlin"
"marytts/marytts" -> "MycroftAI/mimic1"
"marytts/marytts" -> "mozilla/TTS"
"marytts/marytts" -> "cmusphinx/sphinx4" ["e"=1]
"marytts/marytts" -> "AndroidMaryTTS/AndroidMaryTTS"
"marytts/marytts" -> "rhdunn/espeak"
"marytts/marytts" -> "festvox/festival"
"marytts/marytts" -> "keithito/tacotron"
"marytts/marytts" -> "Kyubyong/tacotron"
"marytts/marytts" -> "Rayhane-mamah/Tacotron-2"
"marytts/marytts" -> "marytts/marytts-txt2wav"
"marytts/marytts" -> "google/voice-builder"
"marytts/marytts" -> "MycroftAI/mimic2"
"yl4579/StarGANv2-VC" -> "maum-ai/assem-vc" ["e"=1]
"yl4579/StarGANv2-VC" -> "Wendison/VQMIVC"
"yl4579/StarGANv2-VC" -> "liusongxiang/ppg-vc"
"yl4579/StarGANv2-VC" -> "OlaWod/FreeVC" ["e"=1]
"yl4579/StarGANv2-VC" -> "jjery2243542/adaptive_voice_conversion"
"yl4579/StarGANv2-VC" -> "yl4579/PitchExtractor"
"yl4579/StarGANv2-VC" -> "yl4579/StyleTTS-VC"
"yl4579/StarGANv2-VC" -> "wenet-e2e/speech-synthesis-paper"
"yl4579/StarGANv2-VC" -> "bshall/knn-vc" ["e"=1]
"yl4579/StarGANv2-VC" -> "bshall/soft-vc" ["e"=1]
"yl4579/StarGANv2-VC" -> "liusongxiang/StarGAN-Voice-Conversion"
"yl4579/StarGANv2-VC" -> "Edresson/YourTTS" ["e"=1]
"yl4579/StarGANv2-VC" -> "yl4579/AuxiliaryASR"
"yl4579/StarGANv2-VC" -> "auspicious3000/autovc"
"yl4579/StarGANv2-VC" -> "hhguo/EA-SVC"
"ranchlai/mandarin-tts" -> "lturing/tacotronv2_wavernn_chinese"
"ranchlai/mandarin-tts" -> "atomicoo/FCH-TTS"
"ranchlai/mandarin-tts" -> "Z-yq/TensorflowTTS"
"ranchlai/mandarin-tts" -> "atomicoo/tacotron2-mandarin"
"ranchlai/mandarin-tts" -> "kuangdd/ttskit"
"ranchlai/mandarin-tts" -> "ming024/FastSpeech2"
"ranchlai/mandarin-tts" -> "JasonWei512/Tacotron-2-Chinese"
"ranchlai/mandarin-tts" -> "Jackiexiao/zhtts"
"ranchlai/mandarin-tts" -> "PlayVoice/vits_chinese" ["e"=1]
"ranchlai/mandarin-tts" -> "foamliu/Tacotron2-Mandarin"
"ranchlai/mandarin-tts" -> "X-CCS/mandarin_tacotron_GL"
"ranchlai/mandarin-tts" -> "aidreamwin/TTS-Clone-Chinese"
"ranchlai/mandarin-tts" -> "Jackiexiao/MTTS"
"ranchlai/mandarin-tts" -> "cnlinxi/book-text-to-speech" ["e"=1]
"ranchlai/mandarin-tts" -> "thuhcsi/Crystal"
"OpenVoiceOS/ovos-core" -> "NeonGeckoCom/NeonCore"
"OpenVoiceOS/ovos-core" -> "OpenVoiceOS/ovos-installer"
"OpenVoiceOS/ovos-core" -> "OpenVoiceOS/ovos-docker"
"OpenVoiceOS/ovos-core" -> "OpenVoiceOS/ovos-buildroot"
"LEEYOONHYUNG/BVAE-TTS" -> "thuhcsi/VAENAR-TTS"
"LEEYOONHYUNG/BVAE-TTS" -> "YunahJang/IterCQR"
"LEEYOONHYUNG/BVAE-TTS" -> "hwanheelee1993/MFMA"
"LEEYOONHYUNG/BVAE-TTS" -> "klee972/exec-filter"
"LEEYOONHYUNG/BVAE-TTS" -> "Deepest-Project/AlignTTS"
"LEEYOONHYUNG/BVAE-TTS" -> "DongryeolLee96/AskCQ"
"LEEYOONHYUNG/BVAE-TTS" -> "hwanheelee1993/UMIC"
"thuhcsi/VAENAR-TTS" -> "keonlee9420/VAENAR-TTS" ["e"=1]
"thuhcsi/VAENAR-TTS" -> "LEEYOONHYUNG/BVAE-TTS"
"thuhcsi/VAENAR-TTS" -> "LeoniusChen/Attentions-in-Tacotron"
"thuhcsi/VAENAR-TTS" -> "keonlee9420/StyleSpeech" ["e"=1]
"thuhcsi/VAENAR-TTS" -> "zceng/LVCNet"
"keonlee9420/WaveGrad2" -> "rishikksh20/UnivNet-pytorch"
"audeering/opensmile" -> "audeering/opensmile-python"
"audeering/opensmile" -> "covarep/covarep"
"audeering/opensmile" -> "naxingyu/opensmile"
"audeering/opensmile" -> "auDeep/auDeep"
"audeering/opensmile" -> "zlzhang1124/AcousticFeatureExtraction" ["e"=1]
"audeering/opensmile" -> "declare-lab/MELD" ["e"=1]
"audeering/opensmile" -> "ddlBoJack/emotion2vec" ["e"=1]
"audeering/opensmile" -> "jcvasquezc/DisVoice"
"audeering/opensmile" -> "DeepSpectrum/DeepSpectrum"
"audeering/opensmile" -> "CheyneyComputerScience/CREMA-D" ["e"=1]
"audeering/opensmile" -> "YannickJadoul/Parselmouth"
"audeering/opensmile" -> "iver56/audiomentations" ["e"=1]
"audeering/opensmile" -> "aliutkus/speechmetrics" ["e"=1]
"audeering/opensmile" -> "Demfier/multimodal-speech-emotion-recognition" ["e"=1]
"audeering/opensmile" -> "openXBOW/openXBOW"
"aparrish/pronouncingpy" -> "mphilli/English-to-IPA"
"aparrish/pronouncingpy" -> "prosegrinder/python-cmudict"
"aparrish/pronouncingpy" -> "aparrish/pycorpora" ["e"=1]
"aparrish/pronouncingpy" -> "aparrish/rwet" ["e"=1]
"aparrish/pronouncingpy" -> "hyperreality/Poetry-Tools"
"aparrish/pronouncingpy" -> "aparrish/pronouncingjs"
"aparrish/pronouncingpy" -> "cmusphinx/cmudict"
"sidmulajkar/sentiment-predictor-for-stress-detection" -> "hariharitha21/Detection-of-Anxiety-and-Depression"
"Akshat2430/Detecting-Depression-From-Speech-Signals" -> "AmirHoseein99/Depression-Engine"
"Akshat2430/Detecting-Depression-From-Speech-Signals" -> "chanjunweimy/FYP_Submission"
"hariharitha21/Detection-of-Anxiety-and-Depression" -> "Akshat2430/Detecting-Depression-From-Speech-Signals"
"hariharitha21/Detection-of-Anxiety-and-Depression" -> "notmanan/Depression-Detection-Through-Multi-Modal-Data"
"rhasspy/gruut" -> "spring-media/DeepPhonemizer"
"rhasspy/gruut" -> "rishikksh20/Avocodo-pytorch" ["e"=1]
"rhasspy/gruut" -> "Kyubyong/css10"
"rhasspy/gruut" -> "maum-ai/univnet" ["e"=1]
"rhasspy/gruut" -> "k2-fsa/libriheavy" ["e"=1]
"rhasspy/gruut" -> "keonlee9420/Parallel-Tacotron2"
"auspicious3000/AutoPST" -> "auspicious3000/SpeechSplit"
"auspicious3000/AutoPST" -> "auspicious3000/autovc"
"auspicious3000/AutoPST" -> "auspicious3000/contentvec" ["e"=1]
"auspicious3000/AutoPST" -> "Wendison/VQMIVC"
"auspicious3000/AutoPST" -> "ebadawy/voice_conversion"
"auspicious3000/AutoPST" -> "bshall/VectorQuantizedCPC"
"auspicious3000/AutoPST" -> "cyhuang-tw/AdaIN-VC"
"auspicious3000/AutoPST" -> "thuhcsi/VAENAR-TTS"
"zceng/LVCNet" -> "ViEm-ccy/GEDLoss_pytorch"
"spring-media/DeepPhonemizer" -> "rhasspy/gruut"
"spring-media/DeepPhonemizer" -> "xinjli/transphone"
"spring-media/DeepPhonemizer" -> "lingjzhu/CharsiuG2P" ["e"=1]
"spring-media/DeepPhonemizer" -> "Kyubyong/g2p"
"spring-media/DeepPhonemizer" -> "bootphon/phonemizer"
"spring-media/DeepPhonemizer" -> "NeuralVox/OpenPhonemizer"
"spring-media/DeepPhonemizer" -> "CUNY-CL/wikipron"
"spring-media/DeepPhonemizer" -> "roedoejet/g2p"
"spring-media/DeepPhonemizer" -> "ASR-project/Multilingual-PR"
"spring-media/DeepPhonemizer" -> "liusongxiang/Large-Audio-Models" ["e"=1]
"spring-media/DeepPhonemizer" -> "maxrmorrison/torchcrepe" ["e"=1]
"rishikksh20/Fre-GAN-pytorch" -> "rishikksh20/UnivNet-pytorch"
"rishikksh20/Fre-GAN-pytorch" -> "xcmyz/FastVocoder"
"rishikksh20/UnivNet-pytorch" -> "keonlee9420/WaveGrad2"
"Jackiexiao/zhtts" -> "LingLing-Speech/mandarin-tts-frontend-python"
"Jackiexiao/zhtts" -> "thuhcsi/Crystal"
"Jackiexiao/zhtts" -> "lturing/tacotronv2_wavernn_chinese"
"liusongxiang/ppg-vc" -> "Wendison/VQMIVC"
"liusongxiang/ppg-vc" -> "maum-ai/assem-vc" ["e"=1]
"liusongxiang/ppg-vc" -> "jxzhanggg/nonparaSeq2seqVC_code"
"liusongxiang/ppg-vc" -> "hhguo/EA-SVC"
"liusongxiang/ppg-vc" -> "OlaWod/FreeVC" ["e"=1]
"liusongxiang/ppg-vc" -> "thuhcsi/VAENAR-TTS"
"liusongxiang/ppg-vc" -> "huawei-noah/Speech-Backbones" ["e"=1]
"liusongxiang/ppg-vc" -> "jjery2243542/adaptive_voice_conversion"
"liusongxiang/ppg-vc" -> "bshall/ZeroSpeech"
"SongRongLee/mir-svc" -> "johndpope/Singing-Voice-Conversion-with-conditional-VAW-GAN"
"SongRongLee/mir-svc" -> "hhguo/EA-SVC"
"rishikksh20/AdaSpeech" -> "tuanh123789/AdaSpeech"
"rishikksh20/AdaSpeech" -> "rishikksh20/AdaSpeech2"
"rishikksh20/AdaSpeech" -> "rishikksh20/VocGAN"
"yistLin/FragmentVC" -> "yistLin/universal-vocoder"
"yistLin/FragmentVC" -> "howard1337/S2VC"
"yistLin/FragmentVC" -> "ericwudayi/SkipVQVC"
"yistLin/FragmentVC" -> "jxzhanggg/nonparaSeq2seqVC_code"
"yistLin/FragmentVC" -> "cyhuang-tw/AdaIN-VC"
"yistLin/FragmentVC" -> "k2kobayashi/crank"
"yistLin/FragmentVC" -> "KimythAnly/AGAIN-VC"
"yistLin/FragmentVC" -> "jjery2243542/adaptive_voice_conversion"
"yistLin/FragmentVC" -> "tzuhsien/Voice-conversion-evaluation"
"yistLin/FragmentVC" -> "Wendison/VQMIVC"
"yistLin/FragmentVC" -> "liusongxiang/ppg-vc"
"zzw922cn/LPC_for_TTS" -> "Yablon/auorange"
"tts-tutorial/survey" -> "wenet-e2e/speech-synthesis-paper"
"tts-tutorial/survey" -> "jaywalnut310/glow-tts"
"tts-tutorial/survey" -> "LEEYOONHYUNG/BVAE-TTS"
"tts-tutorial/survey" -> "keonlee9420/StyleSpeech" ["e"=1]
"tts-tutorial/survey" -> "thuhcsi/VAENAR-TTS"
"tts-tutorial/survey" -> "maum-ai/univnet" ["e"=1]
"tts-tutorial/survey" -> "KevinMIN95/StyleSpeech" ["e"=1]
"tts-tutorial/survey" -> "NATSpeech/NATSpeech" ["e"=1]
"tts-tutorial/survey" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"tts-tutorial/survey" -> "nii-yamagishilab/project-NN-Pytorch-scripts" ["e"=1]
"tts-tutorial/survey" -> "facebookresearch/speech-resynthesis" ["e"=1]
"tts-tutorial/survey" -> "NVIDIA/mellotron"
"tts-tutorial/survey" -> "Wendison/VQMIVC"
"tts-tutorial/survey" -> "kan-bayashi/ParallelWaveGAN"
"tts-tutorial/survey" -> "aliutkus/speechmetrics" ["e"=1]
"Wendison/VQMIVC" -> "bshall/VectorQuantizedCPC"
"Wendison/VQMIVC" -> "liusongxiang/ppg-vc"
"Wendison/VQMIVC" -> "maum-ai/assem-vc" ["e"=1]
"Wendison/VQMIVC" -> "thuhcsi/VAENAR-TTS"
"Wendison/VQMIVC" -> "jxzhanggg/nonparaSeq2seqVC_code"
"Wendison/VQMIVC" -> "facebookresearch/speech-resynthesis" ["e"=1]
"Wendison/VQMIVC" -> "cyhuang-tw/AdaIN-VC"
"Wendison/VQMIVC" -> "auspicious3000/SpeechSplit"
"Wendison/VQMIVC" -> "k2kobayashi/crank"
"Wendison/VQMIVC" -> "bshall/ZeroSpeech"
"Wendison/VQMIVC" -> "KimythAnly/AGAIN-VC"
"Wendison/VQMIVC" -> "auspicious3000/autovc"
"Wendison/VQMIVC" -> "ericwudayi/SkipVQVC"
"Wendison/VQMIVC" -> "keonlee9420/StyleSpeech" ["e"=1]
"Wendison/VQMIVC" -> "yistLin/FragmentVC"
"audeering/opensmile-python" -> "audeering/opensmile"
"audeering/opensmile-python" -> "naxingyu/opensmile"
"audeering/opensmile-python" -> "jcvasquezc/DisVoice"
"audeering/opensmile-python" -> "novoic/surfboard" ["e"=1]
"audeering/opensmile-python" -> "auDeep/auDeep"
"keonlee9420/Parallel-Tacotron2" -> "rishikksh20/VocGAN"
"keonlee9420/Parallel-Tacotron2" -> "keonlee9420/StyleSpeech" ["e"=1]
"keonlee9420/Parallel-Tacotron2" -> "thuhcsi/VAENAR-TTS"
"keonlee9420/Parallel-Tacotron2" -> "keonlee9420/Comprehensive-Transformer-TTS" ["e"=1]
"keonlee9420/Parallel-Tacotron2" -> "ivanvovk/WaveGrad"
"keonlee9420/Parallel-Tacotron2" -> "maum-ai/univnet" ["e"=1]
"sotelo/parrot" -> "soroushmehr/sampleRNN_ICLR2017"
"sotelo/parrot" -> "CSTR-Edinburgh/merlin"
"sotelo/parrot" -> "azraelkuan/parallel_wavenet_vocoder"
"sotelo/parrot" -> "israelg99/deepvoice"
"sotelo/parrot" -> "google/tacotron"
"sotelo/parrot" -> "tomlepaine/fast-wavenet"
"sotelo/parrot" -> "Zeta36/tensorflow-tex-wavenet"
"sotelo/parrot" -> "facebookarchive/loop"
"sotelo/parrot" -> "Kyubyong/tacotron"
"sotelo/parrot" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"sotelo/parrot" -> "ksw0306/ClariNet"
"sotelo/parrot" -> "google/yang_vocoder"
"sotelo/parrot" -> "tiberiu44/TTS-Cube"
"sotelo/parrot" -> "ksw0306/FloWaveNet"
"sotelo/parrot" -> "NVIDIA/nv-wavenet"
"cyhuang-tw/AutoVC" -> "grtzsohalf/buy_vs_rent_and_invest"
"cyhuang-tw/AutoVC" -> "pohanchi/huggingface_albert"
"adbailey1/DepAudioNet_reproduction" -> "adbailey1/daic_woz_process"
"adbailey1/DepAudioNet_reproduction" -> "speechandlanguageprocessing/ICASSP2022-Depression"
"adbailey1/DepAudioNet_reproduction" -> "PingCheng-Wei/DepressionEstimation"
"adbailey1/DepAudioNet_reproduction" -> "kingformatty/NUSD"
"adbailey1/DepAudioNet_reproduction" -> "derong97/automatic-depression-detector"
"adbailey1/DepAudioNet_reproduction" -> "CMDC-corpus/CMDC-Baseline"
"adbailey1/DepAudioNet_reproduction" -> "cyZhu98/depression_papers"
"bshall/Tacotron" -> "LeoniusChen/Attentions-in-Tacotron"
"bshall/Tacotron" -> "bshall/UniversalVocoding"
"bshall/Tacotron" -> "thuhcsi/VAENAR-TTS"
"cyhuang-tw/AdaIN-VC" -> "howard1337/S2VC"
"cyhuang-tw/AdaIN-VC" -> "yistLin/universal-vocoder"
"cyhuang-tw/AdaIN-VC" -> "cyhuang-tw/AutoVC"
"KimythAnly/AGAIN-VC" -> "ericwudayi/SkipVQVC"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/controllable_evc_code"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/Emovox"
"KunZhou9646/seq2seq-EVC" -> "ktho22/vctts"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"KunZhou9646/seq2seq-EVC" -> "CZ26/CycleTransGAN-EVC"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/Mixed_Emotions" ["e"=1]
"KunZhou9646/seq2seq-EVC" -> "bottlecapper/EmoMUNIT"
"KunZhou9646/seq2seq-EVC" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"GANtastic3/MaskCycleGAN-VC" -> "jackaduma/CycleGAN-VC3"
"GANtastic3/MaskCycleGAN-VC" -> "Wendison/VQMIVC"
"GANtastic3/MaskCycleGAN-VC" -> "jackaduma/CycleGAN-VC2"
"GANtastic3/MaskCycleGAN-VC" -> "k2kobayashi/crank"
"GANtastic3/MaskCycleGAN-VC" -> "SongRongLee/mir-svc"
"howard1337/S2VC" -> "cyhuang-tw/AdaIN-VC"
"howard1337/S2VC" -> "yistLin/FragmentVC"
"howard1337/S2VC" -> "yistLin/universal-vocoder"
"howard1337/S2VC" -> "tzuhsien/Voice-conversion-evaluation"
"howard1337/S2VC" -> "grtzsohalf/buy_vs_rent_and_invest"
"CZ26/CycleTransGAN-EVC" -> "KunZhou9646/Emovox"
"CZ26/CycleTransGAN-EVC" -> "ktho22/vctts"
"CZ26/CycleTransGAN-EVC" -> "gayanechilingar/Change-Emotions"
"CZ26/CycleTransGAN-EVC" -> "KunZhou9646/seq2seq-EVC"
"CZ26/CycleTransGAN-EVC" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"derong97/automatic-depression-detector" -> "linlemn/DepressionDectection"
"derong97/automatic-depression-detector" -> "HimangiM/Depression_Detection"
"jackaduma/CycleGAN-VC3" -> "jackaduma/CycleGAN-VC2"
"jackaduma/CycleGAN-VC3" -> "ChaoWANG0511/CycleGAN-VC3"
"jackaduma/CycleGAN-VC3" -> "GANtastic3/MaskCycleGAN-VC"
"jackaduma/CycleGAN-VC3" -> "SamuelBroughton/StarGAN-Voice-Conversion-2"
"atomicoo/FCH-TTS" -> "atomicoo/tacotron2-mandarin"
"atomicoo/FCH-TTS" -> "lturing/tacotronv2_wavernn_chinese"
"atomicoo/FCH-TTS" -> "ranchlai/mandarin-tts"
"atomicoo/FCH-TTS" -> "atomicoo/PTTS-WebAPP"
"atomicoo/FCH-TTS" -> "thuhcsi/Crystal"
"atomicoo/FCH-TTS" -> "Tendol/Bo-Eng-Machine-Transation"
"rhasspy/gruut-ipa" -> "pettarin/ipapy"
"xcmyz/FastVocoder" -> "xcmyz/ConvTasNet4BasisMelGAN"
"pohanchi/AALBERT" -> "grtzsohalf/buy_vs_rent_and_invest"
"pohanchi/AALBERT" -> "yistLin/universal-vocoder"
"hwanheelee1993/KPQA" -> "klee972/exec-filter"
"hwanheelee1993/KPQA" -> "hwanheelee1993/MFMA"
"hwanheelee1993/KPQA" -> "DongryeolLee96/AskCQ"
"hwanheelee1993/KPQA" -> "hwanheelee1993/UMIC"
"hwanheelee1993/KPQA" -> "YunahJang/IterCQR"
"hwanheelee1993/KPQA" -> "hyukhunkoh-ai/multi_view_zero_shot_open_intent_induction"
"hwanheelee1993/KPQA" -> "minbeomkim/CriticControl"
"hwanheelee1993/UMIC" -> "klee972/exec-filter"
"hwanheelee1993/UMIC" -> "hwanheelee1993/MFMA"
"hwanheelee1993/UMIC" -> "hwanheelee1993/KPQA"
"hwanheelee1993/UMIC" -> "DongryeolLee96/AskCQ"
"hwanheelee1993/UMIC" -> "YunahJang/IterCQR"
"hwanheelee1993/UMIC" -> "hyukhunkoh-ai/multi_view_zero_shot_open_intent_induction"
"hwanheelee1993/UMIC" -> "minbeomkim/CriticControl"
"L0SG/NanoFlow" -> "ysw1021/AGG"
"LeoniusChen/Attentions-in-Tacotron" -> "thuhcsi/tacotron"
"LeoniusChen/Attentions-in-Tacotron" -> "thuhcsi/VAENAR-TTS"
"LeoniusChen/Attentions-in-Tacotron" -> "bshall/Tacotron"
"thuhcsi/tacotron" -> "LeoniusChen/Attentions-in-Tacotron"
"thuhcsi/tacotron" -> "zerlinwang/synthetic-corpus-vocoder"
"RussellSB/tt-vae-gan" -> "ebadawy/voice_conversion"
"ethan-nicholas-tsai/SWDD" -> "ethan-nicholas-tsai/DepressionDetection"
"xinjli/ucla-phonetic-corpus" -> "xinjli/phonepiece"
"manexagirrezabal/zeuscansion" -> "jproft/Scandroid"
"KimythAnly/qqdm" -> "grtzsohalf/buy_vs_rent_and_invest"
"OpenVoiceOS/ovos-ww-plugin-vosk" -> "ChanceNCounter/awesome-mycroft-community"
"KunZhou9646/controllable_evc_code" -> "KunZhou9646/seq2seq-EVC"
"KunZhou9646/controllable_evc_code" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"KunZhou9646/controllable_evc_code" -> "ktho22/vctts"
"KunZhou9646/controllable_evc_code" -> "gayanechilingar/Change-Emotions"
"kichanll/Chinese_Prosody_Predict" -> "Zeqiang-Lai/Prosody_Prediction"
"kuangdd/ttskit" -> "ranchlai/mandarin-tts"
"kuangdd/ttskit" -> "lturing/tacotronv2_wavernn_chinese"
"kuangdd/ttskit" -> "PlayVoice/vits_chinese" ["e"=1]
"kuangdd/ttskit" -> "shibing624/parrots"
"kuangdd/ttskit" -> "modelscope/KAN-TTS" ["e"=1]
"kuangdd/ttskit" -> "TensorSpeech/TensorFlowTTS"
"kuangdd/ttskit" -> "aidreamwin/TTS-Clone-Chinese"
"kuangdd/ttskit" -> "atomicoo/FCH-TTS"
"kuangdd/ttskit" -> "huakunyang/SummerTTS" ["e"=1]
"kuangdd/ttskit" -> "JasonWei512/Tacotron-2-Chinese"
"kuangdd/ttskit" -> "chenkui164/FastASR" ["e"=1]
"kuangdd/ttskit" -> "RapidAI/RapidASR" ["e"=1]
"kuangdd/ttskit" -> "Jackiexiao/zhtts"
"kuangdd/ttskit" -> "funnyzak/tts-now" ["e"=1]
"kuangdd/ttskit" -> "innnky/emotional-vits" ["e"=1]
"mmorise/World" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"mmorise/World" -> "CSTR-Edinburgh/merlin"
"mmorise/World" -> "xiph/LPCNet"
"mmorise/World" -> "nnsvs/nnsvs" ["e"=1]
"mmorise/World" -> "kan-bayashi/ParallelWaveGAN"
"mmorise/World" -> "descriptinc/melgan-neurips"
"mmorise/World" -> "k2kobayashi/sprocket"
"mmorise/World" -> "r9y9/pysptk"
"mmorise/World" -> "jik876/hifi-gan"
"mmorise/World" -> "google/REAPER"
"mmorise/World" -> "fatchord/WaveRNN"
"mmorise/World" -> "r9y9/wavenet_vocoder"
"mmorise/World" -> "HidekiKawahara/legacy_STRAIGHT"
"mmorise/World" -> "r9y9/nnmnkwii"
"mmorise/World" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"xinjli/transphone" -> "xinjli/alqalign"
"xinjli/transphone" -> "xinjli/ucla-phonetic-corpus"
"xinjli/transphone" -> "lingjzhu/CharsiuG2P" ["e"=1]
"xinjli/transphone" -> "xinjli/asr2k"
"xinjli/transphone" -> "xinjli/phonepiece"
"ASR-project/Multilingual-PR" -> "xinjli/transphone"
"ASR-project/Multilingual-PR" -> "spring-media/DeepPhonemizer"
"ASR-project/Multilingual-PR" -> "thuhcsi/NeuFA"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "pettarin/forced-alignment-tools"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "kan-bayashi/ParallelWaveGAN"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "ming024/FastSpeech2"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "wenet-e2e/speech-synthesis-paper"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "jik876/hifi-gan"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "bootphon/phonemizer"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "Kyubyong/g2p"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "xcmyz/FastSpeech"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "jaywalnut310/glow-tts"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "strob/gentle"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "s3prl/s3prl"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "kakaobrain/g2pm"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "xiph/LPCNet"
"MontrealCorpusTools/Montreal-Forced-Aligner" -> "YannickJadoul/Parselmouth"
"strob/gentle" -> "pettarin/forced-alignment-tools"
"strob/gentle" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"strob/gentle" -> "readbeyond/aeneas"
"strob/gentle" -> "prosodylab/Prosodylab-Aligner"
"strob/gentle" -> "YannickJadoul/Parselmouth"
"strob/gentle" -> "coqui-ai/open-speech-corpora"
"strob/gentle" -> "mozilla/DSAlign"
"strob/gentle" -> "syhw/wer_are_we" ["e"=1]
"strob/gentle" -> "Kyubyong/g2p"
"strob/gentle" -> "AdolfVonKleist/Phonetisaurus"
"strob/gentle" -> "CSTR-Edinburgh/merlin"
"strob/gentle" -> "lhotse-speech/lhotse" ["e"=1]
"strob/gentle" -> "pykaldi/pykaldi" ["e"=1]
"strob/gentle" -> "facebookresearch/WavAugment" ["e"=1]
"strob/gentle" -> "wiseman/py-webrtcvad" ["e"=1]
"MycroftAI/adapt" -> "MycroftAI/mimic1"
"MycroftAI/adapt" -> "MycroftAI/ZZZ-RETIRED__openstt"
"MycroftAI/adapt" -> "MycroftAI/padatious"
"MycroftAI/adapt" -> "MycroftAI/mycroft-skills"
"MycroftAI/adapt" -> "MycroftAI/mycroft-core" ["e"=1]
"MycroftAI/adapt" -> "MycroftAI/ZZZ-RETIRED__mycroft-core-documentation"
"MycroftAI/adapt" -> "MycroftAI/hardware-mycroft-mark-1"
"MycroftAI/adapt" -> "MycroftAI/mimic2"
"MycroftAI/adapt" -> "MycroftAI/ZZZ-RETIRED__adapt-documentation"
"MycroftAI/adapt" -> "MycroftAI/Mycroft-Android"
"MycroftAI/adapt" -> "MycroftAI/linux"
"MycroftAI/adapt" -> "MycroftAI/ZZZ-RETIRED__pychromecast"
"MycroftAI/adapt" -> "MycroftAI/docker-mycroft"
"MycroftAI/adapt" -> "MycroftAI/enclosure-picroft"
"MycroftAI/adapt" -> "AIIX/Mycroft-AI-Gnome-Shell-Extension"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "adbailey1/DepAudioNet_reproduction"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "adbailey1/daic_woz_process"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "PingCheng-Wei/DepressionEstimation"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "CMDC-corpus/CMDC-Baseline"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "nextDeve/Depression-detect-ResNet"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "cosmaadrian/time-enriched-multimodal-depression-detection"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "derong97/automatic-depression-detector"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "aidenwang9867/Weibo-User-Depression-Detection-Dataset"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "cosmaadrian/multimodal-depression-from-video"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "linlemn/DepressionDectection"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "sukesh167/Depression-Detection-in-speech"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "AudioVisualEmotionChallenge/AVEC2019"
"speechandlanguageprocessing/ICASSP2022-Depression" -> "HappyColor/SpeechFormer" ["e"=1]
"speechandlanguageprocessing/ICASSP2022-Depression" -> "kykiefer/depression-detect"
"sequitur-g2p/sequitur-g2p" -> "AdolfVonKleist/Phonetisaurus"
"sequitur-g2p/sequitur-g2p" -> "cmusphinx/g2p-seq2seq" ["e"=1]
"sequitur-g2p/sequitur-g2p" -> "YiwenShaoStephen/pychain" ["e"=1]
"cyZhu98/depression_papers" -> "CMDC-corpus/CMDC-Baseline"
"timmahrt/ProMo" -> "timmahrt/praatIO"
"google/language-resources" -> "google/sparrowhawk"
"google/language-resources" -> "coqui-ai/open-speech-corpora"
"google/language-resources" -> "CUNY-CL/wikipron"
"google/language-resources" -> "google-research-datasets/dakshina" ["e"=1]
"ttslr/StrengthNet" -> "KunZhou9646/Emovox"
"benjaminwan/ChineseTtsTflite" -> "tatans-coder/TensorflowTTS_chinese"
"benjaminwan/ChineseTtsTflite" -> "AndroidMaryTTS/AndroidMaryTTS"
"benjaminwan/ChineseTtsTflite" -> "jinguangyang/ChineseTTS"
"benjaminwan/ChineseTtsTflite" -> "cczhr/voice-tts"
"MycroftAI/ZZZ-RETIRED__adapt-documentation" -> "MycroftAI/linux"
"r9y9/pysptk" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"r9y9/pysptk" -> "r9y9/nnmnkwii"
"r9y9/pysptk" -> "lochenchou/MOSNet" ["e"=1]
"r9y9/pysptk" -> "bigpon/vcc20_baseline_cyclevae"
"r9y9/pysptk" -> "k2kobayashi/sprocket"
"r9y9/pysptk" -> "mmorise/World"
"r9y9/pysptk" -> "kan-bayashi/PytorchWaveNetVocoder"
"r9y9/pysptk" -> "auspicious3000/SpeechSplit"
"r9y9/pysptk" -> "liusongxiang/StarGAN-Voice-Conversion"
"r9y9/pysptk" -> "xiph/LPCNet"
"r9y9/pysptk" -> "kan-bayashi/ParallelWaveGAN"
"r9y9/pysptk" -> "descriptinc/melgan-neurips"
"r9y9/pysptk" -> "NVIDIA/mellotron"
"r9y9/pysptk" -> "aliutkus/speechmetrics" ["e"=1]
"r9y9/pysptk" -> "mkotha/WaveRNN"
"cyZhu98/Depression_Baseline" -> "aidenwang9867/Weibo-User-Depression-Detection-Dataset"
"BrightGu/MediumVC" -> "BrightGu/SingleVC"
"KunZhou9646/Emovox" -> "CZ26/CycleTransGAN-EVC"
"KunZhou9646/Emovox" -> "KunZhou9646/seq2seq-EVC"
"KunZhou9646/Emovox" -> "KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT"
"KunZhou9646/Emovox" -> "glam-imperial/EmotionalConversionStarGAN"
"KunZhou9646/Emovox" -> "KunZhou9646/Mixed_Emotions" ["e"=1]
"KunZhou9646/Emovox" -> "KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0"
"KunZhou9646/Emovox" -> "ktho22/vctts"
"KunZhou9646/Emovox" -> "ttslr/StrengthNet"
"KunZhou9646/Emovox" -> "KunZhou9646/controllable_evc_code"
"nextDeve/Depression-detect-ResNet" -> "PingCheng-Wei/DepressionEstimation"
"PingCheng-Wei/DepressionEstimation" -> "adbailey1/DepAudioNet_reproduction"
"PingCheng-Wei/DepressionEstimation" -> "adbailey1/daic_woz_process"
"PingCheng-Wei/DepressionEstimation" -> "linlemn/DepressionDectection"
"PingCheng-Wei/DepressionEstimation" -> "speechandlanguageprocessing/ICASSP2022-Depression"
"PingCheng-Wei/DepressionEstimation" -> "derong97/automatic-depression-detector"
"PingCheng-Wei/DepressionEstimation" -> "nextDeve/Depression-detect-ResNet"
"PingCheng-Wei/DepressionEstimation" -> "cosmaadrian/multimodal-depression-from-video"
"PingCheng-Wei/DepressionEstimation" -> "cosmaadrian/time-enriched-multimodal-depression-detection"
"tuanh123789/AdaSpeech" -> "rishikksh20/AdaSpeech"
"lingjzhu/charsiu" -> "lingjzhu/CharsiuG2P" ["e"=1]
"lingjzhu/charsiu" -> "interactiveaudiolab/ppgs"
"lingjzhu/charsiu" -> "kakaobrain/g2pm"
"lingjzhu/charsiu" -> "thuhcsi/NeuFA"
"lingjzhu/charsiu" -> "AdolfVonKleist/Phonetisaurus"
"CMDC-corpus/CMDC-Baseline" -> "cyZhu98/depression_papers"
"hwanheelee1993/MFMA" -> "klee972/exec-filter"
"hwanheelee1993/MFMA" -> "DongryeolLee96/AskCQ"
"hwanheelee1993/MFMA" -> "YunahJang/IterCQR"
"MycroftAI/ZZZ-RETIRED__pychromecast" -> "MycroftAI/linux"
"naxingyu/opensmile" -> "audeering/opensmile-python"
"maxrmorrison/promonet" -> "maxrmorrison/torbi"
"tkedwards/wiktionarytodict" -> "dohliam/dsl-tools"
"BrightGu/SingleVC" -> "BrightGu/MediumVC"
"MycroftAI/linux" -> "MycroftAI/ZZZ-RETIRED__adapt-documentation"
"MycroftAI/linux" -> "MycroftAI/ZZZ-RETIRED__pychromecast"
"lajlksdf/vtl" -> "BrightGu/SingleVC"
"lajlksdf/vtl" -> "BrightGu/MediumVC"
"zerlinwang/synthetic-corpus-vocoder" -> "William-Zhanng/SenseXAMP"
"ethan-nicholas-tsai/DepressionDetection" -> "ethan-nicholas-tsai/SWDD"
"dmort27/epitran" -> "dmort27/panphon"
"dmort27/epitran" -> "bootphon/phonemizer"
"dmort27/epitran" -> "CUNY-CL/wikipron"
"dmort27/epitran" -> "xinjli/allosaurus"
"dmort27/epitran" -> "xinjli/transphone"
"dmort27/epitran" -> "lingjzhu/CharsiuG2P" ["e"=1]
"dmort27/epitran" -> "AdolfVonKleist/Phonetisaurus"
"dmort27/epitran" -> "festvox/datasets-CMU_Wilderness"
"dmort27/epitran" -> "Kyubyong/g2p"
"dmort27/epitran" -> "pettarin/ipapy"
"dmort27/epitran" -> "spring-media/DeepPhonemizer"
"dmort27/epitran" -> "uiuc-sst/g2ps"
"dmort27/epitran" -> "mphilli/English-to-IPA"
"dmort27/epitran" -> "open-dict-data/ipa-dict"
"dmort27/epitran" -> "Kyubyong/css10"
"bootphon/phonemizer" -> "Kyubyong/g2p"
"bootphon/phonemizer" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"bootphon/phonemizer" -> "dmort27/epitran"
"bootphon/phonemizer" -> "NVIDIA/BigVGAN" ["e"=1]
"bootphon/phonemizer" -> "jik876/hifi-gan"
"bootphon/phonemizer" -> "kan-bayashi/ParallelWaveGAN"
"bootphon/phonemizer" -> "jaywalnut310/glow-tts"
"bootphon/phonemizer" -> "wenet-e2e/speech-synthesis-paper"
"bootphon/phonemizer" -> "shivammehta25/Matcha-TTS" ["e"=1]
"bootphon/phonemizer" -> "s3prl/s3prl"
"bootphon/phonemizer" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"bootphon/phonemizer" -> "microsoft/NeuralSpeech" ["e"=1]
"bootphon/phonemizer" -> "AdolfVonKleist/Phonetisaurus"
"bootphon/phonemizer" -> "ming024/FastSpeech2"
"bootphon/phonemizer" -> "gemelo-ai/vocos" ["e"=1]
"google/sparrowhawk" -> "google-research-datasets/TextNormalizationCoveringGrammars"
"google/sparrowhawk" -> "rwsproat/text-normalization-data"
"google/sparrowhawk" -> "danijel3/SparrowhawkTest"
"google/sparrowhawk" -> "candlewill/CNTN"
"google/sparrowhawk" -> "AdolfVonKleist/Phonetisaurus"
"google/sparrowhawk" -> "speechio/chinese_text_normalization" ["e"=1]
"google/sparrowhawk" -> "thuhcsi/Crystal"
"google/sparrowhawk" -> "bpotard/idlak"
"google/sparrowhawk" -> "npuichigo/voicenet"
"MycroftAI/mimic1" -> "MycroftAI/adapt"
"MycroftAI/mimic1" -> "MycroftAI/ZZZ-RETIRED__openstt"
"MycroftAI/mimic1" -> "MycroftAI/mimic2"
"MycroftAI/mimic1" -> "MycroftAI/mycroft-skills"
"MycroftAI/mimic1" -> "festvox/flite"
"MycroftAI/mimic1" -> "AIIX/Mycroft-AI-Gnome-Shell-Extension"
"MycroftAI/mimic1" -> "MycroftAI/mycroft-core" ["e"=1]
"MycroftAI/mimic1" -> "marytts/marytts"
"MycroftAI/mimic1" -> "CSTR-Edinburgh/merlin"
"MycroftAI/mimic1" -> "MycroftAI/enclosure-picroft"
"MycroftAI/mimic1" -> "MycroftAI/Mycroft-Android"
"MycroftAI/mimic1" -> "MycroftAI/ZZZ-RETIRED__mycroft-core-documentation"
"MycroftAI/mimic1" -> "festvox/festival"
"MycroftAI/mimic1" -> "MycroftAI/docker-mycroft"
"MycroftAI/mimic1" -> "Kyubyong/tacotron"
"dmort27/panphon" -> "dmort27/epitran"
"dmort27/panphon" -> "phoible/dev"
"dmort27/panphon" -> "pettarin/ipapy"
"dmort27/panphon" -> "xinjli/allosaurus"
"dmort27/panphon" -> "AdolfVonKleist/Phonetisaurus"
"dmort27/panphon" -> "uiuc-sst/g2ps"
"dmort27/panphon" -> "CUNY-CL/wikipron"
"dmort27/panphon" -> "roedoejet/g2p"
"dmort27/panphon" -> "xinjli/transphone"
"dmort27/panphon" -> "lingpy/lingpy" ["e"=1]
"dmort27/panphon" -> "lingjzhu/CharsiuG2P" ["e"=1]
"yl4579/AuxiliaryASR" -> "yl4579/PitchExtractor"
"yl4579/AuxiliaryASR" -> "yl4579/PL-BERT" ["e"=1]
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "mmorise/World"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "r9y9/pysptk"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "k2kobayashi/sprocket"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "kan-bayashi/ParallelWaveGAN"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "xiph/LPCNet"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "kan-bayashi/PytorchWaveNetVocoder"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "r9y9/nnmnkwii"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "CSTR-Edinburgh/merlin"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "wenet-e2e/speech-synthesis-paper"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "jjery2243542/adaptive_voice_conversion"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "maxrmorrison/torchcrepe" ["e"=1]
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "descriptinc/melgan-neurips"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "tuanad121/Python-WORLD"
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" -> "NVIDIA/mellotron"
"hgneng/ekho" -> "junzew/HanTTS"
"hgneng/ekho" -> "Jackiexiao/MTTS"
"hgneng/ekho" -> "thuhcsi/Crystal"
"hgneng/ekho" -> "begeekmyfriend/tacotron"
"hgneng/ekho" -> "ranchlai/mandarin-tts"
"hgneng/ekho" -> "TensorSpeech/TensorFlowTTS"
"hgneng/ekho" -> "KuangDD/zhrtvc"
"hgneng/ekho" -> "begeekmyfriend/Tacotron-2"
"hgneng/ekho" -> "xcmyz/FastSpeech"
"hgneng/ekho" -> "JasonWei512/Tacotron-2-Chinese"
"hgneng/ekho" -> "mmorise/World"
"hgneng/ekho" -> "CSTR-Edinburgh/merlin"
"hgneng/ekho" -> "espeak-ng/espeak-ng" ["e"=1]
"hgneng/ekho" -> "marytts/marytts"
"hgneng/ekho" -> "fatchord/WaveRNN"
"cosmaadrian/time-enriched-multimodal-depression-detection" -> "cosmaadrian/multimodal-depression-from-video"
"cosmaadrian/time-enriched-multimodal-depression-detection" -> "helang818/LMVD"
"cosmaadrian/time-enriched-multimodal-depression-detection" -> "PingCheng-Wei/DepressionEstimation"
"yl4579/PitchExtractor" -> "yl4579/AuxiliaryASR"
"pettarin/ipapy" -> "rhasspy/gruut-ipa"
"bpotard/idlak" -> "dophist/DaCiDian-Develop"
"bpotard/idlak" -> "naxingyu/kaldi-gadgets"
"albertfgu/diffwave-sashimi" -> "philsyn/DiffWave-Vocoder"
"albertfgu/diffwave-sashimi" -> "philsyn/DiffWave-unconditional"
"interactiveaudiolab/ppgs" -> "maxrmorrison/promonet"
"MycroftAI/ZZZ-RETIRED__openstt" -> "MycroftAI/adapt"
"MycroftAI/ZZZ-RETIRED__openstt" -> "MycroftAI/mimic1"
"MycroftAI/ZZZ-RETIRED__openstt" -> "MycroftAI/ZZZ-RETIRED__adapt-documentation"
"MycroftAI/ZZZ-RETIRED__openstt" -> "AIIX/Mycroft-AI-Gnome-Shell-Extension"
"openXBOW/openXBOW" -> "DeepSpectrum/DeepSpectrum"
"openXBOW/openXBOW" -> "auDeep/auDeep"
"openXBOW/openXBOW" -> "end2you/end2you"
"sahasourav17/Student-Anxiety-and-Depression-Prediction" -> "faiqali1/Suicidal-Text-Analysis"
"jiaqi-pro/Depression-detection-Graph" -> "divertingPan/STA-DRN"
"jiaqi-pro/Depression-detection-Graph" -> "volatileee/FacialPulse"
"bbTomas/rPraat" -> "IPS-LMU/emuR"
"pettarin/forced-alignment-tools" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"pettarin/forced-alignment-tools" -> "strob/gentle"
"pettarin/forced-alignment-tools" -> "prosodylab/Prosodylab-Aligner"
"pettarin/forced-alignment-tools" -> "readbeyond/aeneas"
"pettarin/forced-alignment-tools" -> "mozilla/DSAlign"
"pettarin/forced-alignment-tools" -> "Kyubyong/g2p"
"pettarin/forced-alignment-tools" -> "coqui-ai/open-speech-corpora"
"pettarin/forced-alignment-tools" -> "open-speech/speech-aligner"
"pettarin/forced-alignment-tools" -> "xiph/LPCNet"
"pettarin/forced-alignment-tools" -> "cmusphinx/g2p-seq2seq" ["e"=1]
"pettarin/forced-alignment-tools" -> "bootphon/phonemizer"
"pettarin/forced-alignment-tools" -> "syhw/wer_are_we" ["e"=1]
"pettarin/forced-alignment-tools" -> "tbright17/kaldi-dnn-ali-gop" ["e"=1]
"pettarin/forced-alignment-tools" -> "xcmyz/FastSpeech"
"pettarin/forced-alignment-tools" -> "YannickJadoul/Parselmouth"
"ibab/tensorflow-wavenet" -> "tomlepaine/fast-wavenet"
"ibab/tensorflow-wavenet" -> "buriburisuri/speech-to-text-wavenet"
"ibab/tensorflow-wavenet" -> "basveeling/wavenet"
"ibab/tensorflow-wavenet" -> "r9y9/wavenet_vocoder"
"ibab/tensorflow-wavenet" -> "Kyubyong/tacotron"
"ibab/tensorflow-wavenet" -> "keithito/tacotron"
"ibab/tensorflow-wavenet" -> "Rayhane-mamah/Tacotron-2"
"ibab/tensorflow-wavenet" -> "CSTR-Edinburgh/merlin"
"ibab/tensorflow-wavenet" -> "magenta/magenta" ["e"=1]
"ibab/tensorflow-wavenet" -> "andabi/deep-voice-conversion"
"ibab/tensorflow-wavenet" -> "NVIDIA/waveglow"
"ibab/tensorflow-wavenet" -> "pannous/tensorflow-speech-recognition" ["e"=1]
"ibab/tensorflow-wavenet" -> "openai/pixel-cnn" ["e"=1]
"ibab/tensorflow-wavenet" -> "carpedm20/DCGAN-tensorflow" ["e"=1]
"ibab/tensorflow-wavenet" -> "google-deepmind/sonnet" ["e"=1]
"MycroftAI/mycroft-skills" -> "MycroftAI/mycroft-core" ["e"=1]
"MycroftAI/mycroft-skills" -> "MycroftAI/enclosure-picroft"
"MycroftAI/mycroft-skills" -> "MycroftAI/hardware-mycroft-mark-1"
"MycroftAI/mycroft-skills" -> "MycroftAI/Mycroft-Android"
"MycroftAI/mycroft-skills" -> "MycroftAI/mycroft-gui"
"MycroftAI/mycroft-skills" -> "MycroftAI/adapt"
"MycroftAI/mycroft-skills" -> "MycroftAI/mimic1"
"MycroftAI/mycroft-skills" -> "btotharye/mycroft-homeassistant"
"MycroftAI/mycroft-skills" -> "MycroftAI/docker-mycroft"
"MycroftAI/mycroft-skills" -> "MycroftAI/documentation"
"MycroftAI/mycroft-skills" -> "OpenVoiceOS/ovos-buildroot"
"MycroftAI/mycroft-skills" -> "MycroftAI/contributors"
"MycroftAI/mycroft-skills" -> "MycroftAI/selene-backend"
"MycroftAI/mycroft-skills" -> "MycroftAI/ZZZ-RETIRED__mycroft-core-documentation"
"MycroftAI/mycroft-skills" -> "forslund/spotify-skill"
"CSTR-Edinburgh/merlin" -> "mmorise/World"
"CSTR-Edinburgh/merlin" -> "Kyubyong/tacotron"
"CSTR-Edinburgh/merlin" -> "xiph/LPCNet"
"CSTR-Edinburgh/merlin" -> "Jackiexiao/MTTS"
"CSTR-Edinburgh/merlin" -> "r9y9/wavenet_vocoder"
"CSTR-Edinburgh/merlin" -> "r9y9/nnmnkwii"
"CSTR-Edinburgh/merlin" -> "sotelo/parrot"
"CSTR-Edinburgh/merlin" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"CSTR-Edinburgh/merlin" -> "keithito/tacotron"
"CSTR-Edinburgh/merlin" -> "Rayhane-mamah/Tacotron-2"
"CSTR-Edinburgh/merlin" -> "r9y9/gantts"
"CSTR-Edinburgh/merlin" -> "xcmyz/FastSpeech"
"CSTR-Edinburgh/merlin" -> "fatchord/WaveRNN"
"CSTR-Edinburgh/merlin" -> "kan-bayashi/ParallelWaveGAN"
"CSTR-Edinburgh/merlin" -> "syang1993/gst-tacotron"
"AIIX/Mycroft-AI-Gnome-Shell-Extension" -> "joshymcd/mycroft-electro"
"AIIX/Mycroft-AI-Gnome-Shell-Extension" -> "lolstring/gnome-shell-extension-mycroft"
"basveeling/wavenet" -> "usernaamee/keras-wavenet"
"basveeling/wavenet" -> "tomlepaine/fast-wavenet"
"basveeling/wavenet" -> "ibab/tensorflow-wavenet"
"basveeling/wavenet" -> "bstriner/keras-adversarial" ["e"=1]
"basveeling/wavenet" -> "vincentherrmann/pytorch-wavenet"
"basveeling/wavenet" -> "sotelo/parrot"
"basveeling/wavenet" -> "soroushmehr/sampleRNN_ICLR2017"
"basveeling/wavenet" -> "Zeta36/tensorflow-tex-wavenet"
"basveeling/wavenet" -> "r9y9/wavenet_vocoder"
"basveeling/wavenet" -> "buriburisuri/speech-to-text-wavenet"
"basveeling/wavenet" -> "coreylynch/async-rl" ["e"=1]
"basveeling/wavenet" -> "Kyubyong/tacotron"
"basveeling/wavenet" -> "CSTR-Edinburgh/merlin"
"basveeling/wavenet" -> "farizrahman4u/seq2seq" ["e"=1]
"basveeling/wavenet" -> "jacobgil/keras-dcgan" ["e"=1]
"sillsdev/aeneas-installer" -> "ozdefir/finetuneas"
"cosmaadrian/multimodal-depression-from-video" -> "cosmaadrian/time-enriched-multimodal-depression-detection"
"cosmaadrian/multimodal-depression-from-video" -> "helang818/LMVD"
"cosmaadrian/multimodal-depression-from-video" -> "divertingPan/STA-DRN"
"cosmaadrian/multimodal-depression-from-video" -> "jiaqi-pro/Depression-detection-Graph"
"cosmaadrian/multimodal-depression-from-video" -> "PingCheng-Wei/DepressionEstimation"
"cosmaadrian/multimodal-depression-from-video" -> "Jiaxin-Ye/DepMamba"
"open-dict-data/ipa-dict" -> "CUNY-CL/wikipron"
"open-dict-data/ipa-dict" -> "dmort27/epitran"
"open-dict-data/ipa-dict" -> "mphilli/English-to-IPA"
"open-dict-data/ipa-dict" -> "open-dsl-dict/ipa-dict-dsl"
"open-dict-data/ipa-dict" -> "cmusphinx/cmudict"
"open-dict-data/ipa-dict" -> "bootphon/phonemizer"
"open-dict-data/ipa-dict" -> "AdolfVonKleist/Phonetisaurus"
"open-dict-data/ipa-dict" -> "Kyubyong/g2p"
"open-dict-data/ipa-dict" -> "xinjli/allosaurus"
"open-dict-data/ipa-dict" -> "JoseLlarena/Britfone"
"open-dict-data/ipa-dict" -> "xinjli/transphone"
"open-dict-data/ipa-dict" -> "Alexir/CMUdict"
"open-dict-data/ipa-dict" -> "uiuc-sst/g2ps"
"open-dict-data/ipa-dict" -> "quadrismegistus/prosodic"
"open-dict-data/ipa-dict" -> "menelik3/cmudict-ipa"
"feelins/Praat_Scripts" -> "stylerw/styler_praat_scripts"
"tomlepaine/fast-wavenet" -> "ibab/tensorflow-wavenet"
"tomlepaine/fast-wavenet" -> "basveeling/wavenet"
"tomlepaine/fast-wavenet" -> "r9y9/wavenet_vocoder"
"tomlepaine/fast-wavenet" -> "buriburisuri/speech-to-text-wavenet"
"tomlepaine/fast-wavenet" -> "NVIDIA/nv-wavenet"
"tomlepaine/fast-wavenet" -> "usernaamee/keras-wavenet"
"tomlepaine/fast-wavenet" -> "Kyubyong/tacotron"
"tomlepaine/fast-wavenet" -> "vincentherrmann/pytorch-wavenet"
"tomlepaine/fast-wavenet" -> "CSTR-Edinburgh/merlin"
"tomlepaine/fast-wavenet" -> "Zeta36/tensorflow-tex-wavenet"
"tomlepaine/fast-wavenet" -> "soroushmehr/sampleRNN_ICLR2017"
"tomlepaine/fast-wavenet" -> "openai/pixel-cnn" ["e"=1]
"tomlepaine/fast-wavenet" -> "sotelo/parrot"
"tomlepaine/fast-wavenet" -> "keithito/tacotron"
"tomlepaine/fast-wavenet" -> "NVIDIA/waveglow"
"usernaamee/keras-wavenet" -> "basveeling/wavenet"
"usernaamee/keras-wavenet" -> "tomlepaine/fast-wavenet"
"usernaamee/keras-wavenet" -> "ritheshkumar95/WaveNet"
"usernaamee/keras-wavenet" -> "Zeta36/tensorflow-tex-wavenet"
"usernaamee/keras-wavenet" -> "Smerity/keras_snli" ["e"=1]
"usernaamee/keras-wavenet" -> "huyouare/WaveNet-Theano"
"usernaamee/keras-wavenet" -> "ibab/tensorflow-wavenet"
"usernaamee/keras-wavenet" -> "phreeza/keras-GAN" ["e"=1]
"usernaamee/keras-wavenet" -> "osh/kerlym" ["e"=1]
"usernaamee/keras-wavenet" -> "musyoku/wavenet"
"echogarden-project/echogarden" -> "feldberlin/timething"
"echogarden-project/echogarden" -> "r4victor/afaligner"
"MycroftAI/Mycroft-Android" -> "MycroftAI/MycroftCore-Android"
"MycroftAI/Mycroft-Android" -> "MycroftAI/mycroft-skills"
"MycroftAI/Mycroft-Android" -> "alexisdiaz008/react-mycroft-gui"
"MycroftAI/Mycroft-Android" -> "OpenVoiceOS/ovos-buildroot"
"MycroftAI/Mycroft-Android" -> "MycroftAI/mycroft-core" ["e"=1]
"MycroftAI/Mycroft-Android" -> "Tadashi-Hikari/Sapphire-Assistant-Framework" ["e"=1]
"MycroftAI/Mycroft-Android" -> "MycroftAI/hardware-mycroft-mark-1"
"MycroftAI/Mycroft-Android" -> "MycroftAI/docker-mycroft"
"MycroftAI/Mycroft-Android" -> "MycroftAI/mycroft-gui"
"AndroidMaryTTS/AndroidMaryTTS" -> "AndroidMaryTTS/AndroidMaryTTS-Client"
"Zeta36/tensorflow-tex-wavenet" -> "Zeta36/tensorflow-image-wavenet"
"Zeta36/tensorflow-tex-wavenet" -> "tomlepaine/fast-wavenet"
"Zeta36/tensorflow-tex-wavenet" -> "sotelo/parrot"
"Zeta36/tensorflow-tex-wavenet" -> "soroushmehr/sampleRNN_ICLR2017"
"Zeta36/tensorflow-tex-wavenet" -> "ibab/tensorflow-wavenet"
"Zeta36/tensorflow-tex-wavenet" -> "usernaamee/keras-wavenet"
"kingformatty/NUSD" -> "adbailey1/daic_woz_process"
"huyouare/WaveNet-Theano" -> "ritheshkumar95/WaveNet"
"musyoku/wavenet" -> "ritheshkumar95/WaveNet"
"minbeomkim/CriticControl" -> "klee972/exec-filter"
"minbeomkim/CriticControl" -> "DongryeolLee96/AskCQ"
"minbeomkim/CriticControl" -> "YunahJang/IterCQR"
"minbeomkim/CriticControl" -> "hyukhunkoh-ai/multi_view_zero_shot_open_intent_induction"
"minbeomkim/CriticControl" -> "hwanheelee1993/MFMA"
"minbeomkim/CriticControl" -> "hwanheelee1993/KPQA"
"William-Zhanng/SenseXAMP" -> "zerlinwang/synthetic-corpus-vocoder"
"divertingPan/STA-DRN" -> "jiaqi-pro/Depression-detection-Graph"
"divertingPan/STA-DRN" -> "muzixingyun/LI-FPN"
"MycroftAI/MycroftCore-Android" -> "MycroftAI/Mycroft-Android"
"maxrmorrison/torbi" -> "maxrmorrison/promonet"
"muzixingyun/LI-FPN" -> "volatileee/FacialPulse"
"jproft/Scandroid" -> "manexagirrezabal/zeuscansion"
"Cadair/mycroft-kodi" -> "joshymcd/mycroft-electro"
"Zeta36/tensorflow-image-wavenet" -> "Zeta36/tensorflow-tex-wavenet"
"buriburisuri/speech-to-text-wavenet" -> "ibab/tensorflow-wavenet"
"buriburisuri/speech-to-text-wavenet" -> "pannous/tensorflow-speech-recognition" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "tomlepaine/fast-wavenet"
"buriburisuri/speech-to-text-wavenet" -> "zzw922cn/Automatic_Speech_Recognition" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "andabi/deep-voice-conversion"
"buriburisuri/speech-to-text-wavenet" -> "Kyubyong/tacotron"
"buriburisuri/speech-to-text-wavenet" -> "facebookresearch/fairseq-lua" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "OpenNMT/OpenNMT" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "MrNothing/AI-Blocks" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "junyanz/iGAN" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "oarriaga/face_classification" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "zzw922cn/awesome-speech-recognition-speech-synthesis-papers" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "flashlight/wav2letter" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "AKSHAYUBHAT/DeepVideoAnalytics" ["e"=1]
"buriburisuri/speech-to-text-wavenet" -> "SeanNaren/deepspeech.pytorch" ["e"=1]
"MycroftAI/enclosure-picroft" -> "MycroftAI/mycroft-skills"
"MycroftAI/enclosure-picroft" -> "MycroftAI/mycroft-core" ["e"=1]
"MycroftAI/enclosure-picroft" -> "MycroftAI/mimic1"
"MycroftAI/enclosure-picroft" -> "OpenVoiceOS/ovos-buildroot"
"MycroftAI/enclosure-picroft" -> "MycroftAI/Mycroft-Android"
"MycroftAI/enclosure-picroft" -> "MycroftAI/adapt"
"MycroftAI/enclosure-picroft" -> "MycroftAI/hardware-mycroft-mark-1"
"MycroftAI/enclosure-picroft" -> "MycroftAI/docker-mycroft"
"MycroftAI/enclosure-picroft" -> "MycroftAI/mimic2"
"MycroftAI/enclosure-picroft" -> "MycroftAI/mycroft-gui"
"MycroftAI/enclosure-picroft" -> "MycroftAI/Precise-Community-Data"
"MycroftAI/enclosure-picroft" -> "OpenVoiceOS/ovos-personal-backend"
"MycroftAI/enclosure-picroft" -> "shivasiddharth/GassistPi" ["e"=1]
"MycroftAI/enclosure-picroft" -> "MycroftAI/documentation"
"MycroftAI/enclosure-picroft" -> "MycroftAI/skill-homeassistant"
"soroushmehr/sampleRNN_ICLR2017" -> "sotelo/parrot"
"soroushmehr/sampleRNN_ICLR2017" -> "deepsound-project/samplernn-pytorch"
"soroushmehr/sampleRNN_ICLR2017" -> "richardassar/SampleRNN_torch"
"soroushmehr/sampleRNN_ICLR2017" -> "Unisound/SampleRNN"
"soroushmehr/sampleRNN_ICLR2017" -> "tomlepaine/fast-wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "CSTR-Edinburgh/merlin"
"soroushmehr/sampleRNN_ICLR2017" -> "Zeta36/tensorflow-tex-wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "basveeling/wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "ZVK/sampleRNN_ICLR2017"
"soroushmehr/sampleRNN_ICLR2017" -> "ibab/tensorflow-wavenet"
"soroushmehr/sampleRNN_ICLR2017" -> "Kyubyong/tacotron"
"soroushmehr/sampleRNN_ICLR2017" -> "huyouare/WaveNet-Theano"
"soroushmehr/sampleRNN_ICLR2017" -> "rncm-prism/prism-samplernn"
"soroushmehr/sampleRNN_ICLR2017" -> "ksw0306/FloWaveNet"
"soroushmehr/sampleRNN_ICLR2017" -> "barronalex/Tacotron"
"ex3ndr/supervoice-voicebox" -> "NeuralVox/OpenPhonemizer"
"OpenVoiceOS/ovos-installer" -> "OpenVoiceOS/ovos-docker"
"OpenVoiceOS/ovos-installer" -> "OpenVoiceOS/raspOVOS"
"OpenVoiceOS/ovos-installer" -> "NeonGeckoCom/NeonCore"
"OpenVoiceOS/ovos-installer" -> "OpenVoiceOS/ovos-core"
"DmitryUlyanov/neural-style-audio-tf" -> "rupeshs/neuralsongstyle"
"DmitryUlyanov/neural-style-audio-tf" -> "vadim-v-lebedev/audio_style_tranfer"
"DmitryUlyanov/neural-style-audio-tf" -> "DmitryUlyanov/neural-style-audio-torch"
"DmitryUlyanov/neural-style-audio-tf" -> "pkmital/time-domain-neural-audio-style-transfer"
"DmitryUlyanov/neural-style-audio-tf" -> "alishdipani/Neural-Style-Transfer-Audio"
"DmitryUlyanov/neural-style-audio-tf" -> "sotelo/parrot"
"DmitryUlyanov/neural-style-audio-tf" -> "sumuzhao/CycleGAN-Music-Style-Transfer"
"DmitryUlyanov/neural-style-audio-tf" -> "facebookresearch/music-translation"
"DmitryUlyanov/neural-style-audio-tf" -> "soroushmehr/sampleRNN_ICLR2017"
"DmitryUlyanov/neural-style-audio-tf" -> "frhrdr/generative_audio"
"rupeshs/neuralsongstyle" -> "DmitryUlyanov/neural-style-audio-tf"
"rupeshs/neuralsongstyle" -> "vadim-v-lebedev/audio_style_tranfer"
"NeuralVox/OpenPhonemizer" -> "ex3ndr/supervoice-voicebox"
"albertaparicio/tfg-voice-conversion" -> "shamidreza/dnnmapper"
"albertaparicio/tfg-voice-conversion" -> "Kyubyong/cross_vc"
"albertaparicio/tfg-voice-conversion" -> "JeremyCCHsu/vae-npvc"
"rwsproat/text-normalization-data" -> "shauryr/google_text_normalization"
"rwsproat/text-normalization-data" -> "amazon-science/proteno"
"DongryeolLee96/AskCQ" -> "klee972/exec-filter"
"YunahJang/IterCQR" -> "klee972/exec-filter"
"YunahJang/IterCQR" -> "DongryeolLee96/AskCQ"
"YunahJang/IterCQR" -> "hyukhunkoh-ai/multi_view_zero_shot_open_intent_induction"
"YunahJang/IterCQR" -> "hwanheelee1993/MFMA"
"YunahJang/IterCQR" -> "minbeomkim/CriticControl"
"open-dsl-dict/ipa-dict-dsl" -> "dohliam/dsl-tools"
"open-dsl-dict/ipa-dict-dsl" -> "xiaoyifang/Capture2Text"
"Jiaxin-Ye/DepMamba" -> "helang818/LMVD"
"Jiaxin-Ye/DepMamba" -> "cosmaadrian/multimodal-depression-from-video"
"vincentherrmann/pytorch-wavenet" -> "r9y9/wavenet_vocoder"
"vincentherrmann/pytorch-wavenet" -> "tomlepaine/fast-wavenet"
"vincentherrmann/pytorch-wavenet" -> "golbin/WaveNet"
"vincentherrmann/pytorch-wavenet" -> "basveeling/wavenet"
"vincentherrmann/pytorch-wavenet" -> "NVIDIA/nv-wavenet"
"vincentherrmann/pytorch-wavenet" -> "descriptinc/melgan-neurips"
"vincentherrmann/pytorch-wavenet" -> "dhpollack/fast-wavenet.pytorch"
"vincentherrmann/pytorch-wavenet" -> "kan-bayashi/PytorchWaveNetVocoder"
"vincentherrmann/pytorch-wavenet" -> "ibab/tensorflow-wavenet"
"vincentherrmann/pytorch-wavenet" -> "kan-bayashi/ParallelWaveGAN"
"vincentherrmann/pytorch-wavenet" -> "fatchord/WaveRNN"
"vincentherrmann/pytorch-wavenet" -> "NVIDIA/waveglow"
"vincentherrmann/pytorch-wavenet" -> "chrisdonahue/wavegan"
"vincentherrmann/pytorch-wavenet" -> "ksw0306/FloWaveNet"
"vincentherrmann/pytorch-wavenet" -> "zcaceres/spec_augment" ["e"=1]
"ex3ndr/supervoice-vall-e-2" -> "keonlee9420/evaluate-zero-shot-tts"
"ex3ndr/supervoice-vall-e-2" -> "ex3ndr/supervoice-voicebox"
"richardassar/SampleRNN_torch" -> "soroushmehr/sampleRNN_ICLR2017"
"richardassar/SampleRNN_torch" -> "Unisound/SampleRNN"
"richardassar/SampleRNN_torch" -> "deepsound-project/samplernn-pytorch"
"barronalex/Tacotron" -> "Kyubyong/tacotron"
"barronalex/Tacotron" -> "candlewill/Tacotron-2"
"barronalex/Tacotron" -> "Kyubyong/tacotron_asr"
"barronalex/Tacotron" -> "zuoxiang95/tacotron-1"
"barronalex/Tacotron" -> "keithito/tacotron"
"barronalex/Tacotron" -> "riverphoenix/tacotron2"
"mhsabbagh/green-recorder" -> "AIIX/Mycroft-AI-Gnome-Shell-Extension"
"mhsabbagh/green-recorder" -> "Programmica/python-gtk3-tutorial"
"israelg99/deepvoice" -> "Kyubyong/deepvoice3"
"israelg99/deepvoice" -> "sotelo/parrot"
"israelg99/deepvoice" -> "baidu-research/deep-voice"
"israelg99/deepvoice" -> "Kyubyong/tacotron"
"israelg99/deepvoice" -> "candlewill/AiVoice"
"israelg99/deepvoice" -> "CSTR-Edinburgh/merlin"
"google/tacotron" -> "sotelo/parrot"
"google/tacotron" -> "KinglittleQ/GST-Tacotron"
"google/tacotron" -> "keithito/tacotron"
"google/tacotron" -> "Kyubyong/tacotron"
"google/tacotron" -> "CSTR-Edinburgh/merlin"
"google/tacotron" -> "Kyubyong/expressive_tacotron"
"google/tacotron" -> "google/sparrowhawk"
"google/tacotron" -> "xiph/LPCNet"
"google/tacotron" -> "jinhan/tacotron2-vae"
"google/tacotron" -> "jxzhanggg/nonparaSeq2seqVC_code"
"google/tacotron" -> "syang1993/gst-tacotron"
"google/tacotron" -> "NVIDIA/mellotron"
"google/tacotron" -> "barronalex/Tacotron"
"google/tacotron" -> "Rayhane-mamah/Tacotron-2"
"google/tacotron" -> "Kyubyong/deepvoice3"
"lennes/spect" -> "mauriciofigueroa/praatSublimeSyntax"
"persephone-tools/persephone" -> "CoEDL/elpis"
"persephone-tools/persephone" -> "alexis-michaud/na"
"mphilli/English-to-IPA" -> "aparrish/pronouncingpy"
"mphilli/English-to-IPA" -> "menelik3/cmudict-ipa"
"mphilli/English-to-IPA" -> "dmort27/epitran"
"mphilli/English-to-IPA" -> "open-dict-data/ipa-dict"
"mphilli/English-to-IPA" -> "pettarin/ipapy"
"mphilli/English-to-IPA" -> "cmusphinx/cmudict"
"mphilli/English-to-IPA" -> "YuanGongND/gopt" ["e"=1]
"mphilli/English-to-IPA" -> "dmort27/panphon"
"mphilli/English-to-IPA" -> "JoseLlarena/Britfone"
"mphilli/English-to-IPA" -> "hbuschme/TextGridTools"
"mphilli/English-to-IPA" -> "spring-media/DeepPhonemizer"
"ryokamoi/ppg_vc" -> "Kyubyong/cross_vc"
"ryokamoi/ppg_vc" -> "guanlongzhao/fac-via-ppg"
"ryokamoi/ppg_vc" -> "JeremyCCHsu/vae-npvc"
"ryokamoi/ppg_vc" -> "andi611/ZeroSpeech-TTS-without-T"
"volatileee/FacialPulse" -> "muzixingyun/LI-FPN"
"YannickJadoul/Parselmouth" -> "praat/praat"
"YannickJadoul/Parselmouth" -> "timmahrt/praatIO"
"YannickJadoul/Parselmouth" -> "drfeinberg/PraatScripts"
"YannickJadoul/Parselmouth" -> "MontrealCorpusTools/Montreal-Forced-Aligner"
"YannickJadoul/Parselmouth" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"YannickJadoul/Parselmouth" -> "kan-bayashi/ParallelWaveGAN"
"YannickJadoul/Parselmouth" -> "Shahabks/myprosody"
"YannickJadoul/Parselmouth" -> "google/REAPER"
"YannickJadoul/Parselmouth" -> "marl/crepe" ["e"=1]
"YannickJadoul/Parselmouth" -> "bootphon/phonemizer"
"YannickJadoul/Parselmouth" -> "kylebgorman/textgrid"
"YannickJadoul/Parselmouth" -> "jik876/hifi-gan"
"YannickJadoul/Parselmouth" -> "NVIDIA/BigVGAN" ["e"=1]
"YannickJadoul/Parselmouth" -> "wenet-e2e/speech-synthesis-paper"
"YannickJadoul/Parselmouth" -> "jcvasquezc/DisVoice"
"jcvasquezc/DisVoice" -> "jcvasquezc/NeuroSpeech"
"jcvasquezc/DisVoice" -> "Shahabks/myprosody"
"jcvasquezc/DisVoice" -> "Shahabks/my-voice-analysis"
"jcvasquezc/DisVoice" -> "timmahrt/praatIO"
"jcvasquezc/DisVoice" -> "drfeinberg/PraatScripts"
"jcvasquezc/DisVoice" -> "novoic/surfboard" ["e"=1]
"jcvasquezc/DisVoice" -> "audeering/opensmile-python"
"jcvasquezc/DisVoice" -> "r9y9/pysptk"
"jcvasquezc/DisVoice" -> "numediart/EmoV-DB" ["e"=1]
"jcvasquezc/DisVoice" -> "YannickJadoul/Parselmouth"
"jcvasquezc/DisVoice" -> "Helsinki-NLP/prosody"
"jcvasquezc/DisVoice" -> "lochenchou/MOSNet" ["e"=1]
"jcvasquezc/DisVoice" -> "jcvasquezc/phonet"
"keithito/tacotron" -> "Rayhane-mamah/Tacotron-2"
"keithito/tacotron" -> "Kyubyong/tacotron"
"keithito/tacotron" -> "r9y9/wavenet_vocoder"
"keithito/tacotron" -> "NVIDIA/tacotron2"
"keithito/tacotron" -> "fatchord/WaveRNN"
"keithito/tacotron" -> "NVIDIA/waveglow"
"keithito/tacotron" -> "r9y9/deepvoice3_pytorch"
"keithito/tacotron" -> "CSTR-Edinburgh/merlin"
"keithito/tacotron" -> "ibab/tensorflow-wavenet"
"keithito/tacotron" -> "syang1993/gst-tacotron"
"keithito/tacotron" -> "begeekmyfriend/tacotron"
"keithito/tacotron" -> "kan-bayashi/ParallelWaveGAN"
"keithito/tacotron" -> "xcmyz/FastSpeech"
"keithito/tacotron" -> "TensorSpeech/TensorFlowTTS"
"keithito/tacotron" -> "mozilla/TTS"
"uiuc-sst/g2ps" -> "timmahrt/pysle"
"uiuc-sst/g2ps" -> "AdolfVonKleist/Phonetisaurus"
"Kyubyong/tacotron" -> "keithito/tacotron"
"Kyubyong/tacotron" -> "Rayhane-mamah/Tacotron-2"
"Kyubyong/tacotron" -> "barronalex/Tacotron"
"Kyubyong/tacotron" -> "CSTR-Edinburgh/merlin"
"Kyubyong/tacotron" -> "Kyubyong/dc_tts"
"Kyubyong/tacotron" -> "Kyubyong/deepvoice3"
"Kyubyong/tacotron" -> "r9y9/wavenet_vocoder"
"Kyubyong/tacotron" -> "ibab/tensorflow-wavenet"
"Kyubyong/tacotron" -> "tomlepaine/fast-wavenet"
"Kyubyong/tacotron" -> "r9y9/deepvoice3_pytorch"
"Kyubyong/tacotron" -> "fatchord/WaveRNN"
"Kyubyong/tacotron" -> "buriburisuri/speech-to-text-wavenet"
"Kyubyong/tacotron" -> "NVIDIA/waveglow"
"Kyubyong/tacotron" -> "andabi/deep-voice-conversion"
"Kyubyong/tacotron" -> "NVIDIA/tacotron2"
"kykiefer/depression-detect" -> "talhanai/redbud-tree-depression"
"kykiefer/depression-detect" -> "sukesh167/Depression-Detection-in-speech"
"kykiefer/depression-detect" -> "adbailey1/DepAudioNet_reproduction"
"kykiefer/depression-detect" -> "speechandlanguageprocessing/ICASSP2022-Depression"
"kykiefer/depression-detect" -> "derong97/automatic-depression-detector"
"kykiefer/depression-detect" -> "adbailey1/daic_woz_process"
"kykiefer/depression-detect" -> "chanjunweimy/FYP_Submission"
"kykiefer/depression-detect" -> "halolimat/Social-media-Depression-Detector"
"kykiefer/depression-detect" -> "niquejoe/Classification-of-Depression-on-Social-Media-Using-Text-Mining"
"kykiefer/depression-detect" -> "Jackustc/Question-Level-Feature-Extraction-on-DAIC-WOZ-dataset"
"kykiefer/depression-detect" -> "AudioVisualEmotionChallenge/AVEC2019"
"kykiefer/depression-detect" -> "Akshat2430/Detecting-Depression-From-Speech-Signals"
"kykiefer/depression-detect" -> "PingCheng-Wei/DepressionEstimation"
"kykiefer/depression-detect" -> "isrugeek/depression_detection"
"kykiefer/depression-detect" -> "RicherMans/text_based_depression"
"Unisound/SampleRNN" -> "richardassar/SampleRNN_torch"
"Unisound/SampleRNN" -> "soroushmehr/sampleRNN_ICLR2017"
"Unisound/SampleRNN" -> "szcom/samplernn"
"Unisound/SampleRNN" -> "deepsound-project/samplernn-pytorch"
"deepsound-project/samplernn-pytorch" -> "soroushmehr/sampleRNN_ICLR2017"
"deepsound-project/samplernn-pytorch" -> "richardassar/SampleRNN_torch"
"deepsound-project/samplernn-pytorch" -> "deepsound-project/pggan-pytorch"
"deepsound-project/samplernn-pytorch" -> "Unisound/SampleRNN"
"deepsound-project/samplernn-pytorch" -> "gcunhase/samplernn-pytorch"
"deepsound-project/samplernn-pytorch" -> "Cortexelus/dadabots_sampleRNN"
"deepsound-project/samplernn-pytorch" -> "rncm-prism/prism-samplernn"
"deepsound-project/samplernn-pytorch" -> "NVIDIA/nv-wavenet"
"auDeep/auDeep" -> "DeepSpectrum/DeepSpectrum"
"auDeep/auDeep" -> "end2you/end2you"
"auDeep/auDeep" -> "openXBOW/openXBOW"
"r9y9/nnmnkwii" -> "r9y9/nnmnkwii_gallery"
"r9y9/nnmnkwii" -> "r9y9/pysptk"
"r9y9/nnmnkwii" -> "CSTR-Edinburgh/merlin"
"r9y9/nnmnkwii" -> "r9y9/gantts"
"r9y9/nnmnkwii" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"r9y9/nnmnkwii" -> "kan-bayashi/PytorchWaveNetVocoder"
"r9y9/nnmnkwii" -> "jxzhanggg/nonparaSeq2seqVC_code"
"r9y9/nnmnkwii" -> "nnsvs/nnsvs" ["e"=1]
"r9y9/nnmnkwii" -> "mmorise/World"
"r9y9/nnmnkwii" -> "soobinseo/Transformer-TTS"
"r9y9/nnmnkwii" -> "syang1993/gst-tacotron"
"r9y9/nnmnkwii" -> "seungwonpark/melgan"
"r9y9/nnmnkwii" -> "seaniezhao/torch_npss" ["e"=1]
"r9y9/nnmnkwii" -> "liusongxiang/StarGAN-Voice-Conversion"
"r9y9/nnmnkwii" -> "kan-bayashi/ParallelWaveGAN"
"JeremyCCHsu/vae-npvc" -> "unilight/cdvae-vc"
"JeremyCCHsu/vae-npvc" -> "jxzhanggg/nonparaSeq2seqVC_code"
"JeremyCCHsu/vae-npvc" -> "ryokamoi/ppg_vc"
"JeremyCCHsu/vae-npvc" -> "Kyubyong/cross_vc"
"JeremyCCHsu/vae-npvc" -> "JeremyCCHsu/vc-vawgan"
"JeremyCCHsu/vae-npvc" -> "JeremyCCHsu/vqvae-speech"
"JeremyCCHsu/vae-npvc" -> "guanlongzhao/fac-via-ppg"
"JeremyCCHsu/vae-npvc" -> "albertaparicio/tfg-voice-conversion"
"JeremyCCHsu/vae-npvc" -> "jjery2243542/voice_conversion"
"yistLin/H264-Encoder" -> "grtzsohalf/buy_vs_rent_and_invest"
"yistLin/H264-Encoder" -> "cyhuang-tw/AutoVC"
"liangstein/Chinese-speech-to-text" -> "CynthiaSuwi/ASR-for-Chinese-Pipeline"
"junzew/HanTTS" -> "Jackiexiao/MTTS"
"junzew/HanTTS" -> "hgneng/ekho"
"junzew/HanTTS" -> "alexram1313/text-to-speech-sample"
"junzew/HanTTS" -> "JasonWei512/Tacotron-2-Chinese"
"k2kobayashi/sprocket" -> "liusongxiang/StarGAN-Voice-Conversion"
"k2kobayashi/sprocket" -> "JeremyCCHsu/Python-Wrapper-for-World-Vocoder"
"k2kobayashi/sprocket" -> "r9y9/gantts"
"k2kobayashi/sprocket" -> "jjery2243542/adaptive_voice_conversion"
"k2kobayashi/sprocket" -> "jxzhanggg/nonparaSeq2seqVC_code"
"k2kobayashi/sprocket" -> "leimao/Voice-Converter-CycleGAN"
"k2kobayashi/sprocket" -> "kan-bayashi/PytorchWaveNetVocoder"
"k2kobayashi/sprocket" -> "k2kobayashi/crank"
"k2kobayashi/sprocket" -> "jjery2243542/voice_conversion"
"k2kobayashi/sprocket" -> "mmorise/World"
"k2kobayashi/sprocket" -> "r9y9/pysptk"
"k2kobayashi/sprocket" -> "auspicious3000/autovc"
"k2kobayashi/sprocket" -> "JeremyCCHsu/vae-npvc"
"k2kobayashi/sprocket" -> "kan-bayashi/ParallelWaveGAN"
"k2kobayashi/sprocket" -> "hujinsen/StarGAN-Voice-Conversion"
"MycroftAI/padatious" -> "MycroftAI/adapt"
"MycroftAI/padatious" -> "deepmipt/intent_classifier"
"npuichigo/voicenet" -> "npuichigo/grpc_gateway_demo"
"npuichigo/voicenet" -> "npuichigo/ttsflow"
"forslund/spotify-skill" -> "forslund/gcalendar_skill"
"AshwanthRamji/Depression-Sentiment-Analysis-with-Twitter-Data" -> "viritaromero/Detecting-Depression-in-Tweets"
"AshwanthRamji/Depression-Sentiment-Analysis-with-Twitter-Data" -> "peijoy/DetectDepressionInTwitterPosts"
"r9y9/gantts" ["l"="37.248,2.558"]
"leimao/Voice-Converter-CycleGAN" ["l"="37.272,2.645"]
"k2kobayashi/sprocket" ["l"="37.249,2.601"]
"r9y9/nnmnkwii" ["l"="37.242,2.529"]
"liusongxiang/StarGAN-Voice-Conversion" ["l"="37.285,2.615"]
"yanggeng1995/GAN-TTS" ["l"="37.346,2.507"]
"CSTR-Edinburgh/merlin" ["l"="37.187,2.493"]
"JeremyCCHsu/Python-Wrapper-for-World-Vocoder" ["l"="37.244,2.49"]
"r9y9/wavenet_vocoder" ["l"="37.174,2.512"]
"r9y9/pysptk" ["l"="37.241,2.455"]
"NVIDIA/mellotron" ["l"="37.278,2.509"]
"andabi/parallel-wavenet-vocoder" ["l"="37.316,2.584"]
"kan-bayashi/ParallelWaveGAN" ["l"="37.256,2.466"]
"jjery2243542/voice_conversion" ["l"="37.279,2.658"]
"ksw0306/FloWaveNet" ["l"="37.269,2.56"]
"r9y9/deepvoice3_pytorch" ["l"="37.171,2.532"]
"xcmyz/FastSpeech" ["l"="37.273,2.458"]
"ming024/FastSpeech2" ["l"="37.269,2.404"]
"soobinseo/Transformer-TTS" ["l"="37.295,2.517"]
"wenet-e2e/speech-synthesis-paper" ["l"="37.286,2.43"]
"syang1993/gst-tacotron" ["l"="37.29,2.503"]
"xiph/LPCNet" ["l"="37.232,2.476"]
"seungwonpark/melgan" ["l"="37.302,2.495"]
"jaywalnut310/glow-tts" ["l"="37.317,2.458"]
"KinglittleQ/GST-Tacotron" ["l"="37.326,2.532"]
"descriptinc/melgan-neurips" ["l"="37.251,2.513"]
"speechio/chinese_text_normalization" ["l"="35.722,2.397"]
"NVIDIA/waveglow" ["l"="37.21,2.503"]
"MontrealCorpusTools/Montreal-Forced-Aligner" ["l"="37.253,2.37"]
"fatchord/WaveRNN" ["l"="37.204,2.474"]
"facebookresearch/music-translation" ["l"="37.217,2.672"]
"sumuzhao/CycleGAN-Music-Style-Transfer" ["l"="37.16,2.789"]
"joansj/blow" ["l"="37.297,2.645"]
"NVIDIA/nv-wavenet" ["l"="37.211,2.554"]
"marcoppasini/MelGAN-VC" ["l"="37.297,2.69"]
"jason9693/MusicTransformer-tensorflow2.0" ["l"="38.753,4.113"]
"gabolsgabs/DALI" ["l"="38.623,4.07"]
"mairaksi/PiENet" ["l"="37.201,2.777"]
"DmitryUlyanov/neural-style-audio-tf" ["l"="37.126,2.745"]
"ksw0306/ClariNet" ["l"="37.314,2.566"]
"huangsicong/TimbreTron" ["l"="37.183,2.745"]
"MTG/WGANSing" ["l"="38.558,2.318"]
"jordipons/music-audio-tagging-at-scale-models" ["l"="38.359,4.119"]
"pndurette/gTTS" ["l"="37.02,2.468"]
"nateshmbhat/pyttsx3" ["l"="35.267,2.375"]
"Uberi/speech_recognition" ["l"="35.342,2.39"]
"RapidWareTech/pyttsx" ["l"="36.918,2.357"]
"ssut/py-googletrans" ["l"="52.858,26.109"]
"mozilla/TTS" ["l"="37.109,2.441"]
"jiaaro/pydub" ["l"="35.301,2.29"]
"readbeyond/aeneas" ["l"="37.13,2.372"]
"TaylorSMarks/playsound" ["l"="-1.212,2.255"]
"marytts/marytts" ["l"="37.062,2.41"]
"buriburisuri/speech-to-text-wavenet" ["l"="37.042,2.54"]
"rany2/edge-tts" ["l"="38.519,1.512"]
"goldsmith/Wikipedia" ["l"="-1.128,-42.16"]
"desbma/GoogleSpeech" ["l"="36.95,2.502"]
"Kyubyong/tacotron" ["l"="37.134,2.54"]
"quadrismegistus/prosodic" ["l"="37.426,2.227"]
"quadrismegistus/poesy" ["l"="37.482,2.19"]
"Helsinki-NLP/prosody" ["l"="37.354,2.338"]
"manexagirrezabal/zeuscansion" ["l"="37.463,2.192"]
"quadrismegistus/litlab-poetry" ["l"="37.459,2.211"]
"hyperreality/Poetry-Tools" ["l"="37.431,2.17"]
"a-coles/deep-rhyme-detection" ["l"="37.477,2.209"]
"jproft/Scandroid" ["l"="37.449,2.197"]
"coqui-ai/TTS" ["l"="38.433,1.412"]
"TensorSpeech/TensorFlowTTS" ["l"="37.227,2.428"]
"NVIDIA/tacotron2" ["l"="37.171,2.451"]
"mozilla/DeepSpeech" ["l"="35.353,2.34"]
"keithito/tacotron" ["l"="37.153,2.502"]
"espnet/espnet" ["l"="35.458,2.306"]
"Rayhane-mamah/Tacotron-2" ["l"="37.17,2.478"]
"neonbjb/tortoise-tts" ["l"="38.592,1.538"]
"jaywalnut310/vits" ["l"="38.324,1.753"]
"espeak-ng/espeak-ng" ["l"="38.548,1.545"]
"niquejoe/Classification-of-Depression-on-Social-Media-Using-Text-Mining" ["l"="37.21,1.859"]
"isrugeek/depression_detection" ["l"="37.188,1.88"]
"halolimat/Social-media-Depression-Detector" ["l"="37.204,1.887"]
"AshwanthRamji/Depression-Sentiment-Analysis-with-Twitter-Data" ["l"="37.232,1.781"]
"jinhan/tacotron2-gst" ["l"="37.266,2.626"]
"jinhan/tacotron2-vae" ["l"="37.337,2.566"]
"rishikksh20/vae_tacotron2" ["l"="37.357,2.546"]
"Wendison/VQMIVC" ["l"="37.33,2.615"]
"nii-yamagishilab/multi-speaker-tacotron" ["l"="37.33,2.509"]
"keonlee9420/Parallel-Tacotron2" ["l"="37.395,2.486"]
"yanggeng1995/vae_tacotron" ["l"="37.408,2.564"]
"rishikksh20/gmvae_tacotron" ["l"="37.419,2.556"]
"ide8/tacotron2" ["l"="37.382,2.573"]
"jjery2243542/adaptive_voice_conversion" ["l"="37.302,2.614"]
"keonlee9420/STYLER" ["l"="38.438,2.273"]
"auspicious3000/autovc" ["l"="37.279,2.592"]
"jxzhanggg/nonparaSeq2seqVC_code" ["l"="37.312,2.642"]
"auspicious3000/SpeechSplit" ["l"="37.295,2.557"]
"yistLin/FragmentVC" ["l"="37.317,2.689"]
"KimythAnly/AGAIN-VC" ["l"="37.334,2.663"]
"bshall/ZeroSpeech" ["l"="37.331,2.638"]
"liusongxiang/ppg-vc" ["l"="37.344,2.624"]
"cyhuang-tw/AdaIN-VC" ["l"="37.307,2.727"]
"ericwudayi/SkipVQVC" ["l"="37.317,2.673"]
"Hiroshiba/realtime-yukarin" ["l"="37.205,2.7"]
"Hiroshiba/yukarin" ["l"="37.174,2.703"]
"Hiroshiba/become-yukarin" ["l"="37.187,2.655"]
"k2kobayashi/crank" ["l"="37.322,2.649"]
"mmorise/kiritan_singing" ["l"="37.161,2.73"]
"OpenVoiceOS/ovos-buildroot" ["l"="36.789,2.496"]
"JarbasHiveMind/ZZZ-HiveMind-core" ["l"="36.767,2.486"]
"MycroftAI/mycroft-gui" ["l"="36.861,2.482"]
"OpenVoiceOS/ovos-personal-backend" ["l"="36.807,2.509"]
"ChanceNCounter/awesome-mycroft-community" ["l"="36.779,2.529"]
"OpenVoiceOS/ovos-core" ["l"="36.725,2.495"]
"alexisdiaz008/react-mycroft-gui" ["l"="36.807,2.476"]
"OpenVoiceOS/raspOVOS" ["l"="36.745,2.52"]
"OpenVoiceOS/ovos-installer" ["l"="36.719,2.515"]
"LinusSkucas/communications-skill" ["l"="36.78,2.471"]
"OpenVoiceOS/ovos-docker" ["l"="36.743,2.502"]
"MycroftAI/Precise-Community-Data" ["l"="36.82,2.49"]
"gras64/learning-skill" ["l"="36.793,2.479"]
"pcwii/mesh-skill" ["l"="36.749,2.541"]
"OpenVoiceOS/ovos-skill-fallback-chatgpt" ["l"="36.764,2.51"]
"el-tocino/localcroft" ["l"="36.764,2.529"]
"MattShannon/mcd" ["l"="37.519,2.649"]
"SamuelBroughton/Mel-Cepstral-Distortion" ["l"="37.519,2.682"]
"ttslr/python-MCD" ["l"="37.545,2.682"]
"thuhcsi/VAENAR-TTS" ["l"="37.388,2.541"]
"jik876/hifi-gan" ["l"="37.255,2.425"]
"NVIDIA/flowtron" ["l"="37.287,2.475"]
"Kyubyong/dc_tts" ["l"="37.195,2.527"]
"Kyubyong/deepvoice3" ["l"="37.219,2.573"]
"Kyubyong/speaker_adapted_tts" ["l"="37.201,2.615"]
"parente/espeakbox" ["l"="36.864,2.318"]
"ibab/tensorflow-wavenet" ["l"="37.086,2.543"]
"tomlepaine/fast-wavenet" ["l"="37.112,2.568"]
"kan-bayashi/PytorchWaveNetVocoder" ["l"="37.261,2.543"]
"opensourceteams/google-sdk-speech-to-text" ["l"="37.673,2.264"]
"Renovamen/Speech-and-Text" ["l"="37.64,2.302"]
"kylebgorman/textgrid" ["l"="37.221,2.279"]
"hbuschme/TextGridTools" ["l"="37.224,2.24"]
"timmahrt/praatIO" ["l"="37.195,2.253"]
"YannickJadoul/Parselmouth" ["l"="37.219,2.299"]
"begeekmyfriend/tacotron" ["l"="37.278,2.492"]
"begeekmyfriend/Tacotron-2" ["l"="37.348,2.52"]
"Jackiexiao/MTTS" ["l"="37.337,2.437"]
"atomicoo/tacotron2-mandarin" ["l"="37.448,2.43"]
"JasonWei512/Tacotron-2-Chinese" ["l"="37.391,2.448"]
"zuoxiang95/tacotron-1" ["l"="37.194,2.594"]
"open-speech/speech-aligner" ["l"="37.336,2.384"]
"Kyubyong/g2pC" ["l"="37.392,2.363"]
"aishell-foundation/DaCiDian" ["l"="35.668,2.381"]
"KuangDD/phkit" ["l"="37.377,2.389"]
"kakaobrain/g2pm" ["l"="37.36,2.376"]
"KuangDD/zhvoice" ["l"="37.373,2.408"]
"open-speech/cn-text-normalizer" ["l"="37.415,2.332"]
"thuhcsi/Crystal" ["l"="37.409,2.374"]
"npuichigo/waveglow" ["l"="37.327,2.554"]
"XiaoMi/kaldi-onnx" ["l"="35.702,2.408"]
"speechio/BigCiDian" ["l"="35.733,2.365"]
"thu-spmi/CAT" ["l"="35.708,2.337"]
"andabi/deep-voice-conversion" ["l"="37.144,2.58"]
"MrNothing/AI-Blocks" ["l"="45.615,29.223"]
"yunjey/stargan" ["l"="45.699,29.098"]
"oarriaga/face_classification" ["l"="55.996,27.258"]
"junyanz/iGAN" ["l"="45.716,29.249"]
"hujinsen/pytorch-StarGAN-VC" ["l"="37.297,2.659"]
"hujinsen/StarGAN-Voice-Conversion" ["l"="37.283,2.673"]
"SamuelBroughton/StarGAN-Voice-Conversion-2" ["l"="37.336,2.677"]
"jackaduma/CycleGAN-VC2" ["l"="37.343,2.595"]
"TaiChunYen/Pytorch-CycleGAN-VC2" ["l"="37.339,2.716"]
"Oscarshu0719/pytorch-StarGAN-VC2" ["l"="37.307,2.704"]
"bigpon/vcc20_baseline_cyclevae" ["l"="37.312,2.662"]
"auspicious3000/AutoPST" ["l"="37.346,2.642"]
"jaywalnut310/waveglow-vqvae" ["l"="37.358,2.68"]
"MycroftAI/mimic2" ["l"="37.064,2.438"]
"MycroftAI/mimic-recording-studio" ["l"="37.141,2.343"]
"MycroftAI/mimic1" ["l"="36.973,2.424"]
"MycroftAI/selene-backend" ["l"="36.908,2.383"]
"google/voice-builder" ["l"="37.161,2.423"]
"MycroftAI/adapt" ["l"="36.901,2.421"]
"thorstenMueller/Thorsten-Voice" ["l"="37.292,2.332"]
"spring-media/TransformerTTS" ["l"="37.296,2.454"]
"rishikksh20/VocGAN" ["l"="37.39,2.502"]
"tiberiu44/TTS-Cube" ["l"="37.303,2.578"]
"bfs18/nsynth_wavenet" ["l"="37.324,2.597"]
"mkotha/WaveRNN" ["l"="37.331,2.585"]
"h-meru/Tacotron-WaveRNN" ["l"="37.381,2.597"]
"geneing/WaveRNN-Pytorch" ["l"="37.368,2.601"]
"erogol/FFTNet" ["l"="37.356,2.697"]
"G-Wang/WaveRNN-Pytorch" ["l"="37.385,2.611"]
"azraelkuan/parallel_wavenet_vocoder" ["l"="37.306,2.592"]
"syang1993/FFTNet" ["l"="37.302,2.675"]
"SforAiDl/Neural-Voice-Cloning-With-Few-Samples" ["l"="37.313,2.54"]
"Sharad24/Neural-Voice-Cloning-with-Few-Samples" ["l"="37.392,2.557"]
"deterministic-algorithms-lab/Cross-Lingual-Voice-Cloning" ["l"="37.373,2.533"]
"IEEE-NITK/Neural-Voice-Cloning" ["l"="37.435,2.575"]
"Tomiinek/Multilingual_Text_to_Speech" ["l"="37.324,2.474"]
"vlomme/Multi-Tacotron-Voice-Cloning" ["l"="37.364,2.482"]
"nii-yamagishilab/self-attention-tacotron" ["l"="37.547,2.602"]
"nii-yamagishilab/tacotron2" ["l"="37.587,2.611"]
"numediart/MBROLA" ["l"="36.988,2.25"]
"numediart/MBROLA-voices" ["l"="37.01,2.258"]
"numediart/MBROLATOR" ["l"="36.981,2.227"]
"hadware/voxpopuli" ["l"="36.954,2.228"]
"chrisdonahue/wavegan" ["l"="37.13,2.478"]
"mostafaelaraby/wavegan-pytorch" ["l"="37.003,2.425"]
"magenta/ddsp" ["l"="38.563,4.061"]
"chaeyoung-lee/cwavegan" ["l"="37.01,2.403"]
"santi-pdp/segan" ["l"="36.707,4.44"]
"f90/Wave-U-Net" ["l"="36.634,4.374"]
"acids-ircam/RAVE" ["l"="38.789,1.976"]
"vincentherrmann/pytorch-wavenet" ["l"="37.124,2.522"]
"HaiFengZeng/clari_wavenet_vocoder" ["l"="37.493,2.628"]
"dhgrs/chainer-ClariNet" ["l"="37.557,2.655"]
"facebookresearch/SING" ["l"="37.441,2.666"]
"tuan3w/cnn_vocoder" ["l"="37.46,2.692"]
"mmorise/World" ["l"="37.212,2.451"]
"aliutkus/speechmetrics" ["l"="36.784,4.347"]
"ksw0306/WaveVAE" ["l"="37.354,2.609"]
"nii-yamagishilab/TSNetVocoder" ["l"="37.348,2.575"]
"npuichigo/voicenet" ["l"="37.375,2.37"]
"Kyubyong/expressive_tacotron" ["l"="37.261,2.578"]
"bshall/UniversalVocoding" ["l"="37.371,2.585"]
"tianrengao/SqueezeWave" ["l"="37.353,2.534"]
"cnlinxi/style-token_tacotron2" ["l"="37.482,2.604"]
"hrbigelow/ae-wavenet" ["l"="37.381,2.69"]
"swasun/VQ-VAE-Speech" ["l"="37.356,2.663"]
"DongyaoZhu/VQ-VAE-WaveNet" ["l"="37.394,2.71"]
"nii-yamagishilab/Extended_VQVAE" ["l"="38.387,2.429"]
"alokprasad/LPCTron" ["l"="37.404,2.598"]
"keonlee9420/Robust_Fine_Grained_Prosody_Control" ["l"="37.241,2.696"]
"CODEJIN/GST_Tacotron" ["l"="37.241,2.677"]
"s3prl/s3prl" ["l"="37.227,2.374"]
"speechbrain/speechbrain" ["l"="35.41,2.293"]
"k2-fsa/k2" ["l"="35.642,2.36"]
"wenet-e2e/wenet" ["l"="35.55,2.34"]
"ga642381/speech-trident" ["l"="38.479,2.021"]
"lhotse-speech/lhotse" ["l"="35.675,2.368"]
"asteroid-team/asteroid" ["l"="36.723,4.312"]
"facebookresearch/WavAugment" ["l"="38.519,3.981"]
"mravanelli/pytorch-kaldi" ["l"="35.563,2.316"]
"facebookresearch/encodec" ["l"="38.609,1.939"]
"TencentGameMate/chinese_speech_pretrain" ["l"="38.436,1.986"]
"coqui-ai/open-speech-corpora" ["l"="37.272,2.355"]
"jim-schwoebel/voice_datasets" ["l"="37.238,2.339"]
"SpeechColab/GigaSpeech" ["l"="35.705,2.368"]
"bootphon/phonemizer" ["l"="37.266,2.33"]
"freewym/espresso" ["l"="35.692,2.353"]
"syhw/wer_are_we" ["l"="35.584,2.32"]
"coqui-ai/TTS-papers" ["l"="37.329,2.408"]
"Kyubyong/css10" ["l"="37.322,2.428"]
"google/language-resources" ["l"="37.265,2.299"]
"sotelo/parrot" ["l"="37.174,2.584"]
"google/sparrowhawk" ["l"="37.327,2.323"]
"synesthesiam/opentts" ["l"="-14.357,-39.041"]
"festvox/festvox" ["l"="37.072,2.353"]
"shibing624/parrots" ["l"="37.543,2.353"]
"xxbb1234021/speech_recognition" ["l"="35.566,2.17"]
"CynthiaSuwi/ASR-for-Chinese-Pipeline" ["l"="37.599,2.338"]
"ranchlai/mandarin-tts" ["l"="37.431,2.398"]
"liangstein/Chinese-speech-to-text" ["l"="37.611,2.318"]
"kuangdd/ttskit" ["l"="37.459,2.389"]
"Z-yq/TensorflowTTS" ["l"="37.514,2.364"]
"yeyupiaoling/MASR" ["l"="35.622,2.188"]
"atomicoo/FCH-TTS" ["l"="37.477,2.401"]
"Z-yq/TensorflowASR" ["l"="35.651,2.249"]
"xdcesc/my_ch_speech_recognition" ["l"="35.556,2.136"]
"yeyupiaoling/PPASR" ["l"="35.615,2.21"]
"binzhouchn/masr" ["l"="-54.574,-14.53"]
"yeyupiaoling/PaddlePaddle-DeepSpeech" ["l"="35.607,2.047"]
"kensun0/Parallel-Wavenet" ["l"="37.356,2.59"]
"zhf459/P_wavenet_vocoder" ["l"="37.371,2.618"]
"festvox/festival" ["l"="37.048,2.353"]
"festvox/flite" ["l"="37.092,2.325"]
"festvox/speech_tools" ["l"="37.052,2.33"]
"festvox/datasets-CMU_Wilderness" ["l"="37.242,2.275"]
"isletennos/MMVC_Trainer" ["l"="4.571,-40.503"]
"pstuvwx/Deep_VoiceChanger" ["l"="37.194,2.68"]
"isletennos/MMVC_Client" ["l"="4.532,-40.504"]
"omegasisters/homepage" ["l"="5.96,-39.941"]
"yui540/satella.io" ["l"="-43.563,11.032"]
"VOICEVOX/voicevox" ["l"="6.03,-40.067"]
"hidefuku/AnimeEffects" ["l"="-32.961,-29.408"]
"ksasao/Gochiusearch" ["l"="5.889,-40.325"]
"BogiHsu/Tacotron2-PyTorch" ["l"="37.255,2.747"]
"ttaoREtw/Tacotron-pytorch" ["l"="37.231,2.772"]
"BogiHsu/WG-WaveNet" ["l"="37.255,2.82"]
"andi611/ZeroSpeech-TTS-without-T" ["l"="37.236,2.747"]
"keonlee9420/Comprehensive-Transformer-TTS" ["l"="38.46,2.255"]
"praat/praat" ["l"="37.195,2.279"]
"marl/crepe" ["l"="38.562,4.111"]
"stylerw/styler_praat_scripts" ["l"="37.126,2.179"]
"covarep/covarep" ["l"="37.194,2.163"]
"strob/gentle" ["l"="37.19,2.329"]
"pettarin/forced-alignment-tools" ["l"="37.211,2.345"]
"xinjli/allosaurus" ["l"="37.266,2.262"]
"feelins/Praat_Scripts" ["l"="37.126,2.209"]
"jckane/Voice_Analysis_Toolkit" ["l"="37.225,2.118"]
"adbailey1/DepAudioNet_reproduction" ["l"="37.166,1.942"]
"A2Zadeh/CMU-MultimodalSDK" ["l"="56.618,28.005"]
"AudioVisualEmotionChallenge/AVEC2019" ["l"="37.157,1.961"]
"google/REAPER" ["l"="37.185,2.36"]
"audeering/opensmile" ["l"="37.201,2.114"]
"kykiefer/depression-detect" ["l"="37.176,1.922"]
"ImperialCollegeLondon/sap-voicebox" ["l"="36.814,4.439"]
"thuiar/Cross-Modal-BERT" ["l"="56.598,28.054"]
"naxingyu/opensmile" ["l"="37.177,2.102"]
"audeering/opensmile-python" ["l"="37.197,2.138"]
"pranaymanocha/PerceptualAudio" ["l"="38.535,3.947"]
"jcvasquezc/DisVoice" ["l"="37.218,2.219"]
"happyalu/Flite-TTS-Engine-for-Android" ["l"="37.007,2.294"]
"dmort27/epitran" ["l"="37.287,2.257"]
"huakunyang/SummerTTS" ["l"="38.307,2.038"]
"Kyubyong/g2p" ["l"="37.292,2.362"]
"festvox/bard" ["l"="37.032,2.308"]
"AdolfVonKleist/Phonetisaurus" ["l"="37.286,2.279"]
"israelg99/deepvoice" ["l"="37.147,2.624"]
"riverphoenix/tacotron2" ["l"="37.173,2.632"]
"ttsunion/Deep-Expression" ["l"="37.235,2.653"]
"mazzzystar/WaveGAN-pytorch" ["l"="36.98,2.4"]
"auroracramer/wavegan" ["l"="36.959,2.398"]
"KunZhou9646/emotional-voice-conversion-with-CycleGAN-and-CWT-for-Spectrum-and-F0" ["l"="37.329,2.747"]
"mazzzystar/randomCNN-voice-transfer" ["l"="37.265,2.67"]
"pritishyuvraj/Voice-Conversion-GAN" ["l"="37.27,2.699"]
"JeremyCCHsu/vae-npvc" ["l"="37.285,2.714"]
"Zws-China/VoiceToWords" ["l"="37.674,2.308"]
"weineng-zhou/text2voice" ["l"="37.674,2.289"]
"wulee510505/Text2Speach" ["l"="37.697,2.288"]
"alishdipani/Neural-Style-Transfer-Audio" ["l"="37.181,2.764"]
"microsoft/DNS-Challenge" ["l"="36.768,4.346"]
"iver56/audiomentations" ["l"="38.484,3.982"]
"facebookresearch/denoiser" ["l"="36.764,4.31"]
"gabrielmittag/NISQA" ["l"="36.816,4.284"]
"nanahou/Awesome-Speech-Enhancement" ["l"="36.74,4.38"]
"microsoft/MS-SNSD" ["l"="36.768,4.364"]
"Yuan-ManX/ai-audio-datasets" ["l"="38.528,2.065"]
"JasonWei512/wavenet_vocoder" ["l"="37.441,2.444"]
"foamliu/Tacotron2-Mandarin" ["l"="37.434,2.423"]
"lturing/tacotronv2_wavernn_chinese" ["l"="37.399,2.435"]
"KuangDD/zhrtvc" ["l"="37.356,2.455"]
"iwater/Real-Time-Voice-Cloning-Chinese" ["l"="37.451,2.454"]
"MachineLP/TensorFlowTTS_chinese" ["l"="37.469,2.442"]
"aidreamwin/TTS-Clone-Chinese" ["l"="37.424,2.442"]
"bbepoch/CuteChineseTTS" ["l"="37.465,2.458"]
"MlWoo/LPCNet" ["l"="37.424,2.615"]
"alokprasad/lpctron-tts-cpp" ["l"="37.462,2.626"]
"llSourcell/Neural_Network_Voices" ["l"="36.98,2.808"]
"baidu-research/deep-voice" ["l"="37.039,2.745"]
"Shahabks/my-voice-analysis" ["l"="37.198,2.197"]
"Shahabks/myprosody" ["l"="37.197,2.224"]
"drfeinberg/PraatScripts" ["l"="37.183,2.219"]
"Shahabks/Speechat" ["l"="37.177,2.187"]
"end2you/end2you" ["l"="37.173,2.033"]
"tzirakis/Multimodal-Emotion-Recognition" ["l"="56.751,27.984"]
"auDeep/auDeep" ["l"="37.185,2.06"]
"openXBOW/openXBOW" ["l"="37.195,2.044"]
"DeepSpectrum/DeepSpectrum" ["l"="37.187,2.023"]
"begeekmyfriend/tacotron2" ["l"="37.412,2.539"]
"begeekmyfriend/WaveRNN" ["l"="37.401,2.517"]
"phoible/dev" ["l"="37.259,2.166"]
"unicode-cookbook/cookbook" ["l"="37.262,2.108"]
"dmort27/panphon" ["l"="37.295,2.21"]
"mlml/autovot" ["l"="37.174,2.169"]
"dmort27/allovera" ["l"="37.253,2.189"]
"jaekookang/p2fa_py3" ["l"="37.163,2.26"]
"prosodylab/Prosodylab-Aligner" ["l"="37.193,2.302"]
"JoFrhwld/FAVE" ["l"="37.147,2.241"]
"prosodylab/prosodylab.dictionaries" ["l"="37.163,2.278"]
"dopefishh/praatalign" ["l"="37.15,2.272"]
"mmcauliffe/Conch-sounds" ["l"="37.136,2.261"]
"tbright17/kaldi-dnn-ali-gop" ["l"="35.835,2.405"]
"nassosoassos/sail_align" ["l"="37.123,2.245"]
"asuni/wavelet_prosody_toolkit" ["l"="37.353,2.289"]
"BoragoCode/AttentionBasedProsodyPrediction" ["l"="37.402,2.346"]
"Daisyqk/Automatic-Prosody-Annotation" ["l"="37.407,2.284"]
"Riroaki/Chinese-Rhythm-Predictor" ["l"="37.416,2.299"]
"Zeqiang-Lai/Prosody_Prediction" ["l"="37.419,2.313"]
"bshall/VectorQuantizedCPC" ["l"="37.361,2.632"]
"hhguo/EA-SVC" ["l"="37.384,2.663"]
"guanlongzhao/fac-via-ppg" ["l"="37.322,2.711"]
"wnhsu/SpeechVAE" ["l"="37.426,2.846"]
"edchengg/generative_model_speech" ["l"="37.441,2.878"]
"wnhsu/FactorizedHierarchicalVAE" ["l"="37.391,2.772"]
"wnhsu/ScalableFHVAE" ["l"="37.425,2.822"]
"LearnedVector/A-Hackers-AI-Voice-Assistant" ["l"="35.674,3.284"]
"getalp/ALFFA_PUBLIC" ["l"="37.089,2.292"]
"L0SG/relational-rnn-pytorch" ["l"="37.408,2.649"]
"BogiHsu/Voice-Conversion" ["l"="37.241,2.727"]
"erogol/WaveRNN" ["l"="37.492,2.679"]
"yoyolicoris/pytorch_FFTNet" ["l"="37.533,2.713"]
"ivanvovk/durian-pytorch" ["l"="37.443,2.479"]
"facebookarchive/loop" ["l"="37.194,2.553"]
"xinjli/transphone" ["l"="37.282,2.197"]
"uiuc-sst/g2ps" ["l"="37.28,2.221"]
"xinjli/ucla-phonetic-corpus" ["l"="37.255,2.204"]
"FieldDB/Praat-Scripts" ["l"="37.022,2.084"]
"lennes/spect" ["l"="37.055,2.119"]
"chanjunweimy/FYP_Submission" ["l"="37.151,1.881"]
"RicherMans/text_based_depression" ["l"="37.2,1.928"]
"cmusphinx/g2p-seq2seq" ["l"="35.569,2.419"]
"MycroftAI/selene-ui" ["l"="36.881,2.37"]
"MycroftAI/personal-backend" ["l"="36.875,2.35"]
"ryokamoi/ppg_vc" ["l"="37.279,2.737"]
"HidekiKawahara/legacy_STRAIGHT" ["l"="37.173,2.383"]
"shuaijiang/STRAIGHT" ["l"="37.102,2.359"]
"acetylSv/GST-tacotron" ["l"="54.844,26.604"]
"L0SG/WaveFlow" ["l"="37.399,2.532"]
"viritaromero/Detecting-Depression-in-Tweets" ["l"="37.238,1.745"]
"peijoy/DetectDepressionInTwitterPosts" ["l"="37.245,1.762"]
"swcwang/depression-detection" ["l"="37.242,1.717"]
"JeremyCCHsu/vqvae-speech" ["l"="37.354,2.728"]
"yjlolo/vae-audio" ["l"="38.971,3.637"]
"facebookresearch/speech-resynthesis" ["l"="38.438,2.11"]
"sumuzhao/CycleGAN-Music-Style-Transfer-Refactorization" ["l"="37.141,2.832"]
"cifkao/groove2groove" ["l"="37.133,2.852"]
"brunnergino/MIDI-VAE" ["l"="38.716,4.07"]
"salu133445/lakh-pianoroll-dataset" ["l"="37.161,2.856"]
"YatingMusic/MuseMorphose" ["l"="38.709,4.169"]
"music-x-lab/POP909-Dataset" ["l"="38.713,4.137"]
"jason9693/MusicTransformer-pytorch" ["l"="38.747,4.132"]
"umbrellabeach/music-generation-with-DL" ["l"="1.45,11.935"]
"ChienYuLu/Play-As-You-Like-Timbre-Enhanced-Multi-modal-Music-Style-Transfer" ["l"="54.946,26.669"]
"twidddj/tf-wavenet_vocoder" ["l"="37.483,2.711"]
"azraelkuan/tensorflow_wavenet_vocoder" ["l"="37.414,2.67"]
"ivanvovk/WaveGrad" ["l"="37.371,2.499"]
"rhasspy/gruut" ["l"="37.341,2.36"]
"maum-ai/phaseaug" ["l"="38.439,2.287"]
"keonlee9420/StyleSpeech" ["l"="38.431,2.317"]
"A-Jacobson/tacotron2" ["l"="37.227,2.634"]
"m-toman/tacorn" ["l"="37.392,2.624"]
"CUNY-CL/wikipron" ["l"="37.29,2.236"]
"spring-media/DeepPhonemizer" ["l"="37.32,2.25"]
"roedoejet/g2p" ["l"="37.356,2.143"]
"lumaku/ctc-segmentation" ["l"="35.757,2.357"]
"lingjzhu/charsiu" ["l"="37.259,2.23"]
"lingjzhu/CharsiuG2P" ["l"="38.439,2.17"]
"open-dict-data/ipa-dict" ["l"="37.322,2.227"]
"selap91/Tacotron2" ["l"="37.147,2.676"]
"dhgrs/chainer-VQ-VAE" ["l"="37.614,2.682"]
"geneing/parallel_wavenet_vocoder" ["l"="37.591,2.669"]
"timmahrt/ProMo" ["l"="37.167,2.22"]
"r9y9/tacotron_pytorch" ["l"="37.285,2.575"]
"thuhcsi/tacotron" ["l"="37.407,2.611"]
"Yablon/auorange" ["l"="37.46,2.425"]
"XierHacker/Model_Fusion_Based_Prosody_Prediction" ["l"="37.436,2.322"]
"hjzin/PolyphoneDisambiguation" ["l"="53.146,27.525"]
"atomicoo/Tacotron2-PyTorch" ["l"="37.49,2.423"]
"ArwenFeng/tacotron_mandarin" ["l"="37.526,2.421"]
"wqt2019/tacotron-2_melgan" ["l"="37.499,2.435"]
"wqt2019/tacotron-2_wavernn" ["l"="37.513,2.432"]
"awesome-archive/tacotron_cn" ["l"="37.544,2.434"]
"talhanai/redbud-tree-depression" ["l"="37.187,1.908"]
"linlemn/DepressionDectection" ["l"="37.143,1.894"]
"AudioVisualEmotionChallenge/AVEC2018" ["l"="37.173,1.982"]
"azraelkuan/FFTNet" ["l"="37.482,2.751"]
"drfeinberg/Parselmouth-Guides" ["l"="37.153,2.162"]
"Voice-Lab/VoiceLab" ["l"="37.15,2.183"]
"voicesauce/opensauce-python" ["l"="37.112,2.196"]
"CoEDL/elpis" ["l"="37.49,1.966"]
"persephone-tools/persephone" ["l"="37.51,1.932"]
"onset/lameta" ["l"="37.486,1.939"]
"ReadAlongs/Studio" ["l"="37.428,2.051"]
"lgessler/glam" ["l"="37.521,1.958"]
"ardaillon/FCN-f0" ["l"="37.197,2.813"]
"bshall/Tacotron" ["l"="37.424,2.591"]
"yistLin/universal-vocoder" ["l"="37.314,2.751"]
"LeoniusChen/Attentions-in-Tacotron" ["l"="37.412,2.581"]
"kirbyj/praatsauce" ["l"="37.157,2.202"]
"Liu-Feng-deeplearning/TTS-frontend" ["l"="37.432,2.34"]
"rishikksh20/Phone-Level-Mixture-Density-Network-for-TTS" ["l"="37.473,2.585"]
"r9y9/nnmnkwii_gallery" ["l"="37.217,2.537"]
"ajinkyaT/CNN_Intent_Classification" ["l"="36.726,2.306"]
"deepmipt/intent_classifier" ["l"="36.766,2.333"]
"nrc-cnrc/gramble" ["l"="37.397,2.09"]
"EveryVoiceTTS/EveryVoice" ["l"="37.365,2.1"]
"tuanad121/Python-WORLD" ["l"="37.27,2.532"]
"cognibit/Text-Normalization-Demo" ["l"="37.49,2.237"]
"google-research-datasets/TextNormalizationCoveringGrammars" ["l"="37.431,2.265"]
"mozilla/DSAlign" ["l"="37.153,2.305"]
"mlcommons/peoples-speech" ["l"="37.092,2.247"]
"Jackustc/Question-Level-Feature-Extraction-on-DAIC-WOZ-dataset" ["l"="37.129,1.937"]
"AliceOTHMANI/EmoAudioNet" ["l"="37.1,1.925"]
"chuyuanli/MTL4Depr" ["l"="37.102,1.939"]
"CMDC-corpus/CMDC-Baseline" ["l"="37.147,1.937"]
"adbailey1/daic_woz_process" ["l"="37.153,1.92"]
"soobinseo/Tacotron-pytorch" ["l"="37.435,2.635"]
"soobinseo/bytenet_masked" ["l"="37.47,2.658"]
"KuangDD/aukit" ["l"="37.404,2.406"]
"Kyubyong/vq-vae" ["l"="37.394,2.792"]
"atomicoo/chn_text_norm" ["l"="37.458,2.233"]
"candlewill/CNTN" ["l"="37.407,2.262"]
"guanlongzhao/ppg-gmm" ["l"="37.322,2.772"]
"cjerry1243/TransferLearning-CLVC" ["l"="37.367,2.762"]
"BridgetteSong/ExpressiveTacotron" ["l"="38.421,2.355"]
"peak1995/tacotron-chinese" ["l"="37.571,2.435"]
"karamarieliu/gst_tacotron2_wavenet" ["l"="37.564,2.415"]
"JarbasHiveMind/HiveMind-voice-sat" ["l"="36.737,2.469"]
"vsimkus/vae-voice-conversion" ["l"="37.323,2.731"]
"Suhee05/Zerospeech2019" ["l"="37.319,2.789"]
"hon9g/Text-to-Color" ["l"="36.943,2.343"]
"zamlz/dlcampjeju2018-I2A-cube" ["l"="36.967,2.37"]
"jwkanggist/tpu-tutorial-experiment" ["l"="36.983,2.379"]
"Cortexelus/dadabots_sampleRNN" ["l"="37.003,2.694"]
"ZVK/sampleRNN_ICLR2017" ["l"="37.028,2.677"]
"inzva/Audio-Style-Transfer" ["l"="37.17,2.822"]
"dophist/DaCiDian-Develop" ["l"="37.402,2.236"]
"naxingyu/kaldi-gadgets" ["l"="37.411,2.245"]
"Kyubyong/cross_vc" ["l"="37.266,2.769"]
"unilight/cdvae-vc" ["l"="37.272,2.805"]
"acetylSv/non-parallel-rhythm-flexible-VC" ["l"="37.263,2.843"]
"tifgan/stftGAN" ["l"="37.337,2.898"]
"andimarafioti/tifresi" ["l"="37.341,2.929"]
"kastnerkyle/representation_mixing" ["l"="37.324,2.815"]
"fatchord/FFTNet" ["l"="37.441,2.755"]
"LouisYZK/dds-avec2019" ["l"="37.12,1.924"]
"ihp-lab/Avec2019_DDS" ["l"="37.13,1.947"]
"PingCheng-Wei/DepressionEstimation" ["l"="37.168,1.896"]
"onejiin/CycleGAN-VC2" ["l"="37.363,2.747"]
"MLSpeech/Dr.VOT" ["l"="37.146,2.131"]
"bottlecapper/EmoMUNIT" ["l"="37.36,2.823"]
"bottlecapper/EmoCycleGAN" ["l"="37.354,2.836"]
"ktho22/vctts" ["l"="37.348,2.805"]
"EliasCai/speech_recognition_ctc" ["l"="35.562,2.113"]
"prosegrinder/python-cmudict" ["l"="37.404,2.176"]
"prosegrinder/python-syllables" ["l"="37.438,2.136"]
"LingLing-Speech/mandarin-tts-frontend-python" ["l"="37.482,2.315"]
"IPS-LMU/EMU-webApp" ["l"="37.064,2.053"]
"IPS-LMU/emuR" ["l"="37.089,2.09"]
"xcmyz/DurIAN" ["l"="37.473,2.289"]
"Yangyangii/TPGST-Tacotron" ["l"="37.475,2.568"]
"maozhiqiang/wavernn" ["l"="37.429,2.655"]
"HimangiM/Depression_Detection" ["l"="37.126,1.903"]
"derong97/automatic-depression-detector" ["l"="37.151,1.91"]
"shauryr/google_text_normalization" ["l"="37.452,2.175"]
"rwsproat/text-normalization-data" ["l"="37.428,2.202"]
"german-asr/kaldi-german" ["l"="37.445,2.258"]
"bmilde/german-asr-lm-tools" ["l"="37.469,2.25"]
"ReadAlongs/Studio-Web" ["l"="37.439,2.026"]
"roedoejet/convertextract" ["l"="37.451,2.037"]
"AIIX/youtube-skill" ["l"="36.733,2.566"]
"IPS-LMU/wrassp" ["l"="37.078,2.07"]
"HappyBall/tacotron" ["l"="37.589,2.434"]
"MachineJeff/Chinese_Polyphone_Disambiguation" ["l"="37.49,2.275"]
"npujcong/Chinese_PSP" ["l"="37.455,2.299"]
"maozhiqiang/tacotron_cn" ["l"="37.466,2.673"]
"resemble-ai/Resemblyzer" ["l"="37.201,2.415"]
"wq2012/awesome-diarization" ["l"="37.021,3.203"]
"biggytruck/SpeechSplit2" ["l"="38.334,2.299"]
"auspicious3000/contentvec" ["l"="38.38,2.039"]
"maum-ai/cotatron" ["l"="37.365,2.516"]
"lifeiteng/vall-e" ["l"="38.527,1.915"]
"microsoft/NeuralSpeech" ["l"="38.531,1.994"]
"lucidrains/naturalspeech2-pytorch" ["l"="38.527,1.967"]
"shivammehta25/Matcha-TTS" ["l"="38.492,1.986"]
"NVIDIA/BigVGAN" ["l"="38.519,2.044"]
"LEEYOONHYUNG/BVAE-TTS" ["l"="37.418,2.522"]
"TensorSpeech/TensorFlowASR" ["l"="35.666,2.329"]
"as-ideas/ForwardTacotron" ["l"="37.32,2.492"]
"rishikksh20/FastSpeech2" ["l"="37.435,2.495"]
"rishikksh20/AdaSpeech" ["l"="37.462,2.509"]
"rishikksh20/TFGAN" ["l"="37.466,2.489"]
"HGU-DLLAB/Korean-FastSpeech2-Pytorch" ["l"="-5.18,-22.999"]
"tencent-ailab/bddm" ["l"="38.466,2.299"]
"zceng/LVCNet" ["l"="37.439,2.517"]
"dipjyoti92/SC-WaveRNN" ["l"="37.37,2.56"]
"ebadawy/voice_conversion" ["l"="37.408,2.758"]
"himajin2045/voice-conversion" ["l"="37.268,2.727"]
"lmnt-com/diffwave" ["l"="37.375,2.43"]
"lmnt-com/wavegrad" ["l"="37.415,2.471"]
"huawei-noah/Speech-Backbones" ["l"="38.463,2.133"]
"neillu23/CDiffuSE" ["l"="36.697,4.2"]
"Rongjiehuang/FastDiff" ["l"="38.482,2.216"]
"Rongjiehuang/ProDiff" ["l"="38.47,2.232"]
"sp-uhh/sgmse" ["l"="36.741,4.258"]
"facebookresearch/vocoder-benchmark" ["l"="38.384,2.364"]
"archinetai/audio-diffusion-pytorch" ["l"="38.697,1.983"]
"philsyn/DiffWave-Vocoder" ["l"="37.503,2.339"]
"descriptinc/descript-audio-codec" ["l"="38.554,2.011"]
"r4victor/syncabook" ["l"="37.054,2.292"]
"r4victor/afaligner" ["l"="37.031,2.274"]
"r4victor/synclibrivox" ["l"="37.038,2.258"]
"smoores-dev/storyteller" ["l"="37.025,2.243"]
"kanjieater/SubPlz" ["l"="-36.449,18.627"]
"AASHISHAG/deepspeech-german" ["l"="35.627,2.516"]
"german-asr/megs" ["l"="37.38,2.285"]
"repodiac/german_transliterate" ["l"="37.312,2.296"]
"uhh-lt/kaldi-tuda-de" ["l"="35.661,2.504"]
"monatis/german-tts" ["l"="37.324,2.304"]
"coqui-ai/TTS-recipes" ["l"="37.329,2.284"]
"adbar/German-NLP" ["l"="52.12,25.362"]
"NVIDIA/radtts" ["l"="38.463,2.205"]
"GitYCC/g2pW" ["l"="38.45,2.056"]
"xcmyz/FastVocoder" ["l"="37.547,2.376"]
"KunZhou9646/Speaker-independent-emotional-voice-conversion-based-on-conditional-VAW-GAN-and-CWT" ["l"="37.346,2.758"]
"KunZhou9646/seq2seq-EVC" ["l"="37.344,2.772"]
"patrickltobing/cyclevae-vc-neuralvoco" ["l"="37.339,2.694"]
"maum-ai/assem-vc" ["l"="38.439,2.362"]
"tts-tutorial/survey" ["l"="37.339,2.49"]
"zzw922cn/awesome-speech-recognition-speech-synthesis-papers" ["l"="35.52,2.286"]
"ddlBoJack/Speech-Resources" ["l"="38.441,2.095"]
"yangdongchao/AcademiCodec" ["l"="38.483,2.058"]
"p0p4k/vits2_pytorch" ["l"="38.436,2.032"]
"sequitur-g2p/sequitur-g2p" ["l"="37.343,2.237"]
"HawkAaron/warp-transducer" ["l"="35.689,2.295"]
"cmusphinx/cmudict" ["l"="37.35,2.258"]
"bbTomas/rPraat" ["l"="37.13,2.155"]
"Deepest-Project/AlignTTS" ["l"="37.453,2.526"]
"L0SG/NanoFlow" ["l"="37.499,2.566"]
"janvainer/speedyspeech" ["l"="37.385,2.479"]
"PaddlePaddle/Parakeet" ["l"="37.343,2.474"]
"maum-ai/wavegrad2" ["l"="38.433,2.343"]
"maum-ai/univnet" ["l"="38.422,2.299"]
"grtzsohalf/buy_vs_rent_and_invest" ["l"="37.29,2.818"]
"sukesh167/Depression-Detection-in-speech" ["l"="37.137,1.91"]
"Akshat2430/Detecting-Depression-From-Speech-Signals" ["l"="37.132,1.877"]
"notmanan/Depression-Detection-Through-Multi-Modal-Data" ["l"="37.119,1.886"]
"hariharitha21/Detection-of-Anxiety-and-Depression" ["l"="37.095,1.862"]
"Fancy-Block/ICASSP2022-Depression" ["l"="37.083,1.88"]
"sahasourav17/Student-Anxiety-and-Depression-Prediction" ["l"="37.092,1.843"]
"speechandlanguageprocessing/ICASSP2022-Depression" ["l"="37.154,1.894"]
"cosmaadrian/time-enriched-multimodal-depression-detection" ["l"="37.153,1.862"]
"gemelo-ai/vocos" ["l"="38.493,2.01"]
"athena-team/athena" ["l"="50.614,2.815"]
"thuhcsi/Crystal.TTVS" ["l"="37.482,2.336"]
"thuhcsi/FlatTN" ["l"="37.463,2.321"]
"Alexir/CMUdict" ["l"="37.37,2.217"]
"NeonGeckoCom/NeonCore" ["l"="36.694,2.506"]
"X-CCS/mandarin_tacotron_GL" ["l"="37.46,2.34"]
"cmusphinx/cmudict-tools" ["l"="35.33,2.562"]
"cmusphinx/sphinxbase" ["l"="35.347,2.516"]
"Kyubyong/pron_dictionaries" ["l"="37.398,2.21"]
"aparrish/pronouncingpy" ["l"="37.381,2.185"]
"mphilli/English-to-IPA" ["l"="37.331,2.206"]
"cifkao/ss-vq-vae" ["l"="37.115,2.901"]
"descriptinc/cargan" ["l"="38.453,2.275"]
"rendchevi/nix-tts" ["l"="38.473,2.279"]
"r9y9/pyreaper" ["l"="37.134,2.295"]
"dgaspari/pyrapt" ["l"="37.156,2.323"]
"ronggong/pypYIN" ["l"="37.115,2.305"]
"CSTR-Edinburgh/magphase" ["l"="37.129,2.319"]
"SJTMusicTeam/Muskits" ["l"="38.506,2.229"]
"sp-nitech/SPTK" ["l"="38.511,4.317"]
"r9y9/SPTK" ["l"="37.117,2.285"]
"nsu-ai/russian_g2p" ["l"="-44.134,25.953"]
"snakers4/open_stt" ["l"="-44.14,25.983"]
"sovaai/sova-tts" ["l"="-44.035,25.858"]
"resemble-ai/MelNet" ["l"="37.451,2.592"]
"Deepest-Project/MelNet" ["l"="37.446,2.557"]
"IMLHF/Real-Time-Voice-Cloning" ["l"="37.513,2.607"]
"rahulsharma-rks/DepressionDetection" ["l"="37.246,1.693"]
"jackaduma/CycleGAN-VC3" ["l"="37.375,2.673"]
"GANtastic3/MaskCycleGAN-VC" ["l"="37.37,2.653"]
"yl4579/StarGANv2-VC" ["l"="37.38,2.638"]
"KunZhou9646/controllable_evc_code" ["l"="37.351,2.786"]
"KunZhou9646/Emovox" ["l"="37.36,2.796"]
"CZ26/CycleTransGAN-EVC" ["l"="37.361,2.782"]
"glam-imperial/EmotionalConversionStarGAN" ["l"="37.339,2.794"]
"kingformatty/NUSD" ["l"="37.139,1.928"]
"noetits/ICE-Talk" ["l"="37.445,2.612"]
"yanggeng1995/EATS" ["l"="37.487,2.49"]
"mbinkowski/DeepSpeechDistances" ["l"="37.414,2.502"]
"mmorise/itako_singing" ["l"="37.144,2.765"]
"yistLin/human-evaluation" ["l"="37.306,2.782"]
"howard1337/S2VC" ["l"="37.302,2.76"]
"cyhuang-tw/AutoVC" ["l"="37.299,2.802"]
"ariacat3366/pytorch-StarGAN-VC2-implementation" ["l"="37.378,2.748"]
"Jackiexiao/zhtts" ["l"="37.448,2.369"]
"aidenwang9867/Weibo-User-Depression-Detection-Dataset" ["l"="37.136,1.837"]
"cyZhu98/Depression_Baseline" ["l"="37.133,1.813"]
"ethan-nicholas-tsai/SWDD" ["l"="37.123,1.792"]
"sunlightsgy/MDDL" ["l"="37.113,1.811"]
"jefflai108/pytorch-kaldi-neural-speaker-embeddings" ["l"="37.091,3.336"]
"CODEJIN/multi_speaker_tts" ["l"="37.429,2.537"]
"SamuelBroughton/StarGAN-Voice-Conversion" ["l"="37.435,2.687"]
"MingjieChen/LowResourceVC" ["l"="37.381,2.727"]
"dipjyoti92/StarGAN-Voice-Conversion-2" ["l"="37.37,2.716"]
"joongbo/tta" ["l"="37.51,2.53"]
"hwanheelee1993/MFMA" ["l"="37.493,2.529"]
"YunahJang/IterCQR" ["l"="37.494,2.547"]
"klee972/exec-filter" ["l"="37.485,2.533"]
"DongryeolLee96/AskCQ" ["l"="37.485,2.543"]
"hwanheelee1993/UMIC" ["l"="37.5,2.539"]
"hwanheelee1993/KPQA" ["l"="37.519,2.54"]
"minbeomkim/CriticControl" ["l"="37.511,2.55"]
"hyukhunkoh-ai/multi_view_zero_shot_open_intent_induction" ["l"="37.527,2.549"]
"jiwoohong93/ai_dep" ["l"="64.621,4.538"]
"tarepan/VoiceConversionLab" ["l"="37.282,2.755"]
"patrickltobing/cyclevae-vc" ["l"="37.34,2.733"]
"xcmyz/FastSpeech2" ["l"="37.514,2.294"]
"cywang97/StreamingTransformer" ["l"="35.726,2.329"]
"SongRongLee/mir-svc" ["l"="37.411,2.693"]
"xinjli/phonepiece" ["l"="37.268,2.179"]
"yerfor/SyntaSpeech" ["l"="38.486,2.31"]
"yanggeng1995/FB-MelGAN" ["l"="37.499,2.469"]
"santiagobarreda/FastTrack" ["l"="37.088,2.126"]
"ChristopherCarignan/formant-optimization" ["l"="37.071,2.102"]
"yanggeng1995/Multi-band-WaveRNN" ["l"="37.517,2.461"]
"entn-at/DurIAN-1" ["l"="37.482,2.469"]
"liusongxiang/efficient_tts" ["l"="37.524,2.481"]
"zzw922cn/LPC_for_TTS" ["l"="37.507,2.413"]
"stylerw/usingpraat" ["l"="37.098,2.152"]
"ZDisket/TensorVox" ["l"="37.726,2.345"]
"tulasiram58827/TTS_TFLite" ["l"="37.662,2.357"]
"YuvalBecker/MelNet" ["l"="37.517,2.578"]
"jaeyeun97/MelNet" ["l"="37.499,2.584"]
"HelloChatterbox/wikipedia_for_humans" ["l"="36.712,2.564"]
"ga642381/FastSpeech2" ["l"="37.543,2.495"]
"keonlee9420/Expressive-FastSpeech2" ["l"="38.405,2.27"]
"npuichigo/grpc_gateway_demo" ["l"="37.385,2.339"]
"npuichigo/ttsflow" ["l"="37.392,2.325"]
"timmahrt/pysle" ["l"="37.299,2.141"]
"Madoshakalaka/English-IPA" ["l"="37.305,2.101"]
"pohanchi/huggingface_albert" ["l"="37.287,2.836"]
"kichanll/Chinese_Prosody_Predict" ["l"="37.439,2.295"]
"lucidrains/audiolm-pytorch" ["l"="38.625,1.96"]
"sillsdev/aeneas-installer" ["l"="37.066,2.268"]
"wiseman/py-webrtcvad" ["l"="36.78,4.48"]
"JeffC0628/awesome-voice-conversion" ["l"="37.326,2.85"]
"hayeong0/Diff-HierVC" ["l"="38.299,2.068"]
"RussellSB/tt-vae-gan" ["l"="37.436,2.789"]
"BrightGu/SingleVC" ["l"="37.472,2.836"]
"cmusphinx/sphinx4" ["l"="35.376,2.5"]
"AndroidMaryTTS/AndroidMaryTTS" ["l"="36.91,2.288"]
"rhdunn/espeak" ["l"="36.977,2.342"]
"marytts/marytts-txt2wav" ["l"="37.017,2.377"]
"OlaWod/FreeVC" ["l"="38.425,2.072"]
"yl4579/PitchExtractor" ["l"="37.431,2.714"]
"yl4579/StyleTTS-VC" ["l"="37.465,2.724"]
"bshall/knn-vc" ["l"="38.355,2.089"]
"bshall/soft-vc" ["l"="38.367,2.076"]
"Edresson/YourTTS" ["l"="38.558,1.981"]
"yl4579/AuxiliaryASR" ["l"="37.448,2.706"]
"PlayVoice/vits_chinese" ["l"="38.331,1.913"]
"cnlinxi/book-text-to-speech" ["l"="38.467,2.116"]
"keonlee9420/VAENAR-TTS" ["l"="38.392,2.385"]
"keonlee9420/WaveGrad2" ["l"="37.745,2.398"]
"rishikksh20/UnivNet-pytorch" ["l"="37.716,2.396"]
"zlzhang1124/AcousticFeatureExtraction" ["l"="56.876,28.035"]
"declare-lab/MELD" ["l"="56.668,28.03"]
"ddlBoJack/emotion2vec" ["l"="38.422,2.038"]
"CheyneyComputerScience/CREMA-D" ["l"="31.904,30.466"]
"Demfier/multimodal-speech-emotion-recognition" ["l"="56.819,27.981"]
"aparrish/pycorpora" ["l"="34.222,23.988"]
"aparrish/rwet" ["l"="34.19,24.051"]
"aparrish/pronouncingjs" ["l"="37.412,2.132"]
"sidmulajkar/sentiment-predictor-for-stress-detection" ["l"="37.058,1.833"]
"AmirHoseein99/Depression-Engine" ["l"="37.116,1.852"]
"rishikksh20/Avocodo-pytorch" ["l"="38.466,2.361"]
"k2-fsa/libriheavy" ["l"="38.278,2.153"]
"ViEm-ccy/GEDLoss_pytorch" ["l"="37.494,2.513"]
"NeuralVox/OpenPhonemizer" ["l"="37.381,2.112"]
"ASR-project/Multilingual-PR" ["l"="37.312,2.186"]
"liusongxiang/Large-Audio-Models" ["l"="38.482,2.117"]
"maxrmorrison/torchcrepe" ["l"="38.45,2.205"]
"rishikksh20/Fre-GAN-pytorch" ["l"="37.65,2.39"]
"johndpope/Singing-Voice-Conversion-with-conditional-VAW-GAN" ["l"="37.432,2.733"]
"tuanh123789/AdaSpeech" ["l"="37.517,2.507"]
"rishikksh20/AdaSpeech2" ["l"="37.548,2.522"]
"tzuhsien/Voice-conversion-evaluation" ["l"="37.299,2.743"]
"KevinMIN95/StyleSpeech" ["l"="38.448,2.25"]
"NATSpeech/NATSpeech" ["l"="-54.941,-13.695"]
"nii-yamagishilab/project-NN-Pytorch-scripts" ["l"="38.166,2.422"]
"novoic/surfboard" ["l"="35.883,2.354"]
"soroushmehr/sampleRNN_ICLR2017" ["l"="37.098,2.626"]
"google/tacotron" ["l"="37.231,2.543"]
"Zeta36/tensorflow-tex-wavenet" ["l"="37.076,2.609"]
"google/yang_vocoder" ["l"="37.13,2.659"]
"cyZhu98/depression_papers" ["l"="37.141,1.955"]
"KunZhou9646/Mixed_Emotions" ["l"="38.343,2.26"]
"gayanechilingar/Change-Emotions" ["l"="37.375,2.811"]
"ChaoWANG0511/CycleGAN-VC3" ["l"="37.408,2.713"]
"atomicoo/PTTS-WebAPP" ["l"="37.538,2.4"]
"Tendol/Bo-Eng-Machine-Transation" ["l"="37.516,2.393"]
"rhasspy/gruut-ipa" ["l"="37.323,2.124"]
"pettarin/ipapy" ["l"="37.314,2.166"]
"xcmyz/ConvTasNet4BasisMelGAN" ["l"="37.587,2.373"]
"pohanchi/AALBERT" ["l"="37.309,2.825"]
"ysw1021/AGG" ["l"="37.541,2.573"]
"zerlinwang/synthetic-corpus-vocoder" ["l"="37.47,2.643"]
"ethan-nicholas-tsai/DepressionDetection" ["l"="37.116,1.772"]
"KimythAnly/qqdm" ["l"="37.285,2.874"]
"OpenVoiceOS/ovos-ww-plugin-vosk" ["l"="36.766,2.554"]
"modelscope/KAN-TTS" ["l"="38.361,2.044"]
"chenkui164/FastASR" ["l"="35.641,2.337"]
"RapidAI/RapidASR" ["l"="35.634,2.322"]
"funnyzak/tts-now" ["l"="45.563,-2.055"]
"innnky/emotional-vits" ["l"="38.289,1.879"]
"nnsvs/nnsvs" ["l"="38.544,2.218"]
"xinjli/alqalign" ["l"="37.287,2.121"]
"xinjli/asr2k" ["l"="37.276,2.139"]
"thuhcsi/NeuFA" ["l"="37.286,2.169"]
"pykaldi/pykaldi" ["l"="35.62,2.358"]
"MycroftAI/ZZZ-RETIRED__openstt" ["l"="36.918,2.404"]
"MycroftAI/padatious" ["l"="36.83,2.38"]
"MycroftAI/mycroft-skills" ["l"="36.866,2.438"]
"MycroftAI/mycroft-core" ["l"="35.805,3.154"]
"MycroftAI/ZZZ-RETIRED__mycroft-core-documentation" ["l"="36.919,2.437"]
"MycroftAI/hardware-mycroft-mark-1" ["l"="36.848,2.43"]
"MycroftAI/ZZZ-RETIRED__adapt-documentation" ["l"="36.891,2.398"]
"MycroftAI/Mycroft-Android" ["l"="36.847,2.455"]
"MycroftAI/linux" ["l"="36.875,2.398"]
"MycroftAI/ZZZ-RETIRED__pychromecast" ["l"="36.861,2.404"]
"MycroftAI/docker-mycroft" ["l"="36.899,2.447"]
"MycroftAI/enclosure-picroft" ["l"="36.883,2.468"]
"AIIX/Mycroft-AI-Gnome-Shell-Extension" ["l"="36.851,2.384"]
"nextDeve/Depression-detect-ResNet" ["l"="37.173,1.871"]
"cosmaadrian/multimodal-depression-from-video" ["l"="37.17,1.846"]
"HappyColor/SpeechFormer" ["l"="56.985,27.944"]
"YiwenShaoStephen/pychain" ["l"="35.716,2.322"]
"google-research-datasets/dakshina" ["l"="52.681,25.092"]
"ttslr/StrengthNet" ["l"="37.38,2.837"]
"benjaminwan/ChineseTtsTflite" ["l"="36.815,2.22"]
"tatans-coder/TensorflowTTS_chinese" ["l"="36.776,2.214"]
"jinguangyang/ChineseTTS" ["l"="36.784,2.194"]
"cczhr/voice-tts" ["l"="36.803,2.187"]
"lochenchou/MOSNet" ["l"="36.849,4.269"]
"BrightGu/MediumVC" ["l"="37.489,2.859"]
"interactiveaudiolab/ppgs" ["l"="37.174,2.131"]
"maxrmorrison/promonet" ["l"="37.139,2.077"]
"maxrmorrison/torbi" ["l"="37.123,2.054"]
"tkedwards/wiktionarytodict" ["l"="37.363,2.03"]
"dohliam/dsl-tools" ["l"="37.356,2.064"]
"lajlksdf/vtl" ["l"="37.5,2.876"]
"William-Zhanng/SenseXAMP" ["l"="37.493,2.655"]
"danijel3/SparrowhawkTest" ["l"="37.363,2.305"]
"bpotard/idlak" ["l"="37.385,2.261"]
"lingpy/lingpy" ["l"="1.888,-27.382"]
"yl4579/PL-BERT" ["l"="38.432,2.085"]
"hgneng/ekho" ["l"="37.307,2.414"]
"junzew/HanTTS" ["l"="37.392,2.391"]
"helang818/LMVD" ["l"="37.161,1.83"]
"albertfgu/diffwave-sashimi" ["l"="37.57,2.281"]
"philsyn/DiffWave-unconditional" ["l"="37.599,2.254"]
"faiqali1/Suicidal-Text-Analysis" ["l"="37.081,1.818"]
"jiaqi-pro/Depression-detection-Graph" ["l"="37.164,1.802"]
"divertingPan/STA-DRN" ["l"="37.181,1.799"]
"volatileee/FacialPulse" ["l"="37.164,1.775"]
"basveeling/wavenet" ["l"="37.083,2.58"]
"magenta/magenta" ["l"="45.441,29.307"]
"pannous/tensorflow-speech-recognition" ["l"="35.519,2.312"]
"openai/pixel-cnn" ["l"="45.952,29.278"]
"carpedm20/DCGAN-tensorflow" ["l"="45.808,29.187"]
"google-deepmind/sonnet" ["l"="57.431,17.873"]
"btotharye/mycroft-homeassistant" ["l"="36.815,2.414"]
"MycroftAI/documentation" ["l"="36.84,2.478"]
"MycroftAI/contributors" ["l"="36.825,2.43"]
"forslund/spotify-skill" ["l"="36.787,2.421"]
"joshymcd/mycroft-electro" ["l"="36.806,2.372"]
"lolstring/gnome-shell-extension-mycroft" ["l"="36.814,2.35"]
"usernaamee/keras-wavenet" ["l"="37.039,2.6"]
"bstriner/keras-adversarial" ["l"="46.019,29.224"]
"coreylynch/async-rl" ["l"="57.327,18.126"]
"farizrahman4u/seq2seq" ["l"="55.837,28.49"]
"jacobgil/keras-dcgan" ["l"="45.945,29.213"]
"ozdefir/finetuneas" ["l"="37.038,2.22"]
"Jiaxin-Ye/DepMamba" ["l"="37.18,1.823"]
"open-dsl-dict/ipa-dict-dsl" ["l"="37.346,2.115"]
"JoseLlarena/Britfone" ["l"="37.356,2.179"]
"menelik3/cmudict-ipa" ["l"="37.339,2.17"]
"ritheshkumar95/WaveNet" ["l"="36.993,2.622"]
"Smerity/keras_snli" ["l"="52.809,27.491"]
"huyouare/WaveNet-Theano" ["l"="37.018,2.626"]
"phreeza/keras-GAN" ["l"="46.043,29.249"]
"osh/kerlym" ["l"="57.282,18.125"]
"musyoku/wavenet" ["l"="36.971,2.632"]
"echogarden-project/echogarden" ["l"="36.947,2.187"]
"feldberlin/timething" ["l"="36.91,2.151"]
"MycroftAI/MycroftCore-Android" ["l"="36.807,2.451"]
"Tadashi-Hikari/Sapphire-Assistant-Framework" ["l"="-52.488,9.641"]
"AndroidMaryTTS/AndroidMaryTTS-Client" ["l"="36.882,2.265"]
"Zeta36/tensorflow-image-wavenet" ["l"="37.035,2.639"]
"muzixingyun/LI-FPN" ["l"="37.181,1.772"]
"Cadair/mycroft-kodi" ["l"="36.776,2.363"]
"zzw922cn/Automatic_Speech_Recognition" ["l"="35.539,2.262"]
"facebookresearch/fairseq-lua" ["l"="53.204,25.644"]
"OpenNMT/OpenNMT" ["l"="53.765,24.675"]
"flashlight/wav2letter" ["l"="35.493,2.336"]
"AKSHAYUBHAT/DeepVideoAnalytics" ["l"="46.202,7.121"]
"SeanNaren/deepspeech.pytorch" ["l"="35.566,2.283"]
"shivasiddharth/GassistPi" ["l"="35.585,1.378"]
"MycroftAI/skill-homeassistant" ["l"="36.848,2.519"]
"deepsound-project/samplernn-pytorch" ["l"="37.055,2.668"]
"richardassar/SampleRNN_torch" ["l"="37.076,2.666"]
"Unisound/SampleRNN" ["l"="37.068,2.684"]
"rncm-prism/prism-samplernn" ["l"="37.044,2.694"]
"barronalex/Tacotron" ["l"="37.124,2.608"]
"ex3ndr/supervoice-voicebox" ["l"="37.401,2.056"]
"rupeshs/neuralsongstyle" ["l"="37.105,2.782"]
"vadim-v-lebedev/audio_style_tranfer" ["l"="37.103,2.806"]
"DmitryUlyanov/neural-style-audio-torch" ["l"="37.064,2.805"]
"pkmital/time-domain-neural-audio-style-transfer" ["l"="37.085,2.826"]
"frhrdr/generative_audio" ["l"="37.079,2.777"]
"albertaparicio/tfg-voice-conversion" ["l"="37.256,2.788"]
"shamidreza/dnnmapper" ["l"="37.238,2.842"]
"amazon-science/proteno" ["l"="37.468,2.158"]
"xiaoyifang/Capture2Text" ["l"="37.348,2.08"]
"golbin/WaveNet" ["l"="37.008,2.557"]
"dhpollack/fast-wavenet.pytorch" ["l"="37.025,2.51"]
"zcaceres/spec_augment" ["l"="35.689,2.265"]
"ex3ndr/supervoice-vall-e-2" ["l"="37.421,1.992"]
"keonlee9420/evaluate-zero-shot-tts" ["l"="37.431,1.954"]
"candlewill/Tacotron-2" ["l"="37.095,2.687"]
"Kyubyong/tacotron_asr" ["l"="37.015,2.658"]
"mhsabbagh/green-recorder" ["l"="36.722,2.36"]
"Programmica/python-gtk3-tutorial" ["l"="36.675,2.35"]
"candlewill/AiVoice" ["l"="37.119,2.682"]
"mauriciofigueroa/praatSublimeSyntax" ["l"="37.031,2.104"]
"alexis-michaud/na" ["l"="37.524,1.911"]
"YuanGongND/gopt" ["l"="35.958,2.439"]
"jcvasquezc/NeuroSpeech" ["l"="37.229,2.155"]
"numediart/EmoV-DB" ["l"="38.355,2.333"]
"jcvasquezc/phonet" ["l"="37.222,2.173"]
"szcom/samplernn" ["l"="37.049,2.718"]
"deepsound-project/pggan-pytorch" ["l"="36.993,2.721"]
"gcunhase/samplernn-pytorch" ["l"="37.025,2.699"]
"seaniezhao/torch_npss" ["l"="38.577,2.345"]
"JeremyCCHsu/vc-vawgan" ["l"="37.282,2.781"]
"yistLin/H264-Encoder" ["l"="37.293,2.852"]
"alexram1313/text-to-speech-sample" ["l"="37.477,2.359"]
"forslund/gcalendar_skill" ["l"="36.753,2.417"]
}