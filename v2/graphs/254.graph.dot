digraph G {
"microsoft/unilm" -> "haotian-liu/LLaVA" ["e"=1]
"microsoft/unilm" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"mattbierbaum/arxiv-public-datasets" -> "tingyaohsu/SciCap" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"dmlc/decord" -> "OpenGVLab/InternVideo" ["e"=1]
"315386775/DeepLearing-Interview-Awesome-2024" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"davidmrau/mixture-of-experts" -> "PKU-YuanGroup/MoE-LLaVA" ["e"=1]
"hassony2/torch_videovision" -> "yuxumin/CoRe" ["e"=1]
"hassony2/torch_videovision" -> "xujinglin/FineDiving" ["e"=1]
"hassony2/torch_videovision" -> "baiyang4/aqa_tpt" ["e"=1]
"hassony2/torch_videovision" -> "nzl-thu/MUSDL" ["e"=1]
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "dvlab-research/LISA" ["e"=1]
"coin-dataset/annotation-tool" -> "xujinglin/FineDiving"
"coin-dataset/annotation-tool" -> "425776024/VideoLabeling"
"coin-dataset/annotation-tool" -> "nzl-thu/MUSDL"
"pomonam/AttentionCluster" -> "hazeld/rank-aware-attention-network"
"Maluuba/FigureQA" -> "kushalkafle/DVQA_dataset"
"Maluuba/FigureQA" -> "vmichals/FigureQA-baseline"
"vmichals/FigureQA-baseline" -> "Maluuba/FigureQA"
"vmichals/FigureQA-baseline" -> "kushalkafle/DVQA_dataset"
"LisaAnne/Hallucination" -> "AoiDragon/POPE"
"LisaAnne/Hallucination" -> "RUCAIBox/POPE"
"LisaAnne/Hallucination" -> "BillChan226/HALC"
"ParitoshParmar/MTL-AQA" -> "nzl-thu/MUSDL"
"ParitoshParmar/MTL-AQA" -> "yuxumin/CoRe"
"ParitoshParmar/MTL-AQA" -> "ZhouKanglei/Awesome-AQA"
"ParitoshParmar/MTL-AQA" -> "Shunli-Wang/TSA-Net"
"ParitoshParmar/MTL-AQA" -> "xujinglin/FineDiving"
"ParitoshParmar/MTL-AQA" -> "hazeld/rank-aware-attention-network"
"ParitoshParmar/MTL-AQA" -> "shiyi-zh0408/LOGO"
"ParitoshParmar/MTL-AQA" -> "baiyang4/aqa_tpt"
"ParitoshParmar/MTL-AQA" -> "ParitoshParmar/Fitness-AQA"
"kushalkafle/DVQA_dataset" -> "kushalkafle/PReFIL"
"kushalkafle/DVQA_dataset" -> "Maluuba/FigureQA"
"kushalkafle/DVQA_dataset" -> "NiteshMethani/PlotQA"
"kushalkafle/DVQA_dataset" -> "vmichals/FigureQA-baseline"
"hazeld/rank-aware-attention-network" -> "nzl-thu/MUSDL"
"NielsRogge/Transformers-Tutorials" -> "haotian-liu/LLaVA" ["e"=1]
"NielsRogge/Transformers-Tutorials" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"hyunwoongko/transformer" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"Breakthrough/PySceneDetect" -> "OpenGVLab/InternVideo" ["e"=1]
"Breakthrough/PySceneDetect" -> "DAMO-NLP-SG/Video-LLaMA" ["e"=1]
"IBM/action-recognition-pytorch" -> "Chuhanxx/Temporal_Query_Networks" ["e"=1]
"soCzech/TransNetV2" -> "OpenGVLab/InternVideo" ["e"=1]
"soCzech/TransNetV2" -> "RenShuhuai-Andy/TimeChat" ["e"=1]
"JasonObeid/Chart2Text" -> "vis-nlp/Chart-to-text"
"JasonObeid/Chart2Text" -> "gongliym/data2text-transformer"
"JasonObeid/Chart2Text" -> "tingyaohsu/SciCap"
"JasonObeid/Chart2Text" -> "soap117/DeepRule"
"JasonObeid/Chart2Text" -> "pranonrahman/ChartSumm"
"JasonObeid/Chart2Text" -> "vis-nlp/ChartQA"
"xtaci/gaio" -> "dvlab-research/Lyra" ["e"=1]
"Cvrane/ChartReader" -> "soap117/DeepRule"
"NiteshMethani/PlotQA" -> "kushalkafle/DVQA_dataset"
"NiteshMethani/PlotQA" -> "kushalkafle/PReFIL"
"NiteshMethani/PlotQA" -> "Maluuba/FigureQA"
"SDOlivia/FineGym" -> "Chuhanxx/Temporal_Query_Networks"
"SDOlivia/FineGym" -> "hazeld/rank-aware-attention-network"
"coriverchen/Robust_Steganography" -> "mengtong0110/InferDPT"
"nzl-thu/MUSDL" -> "hazeld/rank-aware-attention-network"
"nzl-thu/MUSDL" -> "yuxumin/CoRe"
"nzl-thu/MUSDL" -> "ParitoshParmar/MTL-AQA"
"nzl-thu/MUSDL" -> "baiyang4/aqa_tpt"
"nzl-thu/MUSDL" -> "Shunli-Wang/TSA-Net"
"openai/CLIP" -> "haotian-liu/LLaVA" ["e"=1]
"openai/CLIP" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"mlfoundations/open_clip" -> "haotian-liu/LLaVA" ["e"=1]
"mlfoundations/open_clip" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"ccfddl/ccf-deadlines" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"microsoft/LoRA" -> "haotian-liu/LLaVA" ["e"=1]
"luo3300612/Visualizer" -> "dvlab-research/LISA" ["e"=1]
"openai/grok" -> "LargeWorldModel/LWM" ["e"=1]
"google-research/scenic" -> "OpenGVLab/InternVideo" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "yunlong10/Awesome-LLMs-for-Video-Understanding" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "OpenGVLab/InternVideo" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "mbzuai-oryx/Video-ChatGPT" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "DAMO-NLP-SG/Video-LLaMA" ["e"=1]
"SwinTransformer/Video-Swin-Transformer" -> "OpenGVLab/InternVideo" ["e"=1]
"m-bain/webvid" -> "OpenGVLab/InternVideo" ["e"=1]
"m-bain/webvid" -> "mbzuai-oryx/Video-ChatGPT" ["e"=1]
"cvdfoundation/kinetics-dataset" -> "OpenGVLab/InternVideo" ["e"=1]
"DirtyHarryLYL/Transformer-in-Vision" -> "DirtyHarryLYL/LLM-in-Vision" ["e"=1]
"FuxiaoLiu/VisualNews-Repository" -> "FuxiaoLiu/DocumentCLIP" ["e"=1]
"FuxiaoLiu/VisualNews-Repository" -> "FuxiaoLiu/Twitter-Video-dataset" ["e"=1]
"soap117/DeepRule" -> "Cvrane/ChartReader"
"soap117/DeepRule" -> "vis-nlp/Chart-to-text"
"soap117/DeepRule" -> "TheJaeLal/LineFormer"
"soap117/DeepRule" -> "vis-nlp/ChartQA"
"soap117/DeepRule" -> "JasonObeid/Chart2Text"
"soap117/DeepRule" -> "bloomberg/scatteract"
"Shunli-Wang/TSA-Net" -> "baiyang4/aqa_tpt"
"Shunli-Wang/TSA-Net" -> "Lyman-Smoker/Awesome-AQA"
"Shunli-Wang/TSA-Net" -> "yuxumin/CoRe"
"Shunli-Wang/TSA-Net" -> "xuangch/CVPR22_GDLT"
"Shunli-Wang/TSA-Net" -> "xujinglin/FineDiving"
"Shunli-Wang/TSA-Net" -> "ParitoshParmar/MTL-AQA"
"Shunli-Wang/TSA-Net" -> "Luciferbobo/DAE-AQA"
"doc-doc/NExT-QA" -> "egoschema/EgoSchema" ["e"=1]
"Luciferbobo/DAE-AQA" -> "baiyang4/aqa_tpt"
"Luciferbobo/DAE-AQA" -> "xuangch/CVPR22_GDLT"
"Luciferbobo/DAE-AQA" -> "Shunli-Wang/TSA-Net"
"Luciferbobo/DAE-AQA" -> "yuxumin/CoRe"
"archiki/Robust-E2E-ASR" -> "YUCHEN005/DPSL-ASR"
"archiki/Robust-E2E-ASR" -> "bliunlpr/Robust_e2e_gan"
"yuxumin/CoRe" -> "baiyang4/aqa_tpt"
"yuxumin/CoRe" -> "xujinglin/FineDiving"
"yuxumin/CoRe" -> "nzl-thu/MUSDL"
"yuxumin/CoRe" -> "Shunli-Wang/TSA-Net"
"yuxumin/CoRe" -> "ZhouKanglei/Awesome-AQA"
"yuxumin/CoRe" -> "ParitoshParmar/MTL-AQA"
"yuxumin/CoRe" -> "shiyi-zh0408/LOGO"
"yuxumin/CoRe" -> "xuangch/CVPR22_GDLT"
"yuxumin/CoRe" -> "Luciferbobo/DAE-AQA"
"yuxumin/CoRe" -> "ParitoshParmar/Fitness-AQA"
"yuxumin/CoRe" -> "ZhouKanglei/HGCN_AQA"
"kushalkafle/PReFIL" -> "kushalkafle/DVQA_dataset"
"THUDM/SwissArmyTransformer" -> "THUDM/CogVLM" ["e"=1]
"mli/paper-reading" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"salesforce/BLIP" -> "haotian-liu/LLaVA" ["e"=1]
"salesforce/BLIP" -> "QwenLM/Qwen-VL" ["e"=1]
"salesforce/BLIP" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"OFA-Sys/OFA" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"isl-org/lang-seg" -> "dvlab-research/LISA" ["e"=1]
"MLNLP-World/Paper-Writing-Tips" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"microsoft/GLIP" -> "dvlab-research/LISA" ["e"=1]
"MCG-NJU/VideoMAE" -> "OpenGVLab/InternVideo" ["e"=1]
"deepdoctection/deepdoctection" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"cmhungsteve/Awesome-Transformer-Attention" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"diff-usion/Awesome-Diffusion-Models" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"open-mmlab/mmengine" -> "open-mmlab/Multimodal-GPT" ["e"=1]
"facebookresearch/Mask2Former" -> "dvlab-research/LISA" ["e"=1]
"kahnchana/svt" -> "kahnchana/clippy"
"google-research/big_vision" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"google-research/big_vision" -> "cambrian-mllm/cambrian" ["e"=1]
"showlab/all-in-one" -> "Yui010206/SeViLA" ["e"=1]
"codecaution/Awesome-Mixture-of-Experts-Papers" -> "swordlidev/Efficient-Multimodal-LLMs-Survey" ["e"=1]
"lyc8503/2021-nju-software-engineering-textbook" -> "SYuan03/MM-IFEngine" ["e"=1]
"timojl/clipseg" -> "dvlab-research/LISA" ["e"=1]
"vis-nlp/ChartQA" -> "vis-nlp/Chart-to-text"
"vis-nlp/ChartQA" -> "soap117/DeepRule"
"vis-nlp/ChartQA" -> "NiteshMethani/PlotQA"
"vis-nlp/ChartQA" -> "tingxueronghua/ChartLlama-code"
"vis-nlp/ChartQA" -> "vis-nlp/UniChart"
"vis-nlp/ChartQA" -> "google-research/pix2struct" ["e"=1]
"vis-nlp/ChartQA" -> "JasonObeid/Chart2Text"
"vis-nlp/ChartQA" -> "tingyaohsu/SciCap"
"vis-nlp/ChartQA" -> "mitvis/vistext"
"vis-nlp/ChartQA" -> "FuxiaoLiu/MMC"
"vis-nlp/ChartQA" -> "zengxingchen/ChartQA-MLLM" ["e"=1]
"vis-nlp/ChartQA" -> "OpenGVLab/ChartAst"
"vis-nlp/ChartQA" -> "princeton-nlp/CharXiv"
"vis-nlp/ChartQA" -> "levymsn/CQA-CRCT"
"vis-nlp/ChartQA" -> "vis-nlp/ChartGemma"
"XueFuzhao/awesome-mixture-of-experts" -> "PKU-YuanGroup/MoE-LLaVA" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "DirtyHarryLYL/LLM-in-Vision" ["e"=1]
"google-research/pix2seq" -> "OpenGVLab/VisionLLM" ["e"=1]
"ZwwWayne/K-Net" -> "jshilong/GPT4RoI" ["e"=1]
"facebookresearch/multimodal" -> "open-mmlab/Multimodal-GPT" ["e"=1]
"microsoft/XPretrain" -> "rese1f/MovieChat" ["e"=1]
"Thinklab-SJTU/pygmtools" -> "Alpha-Innovator/ChartVLM" ["e"=1]
"vis-nlp/Chart-to-text" -> "JasonObeid/Chart2Text"
"vis-nlp/Chart-to-text" -> "pranonrahman/ChartSumm"
"vis-nlp/Chart-to-text" -> "vis-nlp/ChartQA"
"vis-nlp/Chart-to-text" -> "mitvis/vistext"
"vis-nlp/Chart-to-text" -> "tingyaohsu/SciCap"
"vis-nlp/Chart-to-text" -> "soap117/DeepRule"
"vis-nlp/Chart-to-text" -> "vis-nlp/UniChart"
"vis-nlp/Chart-to-text" -> "vis-nlp/ChartInstruct"
"Soldelli/MAD" -> "lntzm/MESM" ["e"=1]
"ParitoshParmar/Fitness-AQA" -> "baiyang4/aqa_tpt"
"ParitoshParmar/Fitness-AQA" -> "yuxumin/CoRe"
"ParitoshParmar/Fitness-AQA" -> "Luciferbobo/DAE-AQA"
"ParitoshParmar/Fitness-AQA" -> "Shunli-Wang/TSA-Net"
"ParitoshParmar/Fitness-AQA" -> "ParitoshParmar/MTL-AQA"
"ParitoshParmar/Fitness-AQA" -> "xuangch/CVPR22_GDLT"
"ParitoshParmar/Fitness-AQA" -> "ZhouKanglei/Awesome-AQA"
"VRU-NExT/VideoQA" -> "Yui010206/SeViLA" ["e"=1]
"jshilong/DDQ" -> "jshilong/GPT4RoI" ["e"=1]
"neillu23/CDiffuSE" -> "YUCHEN005/NASE" ["e"=1]
"YUCHEN005/DPSL-ASR" -> "YUCHEN005/RATS-Channel-A-Speech-Data"
"YUCHEN005/DPSL-ASR" -> "YUCHEN005/Gradient-Remedy"
"YUCHEN005/DPSL-ASR" -> "YUCHEN005/UNA-GAN"
"YUCHEN005/DPSL-ASR" -> "archiki/Robust-E2E-ASR"
"LibRerank-Community/LibRerank" -> "lyingCS/Controllable-Multi-Objective-Reranking" ["e"=1]
"xujinglin/FineDiving" -> "yuxumin/CoRe"
"xujinglin/FineDiving" -> "Shunli-Wang/TSA-Net"
"xujinglin/FineDiving" -> "ZhouKanglei/Awesome-AQA"
"xujinglin/FineDiving" -> "shiyi-zh0408/LOGO"
"xujinglin/FineDiving" -> "ParitoshParmar/MTL-AQA"
"xujinglin/FineDiving" -> "baiyang4/aqa_tpt"
"xujinglin/FineDiving" -> "nzl-thu/MUSDL"
"xujinglin/FineDiving" -> "xuangch/CVPR22_GDLT"
"xujinglin/FineDiving" -> "coin-dataset/annotation-tool"
"xujinglin/FineDiving" -> "Luciferbobo/DAE-AQA"
"xujinglin/FineDiving" -> "Lyman-Smoker/Awesome-AQA"
"Ethan00Si/Instrumental-variables-for-recommendation" -> "Ethan00Si/KuaiSAR"
"Ethan00Si/Instrumental-variables-for-recommendation" -> "Ethan00Si/SESREC-SIGIR-2023"
"FuxiaoLiu/Twitter-Video-dataset" -> "FuxiaoLiu/DocumentCLIP"
"xuangch/CVPR22_GDLT" -> "baiyang4/aqa_tpt"
"YUCHEN005/RATS-Channel-A-Speech-Data" -> "YUCHEN005/UNA-GAN"
"YUCHEN005/RATS-Channel-A-Speech-Data" -> "YUCHEN005/DPSL-ASR"
"YUCHEN005/RATS-Channel-A-Speech-Data" -> "YUCHEN005/Gradient-Remedy"
"ZhouKanglei/HGCN_AQA" -> "qinghuannn/PAMFN"
"mlfoundations/open_flamingo" -> "open-mmlab/Multimodal-GPT" ["e"=1]
"mlfoundations/open_flamingo" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"mlfoundations/open_flamingo" -> "haotian-liu/LLaVA" ["e"=1]
"mlfoundations/open_flamingo" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"amazon-science/mm-cot" -> "lupantech/ScienceQA" ["e"=1]
"amazon-science/mm-cot" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"huggingface/peft" -> "haotian-liu/LLaVA" ["e"=1]
"baaivision/Painter" -> "dvlab-research/LISA" ["e"=1]
"omriav/blended-latent-diffusion" -> "levymsn/CQA-CRCT" ["e"=1]
"microsoft/GenerativeImage2Text" -> "JialianW/GRiT" ["e"=1]
"microsoft/GenerativeImage2Text" -> "yuweihao/MM-Vet" ["e"=1]
"dair-ai/ML-Papers-of-the-Week" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"microsoft/i-Code" -> "microsoft/UDOP" ["e"=1]
"microsoft/i-Code" -> "NExT-GPT/NExT-GPT"
"microsoft/i-Code" -> "facebookresearch/ImageBind" ["e"=1]
"microsoft/i-Code" -> "thu-ml/unidiffuser" ["e"=1]
"microsoft/i-Code" -> "SHI-Labs/Versatile-Diffusion" ["e"=1]
"microsoft/i-Code" -> "DAMO-NLP-SG/Video-LLaMA"
"microsoft/i-Code" -> "AlibabaResearch/AdvancedLiterateMachinery" ["e"=1]
"microsoft/i-Code" -> "baaivision/Emu" ["e"=1]
"microsoft/i-Code" -> "mlfoundations/open_flamingo" ["e"=1]
"microsoft/i-Code" -> "lucidrains/make-a-video-pytorch" ["e"=1]
"microsoft/i-Code" -> "YingqingHe/LVDM" ["e"=1]
"microsoft/i-Code" -> "AILab-CVC/VideoCrafter" ["e"=1]
"microsoft/i-Code" -> "salesforce/LAVIS" ["e"=1]
"microsoft/i-Code" -> "showlab/Show-o" ["e"=1]
"microsoft/i-Code" -> "gligen/GLIGEN" ["e"=1]
"lupantech/ScienceQA" -> "amazon-science/mm-cot" ["e"=1]
"lupantech/ScienceQA" -> "yuweihao/MM-Vet"
"lupantech/ScienceQA" -> "RUCAIBox/POPE"
"lupantech/ScienceQA" -> "wenhuchen/TheoremQA" ["e"=1]
"lupantech/ScienceQA" -> "lupantech/chameleon-llm" ["e"=1]
"lupantech/ScienceQA" -> "lupantech/MathVista"
"lupantech/ScienceQA" -> "ylsung/VL_adapter" ["e"=1]
"lupantech/ScienceQA" -> "FuxiaoLiu/LRV-Instruction"
"lupantech/ScienceQA" -> "lupantech/dl4math" ["e"=1]
"lupantech/ScienceQA" -> "MMMU-Benchmark/MMMU"
"lupantech/ScienceQA" -> "mandyyyyii/scibench" ["e"=1]
"lupantech/ScienceQA" -> "lupantech/IconQA" ["e"=1]
"lupantech/ScienceQA" -> "Timothyxxx/Chain-of-ThoughtsPapers" ["e"=1]
"lupantech/ScienceQA" -> "vis-nlp/ChartQA"
"lupantech/ScienceQA" -> "LightChen233/M3CoT"
"modelscope/modelscope" -> "QwenLM/Qwen-VL" ["e"=1]
"clovaai/donut" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"salesforce/LAVIS" -> "haotian-liu/LLaVA" ["e"=1]
"salesforce/LAVIS" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"salesforce/LAVIS" -> "QwenLM/Qwen-VL" ["e"=1]
"salesforce/LAVIS" -> "OpenGVLab/InternVL" ["e"=1]
"ShoufaChen/DiffusionDet" -> "dvlab-research/LISA" ["e"=1]
"microsoft/X-Decoder" -> "jshilong/GPT4RoI" ["e"=1]
"microsoft/X-Decoder" -> "dvlab-research/LISA" ["e"=1]
"breezedeus/Pix2Text" -> "Ucas-HaoranWei/Vary" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "QwenLM/Qwen-VL" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "OpenGVLab/InternVL" ["e"=1]
"OFA-Sys/Chinese-CLIP" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"eliahuhorwitz/Academic-project-page-template" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"google-research/pix2struct" -> "vis-nlp/ChartQA" ["e"=1]
"google-research/pix2struct" -> "NiteshMethani/PlotQA" ["e"=1]
"google-research/pix2struct" -> "vis-nlp/UniChart" ["e"=1]
"google-research/pix2struct" -> "SALT-NLP/LLaVAR" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "microsoft/GLIP" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "DirtyHarryLYL/LLM-in-Vision"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "dvlab-research/LISA"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "KaiyangZhou/CoOp" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "IDEA-Research/awesome-detection-transformer" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "mlfoundations/open_flamingo" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "VainF/Awesome-Anything" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "microsoft/X-Decoder" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "jianzongwu/Awesome-Open-Vocabulary" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "LLaVA-VL/LLaVA-NeXT"
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "salesforce/LAVIS" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "TheShadow29/awesome-grounding" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "baaivision/Painter" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "KMnP/vpt" ["e"=1]
"facebookresearch/LaViLa" -> "RenShuhuai-Andy/TimeChat" ["e"=1]
"facebookresearch/LaViLa" -> "OpenGVLab/InternVideo" ["e"=1]
"facebookresearch/LaViLa" -> "huangb23/VTimeLLM" ["e"=1]
"facebookresearch/LaViLa" -> "rese1f/MovieChat" ["e"=1]
"OpenGVLab/InternVideo" -> "OpenGVLab/Ask-Anything"
"OpenGVLab/InternVideo" -> "OpenGVLab/VideoChat-Flash"
"OpenGVLab/InternVideo" -> "OpenGVLab/VideoMAEv2" ["e"=1]
"OpenGVLab/InternVideo" -> "MCG-NJU/VideoMAE" ["e"=1]
"OpenGVLab/InternVideo" -> "DAMO-NLP-SG/Video-LLaMA"
"OpenGVLab/InternVideo" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"OpenGVLab/InternVideo" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"OpenGVLab/InternVideo" -> "PKU-YuanGroup/Video-LLaVA"
"OpenGVLab/InternVideo" -> "OpenGVLab/VideoMamba" ["e"=1]
"OpenGVLab/InternVideo" -> "LLaVA-VL/LLaVA-NeXT"
"OpenGVLab/InternVideo" -> "mbzuai-oryx/Video-ChatGPT"
"OpenGVLab/InternVideo" -> "DAMO-NLP-SG/VideoLLaMA2"
"OpenGVLab/InternVideo" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"OpenGVLab/InternVideo" -> "magic-research/PLLaVA"
"OpenGVLab/InternVideo" -> "SwinTransformer/Video-Swin-Transformer" ["e"=1]
"AlibabaResearch/AdvancedLiterateMachinery" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"AlibabaResearch/AdvancedLiterateMachinery" -> "Ucas-HaoranWei/Vary" ["e"=1]
"JialianW/GRiT" -> "showlab/Image2Paragraph" ["e"=1]
"JialianW/GRiT" -> "jshilong/GPT4RoI"
"JialianW/GRiT" -> "davidnvq/grit" ["e"=1]
"JialianW/GRiT" -> "allenai/unified-io-inference"
"JialianW/GRiT" -> "showlab/EgoVLP" ["e"=1]
"JialianW/GRiT" -> "shikras/shikra"
"JialianW/GRiT" -> "X2FD/LVIS-INSTRUCT4V"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "huangb23/VTimeLLM"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "rese1f/MovieChat"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "Ziyang412/VideoTree"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "boheumd/MA-LMM"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "RenShuhuai-Andy/TimeChat"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "md-mohaiminul/VideoRecap"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "MME-Benchmarks/Video-MME"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "daixiangzi/Awesome-Token-Compress"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "Wang-Xiaodong1899/Open-R1-Video"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "JUNJIE99/MLVU"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "EvolvingLMMs-Lab/LongVA"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "egoschema/EgoSchema"
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "j-min/HiREST" ["e"=1]
"ttengwang/Awesome_Long_Form_Video_Understanding" -> "QQ-MM/Video-CCAM"
"baaivision/EVA" -> "dvlab-research/LISA" ["e"=1]
"Oneirocom/Magick" -> "SkunkworksAI/hydra-moe" ["e"=1]
"iejMac/video2dataset" -> "OpenGVLab/InternVideo" ["e"=1]
"iejMac/video2dataset" -> "rese1f/MovieChat" ["e"=1]
"facebookresearch/ToMe" -> "daixiangzi/Awesome-Token-Compress" ["e"=1]
"allenai/unified-io-inference" -> "allenai/unified-io-2" ["e"=1]
"allenai/unified-io-inference" -> "JialianW/GRiT"
"allenai/unified-io-inference" -> "TencentARC/GVT"
"ch3cook-fdu/Vote2Cap-DETR" -> "Open3DA/LL3DA" ["e"=1]
"ch3cook-fdu/Vote2Cap-DETR" -> "HankYe/AdaptiveDiffusion"
"eliahuhorwitz/Conffusion" -> "levymsn/CQA-CRCT" ["e"=1]
"OpenGVLab/UniFormerV2" -> "OpenGVLab/InternVideo" ["e"=1]
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "showlab/Awesome-MLLM-Hallucination"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "DirtyHarryLYL/LLM-in-Vision"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "DAMO-NLP-SG/VCD"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "HillZhang1999/llm-hallucination-survey" ["e"=1]
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "shikiw/OPERA"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "FuxiaoLiu/LRV-Instruction"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "Fancy-MLLM/R1-Onevision"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "TideDra/lmm-r1"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "SinclairCoder/Instruction-Tuning-Papers" ["e"=1]
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "ChaofanTao/Autoregressive-Models-in-Vision-Survey" ["e"=1]
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "Osilly/Vision-R1"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "open-compass/VLMEvalKit"
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"antoyang/FrozenBiLM" -> "Yui010206/SeViLA" ["e"=1]
"google-deepmind/perception_test" -> "PKU-YuanGroup/Video-Bench" ["e"=1]
"google-deepmind/perception_test" -> "OpenGVLab/VideoChat-Flash"
"google-deepmind/perception_test" -> "Ziyang412/VideoTree"
"google-deepmind/perception_test" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"YUCHEN005/Unified-Enhance-Separation" -> "YUCHEN005/UNA-GAN"
"YUCHEN005/Unified-Enhance-Separation" -> "YUCHEN005/Gradient-Remedy"
"YUCHEN005/Unified-Enhance-Separation" -> "YUCHEN005/RATS-Channel-A-Speech-Data"
"baiyang4/aqa_tpt" -> "xuangch/CVPR22_GDLT"
"baiyang4/aqa_tpt" -> "Luciferbobo/DAE-AQA"
"baiyang4/aqa_tpt" -> "yuxumin/CoRe"
"baiyang4/aqa_tpt" -> "Shunli-Wang/TSA-Net"
"HELLORPG/CV-Framework" -> "MCG-NJU/CaReBench"
"AndyTang15/FLAG3D" -> "AndyTang15/FLAG3Dv2"
"Lyman-Smoker/Awesome-AQA" -> "iSEE-Laboratory/Continual-AQA"
"Lyman-Smoker/Awesome-AQA" -> "Run542968/Awesome-3D-Human-Motion-Generation"
"Lyman-Smoker/Awesome-AQA" -> "iSEE-Laboratory/EgoExo-Fitness"
"YUCHEN005/Gradient-Remedy" -> "YUCHEN005/UNA-GAN"
"YUCHEN005/Gradient-Remedy" -> "YUCHEN005/RATS-Channel-A-Speech-Data"
"FuxiaoLiu/DocumentCLIP" -> "FuxiaoLiu/Twitter-Video-dataset"
"facebookresearch/ImageBind" -> "haotian-liu/LLaVA" ["e"=1]
"facebookresearch/ImageBind" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"facebookresearch/ImageBind" -> "DAMO-NLP-SG/Video-LLaMA" ["e"=1]
"facebookresearch/segment-anything" -> "haotian-liu/LLaVA" ["e"=1]
"IDEA-Research/Grounded-Segment-Anything" -> "haotian-liu/LLaVA" ["e"=1]
"UX-Decoder/Semantic-SAM" -> "dvlab-research/LISA" ["e"=1]
"aiwaves-cn/agents" -> "NExT-GPT/NExT-GPT" ["e"=1]
"Alpha-VLLM/LLaMA2-Accessory" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"Alpha-VLLM/LLaMA2-Accessory" -> "X-PLUG/mPLUG-Owl"
"Alpha-VLLM/LLaMA2-Accessory" -> "LLaVA-VL/LLaVA-NeXT"
"Alpha-VLLM/LLaMA2-Accessory" -> "InternLM/InternLM-XComposer"
"Alpha-VLLM/LLaMA2-Accessory" -> "mlfoundations/open_flamingo" ["e"=1]
"Alpha-VLLM/LLaMA2-Accessory" -> "baaivision/Emu" ["e"=1]
"Alpha-VLLM/LLaMA2-Accessory" -> "QwenLM/Qwen-VL"
"Alpha-VLLM/LLaMA2-Accessory" -> "EvolvingLMMs-Lab/Otter" ["e"=1]
"Alpha-VLLM/LLaMA2-Accessory" -> "dvlab-research/LISA"
"Alpha-VLLM/LLaMA2-Accessory" -> "DAMO-NLP-SG/Video-LLaMA"
"Alpha-VLLM/LLaMA2-Accessory" -> "salesforce/LAVIS" ["e"=1]
"Alpha-VLLM/LLaMA2-Accessory" -> "PKU-YuanGroup/Video-LLaVA"
"Alpha-VLLM/LLaMA2-Accessory" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"Alpha-VLLM/LLaMA2-Accessory" -> "THUDM/CogVLM"
"Alpha-VLLM/LLaMA2-Accessory" -> "Alpha-VLLM/Lumina-T2X" ["e"=1]
"meta-llama/llama-cookbook" -> "haotian-liu/LLaVA" ["e"=1]
"meta-llama/codellama" -> "haotian-liu/LLaVA" ["e"=1]
"liguodongiot/llm-action" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"Vision-CAIR/MiniGPT-4" -> "haotian-liu/LLaVA" ["e"=1]
"Vision-CAIR/MiniGPT-4" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"RUCAIBox/LLMSurvey" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"open-compass/opencompass" -> "open-compass/VLMEvalKit" ["e"=1]
"IDEA-Research/GroundingDINO" -> "haotian-liu/LLaVA" ["e"=1]
"haotian-liu/LLaVA" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"haotian-liu/LLaVA" -> "salesforce/LAVIS" ["e"=1]
"haotian-liu/LLaVA" -> "openai/CLIP" ["e"=1]
"haotian-liu/LLaVA" -> "Vision-CAIR/MiniGPT-4" ["e"=1]
"haotian-liu/LLaVA" -> "mlfoundations/open_clip" ["e"=1]
"haotian-liu/LLaVA" -> "OpenGVLab/InternVL"
"haotian-liu/LLaVA" -> "IDEA-Research/Grounded-Segment-Anything" ["e"=1]
"haotian-liu/LLaVA" -> "lm-sys/FastChat" ["e"=1]
"haotian-liu/LLaVA" -> "LLaVA-VL/LLaVA-NeXT"
"haotian-liu/LLaVA" -> "QwenLM/Qwen-VL"
"haotian-liu/LLaVA" -> "huggingface/peft" ["e"=1]
"haotian-liu/LLaVA" -> "vllm-project/vllm" ["e"=1]
"haotian-liu/LLaVA" -> "hiyouga/LLaMA-Factory" ["e"=1]
"haotian-liu/LLaVA" -> "THUDM/CogVLM"
"haotian-liu/LLaVA" -> "Dao-AILab/flash-attention" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "mlfoundations/open_flamingo" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "jshilong/GPT4RoI"
"open-mmlab/Multimodal-GPT" -> "microsoft/X-Decoder" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "X-PLUG/mPLUG-Owl"
"open-mmlab/Multimodal-GPT" -> "open-mmlab/mmfewshot" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "EvolvingLMMs-Lab/Otter" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "InternLM/InternLM-techreport" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "open-mmlab/mmengine" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "open-mmlab/mmeval" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "open-mmlab/playground" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "jshilong/GroupRCNN" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "OpenGVLab/Ask-Anything"
"open-mmlab/Multimodal-GPT" -> "baaivision/Painter" ["e"=1]
"open-mmlab/Multimodal-GPT" -> "shikras/shikra"
"InternLM/lmdeploy" -> "OpenGVLab/InternVL" ["e"=1]
"InternLM/xtuner" -> "InternLM/InternLM-XComposer" ["e"=1]
"InternLM/xtuner" -> "OpenGVLab/InternVL" ["e"=1]
"InternLM/xtuner" -> "open-compass/VLMEvalKit" ["e"=1]
"InternLM/xtuner" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"modelscope/ms-swift" -> "OpenGVLab/InternVL" ["e"=1]
"modelscope/ms-swift" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"modelscope/ms-swift" -> "QwenLM/Qwen-VL" ["e"=1]
"modelscope/ms-swift" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"pengsida/learning_research" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"QwenLM/Qwen" -> "QwenLM/Qwen-VL" ["e"=1]
"RiseInRose/MiniGPT-4-ZH" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"Stability-AI/generative-models" -> "haotian-liu/LLaVA" ["e"=1]
"UbiquitousLearning/mllm" -> "Meituan-AutoML/MobileVLM" ["e"=1]
"mistralai/mistral-inference" -> "haotian-liu/LLaVA" ["e"=1]
"google/magika" -> "LargeWorldModel/LWM" ["e"=1]
"facebookresearch/nougat" -> "haotian-liu/LLaVA" ["e"=1]
"KoljaB/RealtimeSTT" -> "OpenBMB/MiniCPM-o" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "haotian-liu/LLaVA"
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "salesforce/LAVIS" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "OpenGVLab/InternVL"
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "QwenLM/Qwen-VL"
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "openai/CLIP" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "RUCAIBox/LLMSurvey" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "pliang279/awesome-multimodal-ml" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "mlfoundations/open_clip" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "huggingface/peft" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "microsoft/unilm" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "QwenLM/Qwen2.5-VL"
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "hiyouga/LLaMA-Factory" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "LLaVA-VL/LLaVA-NeXT"
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "Hannibal046/Awesome-LLM" ["e"=1]
"BradyFU/Awesome-Multimodal-Large-Language-Models" -> "Vision-CAIR/MiniGPT-4" ["e"=1]
"InvincibleWyq/ChatVID" -> "AndyTang15/FLAG3Dv2"
"InvincibleWyq/ChatVID" -> "AndyTang15/FLAG3D"
"InvincibleWyq/ChatVID" -> "SuleBai/SC-CLIP"
"OpenGVLab/InternGPT" -> "OpenGVLab/VisionLLM" ["e"=1]
"OpenGVLab/InternGPT" -> "OpenGVLab/Ask-Anything" ["e"=1]
"OpenGVLab/InternGPT" -> "open-mmlab/Multimodal-GPT" ["e"=1]
"OpenGVLab/InternGPT" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"dvlab-research/LISA" -> "mbzuai-oryx/groundingLMM"
"dvlab-research/LISA" -> "LLaVA-VL/LLaVA-NeXT"
"dvlab-research/LISA" -> "UX-Decoder/Semantic-SAM" ["e"=1]
"dvlab-research/LISA" -> "microsoft/GLIP" ["e"=1]
"dvlab-research/LISA" -> "OpenGVLab/VisionLLM"
"dvlab-research/LISA" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"dvlab-research/LISA" -> "shikras/shikra"
"dvlab-research/LISA" -> "facebookresearch/Mask2Former" ["e"=1]
"dvlab-research/LISA" -> "Liuziyu77/Visual-RFT"
"dvlab-research/LISA" -> "MarkMoHR/Awesome-Referring-Image-Segmentation" ["e"=1]
"dvlab-research/LISA" -> "baaivision/Emu" ["e"=1]
"dvlab-research/LISA" -> "baaivision/Painter" ["e"=1]
"dvlab-research/LISA" -> "baaivision/EVA" ["e"=1]
"dvlab-research/LISA" -> "PKU-YuanGroup/Video-LLaVA"
"dvlab-research/LISA" -> "NVlabs/ODISE" ["e"=1]
"InternLM/InternLM" -> "InternLM/InternLM-XComposer" ["e"=1]
"kaixindelele/ChatPaper" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"microsoft/MM-REACT" -> "shikras/shikra"
"microsoft/MM-REACT" -> "RUCAIBox/POPE"
"microsoft/MM-REACT" -> "allenai/visprog"
"microsoft/MM-REACT" -> "OptimalScale/DetGPT"
"microsoft/MM-REACT" -> "mlfoundations/open_flamingo" ["e"=1]
"microsoft/MM-REACT" -> "allenai/mmc4" ["e"=1]
"microsoft/MM-REACT" -> "jshilong/GPT4RoI"
"microsoft/MM-REACT" -> "yuweihao/MM-Vet"
"microsoft/MM-REACT" -> "mbzuai-oryx/groundingLMM"
"microsoft/MM-REACT" -> "microsoft/X-Decoder" ["e"=1]
"microsoft/MM-REACT" -> "JialianW/GRiT"
"microsoft/MM-REACT" -> "baaivision/Emu" ["e"=1]
"microsoft/MM-REACT" -> "allenai/unified-io-2" ["e"=1]
"microsoft/MM-REACT" -> "Timothyxxx/Chain-of-ThoughtsPapers" ["e"=1]
"microsoft/MM-REACT" -> "lupantech/chameleon-llm" ["e"=1]
"allenai/visprog" -> "cvlab-columbia/viper" ["e"=1]
"allenai/visprog" -> "jshilong/GPT4RoI"
"allenai/visprog" -> "allenai/unified-io-2" ["e"=1]
"allenai/visprog" -> "microsoft/MM-REACT"
"allenai/visprog" -> "ashkamath/mdetr" ["e"=1]
"allenai/visprog" -> "dvlab-research/LISA"
"allenai/visprog" -> "OptimalScale/DetGPT"
"allenai/visprog" -> "JialianW/GRiT"
"allenai/visprog" -> "Computer-Vision-in-the-Wild/CVinW_Readings"
"allenai/visprog" -> "NVlabs/ODISE" ["e"=1]
"allenai/visprog" -> "microsoft/X-Decoder" ["e"=1]
"allenai/visprog" -> "shikras/shikra"
"allenai/visprog" -> "baaivision/Emu" ["e"=1]
"allenai/visprog" -> "Vision-CAIR/ChatCaptioner" ["e"=1]
"allenai/visprog" -> "penghao-wu/vstar"
"facebookresearch/seamless_communication" -> "haotian-liu/LLaVA" ["e"=1]
"Hannibal046/Awesome-LLM" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"Mooler0410/LLMsPracticalGuide" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"THUDM/VisualGLM-6B" -> "THUDM/CogVLM" ["e"=1]
"THUDM/VisualGLM-6B" -> "QwenLM/Qwen-VL" ["e"=1]
"ctlllll/LLM-ToolMaker" -> "lyuchenyang/Macaw-LLM" ["e"=1]
"WooooDyy/LLM-Agent-Paper-List" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"baaivision/Emu" -> "dvlab-research/LISA" ["e"=1]
"baaivision/Emu" -> "InternLM/InternLM-XComposer" ["e"=1]
"baaivision/Emu" -> "shikras/shikra" ["e"=1]
"baaivision/Emu" -> "cambrian-mllm/cambrian" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "DAMO-NLP-SG/Video-LLaMA"
"lyuchenyang/Macaw-LLM" -> "longyuewangdcu/Chinese-Llama-2" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "yxuansu/PandaGPT"
"lyuchenyang/Macaw-LLM" -> "mbzuai-oryx/Video-ChatGPT"
"lyuchenyang/Macaw-LLM" -> "EvolvingLMMs-Lab/Otter" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "X-PLUG/mPLUG-Owl"
"lyuchenyang/Macaw-LLM" -> "OpenGVLab/Ask-Anything"
"lyuchenyang/Macaw-LLM" -> "mlfoundations/open_flamingo" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "Alpha-VLLM/LLaMA2-Accessory"
"lyuchenyang/Macaw-LLM" -> "wxjiao/ParroT" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "bytedance/SALMONN" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "Text-to-Audio/Make-An-Audio" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "PeiranLi0930/TorchProject" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"lyuchenyang/Macaw-LLM" -> "thunlp/WebCPM" ["e"=1]
"OpenGVLab/LLaMA-Adapter" -> "Alpha-VLLM/LLaMA2-Accessory" ["e"=1]
"OpenGVLab/LLaMA-Adapter" -> "haotian-liu/LLaVA" ["e"=1]
"OpenGVLab/LLaMA-Adapter" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"OpenGVLab/LLaMA-Adapter" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"Dicklesworthstone/llm_aided_ocr" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"spcl/graph-of-thoughts" -> "NExT-GPT/NExT-GPT" ["e"=1]
"VainF/Awesome-Anything" -> "Computer-Vision-in-the-Wild/CVinW_Readings" ["e"=1]
"NExT-GPT/NExT-GPT" -> "aiwaves-cn/agents" ["e"=1]
"NExT-GPT/NExT-GPT" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"NExT-GPT/NExT-GPT" -> "baaivision/Emu" ["e"=1]
"NExT-GPT/NExT-GPT" -> "QwenLM/Qwen-VL"
"NExT-GPT/NExT-GPT" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"NExT-GPT/NExT-GPT" -> "haotian-liu/LLaVA"
"NExT-GPT/NExT-GPT" -> "PKU-YuanGroup/Video-LLaVA"
"NExT-GPT/NExT-GPT" -> "facebookresearch/ImageBind" ["e"=1]
"NExT-GPT/NExT-GPT" -> "DAMO-NLP-SG/Video-LLaMA"
"NExT-GPT/NExT-GPT" -> "THUDM/CogVLM"
"NExT-GPT/NExT-GPT" -> "Alpha-VLLM/LLaMA2-Accessory"
"NExT-GPT/NExT-GPT" -> "LLaVA-VL/LLaVA-NeXT"
"NExT-GPT/NExT-GPT" -> "williamyang1991/Rerender_A_Video" ["e"=1]
"NExT-GPT/NExT-GPT" -> "salesforce/LAVIS" ["e"=1]
"NExT-GPT/NExT-GPT" -> "OpenMOSS/AnyGPT" ["e"=1]
"QwenLM/Qwen-VL" -> "OpenGVLab/InternVL"
"QwenLM/Qwen-VL" -> "QwenLM/Qwen2.5-VL"
"QwenLM/Qwen-VL" -> "THUDM/CogVLM"
"QwenLM/Qwen-VL" -> "haotian-liu/LLaVA"
"QwenLM/Qwen-VL" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"QwenLM/Qwen-VL" -> "InternLM/InternLM-XComposer"
"QwenLM/Qwen-VL" -> "salesforce/LAVIS" ["e"=1]
"QwenLM/Qwen-VL" -> "LLaVA-VL/LLaVA-NeXT"
"QwenLM/Qwen-VL" -> "QwenLM/Qwen" ["e"=1]
"QwenLM/Qwen-VL" -> "modelscope/ms-swift" ["e"=1]
"QwenLM/Qwen-VL" -> "X-PLUG/mPLUG-Owl"
"QwenLM/Qwen-VL" -> "open-compass/VLMEvalKit"
"QwenLM/Qwen-VL" -> "mlfoundations/open_clip" ["e"=1]
"QwenLM/Qwen-VL" -> "IDEA-Research/GroundingDINO" ["e"=1]
"QwenLM/Qwen-VL" -> "salesforce/BLIP" ["e"=1]
"DAMO-NLP-SG/Video-LLaMA" -> "PKU-YuanGroup/Video-LLaVA"
"DAMO-NLP-SG/Video-LLaMA" -> "OpenGVLab/Ask-Anything"
"DAMO-NLP-SG/Video-LLaMA" -> "mbzuai-oryx/Video-ChatGPT"
"DAMO-NLP-SG/Video-LLaMA" -> "DAMO-NLP-SG/VideoLLaMA2"
"DAMO-NLP-SG/Video-LLaMA" -> "OpenGVLab/InternVideo"
"DAMO-NLP-SG/Video-LLaMA" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"DAMO-NLP-SG/Video-LLaMA" -> "salesforce/LAVIS" ["e"=1]
"DAMO-NLP-SG/Video-LLaMA" -> "LLaVA-VL/LLaVA-NeXT"
"DAMO-NLP-SG/Video-LLaMA" -> "dvlab-research/LLaMA-VID"
"DAMO-NLP-SG/Video-LLaMA" -> "X-PLUG/mPLUG-Owl"
"DAMO-NLP-SG/Video-LLaMA" -> "QwenLM/Qwen-VL"
"DAMO-NLP-SG/Video-LLaMA" -> "PKU-YuanGroup/LanguageBind"
"DAMO-NLP-SG/Video-LLaMA" -> "lyuchenyang/Macaw-LLM"
"DAMO-NLP-SG/Video-LLaMA" -> "mlfoundations/open_flamingo" ["e"=1]
"DAMO-NLP-SG/Video-LLaMA" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"X-PLUG/mPLUG-Owl" -> "QwenLM/Qwen-VL"
"X-PLUG/mPLUG-Owl" -> "mlfoundations/open_flamingo" ["e"=1]
"X-PLUG/mPLUG-Owl" -> "salesforce/LAVIS" ["e"=1]
"X-PLUG/mPLUG-Owl" -> "OpenGVLab/Ask-Anything"
"X-PLUG/mPLUG-Owl" -> "InternLM/InternLM-XComposer"
"X-PLUG/mPLUG-Owl" -> "X-PLUG/mPLUG-DocOwl"
"X-PLUG/mPLUG-Owl" -> "DAMO-NLP-SG/Video-LLaMA"
"X-PLUG/mPLUG-Owl" -> "LLaVA-VL/LLaVA-NeXT"
"X-PLUG/mPLUG-Owl" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"X-PLUG/mPLUG-Owl" -> "Alpha-VLLM/LLaMA2-Accessory"
"X-PLUG/mPLUG-Owl" -> "open-compass/VLMEvalKit"
"X-PLUG/mPLUG-Owl" -> "OpenGVLab/InternVideo"
"X-PLUG/mPLUG-Owl" -> "haotian-liu/LLaVA"
"X-PLUG/mPLUG-Owl" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"X-PLUG/mPLUG-Owl" -> "baaivision/EVA" ["e"=1]
"AoiDragon/POPE" -> "RUCAIBox/POPE"
"AoiDragon/POPE" -> "LisaAnne/Hallucination"
"AoiDragon/POPE" -> "junyangwang0410/AMBER"
"AILab-CVC/SEED" -> "AILab-CVC/SEED-Bench" ["e"=1]
"facebookresearch/dinov2" -> "haotian-liu/LLaVA" ["e"=1]
"ttengwang/Caption-Anything" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"ttengwang/Caption-Anything" -> "shikras/shikra" ["e"=1]
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "PKU-YuanGroup/Video-LLaVA"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "DAMO-NLP-SG/VideoLLaMA2"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "LLaVA-VL/LLaVA-NeXT"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "OpenGVLab/InternVideo"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "DAMO-NLP-SG/Video-LLaMA"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "mbzuai-oryx/Video-ChatGPT"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "OpenGVLab/Ask-Anything"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "open-compass/VLMEvalKit"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "MME-Benchmarks/Video-MME"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "RenShuhuai-Andy/TimeChat"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "rese1f/MovieChat"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"yunlong10/Awesome-LLMs-for-Video-Understanding" -> "dvlab-research/LLaMA-VID"
"modal-labs/quillman" -> "lxe/llavavision" ["e"=1]
"yxuansu/PandaGPT" -> "CASIA-IVA-Lab/AnomalyGPT" ["e"=1]
"yxuansu/PandaGPT" -> "lyuchenyang/Macaw-LLM"
"yxuansu/PandaGPT" -> "zeroQiaoba/AffectGPT" ["e"=1]
"yxuansu/PandaGPT" -> "luogen1996/LaVIN"
"yxuansu/PandaGPT" -> "mbzuai-oryx/Video-ChatGPT"
"yxuansu/PandaGPT" -> "OptimalScale/DetGPT"
"yxuansu/PandaGPT" -> "open-mmlab/Multimodal-GPT"
"yxuansu/PandaGPT" -> "OpenGVLab/LAMM"
"yxuansu/PandaGPT" -> "0nutation/SpeechGPT" ["e"=1]
"yxuansu/PandaGPT" -> "X-PLUG/mPLUG-Owl"
"yxuansu/PandaGPT" -> "baaivision/Emu" ["e"=1]
"yxuansu/PandaGPT" -> "HenryHZY/Awesome-Multimodal-LLM"
"yxuansu/PandaGPT" -> "microsoft/i-Code"
"yxuansu/PandaGPT" -> "csuhan/OneLLM"
"yxuansu/PandaGPT" -> "LinkSoul-AI/LLaSM" ["e"=1]
"OFA-Sys/ONE-PEACE" -> "PKU-YuanGroup/LanguageBind" ["e"=1]
"OFA-Sys/ONE-PEACE" -> "DAMO-NLP-SG/Video-LLaMA" ["e"=1]
"abacaj/code-eval" -> "AblateIt/finetune-study" ["e"=1]
"THUDM/CogVLM" -> "QwenLM/Qwen-VL"
"THUDM/CogVLM" -> "haotian-liu/LLaVA"
"THUDM/CogVLM" -> "THUDM/CogVLM2"
"THUDM/CogVLM" -> "THUDM/VisualGLM-6B" ["e"=1]
"THUDM/CogVLM" -> "OpenGVLab/InternVL"
"THUDM/CogVLM" -> "salesforce/LAVIS" ["e"=1]
"THUDM/CogVLM" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"THUDM/CogVLM" -> "InternLM/InternLM-XComposer"
"THUDM/CogVLM" -> "QwenLM/Qwen2.5-VL"
"THUDM/CogVLM" -> "IDEA-Research/GroundingDINO" ["e"=1]
"THUDM/CogVLM" -> "modelscope/ms-swift" ["e"=1]
"THUDM/CogVLM" -> "LLaVA-VL/LLaVA-NeXT"
"THUDM/CogVLM" -> "mlfoundations/open_clip" ["e"=1]
"THUDM/CogVLM" -> "THUDM/ChatGLM3" ["e"=1]
"THUDM/CogVLM" -> "THUDM/GLM-4" ["e"=1]
"xinyu1205/recognize-anything" -> "OpenGVLab/Ask-Anything" ["e"=1]
"xinyu1205/recognize-anything" -> "dvlab-research/LISA" ["e"=1]
"xinyu1205/recognize-anything" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"invictus717/MetaTransformer" -> "csuhan/OneLLM"
"invictus717/MetaTransformer" -> "facebookresearch/ImageBind" ["e"=1]
"invictus717/MetaTransformer" -> "PKU-YuanGroup/LanguageBind"
"invictus717/MetaTransformer" -> "baaivision/Emu" ["e"=1]
"invictus717/MetaTransformer" -> "NExT-GPT/NExT-GPT"
"invictus717/MetaTransformer" -> "dvlab-research/LISA"
"invictus717/MetaTransformer" -> "Alpha-VLLM/LLaMA2-Accessory"
"invictus717/MetaTransformer" -> "baaivision/Uni3D" ["e"=1]
"invictus717/MetaTransformer" -> "baaivision/EVA" ["e"=1]
"invictus717/MetaTransformer" -> "mlfoundations/open_flamingo" ["e"=1]
"invictus717/MetaTransformer" -> "DAMO-NLP-SG/Video-LLaMA"
"invictus717/MetaTransformer" -> "thu-ml/unidiffuser" ["e"=1]
"invictus717/MetaTransformer" -> "facebookresearch/multimodal" ["e"=1]
"invictus717/MetaTransformer" -> "OpenGVLab/VisionLLM"
"invictus717/MetaTransformer" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"Yuliang-Liu/MultimodalOCR" -> "SALT-NLP/LLaVAR" ["e"=1]
"Yuliang-Liu/MultimodalOCR" -> "Ucas-HaoranWei/Vary" ["e"=1]
"Yuliang-Liu/MultimodalOCR" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"Yuliang-Liu/MultimodalOCR" -> "nttmdlab-nlp/InstructDoc" ["e"=1]
"QwenLM/Qwen-Agent" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"QwenLM/Qwen-Agent" -> "QwenLM/Qwen-VL" ["e"=1]
"InternLM/InternLM-XComposer" -> "OpenGVLab/InternVL"
"InternLM/InternLM-XComposer" -> "QwenLM/Qwen-VL"
"InternLM/InternLM-XComposer" -> "open-compass/VLMEvalKit"
"InternLM/InternLM-XComposer" -> "InternLM/InternLM" ["e"=1]
"InternLM/InternLM-XComposer" -> "LLaVA-VL/LLaVA-NeXT"
"InternLM/InternLM-XComposer" -> "InternLM/xtuner" ["e"=1]
"InternLM/InternLM-XComposer" -> "InternLM/lagent" ["e"=1]
"InternLM/InternLM-XComposer" -> "InternLM/lmdeploy" ["e"=1]
"InternLM/InternLM-XComposer" -> "THUDM/CogVLM"
"InternLM/InternLM-XComposer" -> "Yuliang-Liu/Monkey" ["e"=1]
"InternLM/InternLM-XComposer" -> "cambrian-mllm/cambrian"
"InternLM/InternLM-XComposer" -> "X-PLUG/mPLUG-Owl"
"InternLM/InternLM-XComposer" -> "baaivision/Emu" ["e"=1]
"InternLM/InternLM-XComposer" -> "THUDM/CogVLM2"
"InternLM/InternLM-XComposer" -> "PKU-YuanGroup/Video-LLaVA"
"NVlabs/ODISE" -> "dvlab-research/LISA" ["e"=1]
"open-mmlab/playground" -> "open-mmlab/Multimodal-GPT" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "DirtyHarryLYL/LLM-in-Vision" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "mbzuai-oryx/Video-LLaVA" ["e"=1]
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "DirtyHarryLYL/LLM-in-Vision" ["e"=1]
"mbzuai-oryx/Video-ChatGPT" -> "PKU-YuanGroup/Video-LLaVA"
"mbzuai-oryx/Video-ChatGPT" -> "DAMO-NLP-SG/Video-LLaMA"
"mbzuai-oryx/Video-ChatGPT" -> "OpenGVLab/Ask-Anything"
"mbzuai-oryx/Video-ChatGPT" -> "mbzuai-oryx/VideoGPT-plus"
"mbzuai-oryx/Video-ChatGPT" -> "DAMO-NLP-SG/VideoLLaMA2"
"mbzuai-oryx/Video-ChatGPT" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"mbzuai-oryx/Video-ChatGPT" -> "rese1f/MovieChat"
"mbzuai-oryx/Video-ChatGPT" -> "RenShuhuai-Andy/TimeChat"
"mbzuai-oryx/Video-ChatGPT" -> "OpenGVLab/InternVideo"
"mbzuai-oryx/Video-ChatGPT" -> "dvlab-research/LLaMA-VID"
"mbzuai-oryx/Video-ChatGPT" -> "mbzuai-oryx/Video-LLaVA"
"mbzuai-oryx/Video-ChatGPT" -> "mbzuai-oryx/groundingLMM"
"mbzuai-oryx/Video-ChatGPT" -> "PKU-YuanGroup/Chat-UniVi" ["e"=1]
"mbzuai-oryx/Video-ChatGPT" -> "LLaVA-VL/LLaVA-NeXT"
"mbzuai-oryx/Video-ChatGPT" -> "Vision-CAIR/MiniGPT4-video"
"facebookresearch/MetaCLIP" -> "tsb0601/MMVP" ["e"=1]
"facebookresearch/MetaCLIP" -> "cambrian-mllm/cambrian" ["e"=1]
"Jamie-Stirling/RetNet" -> "invictus717/MetaTransformer" ["e"=1]
"OpenGVLab/VideoMAEv2" -> "OpenGVLab/InternVideo" ["e"=1]
"voidism/DoLa" -> "DAMO-NLP-SG/VCD" ["e"=1]
"voidism/DoLa" -> "shikiw/OPERA" ["e"=1]
"voidism/DoLa" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination" ["e"=1]
"voidism/DoLa" -> "BillChan226/HALC" ["e"=1]
"YiyangZhou/LURE" -> "BillChan226/HALC"
"YiyangZhou/LURE" -> "RUCAIBox/POPE"
"YiyangZhou/LURE" -> "YiyangZhou/CSR"
"YiyangZhou/LURE" -> "DAMO-NLP-SG/VCD"
"YiyangZhou/LURE" -> "YiyangZhou/POVID"
"YiyangZhou/LURE" -> "shikiw/OPERA"
"YiyangZhou/LURE" -> "FuxiaoLiu/LRV-Instruction"
"YiyangZhou/LURE" -> "AoiDragon/POPE"
"YiyangZhou/LURE" -> "tianyi-lab/HallusionBench"
"YiyangZhou/LURE" -> "junyangwang0410/AMBER"
"YiyangZhou/LURE" -> "Purshow/Awesome-LVLM-Hallucination"
"X-PLUG/mPLUG-DocOwl" -> "AlibabaResearch/AdvancedLiterateMachinery" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "Yuliang-Liu/Monkey" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "X-PLUG/mPLUG-Owl"
"X-PLUG/mPLUG-DocOwl" -> "Ucas-HaoranWei/Vary"
"X-PLUG/mPLUG-DocOwl" -> "Ucas-HaoranWei/GOT-OCR2.0" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "Yuliang-Liu/MultimodalOCR" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "QwenLM/Qwen-VL"
"X-PLUG/mPLUG-DocOwl" -> "clovaai/donut" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "InternLM/InternLM-XComposer"
"X-PLUG/mPLUG-DocOwl" -> "LukeForeverYoung/UReader" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "OpenGVLab/InternVL"
"X-PLUG/mPLUG-DocOwl" -> "open-compass/VLMEvalKit"
"X-PLUG/mPLUG-DocOwl" -> "opendatalab/DocLayout-YOLO" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "illuin-tech/colpali" ["e"=1]
"X-PLUG/mPLUG-DocOwl" -> "hikopensource/DAVAR-Lab-OCR" ["e"=1]
"FuxiaoLiu/MMC" -> "vis-nlp/UniChart"
"FuxiaoLiu/MMC" -> "FuxiaoLiu/Twitter-Video-dataset"
"FuxiaoLiu/MMC" -> "tingxueronghua/ChartLlama-code"
"FuxiaoLiu/MMC" -> "FuxiaoLiu/DocumentCLIP"
"OpenBMB/VisCPM" -> "shikras/shikra" ["e"=1]
"OpenBMB/VisCPM" -> "RLHF-V/RLAIF-V" ["e"=1]
"OpenBMB/VisCPM" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"OpenGVLab/VisionLLM" -> "dvlab-research/LISA"
"OpenGVLab/VisionLLM" -> "OpenGVLab/InternGPT" ["e"=1]
"OpenGVLab/VisionLLM" -> "jshilong/GPT4RoI"
"OpenGVLab/VisionLLM" -> "OptimalScale/DetGPT"
"OpenGVLab/VisionLLM" -> "DirtyHarryLYL/LLM-in-Vision"
"OpenGVLab/VisionLLM" -> "microsoft/GLIP" ["e"=1]
"OpenGVLab/VisionLLM" -> "cientgu/InstructDiffusion" ["e"=1]
"OpenGVLab/VisionLLM" -> "shikras/shikra"
"OpenGVLab/VisionLLM" -> "SkyworkAI/Vitron"
"OpenGVLab/VisionLLM" -> "baaivision/EVA" ["e"=1]
"OpenGVLab/VisionLLM" -> "cambrian-mllm/cambrian"
"OpenGVLab/VisionLLM" -> "microsoft/X-Decoder" ["e"=1]
"OpenGVLab/VisionLLM" -> "lxtGH/OMG-Seg" ["e"=1]
"OpenGVLab/VisionLLM" -> "yuhangzang/ContextDET"
"OpenGVLab/VisionLLM" -> "google-research/pix2seq" ["e"=1]
"YUCHEN005/GILA" -> "YUCHEN005/MIR-GAN"
"YUCHEN005/GILA" -> "YUCHEN005/RATS-Channel-A-Speech-Data"
"YUCHEN005/GILA" -> "YUCHEN005/UNA-GAN"
"YUCHEN005/GILA" -> "YUCHEN005/UniVPM"
"EvolvingLMMs-Lab/RelateAnything" -> "JialianW/GRiT" ["e"=1]
"PKU-YuanGroup/LanguageBind" -> "PKU-YuanGroup/Video-LLaVA"
"PKU-YuanGroup/LanguageBind" -> "PKU-YuanGroup/Video-Bench" ["e"=1]
"PKU-YuanGroup/LanguageBind" -> "PKU-YuanGroup/MoE-LLaVA"
"PKU-YuanGroup/LanguageBind" -> "PKU-YuanGroup/Chat-UniVi" ["e"=1]
"PKU-YuanGroup/LanguageBind" -> "OpenGVLab/InternVideo"
"PKU-YuanGroup/LanguageBind" -> "csuhan/OneLLM"
"PKU-YuanGroup/LanguageBind" -> "mbzuai-oryx/Video-ChatGPT"
"PKU-YuanGroup/LanguageBind" -> "DAMO-NLP-SG/Video-LLaMA"
"PKU-YuanGroup/LanguageBind" -> "fabawi/ImageBind-LoRA" ["e"=1]
"PKU-YuanGroup/LanguageBind" -> "dvlab-research/LLaMA-VID"
"PKU-YuanGroup/LanguageBind" -> "DAMO-NLP-SG/VideoLLaMA2"
"PKU-YuanGroup/LanguageBind" -> "snap-research/Panda-70M" ["e"=1]
"PKU-YuanGroup/LanguageBind" -> "TXH-mercury/VALOR" ["e"=1]
"PKU-YuanGroup/LanguageBind" -> "RenShuhuai-Andy/TimeChat"
"PKU-YuanGroup/LanguageBind" -> "TXH-mercury/VAST" ["e"=1]
"microsoft/LLaVA-Med" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"allenai/mmc4" -> "AILab-CVC/SEED-Bench" ["e"=1]
"allenai/mmc4" -> "allenai/unified-io-inference" ["e"=1]
"allenai/mmc4" -> "HaozheZhao/MIC" ["e"=1]
"allenai/mmc4" -> "JialianW/GRiT" ["e"=1]
"tsujuifu/pytorch_mgie" -> "apple/ml-mgie"
"tsujuifu/pytorch_mgie" -> "TencentARC/SmartEdit" ["e"=1]
"yuhangzang/ContextDET" -> "jshilong/GPT4RoI"
"rese1f/MovieChat" -> "RenShuhuai-Andy/TimeChat"
"rese1f/MovieChat" -> "boheumd/MA-LMM"
"rese1f/MovieChat" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"rese1f/MovieChat" -> "mbzuai-oryx/Video-ChatGPT"
"rese1f/MovieChat" -> "huangb23/VTimeLLM"
"rese1f/MovieChat" -> "dvlab-research/LLaMA-VID"
"rese1f/MovieChat" -> "magic-research/PLLaVA"
"rese1f/MovieChat" -> "PKU-YuanGroup/Chat-UniVi" ["e"=1]
"rese1f/MovieChat" -> "MME-Benchmarks/Video-MME"
"rese1f/MovieChat" -> "Vision-CAIR/LongVU"
"rese1f/MovieChat" -> "EvolvingLMMs-Lab/LongVA"
"rese1f/MovieChat" -> "VectorSpaceLab/Video-XL"
"rese1f/MovieChat" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"rese1f/MovieChat" -> "DCDmllm/Momentor"
"rese1f/MovieChat" -> "Yui010206/SeViLA"
"modelscope/motionagent" -> "modelscope/DiffSynth-Engine"
"jy0205/LaVIT" -> "shikras/shikra" ["e"=1]
"showlab/Image2Paragraph" -> "JialianW/GRiT" ["e"=1]
"showlab/Image2Paragraph" -> "shikras/shikra" ["e"=1]
"LinkSoul-AI/Chinese-LLaVA" -> "SALT-NLP/LLaVAR" ["e"=1]
"OpenGVLab/unmasked_teacher" -> "OpenGVLab/VideoChat-Flash" ["e"=1]
"OpenGVLab/unmasked_teacher" -> "OpenGVLab/InternVideo" ["e"=1]
"HaozheZhao/MIC" -> "pkunlp-icler/MIC"
"HaozheZhao/MIC" -> "HaozheZhao/MIC_tool"
"HaozheZhao/MIC" -> "pkunlp-icler/PCA-EVAL" ["e"=1]
"HaozheZhao/MIC" -> "X2FD/LVIS-INSTRUCT4V"
"HaozheZhao/MIC" -> "shikras/shikra"
"HaozheZhao/MIC" -> "VT-NLP/MultiInstruct"
"HaozheZhao/MIC" -> "HenryHZY/Awesome-Multimodal-LLM"
"HaozheZhao/MIC" -> "DCDmllm/Cheetah"
"HaozheZhao/MIC" -> "allenai/mmc4" ["e"=1]
"HaozheZhao/MIC" -> "RLHF-V/RLHF-V"
"OpenGVLab/Ask-Anything" -> "DAMO-NLP-SG/Video-LLaMA"
"OpenGVLab/Ask-Anything" -> "OpenGVLab/InternVideo"
"OpenGVLab/Ask-Anything" -> "mbzuai-oryx/Video-ChatGPT"
"OpenGVLab/Ask-Anything" -> "PKU-YuanGroup/Video-LLaVA"
"OpenGVLab/Ask-Anything" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"OpenGVLab/Ask-Anything" -> "LLaVA-VL/LLaVA-NeXT"
"OpenGVLab/Ask-Anything" -> "X-PLUG/mPLUG-Owl"
"OpenGVLab/Ask-Anything" -> "salesforce/LAVIS" ["e"=1]
"OpenGVLab/Ask-Anything" -> "rese1f/MovieChat"
"OpenGVLab/Ask-Anything" -> "DAMO-NLP-SG/VideoLLaMA2"
"OpenGVLab/Ask-Anything" -> "dvlab-research/LLaMA-VID"
"OpenGVLab/Ask-Anything" -> "OpenGVLab/InternVL"
"OpenGVLab/Ask-Anything" -> "xinyu1205/recognize-anything" ["e"=1]
"OpenGVLab/Ask-Anything" -> "OpenGVLab/InternGPT" ["e"=1]
"OpenGVLab/Ask-Anything" -> "mlfoundations/open_flamingo" ["e"=1]
"cvlab-columbia/viper" -> "allenai/visprog" ["e"=1]
"cvlab-columbia/viper" -> "mbzuai-oryx/Video-ChatGPT" ["e"=1]
"cvlab-columbia/viper" -> "dvlab-research/LISA" ["e"=1]
"lupantech/chameleon-llm" -> "lupantech/ScienceQA" ["e"=1]
"InternLM/InternLM-techreport" -> "InternLM/InternLM-XComposer" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "jshilong/GPT4RoI"
"OpenGVLab/Multi-Modality-Arena" -> "OpenGVLab/LAMM"
"OpenGVLab/Multi-Modality-Arena" -> "baaivision/Emu" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "RUCAIBox/POPE"
"OpenGVLab/Multi-Modality-Arena" -> "shikras/shikra"
"OpenGVLab/Multi-Modality-Arena" -> "FreedomIntelligence/HuatuoGPT-Vision" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "microsoft/LLaVA-Med" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "xiaoman-zhang/PMC-VQA" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "AILab-CVC/SEED" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "OpenGVLab/MMT-Bench"
"OpenGVLab/Multi-Modality-Arena" -> "AILab-CVC/SEED-Bench"
"OpenGVLab/Multi-Modality-Arena" -> "Vision-CAIR/ChatCaptioner" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "BAAI-DCAI/M3D" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "RunpeiDong/DreamLLM" ["e"=1]
"OpenGVLab/Multi-Modality-Arena" -> "HaozheZhao/MIC"
"OpenGVLab/all-seeing" -> "BAAI-DCAI/Visual-Instruction-Tuning"
"OpenGVLab/all-seeing" -> "jshilong/GPT4RoI"
"OpenGVLab/all-seeing" -> "shikras/shikra"
"OpenGVLab/all-seeing" -> "FreedomIntelligence/ALLaVA"
"OpenGVLab/all-seeing" -> "OpenGVLab/OmniCorpus"
"OpenGVLab/all-seeing" -> "SALT-NLP/LLaVAR"
"OpenGVLab/all-seeing" -> "shikras/d-cube" ["e"=1]
"OpenGVLab/all-seeing" -> "tsb0601/MMVP"
"OpenGVLab/all-seeing" -> "mbzuai-oryx/groundingLMM"
"OpenGVLab/all-seeing" -> "FoundationVision/Groma" ["e"=1]
"OpenGVLab/all-seeing" -> "FuxiaoLiu/LRV-Instruction"
"magic-research/bubogpt" -> "AILab-CVC/Animate-A-Story" ["e"=1]
"magic-research/bubogpt" -> "WisconsinAIVision/ViP-LLaVA"
"magic-research/bubogpt" -> "shikras/shikra"
"magic-research/bubogpt" -> "jshilong/GPT4RoI"
"magic-research/bubogpt" -> "OpenGVLab/all-seeing"
"HillZhang1999/llm-hallucination-survey" -> "showlab/Awesome-MLLM-Hallucination" ["e"=1]
"VITA-MLLM/Woodpecker" -> "MME-Benchmarks/Video-MME"
"VITA-MLLM/Woodpecker" -> "shenyunhang/APE" ["e"=1]
"VITA-MLLM/Woodpecker" -> "RUCAIBox/POPE"
"VITA-MLLM/Woodpecker" -> "shikiw/OPERA"
"VITA-MLLM/Woodpecker" -> "FuxiaoLiu/LRV-Instruction"
"VITA-MLLM/Woodpecker" -> "YiyangZhou/LURE"
"VITA-MLLM/Woodpecker" -> "DAMO-NLP-SG/VCD"
"VITA-MLLM/Woodpecker" -> "showlab/Awesome-MLLM-Hallucination"
"VITA-MLLM/Woodpecker" -> "tianyi-lab/HallusionBench"
"VITA-MLLM/Woodpecker" -> "llava-rlhf/LLaVA-RLHF"
"VITA-MLLM/Woodpecker" -> "shikras/shikra"
"VITA-MLLM/Woodpecker" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"VITA-MLLM/Woodpecker" -> "xjtupanda/Sparrow"
"VITA-MLLM/Woodpecker" -> "VITA-MLLM/VITA" ["e"=1]
"VITA-MLLM/Woodpecker" -> "BillChan226/HALC"
"MrYxJ/calculate-flops.pytorch" -> "daixiangzi/Awesome-Token-Compress" ["e"=1]
"mlpc-ucsd/BLIVA" -> "FreedomIntelligence/ALLaVA"
"ziplab/PTQD" -> "ChangyuanWang17/APQ-DM" ["e"=1]
"liltom-eth/llama2-webui" -> "Alpha-VLLM/LLaMA2-Accessory" ["e"=1]
"jondurbin/airoboros" -> "AblateIt/finetune-study" ["e"=1]
"kohjingyu/gill" -> "HaozheZhao/MIC" ["e"=1]
"egoschema/EgoSchema" -> "doc-doc/NExT-QA" ["e"=1]
"egoschema/EgoSchema" -> "Share14/ShareGemini"
"egoschema/EgoSchema" -> "bigai-nlco/VideoLLaMB"
"jshilong/GPT4RoI" -> "shikras/shikra"
"jshilong/GPT4RoI" -> "OpenGVLab/all-seeing"
"jshilong/GPT4RoI" -> "JialianW/GRiT"
"jshilong/GPT4RoI" -> "jshilong/GroupRCNN" ["e"=1]
"jshilong/GPT4RoI" -> "mbzuai-oryx/groundingLMM"
"jshilong/GPT4RoI" -> "jshilong/DDQ" ["e"=1]
"jshilong/GPT4RoI" -> "microsoft/X-Decoder" ["e"=1]
"jshilong/GPT4RoI" -> "FoundationVision/Groma" ["e"=1]
"jshilong/GPT4RoI" -> "OptimalScale/DetGPT"
"jshilong/GPT4RoI" -> "facebookresearch/VLPart" ["e"=1]
"jshilong/GPT4RoI" -> "jshilong/FisherPruning" ["e"=1]
"jshilong/GPT4RoI" -> "OpenGVLab/VisionLLM"
"jshilong/GPT4RoI" -> "open-mmlab/Multimodal-GPT"
"jshilong/GPT4RoI" -> "OpenGVLab/Multi-Modality-Arena"
"jshilong/GPT4RoI" -> "yuhangzang/ContextDET"
"shenyunhang/APE" -> "VITA-MLLM/Woodpecker" ["e"=1]
"shenyunhang/APE" -> "MME-Benchmarks/Video-MME" ["e"=1]
"shenyunhang/APE" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"hyungkwonko/chart-llm" -> "mitvis/vistext" ["e"=1]
"eric-ai-lab/MiniGPT-5" -> "shikras/shikra" ["e"=1]
"eric-ai-lab/MiniGPT-5" -> "RUCAIBox/POPE" ["e"=1]
"eric-ai-lab/MiniGPT-5" -> "InternLM/InternLM-XComposer" ["e"=1]
"eric-ai-lab/MiniGPT-5" -> "LLaVA-VL/LLaVA-Plus-Codebase" ["e"=1]
"eric-ai-lab/MiniGPT-5" -> "mbzuai-oryx/Video-ChatGPT" ["e"=1]
"eric-ai-lab/MiniGPT-5" -> "VITA-MLLM/Woodpecker" ["e"=1]
"eric-ai-lab/MiniGPT-5" -> "NExT-GPT/NExT-GPT" ["e"=1]
"NVlabs/prismer" -> "microsoft/MM-REACT" ["e"=1]
"X-PLUG/ChatPLUG" -> "X-PLUG/mPLUG-Owl" ["e"=1]
"showlab/VLog" -> "mbzuai-oryx/Video-ChatGPT" ["e"=1]
"showlab/VLog" -> "JialianW/GRiT" ["e"=1]
"showlab/VLog" -> "AILab-CVC/SEED-Bench" ["e"=1]
"berkeley-hipie/HIPIE" -> "aimagelab/freeda" ["e"=1]
"SkunkworksAI/hydra-moe" -> "hydrallm/llama-moe-v1"
"SkunkworksAI/hydra-moe" -> "r-three/phatgoose"
"SkunkworksAI/hydra-moe" -> "AblateIt/finetune-study"
"SkunkworksAI/hydra-moe" -> "taylorai/galactic" ["e"=1]
"SkunkworksAI/hydra-moe" -> "Cohere-Labs-Community/parameter-efficient-moe" ["e"=1]
"SkunkworksAI/hydra-moe" -> "liuqidong07/MOELoRA-peft" ["e"=1]
"SkunkworksAI/hydra-moe" -> "SkunkworksAI/BakLLaVA"
"TXH-mercury/VAST" -> "OpenGVLab/VideoChat-Flash" ["e"=1]
"TXH-mercury/VAST" -> "rese1f/MovieChat" ["e"=1]
"TXH-mercury/VAST" -> "RenShuhuai-Andy/TimeChat" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "mbzuai-oryx/groundingLMM"
"DirtyHarryLYL/LLM-in-Vision" -> "OpenGVLab/VisionLLM"
"DirtyHarryLYL/LLM-in-Vision" -> "baaivision/Emu" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "Computer-Vision-in-the-Wild/CVinW_Readings"
"DirtyHarryLYL/LLM-in-Vision" -> "JindongGu/Awesome-Prompting-on-Vision-Language-Model" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "jy0205/LaVIT" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "Yangyi-Chen/Multimodal-AND-Large-Language-Models"
"DirtyHarryLYL/LLM-in-Vision" -> "HenryHZY/Awesome-Multimodal-LLM"
"DirtyHarryLYL/LLM-in-Vision" -> "DirtyHarryLYL/Transformer-in-Vision" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "dvlab-research/LISA"
"DirtyHarryLYL/LLM-in-Vision" -> "jianzongwu/Awesome-Open-Vocabulary" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "cambrian-mllm/cambrian"
"DirtyHarryLYL/LLM-in-Vision" -> "allenai/unified-io-2" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "ytongbai/LVM" ["e"=1]
"42Shawn/PTQ4DM" -> "ChangyuanWang17/APQ-DM" ["e"=1]
"Ma-Lab-Berkeley/CRATE" -> "invictus717/MetaTransformer" ["e"=1]
"Ma-Lab-Berkeley/CRATE" -> "cambrian-mllm/cambrian" ["e"=1]
"BAAI-DCAI/Visual-Instruction-Tuning" -> "BAAI-DCAI/DataOptim"
"BAAI-DCAI/Visual-Instruction-Tuning" -> "OpenGVLab/all-seeing"
"shikras/shikra" -> "jshilong/GPT4RoI"
"shikras/shikra" -> "OpenGVLab/all-seeing"
"shikras/shikra" -> "dvlab-research/LISA"
"shikras/shikra" -> "mbzuai-oryx/groundingLMM"
"shikras/shikra" -> "baaivision/Emu" ["e"=1]
"shikras/shikra" -> "FuxiaoLiu/LRV-Instruction"
"shikras/shikra" -> "RUCAIBox/POPE"
"shikras/shikra" -> "luogen1996/LaVIN"
"shikras/shikra" -> "jy0205/LaVIT" ["e"=1]
"shikras/shikra" -> "RunpeiDong/DreamLLM" ["e"=1]
"shikras/shikra" -> "JialianW/GRiT"
"shikras/shikra" -> "OptimalScale/DetGPT"
"shikras/shikra" -> "tsb0601/MMVP"
"shikras/shikra" -> "HaozheZhao/MIC"
"shikras/shikra" -> "microsoft/MM-REACT"
"EdinburghNLP/awesome-hallucination-detection" -> "showlab/Awesome-MLLM-Hallucination" ["e"=1]
"VPGTrans/VPGTrans" -> "NExT-ChatV/NExT-Chat"
"HenryHZY/Awesome-Multimodal-LLM" -> "vincentlux/Awesome-Multimodal-LLM"
"HenryHZY/Awesome-Multimodal-LLM" -> "RUCAIBox/POPE"
"HenryHZY/Awesome-Multimodal-LLM" -> "HaozheZhao/MIC"
"HenryHZY/Awesome-Multimodal-LLM" -> "VT-NLP/MultiInstruct"
"HenryHZY/Awesome-Multimodal-LLM" -> "DirtyHarryLYL/LLM-in-Vision"
"HenryHZY/Awesome-Multimodal-LLM" -> "AILab-CVC/SEED" ["e"=1]
"SALT-NLP/LLaVAR" -> "FuxiaoLiu/LRV-Instruction"
"SALT-NLP/LLaVAR" -> "BAAI-DCAI/Visual-Instruction-Tuning"
"SALT-NLP/LLaVAR" -> "luogen1996/LaVIN"
"SALT-NLP/LLaVAR" -> "OpenGVLab/all-seeing"
"SALT-NLP/LLaVAR" -> "Yuliang-Liu/MultimodalOCR" ["e"=1]
"SALT-NLP/LLaVAR" -> "thunlp/LLaVA-UHD"
"SALT-NLP/LLaVAR" -> "YuchenLiu98/COMM"
"facebookresearch/VLPart" -> "jshilong/GPT4RoI" ["e"=1]
"ytongbai/LVM" -> "cambrian-mllm/cambrian" ["e"=1]
"ytongbai/LVM" -> "dvlab-research/LISA" ["e"=1]
"InternLM/lagent" -> "InternLM/InternLM-XComposer" ["e"=1]
"Vision-CAIR/ChatCaptioner" -> "JialianW/GRiT" ["e"=1]
"Vision-CAIR/ChatCaptioner" -> "OpenGVLab/Multi-Modality-Arena" ["e"=1]
"RunpeiDong/DreamLLM" -> "BAAI-DCAI/Visual-Instruction-Tuning" ["e"=1]
"RunpeiDong/DreamLLM" -> "shikras/shikra" ["e"=1]
"RunpeiDong/DreamLLM" -> "WisconsinAIVision/ViP-LLaVA" ["e"=1]
"sail-sg/lorahub" -> "r-three/phatgoose" ["e"=1]
"mathllm/MathCoder" -> "mathllm/MATH-V" ["e"=1]
"apitube/docs" -> "mevbotcrypto/mev-bot"
"mevbotcrypto/mev-bot" -> "apitube/docs"
"opendatalab/WanJuan1.0" -> "OpenGVLab/OmniCorpus" ["e"=1]
"FuxiaoLiu/LRV-Instruction" -> "tianyi-lab/HallusionBench"
"FuxiaoLiu/LRV-Instruction" -> "RUCAIBox/POPE"
"FuxiaoLiu/LRV-Instruction" -> "Yuqifan1117/HalluciDoctor"
"FuxiaoLiu/LRV-Instruction" -> "shikiw/OPERA"
"FuxiaoLiu/LRV-Instruction" -> "YiyangZhou/LURE"
"FuxiaoLiu/LRV-Instruction" -> "junyangwang0410/AMBER"
"FuxiaoLiu/LRV-Instruction" -> "BillChan226/HALC"
"FuxiaoLiu/LRV-Instruction" -> "X2FD/LVIS-INSTRUCT4V"
"FuxiaoLiu/LRV-Instruction" -> "DAMO-NLP-SG/VCD"
"FuxiaoLiu/LRV-Instruction" -> "FuxiaoLiu/MMC"
"FuxiaoLiu/LRV-Instruction" -> "VITA-MLLM/Woodpecker"
"FuxiaoLiu/LRV-Instruction" -> "SALT-NLP/LLaVAR"
"FuxiaoLiu/LRV-Instruction" -> "llava-rlhf/LLaVA-RLHF"
"FuxiaoLiu/LRV-Instruction" -> "FuxiaoLiu/DocumentCLIP"
"FuxiaoLiu/LRV-Instruction" -> "FreedomIntelligence/ALLaVA"
"LuckyyySTA/Awesome-LLM-hallucination" -> "showlab/Awesome-MLLM-Hallucination" ["e"=1]
"LuckyyySTA/Awesome-LLM-hallucination" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination" ["e"=1]
"open-compass/MMBench" -> "yuweihao/MM-Vet"
"open-compass/MMBench" -> "junyangwang0410/AMBER"
"open-compass/MMBench" -> "RUCAIBox/POPE"
"open-compass/MMBench" -> "open-compass/VLMEvalKit"
"open-compass/MMBench" -> "tianyi-lab/HallusionBench"
"uncbiag/Awesome-Foundation-Models" -> "Computer-Vision-in-the-Wild/CVinW_Readings" ["e"=1]
"RupertLuo/Valley" -> "DCDmllm/Momentor"
"RupertLuo/Valley" -> "RenShuhuai-Andy/TimeChat"
"RupertLuo/Valley" -> "bytedance/Shot2Story"
"RupertLuo/Valley" -> "PKU-YuanGroup/Video-Bench" ["e"=1]
"RupertLuo/Valley" -> "TimeMarker-LLM/TimeMarker"
"RupertLuo/Valley" -> "huangb23/VTimeLLM"
"RupertLuo/Valley" -> "EvolvingLMMs-Lab/LongVA"
"RupertLuo/Valley" -> "Share14/ShareGemini"
"RUCAIBox/POPE" -> "junyangwang0410/AMBER"
"RUCAIBox/POPE" -> "AoiDragon/POPE"
"RUCAIBox/POPE" -> "YiyangZhou/LURE"
"RUCAIBox/POPE" -> "FuxiaoLiu/LRV-Instruction"
"RUCAIBox/POPE" -> "LisaAnne/Hallucination"
"RUCAIBox/POPE" -> "tianyi-lab/HallusionBench"
"RUCAIBox/POPE" -> "BillChan226/HALC"
"RUCAIBox/POPE" -> "yuweihao/MM-Vet"
"RUCAIBox/POPE" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"RUCAIBox/POPE" -> "shikiw/OPERA"
"RUCAIBox/POPE" -> "DAMO-NLP-SG/VCD"
"RUCAIBox/POPE" -> "VITA-MLLM/Woodpecker"
"Atomic-man007/Awesome_Multimodel_LLM" -> "showlab/Awesome-MLLM-Hallucination" ["e"=1]
"Atomic-man007/Awesome_Multimodel_LLM" -> "FreedomIntelligence/MLLM-Bench" ["e"=1]
"lyingCS/UOEP" -> "lyingCS/Controllable-Multi-Objective-Reranking"
"lyingCS/UOEP" -> "OpenGVLab/Multitask-Model-Selector"
"antoyang/VidChapters" -> "RenShuhuai-Andy/TimeChat" ["e"=1]
"DCDmllm/Cheetah" -> "DCDmllm/MorphTokens"
"DCDmllm/Cheetah" -> "DCDmllm/AnyEdit" ["e"=1]
"DCDmllm/Cheetah" -> "HaozheZhao/MIC"
"OptimalScale/DetGPT" -> "jshilong/GPT4RoI"
"OptimalScale/DetGPT" -> "microsoft/GLIP" ["e"=1]
"OptimalScale/DetGPT" -> "shikras/shikra"
"OptimalScale/DetGPT" -> "OpenGVLab/VisionLLM"
"OptimalScale/DetGPT" -> "IDEA-Research/OpenSeeD" ["e"=1]
"OptimalScale/DetGPT" -> "microsoft/X-Decoder" ["e"=1]
"OptimalScale/DetGPT" -> "dvlab-research/LISA"
"OptimalScale/DetGPT" -> "microsoft/RegionCLIP" ["e"=1]
"OptimalScale/DetGPT" -> "yuhangzang/ContextDET"
"OptimalScale/DetGPT" -> "microsoft/MM-REACT"
"OptimalScale/DetGPT" -> "open-mmlab/Multimodal-GPT"
"OptimalScale/DetGPT" -> "IDEA-Research/GroundingDINO" ["e"=1]
"OptimalScale/DetGPT" -> "witnessai/Awesome-Open-Vocabulary-Object-Detection" ["e"=1]
"OptimalScale/DetGPT" -> "Saiyan-World/grounded-segment-any-parts" ["e"=1]
"OptimalScale/DetGPT" -> "allenai/visprog"
"llava-rlhf/LLaVA-RLHF" -> "RLHF-V/RLHF-V"
"llava-rlhf/LLaVA-RLHF" -> "RLHF-V/RLAIF-V"
"llava-rlhf/LLaVA-RLHF" -> "FuxiaoLiu/LRV-Instruction"
"llava-rlhf/LLaVA-RLHF" -> "TideDra/VL-RLHF"
"llava-rlhf/LLaVA-RLHF" -> "junyangwang0410/AMBER"
"llava-rlhf/LLaVA-RLHF" -> "shikiw/OPERA"
"llava-rlhf/LLaVA-RLHF" -> "RifleZhang/LLaVA-Hound-DPO"
"llava-rlhf/LLaVA-RLHF" -> "tianyi-lab/HallusionBench"
"llava-rlhf/LLaVA-RLHF" -> "FreedomIntelligence/ALLaVA"
"llava-rlhf/LLaVA-RLHF" -> "FanqingM/R1-Multimodal-Journey"
"llava-rlhf/LLaVA-RLHF" -> "yuweihao/MM-Vet"
"llava-rlhf/LLaVA-RLHF" -> "vlf-silkie/VLFeedback"
"llava-rlhf/LLaVA-RLHF" -> "VT-NLP/MultiInstruct"
"llava-rlhf/LLaVA-RLHF" -> "BAAI-DCAI/Visual-Instruction-Tuning"
"llava-rlhf/LLaVA-RLHF" -> "VITA-MLLM/Woodpecker"
"luogen1996/LaVIN" -> "SALT-NLP/LLaVAR"
"luogen1996/LaVIN" -> "shikras/shikra"
"luogen1996/LaVIN" -> "luogen1996/LLaVA-HR"
"luogen1996/LaVIN" -> "luogen1996/RepAdapter" ["e"=1]
"luogen1996/LaVIN" -> "yuweihao/MM-Vet"
"luogen1996/LaVIN" -> "BAAI-DCAI/Visual-Instruction-Tuning"
"luogen1996/LaVIN" -> "luogen1996/SimREC" ["e"=1]
"luogen1996/LaVIN" -> "OpenGVLab/LAMM"
"luogen1996/LaVIN" -> "seanzhuh/SeqTR" ["e"=1]
"luogen1996/LaVIN" -> "VPGTrans/VPGTrans"
"luogen1996/LaVIN" -> "X-PLUG/mPLUG-Owl"
"luogen1996/LaVIN" -> "jshilong/GPT4RoI"
"luogen1996/LaVIN" -> "baaivision/Emu" ["e"=1]
"luogen1996/LaVIN" -> "FuxiaoLiu/LRV-Instruction"
"luogen1996/LaVIN" -> "VITA-MLLM/Woodpecker"
"OpenGVLab/LAMM" -> "OpenGVLab/Multi-Modality-Arena"
"OpenGVLab/LAMM" -> "EvolvingLMMs-Lab/LongVA"
"OpenGVLab/LAMM" -> "IranQin/MP5" ["e"=1]
"OpenGVLab/LAMM" -> "AILab-CVC/SEED-Bench"
"OpenGVLab/LAMM" -> "FreedomIntelligence/ALLaVA"
"OpenGVLab/LAMM" -> "FuxiaoLiu/LRV-Instruction"
"OpenGVLab/LAMM" -> "embodied-generalist/embodied-generalist" ["e"=1]
"AILab-CVC/SEED-Bench" -> "AILab-CVC/SEED" ["e"=1]
"AILab-CVC/SEED-Bench" -> "tianyi-lab/HallusionBench"
"AILab-CVC/SEED-Bench" -> "ChenYi99/EgoPlan" ["e"=1]
"AILab-CVC/SEED-Bench" -> "palchenli/VL-Instruction-Tuning"
"AILab-CVC/SEED-Bench" -> "yuweihao/MM-Vet"
"AILab-CVC/SEED-Bench" -> "OpenGVLab/MM-Interleaved" ["e"=1]
"AILab-CVC/SEED-Bench" -> "tsb0601/MMVP"
"AILab-CVC/SEED-Bench" -> "lupantech/MathVista"
"AILab-CVC/SEED-Bench" -> "open-compass/MMBench"
"doc-doc/NExT-GQA" -> "Yui010206/SeViLA" ["e"=1]
"Yui010206/SeViLA" -> "antoyang/FrozenBiLM" ["e"=1]
"Yui010206/SeViLA" -> "doc-doc/NExT-GQA" ["e"=1]
"Yui010206/SeViLA" -> "VRU-NExT/VideoQA" ["e"=1]
"Yui010206/SeViLA" -> "minghangz/cpl" ["e"=1]
"Yui010206/SeViLA" -> "Ziyang412/VideoTree"
"Yui010206/SeViLA" -> "j-min/HiREST" ["e"=1]
"Yui010206/SeViLA" -> "mlvlab/Flipped-VQA" ["e"=1]
"Yui010206/SeViLA" -> "klauscc/VindLU" ["e"=1]
"Yui010206/SeViLA" -> "showlab/mist" ["e"=1]
"Yui010206/SeViLA" -> "RenShuhuai-Andy/TimeChat"
"Yui010206/SeViLA" -> "jayleicn/singularity" ["e"=1]
"Yui010206/SeViLA" -> "doc-doc/NExT-QA" ["e"=1]
"Yui010206/SeViLA" -> "showlab/all-in-one" ["e"=1]
"rainym00d/LLM4RS" -> "KID-22/LLM-IR-Bias-Fairness-Survey" ["e"=1]
"Saiyan-World/grounded-segment-any-parts" -> "jshilong/GPT4RoI" ["e"=1]
"yuweihao/MM-Vet" -> "RUCAIBox/POPE"
"yuweihao/MM-Vet" -> "open-compass/MMBench"
"yuweihao/MM-Vet" -> "tianyi-lab/HallusionBench"
"yuweihao/MM-Vet" -> "tsb0601/MMVP"
"yuweihao/MM-Vet" -> "MMMU-Benchmark/MMMU"
"yuweihao/MM-Vet" -> "FreedomIntelligence/ALLaVA"
"yuweihao/MM-Vet" -> "lupantech/ScienceQA"
"yuweihao/MM-Vet" -> "FuxiaoLiu/LRV-Instruction"
"pkunlp-icler/PCA-EVAL" -> "HaozheZhao/MIC_tool" ["e"=1]
"Paranioar/UniPT" -> "JiazuoYu/PathWeave" ["e"=1]
"hydrallm/llama-moe-v1" -> "AblateIt/finetune-study"
"IMCCretrieval/MomentDiff" -> "lntzm/MESM" ["e"=1]
"taylorai/galactic" -> "AblateIt/finetune-study" ["e"=1]
"pkunlp-icler/MIC" -> "HaozheZhao/MIC_tool"
"reactorsh/ambrosia" -> "apitube/docs"
"reactorsh/ambrosia" -> "mevbotcrypto/mev-bot"
"reactorsh/ambrosia" -> "AblateIt/finetune-study"
"thunlp/Muffin" -> "RLHF-V/RLHF-V"
"mitvis/vistext" -> "vis-nlp/Chart-to-text"
"mitvis/vistext" -> "hyungkwonko/chart-llm" ["e"=1]
"Ethan00Si/SESREC-SIGIR-2023" -> "Ethan00Si/KuaiSAR"
"Ethan00Si/SESREC-SIGIR-2023" -> "TengShi-RUC/UniSAR"
"Ethan00Si/SESREC-SIGIR-2023" -> "Ethan00Si/Instrumental-variables-for-recommendation"
"Ethan00Si/SESREC-SIGIR-2023" -> "rucliujn/JDsearch" ["e"=1]
"YUCHEN005/NASE" -> "YUCHEN005/Unified-Enhance-Separation"
"YUCHEN005/NASE" -> "YUCHEN005/RATS-Channel-A-Speech-Data"
"patrick-tssn/Awesome-Colorful-LLM" -> "bigai-nlco/VideoLLaMB"
"yuezih/Movie101" -> "yuezih/SMILE"
"Hypotheses-Paradise/Hypo2Trans" -> "YUCHEN005/RobustGER"
"Hypotheses-Paradise/Hypo2Trans" -> "YUCHEN005/RATS-Channel-A-Speech-Data"
"Hypotheses-Paradise/Hypo2Trans" -> "YUCHEN005/MIR-GAN"
"Hypotheses-Paradise/Hypo2Trans" -> "YUCHEN005/GILA"
"Hypotheses-Paradise/Hypo2Trans" -> "YUCHEN005/DPSL-ASR"
"Hypotheses-Paradise/Hypo2Trans" -> "YUCHEN005/UNA-GAN"
"AblateIt/finetune-study" -> "CarperAI/treasure_trove"
"AblateIt/finetune-study" -> "Alignment-Lab-AI/datagen"
"shiyi-zh0408/LOGO" -> "shiyi-zh0408/NAE_CVPR2024"
"shiyi-zh0408/LOGO" -> "ZhouKanglei/Awesome-AQA"
"shiyi-zh0408/LOGO" -> "xuangch/CVPR22_GDLT"
"shiyi-zh0408/LOGO" -> "yuxumin/CoRe"
"shiyi-zh0408/LOGO" -> "xujinglin/FineDiving"
"shiyi-zh0408/LOGO" -> "baiyang4/aqa_tpt"
"shiyi-zh0408/LOGO" -> "AndyTang15/FLAG3Dv2"
"TheJaeLal/LineFormer" -> "MasterAI-EAM/GraphMaster"
"TheJaeLal/LineFormer" -> "pengyu965/ChartDete"
"YUCHEN005/MIR-GAN" -> "YUCHEN005/GILA"
"YUCHEN005/MIR-GAN" -> "YUCHEN005/UNA-GAN"
"YUCHEN005/MIR-GAN" -> "YUCHEN005/UniVPM"
"salesforce/HIVE" -> "KD-TAO/VidKV" ["e"=1]
"jonathan-roberts1/GPT4GEO" -> "jonathan-roberts1/charting-new-territories"
"jonathan-roberts1/GPT4GEO" -> "jonathan-roberts1/SciFIBench"
"pengyu965/ChartDete" -> "tdsone/extract-line-chart-data"
"pengyu965/ChartDete" -> "TheJaeLal/LineFormer"
"vis-nlp/UniChart" -> "FuxiaoLiu/MMC"
"vis-nlp/UniChart" -> "Alpha-Innovator/SimChart9K"
"vis-nlp/UniChart" -> "vis-nlp/ChartInstruct"
"vis-nlp/UniChart" -> "tingxueronghua/ChartLlama-code"
"vis-nlp/UniChart" -> "pranonrahman/ChartSumm"
"OpenGVLab/Multitask-Model-Selector" -> "lyingCS/UOEP"
"OpenGVLab/Multitask-Model-Selector" -> "lyingCS/Controllable-Multi-Objective-Reranking"
"OpenGVLab/Multitask-Model-Selector" -> "OpenGVLab/MMIU"
"kahnchana/clippy" -> "kahnchana/mvu"
"khuangaf/ZeroFEC" -> "khuangaf/CHOCOLATE"
"lyingCS/Controllable-Multi-Objective-Reranking" -> "lyingCS/UOEP"
"lyingCS/Controllable-Multi-Objective-Reranking" -> "TengShi-RUC/UniSAR"
"lyingCS/Controllable-Multi-Objective-Reranking" -> "OpenGVLab/Multitask-Model-Selector"
"lyingCS/Controllable-Multi-Objective-Reranking" -> "zhengbw0324/LC-Rec"
"lyingCS/Controllable-Multi-Objective-Reranking" -> "OpenGVLab/MMIU"
"lyingCS/Controllable-Multi-Objective-Reranking" -> "E-qin/GEAR"
"YUCHEN005/UniVPM" -> "YUCHEN005/UNA-GAN"
"jaeill/CVPR23-VNE" -> "SNU-DRL/Attribution-ECG"
"Alpha-Innovator/SimChart9K" -> "Alpha-Innovator/TrustGeoGen"
"TencentQQGYLab/AppAgent" -> "THUDM/CogVLM" ["e"=1]
"FujiwaraChoki/MoneyPrinter" -> "apple/ml-mgie" ["e"=1]
"ml-explore/mlx" -> "apple/ml-ferret" ["e"=1]
"OpenBMB/MiniCPM" -> "OpenBMB/MiniCPM-o" ["e"=1]
"apple/ml-ferret" -> "ml-explore/mlx" ["e"=1]
"apple/ml-ferret" -> "cumulo-autumn/StreamDiffusion" ["e"=1]
"apple/ml-ferret" -> "ml-explore/mlx-examples" ["e"=1]
"apple/ml-ferret" -> "haotian-liu/LLaVA"
"apple/ml-ferret" -> "THUDM/CogVLM"
"apple/ml-ferret" -> "apple/ml-mgie"
"apple/ml-ferret" -> "apple/corenet" ["e"=1]
"apple/ml-ferret" -> "TencentQQGYLab/AppAgent" ["e"=1]
"apple/ml-ferret" -> "stanfordnlp/dspy" ["e"=1]
"apple/ml-ferret" -> "mistralai/mistral-inference" ["e"=1]
"apple/ml-ferret" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"apple/ml-ferret" -> "meta-llama/llama-cookbook" ["e"=1]
"apple/ml-ferret" -> "letta-ai/letta" ["e"=1]
"apple/ml-ferret" -> "apple/ml-stable-diffusion" ["e"=1]
"apple/ml-ferret" -> "ShishirPatil/gorilla" ["e"=1]
"QwenLM/Qwen3" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"QwenLM/Qwen3" -> "OpenBMB/MiniCPM-o" ["e"=1]
"QwenLM/Qwen3" -> "haotian-liu/LLaVA" ["e"=1]
"LargeWorldModel/LWM" -> "google/magika" ["e"=1]
"LargeWorldModel/LWM" -> "karpathy/minbpe" ["e"=1]
"LargeWorldModel/LWM" -> "facebookresearch/DiT" ["e"=1]
"LargeWorldModel/LWM" -> "PKU-YuanGroup/Open-Sora-Plan" ["e"=1]
"LargeWorldModel/LWM" -> "haotian-liu/LLaVA"
"LargeWorldModel/LWM" -> "Stability-AI/StableCascade" ["e"=1]
"LargeWorldModel/LWM" -> "hpcaitech/Open-Sora" ["e"=1]
"LargeWorldModel/LWM" -> "dvlab-research/MGM"
"LargeWorldModel/LWM" -> "facebookresearch/jepa" ["e"=1]
"LargeWorldModel/LWM" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"LargeWorldModel/LWM" -> "OpenGVLab/InternVL"
"LargeWorldModel/LWM" -> "QwenLM/Qwen-VL"
"LargeWorldModel/LWM" -> "THUDM/CogVLM"
"LargeWorldModel/LWM" -> "mit-han-lab/streaming-llm" ["e"=1]
"LargeWorldModel/LWM" -> "sgl-project/sglang" ["e"=1]
"meta-llama/llama3" -> "haotian-liu/LLaVA" ["e"=1]
"OpenGVLab/InternVL" -> "QwenLM/Qwen2.5-VL"
"OpenGVLab/InternVL" -> "QwenLM/Qwen-VL"
"OpenGVLab/InternVL" -> "LLaVA-VL/LLaVA-NeXT"
"OpenGVLab/InternVL" -> "haotian-liu/LLaVA"
"OpenGVLab/InternVL" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"OpenGVLab/InternVL" -> "modelscope/ms-swift" ["e"=1]
"OpenGVLab/InternVL" -> "InternLM/InternLM-XComposer"
"OpenGVLab/InternVL" -> "OpenBMB/MiniCPM-o"
"OpenGVLab/InternVL" -> "open-compass/VLMEvalKit"
"OpenGVLab/InternVL" -> "salesforce/LAVIS" ["e"=1]
"OpenGVLab/InternVL" -> "om-ai-lab/VLM-R1"
"OpenGVLab/InternVL" -> "InternLM/lmdeploy" ["e"=1]
"OpenGVLab/InternVL" -> "THUDM/CogVLM"
"OpenGVLab/InternVL" -> "THUDM/CogVLM2"
"OpenGVLab/InternVL" -> "mlfoundations/open_clip" ["e"=1]
"open-compass/VLMEvalKit" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"open-compass/VLMEvalKit" -> "LLaVA-VL/LLaVA-NeXT"
"open-compass/VLMEvalKit" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"open-compass/VLMEvalKit" -> "OpenGVLab/InternVL"
"open-compass/VLMEvalKit" -> "Deep-Agent/R1-V"
"open-compass/VLMEvalKit" -> "InternLM/InternLM-XComposer"
"open-compass/VLMEvalKit" -> "open-compass/opencompass" ["e"=1]
"open-compass/VLMEvalKit" -> "cambrian-mllm/cambrian"
"open-compass/VLMEvalKit" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"open-compass/VLMEvalKit" -> "hiyouga/EasyR1" ["e"=1]
"open-compass/VLMEvalKit" -> "Liuziyu77/Visual-RFT"
"open-compass/VLMEvalKit" -> "TideDra/lmm-r1"
"open-compass/VLMEvalKit" -> "om-ai-lab/VLM-R1"
"open-compass/VLMEvalKit" -> "ModalMinds/MM-EUREKA"
"open-compass/VLMEvalKit" -> "QwenLM/Qwen-VL"
"OpenBMB/MiniCPM-o" -> "OpenBMB/MiniCPM" ["e"=1]
"OpenBMB/MiniCPM-o" -> "OpenGVLab/InternVL"
"OpenBMB/MiniCPM-o" -> "haotian-liu/LLaVA"
"OpenBMB/MiniCPM-o" -> "hiyouga/LLaMA-Factory" ["e"=1]
"OpenBMB/MiniCPM-o" -> "QwenLM/Qwen2.5-VL"
"OpenBMB/MiniCPM-o" -> "hpcaitech/Open-Sora" ["e"=1]
"OpenBMB/MiniCPM-o" -> "fishaudio/fish-speech" ["e"=1]
"OpenBMB/MiniCPM-o" -> "unslothai/unsloth" ["e"=1]
"OpenBMB/MiniCPM-o" -> "vllm-project/vllm" ["e"=1]
"OpenBMB/MiniCPM-o" -> "QwenLM/Qwen3" ["e"=1]
"OpenBMB/MiniCPM-o" -> "stanford-oval/storm" ["e"=1]
"OpenBMB/MiniCPM-o" -> "opendatalab/MinerU" ["e"=1]
"OpenBMB/MiniCPM-o" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"OpenBMB/MiniCPM-o" -> "FoundationAgents/MetaGPT" ["e"=1]
"OpenBMB/MiniCPM-o" -> "2noise/ChatTTS" ["e"=1]
"cumulo-autumn/StreamDiffusion" -> "apple/ml-ferret" ["e"=1]
"google-deepmind/gemma" -> "LargeWorldModel/LWM" ["e"=1]
"Genesis-Embodied-AI/Genesis" -> "haotian-liu/LLaVA" ["e"=1]
"google/gemma_pytorch" -> "LargeWorldModel/LWM" ["e"=1]
"fishaudio/fish-speech" -> "OpenBMB/MiniCPM-o" ["e"=1]
"NVlabs/VILA" -> "LLaVA-VL/LLaVA-NeXT"
"NVlabs/VILA" -> "OpenGVLab/InternVL"
"NVlabs/VILA" -> "open-compass/VLMEvalKit"
"NVlabs/VILA" -> "cambrian-mllm/cambrian"
"NVlabs/VILA" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"NVlabs/VILA" -> "PKU-YuanGroup/Video-LLaVA"
"NVlabs/VILA" -> "InternLM/InternLM-XComposer"
"NVlabs/VILA" -> "mit-han-lab/llm-awq" ["e"=1]
"NVlabs/VILA" -> "openvla/openvla" ["e"=1]
"NVlabs/VILA" -> "showlab/Show-o" ["e"=1]
"NVlabs/VILA" -> "baaivision/Emu3" ["e"=1]
"NVlabs/VILA" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"NVlabs/VILA" -> "DAMO-NLP-SG/VideoLLaMA2"
"NVlabs/VILA" -> "Deep-Agent/R1-V"
"NVlabs/VILA" -> "OpenGVLab/InternVideo"
"hpcaitech/Open-Sora" -> "haotian-liu/LLaVA" ["e"=1]
"hpcaitech/Open-Sora" -> "OpenBMB/MiniCPM-o" ["e"=1]
"apple/ml-mgie" -> "tsujuifu/pytorch_mgie"
"apple/ml-mgie" -> "apple/ml-ferret"
"apple/ml-mgie" -> "FujiwaraChoki/MoneyPrinter" ["e"=1]
"apple/ml-mgie" -> "GoogleCloudPlatform/localllm" ["e"=1]
"apple/ml-mgie" -> "Stability-AI/StableCascade" ["e"=1]
"apple/ml-mgie" -> "metavoiceio/metavoice-src" ["e"=1]
"apple/ml-mgie" -> "ml-explore/mlx" ["e"=1]
"apple/ml-mgie" -> "apple/corenet" ["e"=1]
"apple/ml-mgie" -> "LargeWorldModel/LWM"
"apple/ml-mgie" -> "ml-explore/mlx-examples" ["e"=1]
"apple/ml-mgie" -> "YangLing0818/RPG-DiffusionMaster" ["e"=1]
"apple/ml-mgie" -> "levihsu/OOTDiffusion" ["e"=1]
"apple/ml-mgie" -> "TencentARC/SmartEdit" ["e"=1]
"apple/ml-mgie" -> "Doubiiu/DynamiCrafter" ["e"=1]
"apple/ml-mgie" -> "haotian-liu/LLaVA"
"LLaVA-VL/LLaVA-Interactive-Demo" -> "LLaVA-VL/LLaVA-Plus-Codebase"
"oumi-ai/oumi" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"karpathy/minbpe" -> "LargeWorldModel/LWM" ["e"=1]
"dvlab-research/MGM" -> "LLaVA-VL/LLaVA-NeXT"
"dvlab-research/MGM" -> "cambrian-mllm/cambrian"
"dvlab-research/MGM" -> "InternLM/InternLM-XComposer"
"dvlab-research/MGM" -> "open-compass/VLMEvalKit"
"dvlab-research/MGM" -> "OpenGVLab/InternVL"
"dvlab-research/MGM" -> "QwenLM/Qwen-VL"
"dvlab-research/MGM" -> "THUDM/CogVLM"
"dvlab-research/MGM" -> "NVlabs/VILA"
"dvlab-research/MGM" -> "baaivision/Emu" ["e"=1]
"dvlab-research/MGM" -> "PKU-YuanGroup/MoE-LLaVA"
"dvlab-research/MGM" -> "PKU-YuanGroup/Video-LLaVA"
"dvlab-research/MGM" -> "baaivision/EVA" ["e"=1]
"dvlab-research/MGM" -> "X-PLUG/mPLUG-Owl"
"dvlab-research/MGM" -> "LargeWorldModel/LWM"
"dvlab-research/MGM" -> "FoundationVision/VAR" ["e"=1]
"wnlen/clash-for-linux" -> "om-ai-lab/VLM-R1" ["e"=1]
"apple/corenet" -> "apple/ml-ferret" ["e"=1]
"state-spaces/mamba" -> "haotian-liu/LLaVA" ["e"=1]
"PKU-YuanGroup/Video-LLaVA" -> "DAMO-NLP-SG/Video-LLaMA"
"PKU-YuanGroup/Video-LLaVA" -> "LLaVA-VL/LLaVA-NeXT"
"PKU-YuanGroup/Video-LLaVA" -> "mbzuai-oryx/Video-ChatGPT"
"PKU-YuanGroup/Video-LLaVA" -> "PKU-YuanGroup/MoE-LLaVA"
"PKU-YuanGroup/Video-LLaVA" -> "PKU-YuanGroup/LanguageBind"
"PKU-YuanGroup/Video-LLaVA" -> "DAMO-NLP-SG/VideoLLaMA2"
"PKU-YuanGroup/Video-LLaVA" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"PKU-YuanGroup/Video-LLaVA" -> "OpenGVLab/InternVideo"
"PKU-YuanGroup/Video-LLaVA" -> "PKU-YuanGroup/Chat-UniVi" ["e"=1]
"PKU-YuanGroup/Video-LLaVA" -> "OpenGVLab/Ask-Anything"
"PKU-YuanGroup/Video-LLaVA" -> "dvlab-research/LLaMA-VID"
"PKU-YuanGroup/Video-LLaVA" -> "haotian-liu/LLaVA"
"PKU-YuanGroup/Video-LLaVA" -> "open-compass/VLMEvalKit"
"PKU-YuanGroup/Video-LLaVA" -> "QwenLM/Qwen-VL"
"PKU-YuanGroup/Video-LLaVA" -> "OpenGVLab/InternVL"
"PKU-YuanGroup/MoE-LLaVA" -> "PKU-YuanGroup/Video-LLaVA"
"PKU-YuanGroup/MoE-LLaVA" -> "PKU-YuanGroup/LanguageBind"
"PKU-YuanGroup/MoE-LLaVA" -> "LLaVA-VL/LLaVA-NeXT"
"PKU-YuanGroup/MoE-LLaVA" -> "TinyLLaVA/TinyLLaVA_Factory"
"PKU-YuanGroup/MoE-LLaVA" -> "open-compass/VLMEvalKit"
"PKU-YuanGroup/MoE-LLaVA" -> "InternLM/InternLM-XComposer"
"PKU-YuanGroup/MoE-LLaVA" -> "cambrian-mllm/cambrian"
"PKU-YuanGroup/MoE-LLaVA" -> "DAMO-NLP-SG/Video-LLaMA"
"PKU-YuanGroup/MoE-LLaVA" -> "QwenLM/Qwen-VL"
"PKU-YuanGroup/MoE-LLaVA" -> "THUDM/CogVLM"
"PKU-YuanGroup/MoE-LLaVA" -> "BAAI-DCAI/Bunny"
"PKU-YuanGroup/MoE-LLaVA" -> "PKU-YuanGroup/LLaVA-CoT"
"PKU-YuanGroup/MoE-LLaVA" -> "baaivision/Emu" ["e"=1]
"PKU-YuanGroup/MoE-LLaVA" -> "NVlabs/VILA"
"PKU-YuanGroup/MoE-LLaVA" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"LLaVA-VL/LLaVA-NeXT" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"LLaVA-VL/LLaVA-NeXT" -> "open-compass/VLMEvalKit"
"LLaVA-VL/LLaVA-NeXT" -> "PKU-YuanGroup/Video-LLaVA"
"LLaVA-VL/LLaVA-NeXT" -> "OpenGVLab/InternVL"
"LLaVA-VL/LLaVA-NeXT" -> "NVlabs/VILA"
"LLaVA-VL/LLaVA-NeXT" -> "haotian-liu/LLaVA"
"LLaVA-VL/LLaVA-NeXT" -> "QwenLM/Qwen2.5-VL"
"LLaVA-VL/LLaVA-NeXT" -> "cambrian-mllm/cambrian"
"LLaVA-VL/LLaVA-NeXT" -> "QwenLM/Qwen-VL"
"LLaVA-VL/LLaVA-NeXT" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"LLaVA-VL/LLaVA-NeXT" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"LLaVA-VL/LLaVA-NeXT" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"LLaVA-VL/LLaVA-NeXT" -> "Deep-Agent/R1-V"
"LLaVA-VL/LLaVA-NeXT" -> "om-ai-lab/VLM-R1"
"LLaVA-VL/LLaVA-NeXT" -> "InternLM/InternLM-XComposer"
"01-ai/Yi" -> "QwenLM/Qwen-VL" ["e"=1]
"qinghew/StableIdentity" -> "hmxiong/StreamChat" ["e"=1]
"I-S00N/I-S00N" -> "LargeWorldModel/LWM" ["e"=1]
"ml-explore/mlx-examples" -> "apple/ml-ferret" ["e"=1]
"PKU-YuanGroup/Open-Sora-Plan" -> "haotian-liu/LLaVA" ["e"=1]
"LetheSec/HuggingFace-Download-Accelerator" -> "daixiangzi/Awesome-Token-Compress" ["e"=1]
"LetheSec/HuggingFace-Download-Accelerator" -> "QwenLM/Qwen-VL" ["e"=1]
"LetheSec/HuggingFace-Download-Accelerator" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"wdndev/llm_interview_note" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"Stability-AI/StableCascade" -> "LargeWorldModel/LWM" ["e"=1]
"khanrc/honeybee" -> "WisconsinAIVision/ViP-LLaVA" ["e"=1]
"QwenLM/Qwen2.5-Coder" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"vikhyat/moondream" -> "OpenBMB/MiniCPM-o" ["e"=1]
"vikhyat/moondream" -> "haotian-liu/LLaVA" ["e"=1]
"vikhyat/moondream" -> "THUDM/CogVLM" ["e"=1]
"AILab-CVC/UniRepLKNet" -> "invictus717/MetaTransformer" ["e"=1]
"BAAI-Agents/Cradle" -> "THUDM/CogVLM" ["e"=1]
"boheumd/MA-LMM" -> "RenShuhuai-Andy/TimeChat"
"boheumd/MA-LMM" -> "rese1f/MovieChat"
"boheumd/MA-LMM" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"boheumd/MA-LMM" -> "IVGSZ/Flash-VStream"
"boheumd/MA-LMM" -> "ziplab/LongVLM"
"boheumd/MA-LMM" -> "MME-Benchmarks/Video-MME"
"boheumd/MA-LMM" -> "OpenGVLab/VideoChat-Flash"
"boheumd/MA-LMM" -> "Yui010206/SeViLA"
"boheumd/MA-LMM" -> "Ziyang412/VideoTree"
"boheumd/MA-LMM" -> "EvolvingLMMs-Lab/LongVA"
"boheumd/MA-LMM" -> "TencentARC/ST-LLM"
"boheumd/MA-LMM" -> "mbzuai-oryx/VideoGPT-plus"
"boheumd/MA-LMM" -> "Vision-CAIR/LongVU"
"boheumd/MA-LMM" -> "doc-doc/NExT-GQA" ["e"=1]
"boheumd/MA-LMM" -> "Vision-CAIR/MiniGPT4-video"
"facebookresearch/jepa" -> "LargeWorldModel/LWM" ["e"=1]
"facebookresearch/jepa" -> "PKU-YuanGroup/Video-LLaVA" ["e"=1]
"xk-huang/segment-caption-anything" -> "shiyi-zh0408/NAE_CVPR2024"
"xk-huang/segment-caption-anything" -> "AndyTang15/FLAG3Dv2"
"xk-huang/segment-caption-anything" -> "EternalEvan/DPMesh"
"xk-huang/segment-caption-anything" -> "Tengbo-Yu/AnyBimanual"
"xk-huang/segment-caption-anything" -> "GuanxingLu/vlarl"
"xk-huang/segment-caption-anything" -> "Yxxxb/VoCo-LLaMA"
"xk-huang/segment-caption-anything" -> "SuleBai/SC-CLIP"
"xk-huang/segment-caption-anything" -> "shiyi-zh0408/FlexiAct"
"xk-huang/segment-caption-anything" -> "shiyi-zh0408/LOGO"
"xk-huang/segment-caption-anything" -> "GuanxingLu/ManiGaussian" ["e"=1]
"xk-huang/segment-caption-anything" -> "zhang9302002/Flash-VStream"
"xk-huang/segment-caption-anything" -> "yongliu20/SCAN"
"databricks/dbrx" -> "dvlab-research/MGM" ["e"=1]
"deepseek-ai/DeepSeek-VL" -> "QwenLM/Qwen-VL" ["e"=1]
"deepseek-ai/DeepSeek-VL" -> "InternLM/InternLM-XComposer" ["e"=1]
"deepseek-ai/DeepSeek-VL" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"deepseek-ai/DeepSeek-VL" -> "OpenGVLab/InternVL" ["e"=1]
"deepseek-ai/DeepSeek-VL" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"deepseek-ai/DeepSeek-VL" -> "dvlab-research/MGM" ["e"=1]
"enoch3712/ExtractThinker" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"SkunkworksAI/BakLLaVA" -> "LLaVA-VL/LLaVA-Plus-Codebase"
"SkunkworksAI/BakLLaVA" -> "Fuzzy-Search/realtime-bakllava"
"SkunkworksAI/BakLLaVA" -> "X2FD/LVIS-INSTRUCT4V"
"SkunkworksAI/BakLLaVA" -> "SkunkworksAI/hydra-moe"
"SkunkworksAI/BakLLaVA" -> "dvlab-research/LLaMA-VID"
"SkunkworksAI/BakLLaVA" -> "cognitivecomputations/laserRMT" ["e"=1]
"SkunkworksAI/BakLLaVA" -> "WisconsinAIVision/ViP-LLaVA"
"SkunkworksAI/BakLLaVA" -> "PKU-YuanGroup/MoE-LLaVA"
"Meituan-AutoML/MobileVLM" -> "BAAI-DCAI/Bunny"
"Meituan-AutoML/MobileVLM" -> "TinyLLaVA/TinyLLaVA_Factory"
"Meituan-AutoML/MobileVLM" -> "DLYuanGod/TinyGPT-V"
"Meituan-AutoML/MobileVLM" -> "PKU-YuanGroup/MoE-LLaVA"
"Meituan-AutoML/MobileVLM" -> "open-compass/VLMEvalKit"
"Meituan-AutoML/MobileVLM" -> "LLaVA-VL/LLaVA-NeXT"
"Meituan-AutoML/MobileVLM" -> "xmoanvaf/llava-phi"
"Meituan-AutoML/MobileVLM" -> "THUDM/CogVLM"
"Meituan-AutoML/MobileVLM" -> "TRI-ML/prismatic-vlms" ["e"=1]
"Meituan-AutoML/MobileVLM" -> "chongzhou96/EdgeSAM" ["e"=1]
"Meituan-AutoML/MobileVLM" -> "THUDM/CogVLM2"
"Meituan-AutoML/MobileVLM" -> "LLaVA-VL/LLaVA-Plus-Codebase"
"Meituan-AutoML/MobileVLM" -> "dvlab-research/LISA"
"Meituan-AutoML/MobileVLM" -> "FreedomIntelligence/ALLaVA"
"Meituan-AutoML/MobileVLM" -> "baaivision/Emu" ["e"=1]
"luogen1996/LLaVA-HR" -> "thunlp/LLaVA-UHD"
"luogen1996/LLaVA-HR" -> "YouHuang67/mamba-code-explained" ["e"=1]
"luogen1996/LLaVA-HR" -> "FreedomIntelligence/ALLaVA"
"TinyLLaVA/TinyLLaVA_Factory" -> "BAAI-DCAI/Bunny"
"TinyLLaVA/TinyLLaVA_Factory" -> "xmoanvaf/llava-phi"
"TinyLLaVA/TinyLLaVA_Factory" -> "LLaVA-VL/LLaVA-NeXT"
"TinyLLaVA/TinyLLaVA_Factory" -> "PKU-YuanGroup/MoE-LLaVA"
"TinyLLaVA/TinyLLaVA_Factory" -> "Meituan-AutoML/MobileVLM"
"TinyLLaVA/TinyLLaVA_Factory" -> "mbzuai-oryx/LLaVA-pp"
"TinyLLaVA/TinyLLaVA_Factory" -> "cambrian-mllm/cambrian"
"TinyLLaVA/TinyLLaVA_Factory" -> "zjysteven/lmms-finetune"
"TinyLLaVA/TinyLLaVA_Factory" -> "FreedomIntelligence/ALLaVA"
"TinyLLaVA/TinyLLaVA_Factory" -> "xiaoachen98/Open-LLaVA-NeXT" ["e"=1]
"TinyLLaVA/TinyLLaVA_Factory" -> "daixiangzi/Awesome-Token-Compress"
"TinyLLaVA/TinyLLaVA_Factory" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"TinyLLaVA/TinyLLaVA_Factory" -> "open-compass/VLMEvalKit"
"TinyLLaVA/TinyLLaVA_Factory" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"TinyLLaVA/TinyLLaVA_Factory" -> "TRI-ML/prismatic-vlms" ["e"=1]
"bfshi/scaling_on_scales" -> "luogen1996/LLaVA-HR"
"bfshi/scaling_on_scales" -> "alibaba/conv-llava"
"bfshi/scaling_on_scales" -> "HJYao00/DenseConnector"
"bfshi/scaling_on_scales" -> "WisconsinAIVision/ViP-LLaVA"
"bfshi/scaling_on_scales" -> "thunlp/LLaVA-UHD"
"bfshi/scaling_on_scales" -> "mbzuai-oryx/LLaVA-pp"
"bfshi/scaling_on_scales" -> "FreedomIntelligence/ALLaVA"
"bfshi/scaling_on_scales" -> "baaivision/EVE" ["e"=1]
"bfshi/scaling_on_scales" -> "SHI-Labs/VCoder"
"aixcoder-plugin/aiXcoder-7B" -> "dvlab-research/MGM" ["e"=1]
"AILab-CVC/YOLO-World" -> "OpenGVLab/InternVL" ["e"=1]
"baaivision/tokenize-anything" -> "OpenGVLab/all-seeing" ["e"=1]
"InternLM/Tutorial" -> "InternLM/InternLM-XComposer" ["e"=1]
"FoundationVision/GLEE" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"FoundationVision/GLEE" -> "dvlab-research/LISA" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "gokayfem/ComfyUI_VLM_nodes" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "jingyi0000/VLM_survey" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "DAMO-NLP-SG/VideoLLaMA3"
"gokayfem/awesome-vlm-architectures" -> "JindongGu/Awesome-Prompting-on-Vision-Language-Model" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "NVlabs/VILA"
"gokayfem/awesome-vlm-architectures" -> "open-compass/VLMEvalKit"
"gokayfem/awesome-vlm-architectures" -> "friedrichor/Awesome-Multimodal-Papers" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "LLaVA-VL/LLaVA-NeXT"
"gokayfem/awesome-vlm-architectures" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"gokayfem/awesome-vlm-architectures" -> "zli12321/Vision-Language-Models-Overview"
"gokayfem/awesome-vlm-architectures" -> "TRI-ML/prismatic-vlms" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "Meituan-AutoML/MobileVLM"
"gokayfem/awesome-vlm-architectures" -> "Deep-Agent/R1-V"
"gokayfem/awesome-vlm-architectures" -> "BAAI-DCAI/Bunny"
"mbzuai-oryx/LLaVA-pp" -> "magic-research/PLLaVA"
"mbzuai-oryx/LLaVA-pp" -> "xiaoachen98/Open-LLaVA-NeXT" ["e"=1]
"mbzuai-oryx/LLaVA-pp" -> "TinyLLaVA/TinyLLaVA_Factory"
"mbzuai-oryx/LLaVA-pp" -> "BAAI-DCAI/Bunny"
"mbzuai-oryx/LLaVA-pp" -> "mbzuai-oryx/groundingLMM"
"mbzuai-oryx/LLaVA-pp" -> "LLaVA-VL/LLaVA-NeXT"
"mbzuai-oryx/LLaVA-pp" -> "bfshi/scaling_on_scales"
"mbzuai-oryx/LLaVA-pp" -> "xmoanvaf/llava-phi"
"mbzuai-oryx/LLaVA-pp" -> "PKU-YuanGroup/MoE-LLaVA"
"mbzuai-oryx/LLaVA-pp" -> "TRI-ML/prismatic-vlms" ["e"=1]
"mbzuai-oryx/LLaVA-pp" -> "thunlp/LLaVA-UHD"
"mbzuai-oryx/LLaVA-pp" -> "mbzuai-oryx/Video-LLaVA"
"mbzuai-oryx/LLaVA-pp" -> "cambrian-mllm/cambrian"
"mbzuai-oryx/LLaVA-pp" -> "NVlabs/VILA-archive"
"mbzuai-oryx/LLaVA-pp" -> "swordlidev/Efficient-Multimodal-LLMs-Survey"
"lqtrung1998/mwp_ReFT" -> "TideDra/lmm-r1" ["e"=1]
"THUDM/CogVLM2" -> "THUDM/CogVLM"
"THUDM/CogVLM2" -> "OpenGVLab/InternVL"
"THUDM/CogVLM2" -> "InternLM/InternLM-XComposer"
"THUDM/CogVLM2" -> "THUDM/GLM-4" ["e"=1]
"THUDM/CogVLM2" -> "LLaVA-VL/LLaVA-NeXT"
"THUDM/CogVLM2" -> "QwenLM/Qwen-VL"
"THUDM/CogVLM2" -> "cambrian-mllm/cambrian"
"THUDM/CogVLM2" -> "QwenLM/Qwen2.5-VL"
"THUDM/CogVLM2" -> "THUDM/VisualGLM-6B" ["e"=1]
"THUDM/CogVLM2" -> "modelscope/ms-swift" ["e"=1]
"THUDM/CogVLM2" -> "NVlabs/VILA"
"THUDM/CogVLM2" -> "open-compass/VLMEvalKit"
"THUDM/CogVLM2" -> "X-PLUG/mPLUG-Owl"
"THUDM/CogVLM2" -> "dvlab-research/MGM"
"THUDM/CogVLM2" -> "OpenBMB/MiniCPM-o"
"adamcohenhillel/ADeus" -> "LargeWorldModel/LWM" ["e"=1]
"roboflow/awesome-openai-vision-api-experiments" -> "microsoft/SoM" ["e"=1]
"AviSoori1x/makeMoE" -> "PKU-YuanGroup/MoE-LLaVA" ["e"=1]
"Ucas-HaoranWei/Vary" -> "Ucas-HaoranWei/Vary-toy"
"Ucas-HaoranWei/Vary" -> "Yuliang-Liu/Monkey" ["e"=1]
"Ucas-HaoranWei/Vary" -> "AlibabaResearch/AdvancedLiterateMachinery" ["e"=1]
"Ucas-HaoranWei/Vary" -> "LingyvKong/OneChart"
"Ucas-HaoranWei/Vary" -> "X-PLUG/mPLUG-DocOwl"
"Ucas-HaoranWei/Vary" -> "Yuliang-Liu/MultimodalOCR" ["e"=1]
"Ucas-HaoranWei/Vary" -> "InternLM/InternLM-XComposer"
"Ucas-HaoranWei/Vary" -> "Ucas-HaoranWei/GOT-OCR2.0" ["e"=1]
"Ucas-HaoranWei/Vary" -> "QwenLM/Qwen-VL"
"Ucas-HaoranWei/Vary" -> "THUDM/CogVLM"
"Ucas-HaoranWei/Vary" -> "open-compass/VLMEvalKit"
"Ucas-HaoranWei/Vary" -> "oh-my-ocr/text_renderer" ["e"=1]
"Ucas-HaoranWei/Vary" -> "OpenGVLab/InternVL"
"Ucas-HaoranWei/Vary" -> "dvlab-research/MGM"
"Ucas-HaoranWei/Vary" -> "THUDM/CogVLM2"
"Tele-AI/Telechat" -> "QwenLM/Qwen-VL" ["e"=1]
"likejazz/llama3.np" -> "mustafaaljadery/llama3v" ["e"=1]
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "BillChan226/HALC"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "DAMO-NLP-SG/VCD"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "RUCAIBox/POPE"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "junyangwang0410/AMBER"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "showlab/Awesome-MLLM-Hallucination"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "NishilBalar/Awesome-LVLM-Hallucination"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "LisaAnne/Hallucination"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "LALBJ/PAI"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "Purshow/Awesome-LVLM-Hallucination"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "YiyangZhou/LURE"
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" -> "shikiw/OPERA"
"BillChan226/HALC" -> "YiyangZhou/LURE"
"BillChan226/HALC" -> "shikiw/OPERA"
"BillChan226/HALC" -> "DAMO-NLP-SG/VCD"
"BillChan226/HALC" -> "NishilBalar/Awesome-LVLM-Hallucination"
"BillChan226/HALC" -> "yfzhang114/LLaVA-Align"
"BillChan226/HALC" -> "YiyangZhou/CSR"
"BillChan226/HALC" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"BillChan226/HALC" -> "RUCAIBox/POPE"
"BillChan226/HALC" -> "Ziwei-Zheng/Nullu" ["e"=1]
"BillChan226/HALC" -> "AoiDragon/POPE"
"BillChan226/HALC" -> "Yuqifan1117/HalluciDoctor"
"DAMO-NLP-SG/VCD" -> "shikiw/OPERA"
"DAMO-NLP-SG/VCD" -> "LALBJ/PAI"
"DAMO-NLP-SG/VCD" -> "BillChan226/HALC"
"DAMO-NLP-SG/VCD" -> "YiyangZhou/LURE"
"DAMO-NLP-SG/VCD" -> "tianyi-lab/HallusionBench"
"DAMO-NLP-SG/VCD" -> "showlab/Awesome-MLLM-Hallucination"
"DAMO-NLP-SG/VCD" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"DAMO-NLP-SG/VCD" -> "Purshow/Awesome-LVLM-Hallucination"
"DAMO-NLP-SG/VCD" -> "RUCAIBox/POPE"
"DAMO-NLP-SG/VCD" -> "AoiDragon/POPE"
"DAMO-NLP-SG/VCD" -> "zjunlp/Deco"
"DAMO-NLP-SG/VCD" -> "FuxiaoLiu/LRV-Instruction"
"DAMO-NLP-SG/VCD" -> "voidism/DoLa" ["e"=1]
"DAMO-NLP-SG/VCD" -> "shengliu66/VTI"
"DAMO-NLP-SG/VCD" -> "junyangwang0410/AMBER"
"InternLM/HuixiangDou" -> "InternLM/InternLM-XComposer" ["e"=1]
"gptscript-ai/gptscript" -> "LargeWorldModel/LWM" ["e"=1]
"GoogleCloudPlatform/localllm" -> "apple/ml-mgie" ["e"=1]
"SunzeY/AlphaCLIP" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"SunzeY/AlphaCLIP" -> "Liuziyu77/Visual-RFT" ["e"=1]
"SunzeY/AlphaCLIP" -> "jshilong/GPT4RoI" ["e"=1]
"SunzeY/AlphaCLIP" -> "dvlab-research/LISA" ["e"=1]
"OpenGVLab/VideoMamba" -> "OpenGVLab/InternVideo" ["e"=1]
"OpenGVLab/VideoMamba" -> "OpenGVLab/VideoChat-Flash" ["e"=1]
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "Kwai-YuanQi/MM-RLHF"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "yfzhang114/SliME"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "ModalMinds/MM-EUREKA"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "rongyaofang/GoT" ["e"=1]
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "MME-Benchmarks/MME-RealWorld"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "modelscope/awesome-deep-reasoning"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "TideDra/lmm-r1"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "showlab/Awesome-Unified-Multimodal-Models" ["e"=1]
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "Liuziyu77/Visual-RFT"
"yfzhang114/Awesome-Multimodal-Large-Language-Models" -> "zzli2022/Awesome-System2-Reasoning-LLM" ["e"=1]
"microsoft/PhiCookBook" -> "NVlabs/VILA" ["e"=1]
"FreedomIntelligence/ALLaVA" -> "baaivision/CapsFusion" ["e"=1]
"FreedomIntelligence/ALLaVA" -> "WisconsinAIVision/ViP-LLaVA"
"FreedomIntelligence/ALLaVA" -> "thunlp/Muffin"
"FreedomIntelligence/ALLaVA" -> "FuxiaoLiu/LRV-Instruction"
"RLHF-V/RLAIF-V" -> "RLHF-V/RLHF-V"
"RLHF-V/RLAIF-V" -> "thunlp/Muffin"
"RLHF-V/RLAIF-V" -> "TideDra/VL-RLHF"
"RLHF-V/RLAIF-V" -> "llava-rlhf/LLaVA-RLHF"
"RLHF-V/RLAIF-V" -> "YiyangZhou/POVID"
"RLHF-V/RLAIF-V" -> "FreedomIntelligence/ALLaVA"
"RLHF-V/RLAIF-V" -> "YiyangZhou/CSR"
"RLHF-V/RLAIF-V" -> "tianyi-lab/HallusionBench"
"RLHF-V/RLAIF-V" -> "junyangwang0410/AMBER"
"RLHF-V/RLAIF-V" -> "yihedeng9/STIC"
"RLHF-V/RLAIF-V" -> "luka-group/mDPO"
"RLHF-V/RLAIF-V" -> "yuweihao/MM-Vet"
"RLHF-V/RLAIF-V" -> "vlf-silkie/VLFeedback"
"RLHF-V/RLAIF-V" -> "RifleZhang/LLaVA-Hound-DPO"
"RLHF-V/RLAIF-V" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"SkyworkAI/Vitron" -> "scofield7419/Video-of-Thought" ["e"=1]
"SkyworkAI/Vitron" -> "scofield7419/THOR-ISA" ["e"=1]
"SkyworkAI/Vitron" -> "MaverickRen/PixelLM"
"SkyworkAI/Vitron" -> "AILab-CVC/SEED-X" ["e"=1]
"SkyworkAI/Vitron" -> "OpenGVLab/VisionLLM"
"SkyworkAI/Vitron" -> "FoundationVision/Groma" ["e"=1]
"SkyworkAI/Vitron" -> "TencentARC/SmartEdit" ["e"=1]
"SkyworkAI/Vitron" -> "mbzuai-oryx/groundingLMM"
"SkyworkAI/Vitron" -> "mit-han-lab/vila-u" ["e"=1]
"SkyworkAI/Vitron" -> "showlab/Show-o" ["e"=1]
"SkyworkAI/Vitron" -> "FoundationVision/GLEE" ["e"=1]
"SkyworkAI/Vitron" -> "LLaVA-VL/LLaVA-NeXT"
"apple/ml-4m" -> "cambrian-mllm/cambrian" ["e"=1]
"apple/ml-4m" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"thunlp/LLaVA-UHD" -> "luogen1996/LLaVA-HR"
"thunlp/LLaVA-UHD" -> "RLHF-V/RLHF-V"
"thunlp/LLaVA-UHD" -> "baaivision/EVE" ["e"=1]
"thunlp/LLaVA-UHD" -> "OpenGVLab/OmniCorpus"
"thunlp/LLaVA-UHD" -> "BAAI-DCAI/Bunny"
"thunlp/LLaVA-UHD" -> "SALT-NLP/LLaVAR"
"thunlp/LLaVA-UHD" -> "RUCAIBox/Virgo"
"thunlp/LLaVA-UHD" -> "ParadoxZW/LLaVA-UHD-Better"
"thunlp/LLaVA-UHD" -> "pkunlp-icler/FastV"
"thunlp/LLaVA-UHD" -> "yfzhang114/SliME"
"thunlp/LLaVA-UHD" -> "bfshi/scaling_on_scales"
"thunlp/LLaVA-UHD" -> "RLHF-V/RLAIF-V"
"thunlp/LLaVA-UHD" -> "yuweihao/MM-Vet"
"NVlabs/LITA" -> "gyxxyg/VTG-LLM"
"NVlabs/LITA" -> "WHB139426/Grounded-Video-LLM"
"NVlabs/LITA" -> "huangb23/VTimeLLM"
"NVlabs/LITA" -> "TimeMarker-LLM/TimeMarker"
"NVlabs/LITA" -> "www-Ye/TimeZero"
"NVlabs/LITA" -> "gyxxyg/TRACE"
"NVlabs/LITA" -> "yellow-binary-tree/HawkEye"
"DLYuanGod/TinyGPT-V" -> "Meituan-AutoML/MobileVLM"
"DLYuanGod/TinyGPT-V" -> "xmoanvaf/llava-phi"
"DLYuanGod/TinyGPT-V" -> "TinyLLaVA/TinyLLaVA_Factory"
"DLYuanGod/TinyGPT-V" -> "MILVLG/imp"
"DLYuanGod/TinyGPT-V" -> "PKU-YuanGroup/MoE-LLaVA"
"DLYuanGod/TinyGPT-V" -> "csuhan/OneLLM"
"DLYuanGod/TinyGPT-V" -> "BAAI-DCAI/Bunny"
"DLYuanGod/TinyGPT-V" -> "PKU-YuanGroup/Video-LLaVA"
"DLYuanGod/TinyGPT-V" -> "OSU-NLP-Group/SeeAct" ["e"=1]
"DLYuanGod/TinyGPT-V" -> "Ucas-HaoranWei/Vary-toy"
"DLYuanGod/TinyGPT-V" -> "eric-ai-lab/MiniGPT-5" ["e"=1]
"DLYuanGod/TinyGPT-V" -> "Flossiee/HonestyLLM"
"DLYuanGod/TinyGPT-V" -> "invictus717/MetaTransformer"
"DLYuanGod/TinyGPT-V" -> "ytongbai/LVM" ["e"=1]
"DLYuanGod/TinyGPT-V" -> "dvmazur/mixtral-offloading" ["e"=1]
"RUCAIBox/LC-Rec" -> "zhengbw0324/LC-Rec" ["e"=1]
"Zhen-Tan-dmml/LLM4Annotation" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"letitiabanana/PnP-OVSS" -> "slonetime/EBSeg"
"Xinjie-Q/GaussianImage" -> "ChrisDong-THU/GaussianToken" ["e"=1]
"scofield7419/Video-of-Thought" -> "SkyworkAI/Vitron" ["e"=1]
"scofield7419/Video-of-Thought" -> "Ziyang412/VideoTree" ["e"=1]
"ddupont808/GPT-4V-Act" -> "microsoft/SoM" ["e"=1]
"Open3DA/LL3DA" -> "ch3cook-fdu/Vote2Cap-DETR" ["e"=1]
"NVlabs/RADIO" -> "NVlabs/VILA" ["e"=1]
"NVlabs/RADIO" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"NVlabs/RADIO" -> "bfshi/scaling_on_scales" ["e"=1]
"apple/ml-aim" -> "cambrian-mllm/cambrian" ["e"=1]
"apple/ml-aim" -> "bfshi/scaling_on_scales" ["e"=1]
"apple/ml-mobileclip" -> "Meituan-AutoML/MobileVLM" ["e"=1]
"modelscope/evalscope" -> "open-compass/VLMEvalKit" ["e"=1]
"VikParuchuri/texify" -> "Ucas-HaoranWei/Vary" ["e"=1]
"Ucas-HaoranWei/Vary-toy" -> "Ucas-HaoranWei/Vary"
"Ucas-HaoranWei/Vary-toy" -> "Ucas-HaoranWei/Vary-family"
"Ucas-HaoranWei/Vary-toy" -> "Ucas-HaoranWei/Vary-tiny-600k"
"Ucas-HaoranWei/Vary-toy" -> "LingyvKong/OneChart"
"Ucas-HaoranWei/Vary-toy" -> "MILVLG/imp"
"Ucas-HaoranWei/Vary-toy" -> "thunlp/LLaVA-UHD"
"Ucas-HaoranWei/Vary-toy" -> "TinyLLaVA/TinyLLaVA_Factory"
"Ucas-HaoranWei/Vary-toy" -> "BAAI-DCAI/Bunny"
"Ucas-HaoranWei/Vary-toy" -> "Yuliang-Liu/Monkey" ["e"=1]
"Ucas-HaoranWei/Vary-toy" -> "Coobiw/MPP-LLaVA"
"Ucas-HaoranWei/Vary-toy" -> "ucaslcl/Fox"
"Ucas-HaoranWei/Vary-toy" -> "Meituan-AutoML/MobileVLM"
"Ucas-HaoranWei/Vary-toy" -> "mbzuai-oryx/groundingLMM"
"Ucas-HaoranWei/Vary-toy" -> "shenyunhang/APE" ["e"=1]
"LeapLabTHU/GSVA" -> "zamling/PSALM" ["e"=1]
"NJU-LHRS/LHRS-Bot" -> "xuliu-cyber/RSUniVLM" ["e"=1]
"snap-research/Panda-70M" -> "mbzuai-oryx/Video-ChatGPT" ["e"=1]
"mira-space/MiraData" -> "magic-research/PLLaVA" ["e"=1]
"MMStar-Benchmark/MMStar" -> "shikiw/OPERA" ["e"=1]
"microsoft/SoM" -> "ddupont808/GPT-4V-Act" ["e"=1]
"microsoft/SoM" -> "mbzuai-oryx/groundingLMM"
"microsoft/SoM" -> "UX-Decoder/Semantic-SAM" ["e"=1]
"microsoft/SoM" -> "dvlab-research/LISA"
"microsoft/SoM" -> "zzxslp/SoM-LLaVA"
"microsoft/SoM" -> "jshilong/GPT4RoI"
"microsoft/SoM" -> "huangwl18/VoxPoser" ["e"=1]
"microsoft/SoM" -> "IDEA-Research/OpenSeeD" ["e"=1]
"microsoft/SoM" -> "shikras/shikra"
"microsoft/SoM" -> "microsoft/GLIP" ["e"=1]
"microsoft/SoM" -> "UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["e"=1]
"microsoft/SoM" -> "huangwl18/ReKep" ["e"=1]
"microsoft/SoM" -> "IDEA-Research/Grounding-DINO-1.5-API" ["e"=1]
"microsoft/SoM" -> "LLaVA-VL/LLaVA-NeXT"
"microsoft/SoM" -> "WisconsinAIVision/ViP-LLaVA"
"yfzhang114/LLaVA-Align" -> "BillChan226/HALC"
"magic-research/PLLaVA" -> "bytedance/tarsier"
"magic-research/PLLaVA" -> "mira-space/MiraData" ["e"=1]
"magic-research/PLLaVA" -> "EvolvingLMMs-Lab/LongVA"
"magic-research/PLLaVA" -> "RenShuhuai-Andy/TimeChat"
"magic-research/PLLaVA" -> "TencentARC/ST-LLM"
"magic-research/PLLaVA" -> "DAMO-NLP-SG/VideoLLaMA2"
"magic-research/PLLaVA" -> "MME-Benchmarks/Video-MME"
"magic-research/PLLaVA" -> "huangb23/VTimeLLM"
"magic-research/PLLaVA" -> "rese1f/MovieChat"
"magic-research/PLLaVA" -> "Vision-CAIR/MiniGPT4-video"
"magic-research/PLLaVA" -> "mbzuai-oryx/LLaVA-pp"
"magic-research/PLLaVA" -> "OpenGVLab/VideoChat-Flash"
"magic-research/PLLaVA" -> "OpenGVLab/InternVideo"
"magic-research/PLLaVA" -> "IVGSZ/Flash-VStream"
"magic-research/PLLaVA" -> "imagegridworth/IG-VLM"
"NExT-ChatV/NExT-Chat" -> "UX-Decoder/LLaVA-Grounding" ["e"=1]
"NExT-ChatV/NExT-Chat" -> "zamling/PSALM"
"NExT-ChatV/NExT-Chat" -> "Meituan-AutoML/Lenna"
"NExT-ChatV/NExT-Chat" -> "mbzuai-oryx/groundingLMM"
"NExT-ChatV/NExT-Chat" -> "lzw-lzw/GroundingGPT"
"Coobiw/MPP-LLaVA" -> "IDEA-FinAI/ChartMoE"
"Coobiw/MPP-LLaVA" -> "zjysteven/lmms-finetune"
"Coobiw/MPP-LLaVA" -> "Vision-CAIR/MiniGPT4-video"
"Coobiw/MPP-LLaVA" -> "TinyLLaVA/TinyLLaVA_Factory"
"Coobiw/MPP-LLaVA" -> "FreedomIntelligence/ALLaVA"
"Coobiw/MPP-LLaVA" -> "shikiw/OPERA"
"Coobiw/MPP-LLaVA" -> "FoundationVision/Groma" ["e"=1]
"Coobiw/MPP-LLaVA" -> "Ucas-HaoranWei/Vary-toy"
"lxe/llavavision" -> "Fuzzy-Search/realtime-bakllava"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "LLaVA-VL/LLaVA-Interactive-Demo"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "llava-rlhf/LLaVA-RLHF"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "SkunkworksAI/BakLLaVA"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "WisconsinAIVision/ViP-LLaVA"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "UX-Decoder/LLaVA-Grounding" ["e"=1]
"LLaVA-VL/LLaVA-Plus-Codebase" -> "PKU-YuanGroup/MoE-LLaVA"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "eric-ai-lab/MiniGPT-5" ["e"=1]
"LLaVA-VL/LLaVA-Plus-Codebase" -> "rese1f/MovieChat"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "SHI-Labs/VCoder"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "mbzuai-oryx/groundingLMM"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "microsoft/LLaVA-Med" ["e"=1]
"LLaVA-VL/LLaVA-Plus-Codebase" -> "baaivision/Emu" ["e"=1]
"LLaVA-VL/LLaVA-Plus-Codebase" -> "NExT-ChatV/NExT-Chat"
"LLaVA-VL/LLaVA-Plus-Codebase" -> "RunpeiDong/DreamLLM" ["e"=1]
"LLaVA-VL/LLaVA-Plus-Codebase" -> "FuxiaoLiu/LRV-Instruction"
"xmoanvaf/llava-phi" -> "MILVLG/imp"
"xmoanvaf/llava-phi" -> "TinyLLaVA/TinyLLaVA_Factory"
"xmoanvaf/llava-phi" -> "mbzuai-oryx/LLaVA-pp"
"xmoanvaf/llava-phi" -> "DLYuanGod/TinyGPT-V"
"xmoanvaf/llava-phi" -> "BAAI-DCAI/Bunny"
"xmoanvaf/llava-phi" -> "FreedomIntelligence/ALLaVA"
"OpenMOSS/AnyGPT" -> "csuhan/OneLLM" ["e"=1]
"OpenGVLab/PonderV2" -> "ch3cook-fdu/Vote2Cap-DETR" ["e"=1]
"MaverickRen/PixelLM" -> "zamling/PSALM"
"MaverickRen/PixelLM" -> "LeapLabTHU/GSVA" ["e"=1]
"MaverickRen/PixelLM" -> "mbzuai-oryx/groundingLMM"
"MaverickRen/PixelLM" -> "congvvc/HyperSeg" ["e"=1]
"MaverickRen/PixelLM" -> "berkeley-hipie/segllm"
"MaverickRen/PixelLM" -> "mc-lan/Text4Seg"
"ZhouKanglei/Awesome-AQA" -> "baiyang4/aqa_tpt"
"ZhouKanglei/Awesome-AQA" -> "yuxumin/CoRe"
"ZhouKanglei/Awesome-AQA" -> "shiyi-zh0408/LOGO"
"ZhouKanglei/Awesome-AQA" -> "ParitoshParmar/MTL-AQA"
"ZhouKanglei/Awesome-AQA" -> "ZhouKanglei/HGCN_AQA"
"ZhouKanglei/Awesome-AQA" -> "xujinglin/FineDiving"
"ZhouKanglei/Awesome-AQA" -> "Lyman-Smoker/Awesome-AQA"
"ZhouKanglei/Awesome-AQA" -> "Shunli-Wang/TSA-Net"
"ZhouKanglei/Awesome-AQA" -> "xuangch/CVPR22_GDLT"
"showlab/Awesome-MLLM-Hallucination" -> "shikiw/OPERA"
"showlab/Awesome-MLLM-Hallucination" -> "DAMO-NLP-SG/VCD"
"showlab/Awesome-MLLM-Hallucination" -> "NishilBalar/Awesome-LVLM-Hallucination"
"showlab/Awesome-MLLM-Hallucination" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"showlab/Awesome-MLLM-Hallucination" -> "BillChan226/HALC"
"showlab/Awesome-MLLM-Hallucination" -> "shikiw/Awesome-MLLM-Hallucination" ["e"=1]
"showlab/Awesome-MLLM-Hallucination" -> "junyangwang0410/AMBER"
"showlab/Awesome-MLLM-Hallucination" -> "tianyi-lab/HallusionBench"
"showlab/Awesome-MLLM-Hallucination" -> "RUCAIBox/POPE"
"showlab/Awesome-MLLM-Hallucination" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"showlab/Awesome-MLLM-Hallucination" -> "FuxiaoLiu/LRV-Instruction"
"showlab/Awesome-MLLM-Hallucination" -> "YiyangZhou/LURE"
"showlab/Awesome-MLLM-Hallucination" -> "VITA-MLLM/Woodpecker"
"showlab/Awesome-MLLM-Hallucination" -> "Yangyi-Chen/Multimodal-AND-Large-Language-Models"
"showlab/Awesome-MLLM-Hallucination" -> "LuckyyySTA/Awesome-LLM-hallucination" ["e"=1]
"pkunlp-icler/FastV" -> "daixiangzi/Awesome-Token-Compress"
"pkunlp-icler/FastV" -> "42Shawn/LLaVA-PruMerge"
"pkunlp-icler/FastV" -> "Gumpest/SparseVLMs"
"pkunlp-icler/FastV" -> "Cooperx521/PyramidDrop"
"pkunlp-icler/FastV" -> "dvlab-research/VisionZip"
"pkunlp-icler/FastV" -> "Theia-4869/FasterVLM"
"pkunlp-icler/FastV" -> "SUSTechBruce/LOOK-M"
"pkunlp-icler/FastV" -> "lzhxmu/VTW"
"pkunlp-icler/FastV" -> "Yxxxb/VoCo-LLaMA"
"pkunlp-icler/FastV" -> "Beckschen/LLaVolta"
"pkunlp-icler/FastV" -> "ywh187/FitPrune"
"pkunlp-icler/FastV" -> "shikiw/OPERA"
"pkunlp-icler/FastV" -> "EvolvingLMMs-Lab/LongVA"
"pkunlp-icler/FastV" -> "mrwu-mac/ControlMLLM"
"pkunlp-icler/FastV" -> "liuting20/MustDrop" ["e"=1]
"NishilBalar/Awesome-LVLM-Hallucination" -> "BillChan226/HALC"
"NishilBalar/Awesome-LVLM-Hallucination" -> "showlab/Awesome-MLLM-Hallucination"
"NishilBalar/Awesome-LVLM-Hallucination" -> "shengliu66/VTI"
"NishilBalar/Awesome-LVLM-Hallucination" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"NishilBalar/Awesome-LVLM-Hallucination" -> "shikiw/Awesome-MLLM-Hallucination" ["e"=1]
"NishilBalar/Awesome-LVLM-Hallucination" -> "DAMO-NLP-SG/VCD"
"LALBJ/PAI" -> "DAMO-NLP-SG/VCD"
"LALBJ/PAI" -> "shikiw/OPERA"
"LALBJ/PAI" -> "yuezih/less-is-more"
"LALBJ/PAI" -> "shengliu66/VTI"
"mbzuai-oryx/groundingLMM" -> "FoundationVision/Groma" ["e"=1]
"mbzuai-oryx/groundingLMM" -> "dvlab-research/LISA"
"mbzuai-oryx/groundingLMM" -> "mbzuai-oryx/Video-LLaVA"
"mbzuai-oryx/groundingLMM" -> "UX-Decoder/LLaVA-Grounding" ["e"=1]
"mbzuai-oryx/groundingLMM" -> "zamling/PSALM"
"mbzuai-oryx/groundingLMM" -> "lxtGH/OMG-Seg" ["e"=1]
"mbzuai-oryx/groundingLMM" -> "jshilong/GPT4RoI"
"mbzuai-oryx/groundingLMM" -> "CircleRadon/Osprey" ["e"=1]
"mbzuai-oryx/groundingLMM" -> "mbzuai-oryx/Video-ChatGPT"
"mbzuai-oryx/groundingLMM" -> "MaverickRen/PixelLM"
"mbzuai-oryx/groundingLMM" -> "shikras/shikra"
"mbzuai-oryx/groundingLMM" -> "OpenGVLab/all-seeing"
"mbzuai-oryx/groundingLMM" -> "HarborYuan/ovsam" ["e"=1]
"mbzuai-oryx/groundingLMM" -> "magic-research/Sa2VA" ["e"=1]
"mbzuai-oryx/groundingLMM" -> "lzw-lzw/GroundingGPT"
"deepcs233/Visual-CoT" -> "dongyh20/Insight-V"
"deepcs233/Visual-CoT" -> "RupertLuo/VoCoT"
"deepcs233/Visual-CoT" -> "turningpoint-ai/VisualThinker-R1-Zero"
"deepcs233/Visual-CoT" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"deepcs233/Visual-CoT" -> "YiyangZhou/POVID"
"deepcs233/Visual-CoT" -> "Wang-Xiaodong1899/Open-R1-Video"
"deepcs233/Visual-CoT" -> "tsb0601/MMVP"
"deepcs233/Visual-CoT" -> "Osilly/Vision-R1"
"deepcs233/Visual-CoT" -> "luka-group/mDPO"
"deepcs233/Visual-CoT" -> "chancharikmitra/CCoT" ["e"=1]
"deepcs233/Visual-CoT" -> "ModalMinds/MM-EUREKA"
"deepcs233/Visual-CoT" -> "penghao-wu/vstar"
"ggg0919/cantor" -> "zhourax/VEGA"
"ggg0919/cantor" -> "MAC-AutoML/QuoTA"
"vlf-silkie/VLFeedback" -> "yihedeng9/STIC"
"vlf-silkie/VLFeedback" -> "YiyangZhou/POVID"
"HarborYuan/ovsam" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"HarborYuan/ovsam" -> "dvlab-research/LISA" ["e"=1]
"beichenzbc/Long-CLIP" -> "baaivision/DIVA" ["e"=1]
"beichenzbc/Long-CLIP" -> "shikiw/OPERA" ["e"=1]
"beichenzbc/Long-CLIP" -> "Liuziyu77/Visual-RFT" ["e"=1]
"GAIR-NLP/MathPile" -> "lupantech/MathVista" ["e"=1]
"GuanxingLu/ManiGaussian" -> "Tengbo-Yu/AnyBimanual" ["e"=1]
"GuanxingLu/ManiGaussian" -> "GuanxingLu/vlarl" ["e"=1]
"BAAI-DCAI/Bunny" -> "TinyLLaVA/TinyLLaVA_Factory"
"BAAI-DCAI/Bunny" -> "Meituan-AutoML/MobileVLM"
"BAAI-DCAI/Bunny" -> "thunlp/LLaVA-UHD"
"BAAI-DCAI/Bunny" -> "FreedomIntelligence/ALLaVA"
"BAAI-DCAI/Bunny" -> "mbzuai-oryx/LLaVA-pp"
"BAAI-DCAI/Bunny" -> "PKU-YuanGroup/MoE-LLaVA"
"BAAI-DCAI/Bunny" -> "open-compass/VLMEvalKit"
"BAAI-DCAI/Bunny" -> "xmoanvaf/llava-phi"
"BAAI-DCAI/Bunny" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"BAAI-DCAI/Bunny" -> "OpenGVLab/all-seeing"
"BAAI-DCAI/Bunny" -> "InternLM/InternLM-XComposer"
"BAAI-DCAI/Bunny" -> "xiaoachen98/Open-LLaVA-NeXT" ["e"=1]
"BAAI-DCAI/Bunny" -> "dvlab-research/MGM"
"BAAI-DCAI/Bunny" -> "LLaVA-VL/LLaVA-NeXT"
"BAAI-DCAI/Bunny" -> "cambrian-mllm/cambrian"
"aimagelab/freeda" -> "letitiabanana/PnP-OVSS"
"penghao-wu/vstar" -> "FanqingM/R1-Multimodal-Journey"
"penghao-wu/vstar" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"penghao-wu/vstar" -> "tsb0601/MMVP"
"penghao-wu/vstar" -> "deepcs233/Visual-CoT"
"penghao-wu/vstar" -> "jshilong/GPT4RoI"
"penghao-wu/vstar" -> "thunlp/LLaVA-UHD"
"penghao-wu/vstar" -> "pkunlp-icler/FastV"
"penghao-wu/vstar" -> "allenai/unified-io-2" ["e"=1]
"penghao-wu/vstar" -> "cambrian-mllm/cambrian"
"penghao-wu/vstar" -> "dvlab-research/LISA"
"penghao-wu/vstar" -> "mbzuai-oryx/groundingLMM"
"penghao-wu/vstar" -> "WisconsinAIVision/ViP-LLaVA"
"penghao-wu/vstar" -> "EvolvingLMMs-Lab/LongVA"
"penghao-wu/vstar" -> "open-compass/VLMEvalKit"
"penghao-wu/vstar" -> "shikras/shikra"
"FreedomIntelligence/Apollo" -> "FreedomIntelligence/ApolloMoE"
"FreedomIntelligence/Apollo" -> "FreedomIntelligence/MLLM-Bench"
"FreedomIntelligence/Apollo" -> "FreedomIntelligence/CMB" ["e"=1]
"wangcunxiang/LLM-Factuality-Survey" -> "junyangwang0410/AMBER" ["e"=1]
"OpenGVLab/video-mamba-suite" -> "OpenGVLab/VideoChat-Flash" ["e"=1]
"OpenGVLab/video-mamba-suite" -> "NVlabs/LITA" ["e"=1]
"Vision-CAIR/MiniGPT4-video" -> "magic-research/PLLaVA"
"Vision-CAIR/MiniGPT4-video" -> "mbzuai-oryx/Video-ChatGPT"
"Vision-CAIR/MiniGPT4-video" -> "DAMO-NLP-SG/VideoLLaMA2"
"Vision-CAIR/MiniGPT4-video" -> "boheumd/MA-LMM"
"Vision-CAIR/MiniGPT4-video" -> "PKU-YuanGroup/Video-LLaVA"
"Vision-CAIR/MiniGPT4-video" -> "dvlab-research/LLaMA-VID"
"Vision-CAIR/MiniGPT4-video" -> "OpenGVLab/InternVideo"
"Vision-CAIR/MiniGPT4-video" -> "rese1f/MovieChat"
"Vision-CAIR/MiniGPT4-video" -> "EvolvingLMMs-Lab/LongVA"
"Vision-CAIR/MiniGPT4-video" -> "Deaddawn/MovieLLM-code"
"Vision-CAIR/MiniGPT4-video" -> "mbzuai-oryx/VideoGPT-plus"
"Vision-CAIR/MiniGPT4-video" -> "PKU-YuanGroup/MoE-LLaVA"
"Vision-CAIR/MiniGPT4-video" -> "RenShuhuai-Andy/TimeChat"
"Vision-CAIR/MiniGPT4-video" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"Vision-CAIR/MiniGPT4-video" -> "bytedance/tarsier"
"sming256/OpenTAD" -> "NVlabs/LITA" ["e"=1]
"sming256/OpenTAD" -> "OpenGVLab/VideoChat-Flash" ["e"=1]
"OpenGVLab/VideoChat-Flash" -> "OpenGVLab/InternVideo"
"OpenGVLab/VideoChat-Flash" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"OpenGVLab/VideoChat-Flash" -> "Vision-CAIR/LongVU"
"OpenGVLab/VideoChat-Flash" -> "gls0425/LinVT"
"OpenGVLab/VideoChat-Flash" -> "OpenGVLab/VideoMamba" ["e"=1]
"OpenGVLab/VideoChat-Flash" -> "MME-Benchmarks/Video-MME"
"OpenGVLab/VideoChat-Flash" -> "EvolvingLMMs-Lab/LongVA"
"OpenGVLab/VideoChat-Flash" -> "Wang-Xiaodong1899/Open-R1-Video"
"OpenGVLab/VideoChat-Flash" -> "RenShuhuai-Andy/TimeChat"
"OpenGVLab/VideoChat-Flash" -> "egoschema/EgoSchema"
"OpenGVLab/VideoChat-Flash" -> "bytedance/tarsier"
"OpenGVLab/VideoChat-Flash" -> "VectorSpaceLab/Video-XL"
"OpenGVLab/VideoChat-Flash" -> "Ziyang412/VideoTree"
"OpenGVLab/VideoChat-Flash" -> "magic-research/PLLaVA"
"OpenGVLab/VideoChat-Flash" -> "boheumd/MA-LMM"
"tingxueronghua/ChartLlama-code" -> "vis-nlp/UniChart"
"tingxueronghua/ChartLlama-code" -> "FuxiaoLiu/MMC"
"tingxueronghua/ChartLlama-code" -> "vis-nlp/ChartQA"
"tingxueronghua/ChartLlama-code" -> "OpenGVLab/ChartAst"
"tingxueronghua/ChartLlama-code" -> "hyungkwonko/chart-llm" ["e"=1]
"tingxueronghua/ChartLlama-code" -> "LingyvKong/OneChart"
"tingxueronghua/ChartLlama-code" -> "Alpha-Innovator/SimChart9K"
"tingxueronghua/ChartLlama-code" -> "zengxingchen/ChartQA-MLLM" ["e"=1]
"tingxueronghua/ChartLlama-code" -> "vis-nlp/Chart-to-text"
"tingxueronghua/ChartLlama-code" -> "Alpha-Innovator/ChartVLM"
"tingxueronghua/ChartLlama-code" -> "mitvis/vistext"
"tingxueronghua/ChartLlama-code" -> "findalexli/SciGraphQA" ["e"=1]
"WisconsinAIVision/ViP-LLaVA" -> "FreedomIntelligence/ALLaVA"
"WisconsinAIVision/ViP-LLaVA" -> "BAAI-DCAI/Visual-Instruction-Tuning"
"WisconsinAIVision/ViP-LLaVA" -> "UX-Decoder/LLaVA-Grounding" ["e"=1]
"WisconsinAIVision/ViP-LLaVA" -> "MMStar-Benchmark/MMStar" ["e"=1]
"jun0wanan/awesome-large-multimodal-agents" -> "yaotingwangofficial/Awesome-MCoT" ["e"=1]
"jun0wanan/awesome-large-multimodal-agents" -> "showlab/Awesome-MLLM-Hallucination" ["e"=1]
"LingyvKong/OneChart" -> "ucaslcl/Fox"
"LingyvKong/OneChart" -> "Ucas-HaoranWei/Vary-tiny-600k"
"LingyvKong/OneChart" -> "Alpha-Innovator/ChartVLM"
"LingyvKong/OneChart" -> "Ucas-HaoranWei/Slow-Perception"
"LingyvKong/OneChart" -> "Ucas-HaoranWei/Vary"
"LingyvKong/OneChart" -> "zengxingchen/ChartQA-MLLM" ["e"=1]
"LingyvKong/OneChart" -> "Ucas-HaoranWei/Vary-family"
"LingyvKong/OneChart" -> "tingxueronghua/ChartLlama-code"
"LingyvKong/OneChart" -> "1694439208/GOT-OCR-Inference"
"LingyvKong/OneChart" -> "Ucas-HaoranWei/Vary-toy"
"csuhan/OneLLM" -> "PKU-YuanGroup/LanguageBind"
"csuhan/OneLLM" -> "invictus717/MetaTransformer"
"csuhan/OneLLM" -> "TencentARC/ViT-Lens" ["e"=1]
"csuhan/OneLLM" -> "OpenMOSS/AnyGPT" ["e"=1]
"csuhan/OneLLM" -> "ZrrSkywalker/I2P-MAE" ["e"=1]
"csuhan/OneLLM" -> "showlab/Awesome-Unified-Multimodal-Models" ["e"=1]
"csuhan/OneLLM" -> "Alpha-VLLM/LLaMA2-Accessory"
"csuhan/OneLLM" -> "AILab-CVC/SEED-X" ["e"=1]
"csuhan/OneLLM" -> "baaivision/Emu" ["e"=1]
"csuhan/OneLLM" -> "salesforce/ULIP" ["e"=1]
"csuhan/OneLLM" -> "allenai/unified-io-2" ["e"=1]
"csuhan/OneLLM" -> "PKU-YuanGroup/Chat-UniVi" ["e"=1]
"csuhan/OneLLM" -> "DirtyHarryLYL/LLM-in-Vision"
"EvolvingLMMs-Lab/EgoLife" -> "EvolvingLMMs-Lab/multimodal-search-r1"
"EvolvingLMMs-Lab/EgoLife" -> "EvolvingLMMs-Lab/Aero-1" ["e"=1]
"EvolvingLMMs-Lab/EgoLife" -> "EvolvingLMMs-Lab/LongVA"
"lupantech/MathVista" -> "MMMU-Benchmark/MMMU"
"lupantech/MathVista" -> "mathllm/MATH-V"
"lupantech/MathVista" -> "ZrrSkywalker/MathVerse"
"lupantech/MathVista" -> "vis-nlp/UniChart"
"lupantech/MathVista" -> "HZQ950419/Math-LLaVA"
"lupantech/MathVista" -> "pipilurj/G-LLaVA" ["e"=1]
"lupantech/MathVista" -> "AILab-CVC/SEED-Bench"
"lupantech/MathVista" -> "ZrrSkywalker/MAVIS"
"lupantech/MathVista" -> "tianyi-lab/HallusionBench"
"lupantech/MathVista" -> "vis-nlp/ChartQA"
"mathllm/MATH-V" -> "ZrrSkywalker/MathVerse"
"liangyn22/MCUFormer" -> "ChangyuanWang17/APQ-DM"
"dvlab-research/LLaMA-VID" -> "mbzuai-oryx/Video-ChatGPT"
"dvlab-research/LLaMA-VID" -> "rese1f/MovieChat"
"dvlab-research/LLaMA-VID" -> "PKU-YuanGroup/Chat-UniVi" ["e"=1]
"dvlab-research/LLaMA-VID" -> "PKU-YuanGroup/Video-LLaVA"
"dvlab-research/LLaMA-VID" -> "DAMO-NLP-SG/Video-LLaMA"
"dvlab-research/LLaMA-VID" -> "RenShuhuai-Andy/TimeChat"
"dvlab-research/LLaMA-VID" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"dvlab-research/LLaMA-VID" -> "magic-research/PLLaVA"
"dvlab-research/LLaMA-VID" -> "OpenGVLab/Ask-Anything"
"dvlab-research/LLaMA-VID" -> "LLaVA-VL/LLaVA-NeXT"
"dvlab-research/LLaMA-VID" -> "huangb23/VTimeLLM"
"dvlab-research/LLaMA-VID" -> "PKU-YuanGroup/LanguageBind"
"dvlab-research/LLaMA-VID" -> "dvlab-research/LISA"
"dvlab-research/LLaMA-VID" -> "jzhzhang/NaVid-VLN-CE" ["e"=1]
"dvlab-research/LLaMA-VID" -> "RifleZhang/LLaVA-Hound-DPO"
"ChangyuanWang17/APQ-DM" -> "ChangyuanWang17/QVLM"
"OpenGVLab/ChartAst" -> "lyingCS/Controllable-Multi-Objective-Reranking"
"OpenGVLab/ChartAst" -> "lyingCS/UOEP"
"OpenGVLab/ChartAst" -> "OpenGVLab/MMIU"
"OpenGVLab/ChartAst" -> "TengShi-RUC/UniSAR"
"OpenGVLab/ChartAst" -> "OpenGVLab/PhyGenBench" ["e"=1]
"CeeZh/LLoVi" -> "Ziyang412/VideoTree"
"CeeZh/LLoVi" -> "agentic-learning-ai-lab/lifelong-memory"
"CeeZh/LLoVi" -> "jongwoopark7978/LVNet"
"RLHF-V/RLHF-V" -> "RLHF-V/RLAIF-V"
"RLHF-V/RLHF-V" -> "thunlp/Muffin"
"RLHF-V/RLHF-V" -> "llava-rlhf/LLaVA-RLHF"
"RLHF-V/RLHF-V" -> "TideDra/VL-RLHF"
"RLHF-V/RLHF-V" -> "vlf-silkie/VLFeedback"
"RLHF-V/RLHF-V" -> "yihedeng9/STIC"
"RLHF-V/RLHF-V" -> "RifleZhang/LLaVA-Hound-DPO"
"RLHF-V/RLHF-V" -> "FanqingM/R1-Multimodal-Journey"
"RLHF-V/RLHF-V" -> "dongyh20/Insight-V"
"RLHF-V/RLHF-V" -> "junyangwang0410/AMBER"
"RLHF-V/RLHF-V" -> "opendatalab/HA-DPO"
"RLHF-V/RLHF-V" -> "FreedomIntelligence/ALLaVA"
"RLHF-V/RLHF-V" -> "YiyangZhou/POVID"
"RLHF-V/RLHF-V" -> "thunlp/LLaVA-UHD"
"CircleRadon/Osprey" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"CircleRadon/Osprey" -> "dvlab-research/LISA" ["e"=1]
"CircleRadon/Osprey" -> "jshilong/GPT4RoI" ["e"=1]
"Fuzzy-Search/realtime-bakllava" -> "lxe/llavavision"
"Fuzzy-Search/realtime-bakllava" -> "cocktailpeanut/mirror"
"Fuzzy-Search/realtime-bakllava" -> "SkunkworksAI/BakLLaVA"
"Fuzzy-Search/realtime-bakllava" -> "OneInterface/chat-with-page"
"TRI-ML/prismatic-vlms" -> "FreedomIntelligence/ALLaVA" ["e"=1]
"TRI-ML/prismatic-vlms" -> "open-compass/VLMEvalKit" ["e"=1]
"xiaoachen98/Open-LLaVA-NeXT" -> "Cooperx521/PyramidDrop" ["e"=1]
"EternalEvan/FlowIE" -> "EternalEvan/DPMesh"
"EternalEvan/FlowIE" -> "AndyTang15/FLAG3Dv2"
"EternalEvan/FlowIE" -> "AndyTang15/FLAG3D"
"EternalEvan/FlowIE" -> "Tengbo-Yu/AnyBimanual"
"EternalEvan/FlowIE" -> "InvincibleWyq/ChatVID"
"EternalEvan/FlowIE" -> "SuleBai/SC-CLIP"
"PKU-YuanGroup/Video-Bench" -> "PKU-YuanGroup/LanguageBind" ["e"=1]
"YuzheZhang-1999/DiffTSR" -> "EternalEvan/FlowIE" ["e"=1]
"YUCHEN005/GenTranslate" -> "YUCHEN005/STAR-Adapt"
"YUCHEN005/GenTranslate" -> "YUCHEN005/RobustGER"
"YUCHEN005/GenTranslate" -> "Hypotheses-Paradise/Hypo2Trans"
"Ucas-HaoranWei/Vary-tiny-600k" -> "Ucas-HaoranWei/Vary-family"
"Ucas-HaoranWei/Vary-tiny-600k" -> "LingyvKong/OneChart"
"yellow-binary-tree/HawkEye" -> "gyxxyg/VTG-LLM"
"baaivision/CapsFusion" -> "FreedomIntelligence/ALLaVA" ["e"=1]
"baaivision/CapsFusion" -> "baaivision/DenseFusion" ["e"=1]
"Alpha-Innovator/ChartVLM" -> "HankYe/AdaptiveDiffusion"
"Alpha-Innovator/ChartVLM" -> "LingyvKong/OneChart"
"Alpha-Innovator/ChartVLM" -> "OpenGVLab/ChartAst"
"Alpha-Innovator/ChartVLM" -> "Alpha-Innovator/SimChart9K"
"Alpha-Innovator/ChartVLM" -> "Thinklab-SJTU/ML4CO-Kit" ["e"=1]
"Alpha-Innovator/ChartVLM" -> "Alpha-Innovator/SurveyForge"
"Alpha-Innovator/ChartVLM" -> "vis-nlp/UniChart"
"allenai/unified-io-2" -> "allenai/unified-io-inference" ["e"=1]
"allenai/unified-io-2" -> "shikras/shikra" ["e"=1]
"allenai/unified-io-2" -> "AILab-CVC/SEED-Bench" ["e"=1]
"allenai/unified-io-2" -> "penghao-wu/vstar" ["e"=1]
"allenai/unified-io-2" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"allenai/unified-io-2" -> "allenai/visprog" ["e"=1]
"cocktailpeanut/mirror" -> "Fuzzy-Search/realtime-bakllava"
"wusize/F-LMM" -> "Shengcao-Cao/groundLMM"
"wusize/F-LMM" -> "MICV-yonsei/CASS"
"MMMU-Benchmark/MMMU" -> "lupantech/MathVista"
"MMMU-Benchmark/MMMU" -> "yuweihao/MM-Vet"
"MMMU-Benchmark/MMMU" -> "open-compass/MMBench"
"MMMU-Benchmark/MMMU" -> "FuxiaoLiu/LRV-Instruction"
"MMMU-Benchmark/MMMU" -> "FreedomIntelligence/MLLM-Bench"
"MMMU-Benchmark/MMMU" -> "open-compass/VLMEvalKit"
"MMMU-Benchmark/MMMU" -> "AILab-CVC/SEED-Bench"
"EternalEvan/DPMesh" -> "AndyTang15/FLAG3Dv2"
"EternalEvan/DPMesh" -> "RammusLeo/DPMesh"
"EternalEvan/DPMesh" -> "AndyTang15/FLAG3D"
"EternalEvan/DPMesh" -> "EternalEvan/FlowIE"
"yuanzhoulvpi2017/vscode_debug_transformers" -> "yuanzhoulvpi2017/zero_nlp" ["e"=1]
"yuanzhoulvpi2017/vscode_debug_transformers" -> "DAMO-NLP-SG/VCD"
"yongliu20/UniLSeg" -> "yongliu20/SCAN"
"yongliu20/UniLSeg" -> "SuleBai/SC-CLIP"
"TideDra/VL-RLHF" -> "RLHF-V/RLHF-V"
"TideDra/VL-RLHF" -> "vlf-silkie/VLFeedback"
"TideDra/VL-RLHF" -> "NiuTrans/Vision-LLM-Alignment"
"TideDra/VL-RLHF" -> "njucckevin/MM-Self-Improve"
"TideDra/VL-RLHF" -> "RLHF-V/RLAIF-V"
"TideDra/VL-RLHF" -> "thunlp/Muffin"
"TideDra/VL-RLHF" -> "llava-rlhf/LLaVA-RLHF"
"md-mohaiminul/VideoRecap" -> "Ziyang412/VideoTree"
"md-mohaiminul/VideoRecap" -> "j-min/HiREST" ["e"=1]
"md-mohaiminul/VideoRecap" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"shikiw/OPERA" -> "DAMO-NLP-SG/VCD"
"shikiw/OPERA" -> "BillChan226/HALC"
"shikiw/OPERA" -> "showlab/Awesome-MLLM-Hallucination"
"shikiw/OPERA" -> "shikiw/Awesome-MLLM-Hallucination" ["e"=1]
"shikiw/OPERA" -> "YiyangZhou/LURE"
"shikiw/OPERA" -> "RUCAIBox/POPE"
"shikiw/OPERA" -> "FuxiaoLiu/LRV-Instruction"
"shikiw/OPERA" -> "LALBJ/PAI"
"shikiw/OPERA" -> "tianyi-lab/HallusionBench"
"shikiw/OPERA" -> "AoiDragon/POPE"
"shikiw/OPERA" -> "junyangwang0410/AMBER"
"shikiw/OPERA" -> "shikiw/DAM-VP" ["e"=1]
"shikiw/OPERA" -> "VITA-MLLM/Woodpecker"
"shikiw/OPERA" -> "voidism/DoLa" ["e"=1]
"shikiw/OPERA" -> "MMStar-Benchmark/MMStar" ["e"=1]
"YiyangZhou/POVID" -> "YiyangZhou/CSR"
"YiyangZhou/POVID" -> "yihedeng9/STIC"
"RifleZhang/LLaVA-Hound-DPO" -> "EvolvingLMMs-Lab/LongVA"
"RifleZhang/LLaVA-Hound-DPO" -> "YiyangZhou/POVID"
"RifleZhang/LLaVA-Hound-DPO" -> "yonseivnl/vlm-rlaif" ["e"=1]
"RifleZhang/LLaVA-Hound-DPO" -> "RLHF-V/RLHF-V"
"palchenli/VL-Instruction-Tuning" -> "RUCAIBox/ComVint"
"TencentARC/ST-LLM" -> "llyx97/TempCompass"
"TencentARC/ST-LLM" -> "yellow-binary-tree/HawkEye"
"TencentARC/ST-LLM" -> "Becomebright/GroundVQA" ["e"=1]
"TencentARC/ST-LLM" -> "magic-research/PLLaVA"
"TencentARC/ST-LLM" -> "RenShuhuai-Andy/TimeChat"
"TencentARC/ST-LLM" -> "EvolvingLMMs-Lab/LongVA"
"TencentARC/ST-LLM" -> "TimeMarker-LLM/TimeMarker"
"BAAI-DCAI/DataOptim" -> "BAAI-DCAI/Visual-Instruction-Tuning"
"BAAI-DCAI/DataOptim" -> "BAAI-DCAI/Training-Data-Synthesis"
"tsb0601/MMVP" -> "yuweihao/MM-Vet"
"tsb0601/MMVP" -> "mertyg/vision-language-models-are-bows" ["e"=1]
"tsb0601/MMVP" -> "RUCAIBox/POPE"
"tsb0601/MMVP" -> "FreedomIntelligence/ALLaVA"
"tsb0601/MMVP" -> "bronyayang/Law_of_Vision_Representation_in_MLLMs" ["e"=1]
"tsb0601/MMVP" -> "OpenGVLab/all-seeing"
"tsb0601/MMVP" -> "cambrian-mllm/cambrian"
"tsb0601/MMVP" -> "FuxiaoLiu/LRV-Instruction"
"tsb0601/MMVP" -> "deepcs233/Visual-CoT"
"tsb0601/MMVP" -> "showlab/Awesome-MLLM-Hallucination"
"tsb0601/MMVP" -> "baaivision/DIVA"
"tsb0601/MMVP" -> "penghao-wu/vstar"
"MILVLG/imp" -> "xmoanvaf/llava-phi"
"ZrrSkywalker/MathVerse" -> "ZrrSkywalker/MAVIS"
"ZrrSkywalker/MathVerse" -> "mathllm/MATH-V"
"ZrrSkywalker/MathVerse" -> "HZQ950419/Math-LLaVA"
"ZrrSkywalker/MathVerse" -> "lupantech/MathVista"
"DCDmllm/WorldGPT" -> "DCDmllm/MorphTokens"
"mbzuai-oryx/Video-LLaVA" -> "mbzuai-oryx/VideoGLaMM" ["e"=1]
"mbzuai-oryx/Video-LLaVA" -> "huangb23/VTimeLLM"
"mbzuai-oryx/Video-LLaVA" -> "mbzuai-oryx/groundingLMM"
"mbzuai-oryx/Video-LLaVA" -> "hananshafi/llmblueprint" ["e"=1]
"mbzuai-oryx/Video-LLaVA" -> "mbzuai-oryx/VideoGPT-plus"
"mbzuai-oryx/Video-LLaVA" -> "RenShuhuai-Andy/TimeChat"
"mbzuai-oryx/Video-LLaVA" -> "mbzuai-oryx/Video-ChatGPT"
"mbzuai-oryx/Video-LLaVA" -> "showlab/VideoLISA" ["e"=1]
"mbzuai-oryx/Video-LLaVA" -> "TencentARC/ST-LLM"
"mbzuai-oryx/Video-LLaVA" -> "imagegridworth/IG-VLM"
"mbzuai-oryx/Video-LLaVA" -> "rese1f/MovieChat"
"mbzuai-oryx/Video-LLaVA" -> "mbzuai-oryx/ClimateGPT" ["e"=1]
"zli12321/qa_metrics" -> "zli12321/VideoHallu"
"mistralai/megablocks-public" -> "SkunkworksAI/hydra-moe" ["e"=1]
"YingqingHe/Awesome-LLMs-meet-Multimodal-Generation" -> "mrwu-mac/ControlMLLM" ["e"=1]
"BAAI-DCAI/Training-Data-Synthesis" -> "yongchaoz/diffusion_inversion"
"BAAI-DCAI/Training-Data-Synthesis" -> "BAAI-DCAI/DataOptim"
"JiazuoYu/MoE-Adapters4CL" -> "JiazuoYu/PathWeave" ["e"=1]
"UX-Decoder/DINOv" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"UX-Decoder/DINOv" -> "yongliu20/UniLSeg" ["e"=1]
"shikiw/Awesome-MLLM-Hallucination" -> "shikiw/OPERA" ["e"=1]
"RL4VLM/RL4VLM" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"RL4VLM/RL4VLM" -> "RLHF-V/RLHF-V" ["e"=1]
"zamling/PSALM" -> "MaverickRen/PixelLM"
"zamling/PSALM" -> "LeapLabTHU/GSVA" ["e"=1]
"zamling/PSALM" -> "mbzuai-oryx/groundingLMM"
"zamling/PSALM" -> "NExT-ChatV/NExT-Chat"
"zamling/PSALM" -> "heshuting555/DsHmp" ["e"=1]
"zamling/PSALM" -> "congvvc/LaSagnA" ["e"=1]
"zamling/PSALM" -> "wusize/F-LMM"
"zamling/PSALM" -> "congvvc/HyperSeg" ["e"=1]
"imagegridworth/IG-VLM" -> "SNU-DRL/Attribution-ECG"
"imagegridworth/IG-VLM" -> "Ziyang412/VideoTree"
"imagegridworth/IG-VLM" -> "apple/ml-slowfast-llava"
"ziplab/LongVLM" -> "Ziyang412/VideoTree"
"tianyi-lab/HallusionBench" -> "junyangwang0410/AMBER"
"tianyi-lab/HallusionBench" -> "FuxiaoLiu/LRV-Instruction"
"tianyi-lab/HallusionBench" -> "DAMO-NLP-SG/VCD"
"tianyi-lab/HallusionBench" -> "RUCAIBox/POPE"
"tianyi-lab/HallusionBench" -> "shikiw/OPERA"
"tianyi-lab/HallusionBench" -> "YiyangZhou/LURE"
"tianyi-lab/HallusionBench" -> "yuweihao/MM-Vet"
"tianyi-lab/HallusionBench" -> "FuxiaoLiu/MMC"
"tianyi-lab/HallusionBench" -> "showlab/Awesome-MLLM-Hallucination"
"tianyi-lab/HallusionBench" -> "AILab-CVC/SEED-Bench"
"tianyi-lab/HallusionBench" -> "AoiDragon/POPE"
"tianyi-lab/HallusionBench" -> "xieyuquanxx/awesome-Large-MultiModal-Hallucination"
"tianyi-lab/HallusionBench" -> "llava-rlhf/LLaVA-RLHF"
"tianyi-lab/HallusionBench" -> "mertyg/vision-language-models-are-bows" ["e"=1]
"tianyi-lab/HallusionBench" -> "open-compass/MMBench"
"huangb23/VTimeLLM" -> "gyxxyg/VTG-LLM"
"huangb23/VTimeLLM" -> "RenShuhuai-Andy/TimeChat"
"huangb23/VTimeLLM" -> "sudo-Boris/mr-Blip" ["e"=1]
"huangb23/VTimeLLM" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"huangb23/VTimeLLM" -> "NVlabs/LITA"
"huangb23/VTimeLLM" -> "Ziyang412/VideoTree"
"huangb23/VTimeLLM" -> "DCDmllm/Momentor"
"huangb23/VTimeLLM" -> "mbzuai-oryx/Video-LLaVA"
"huangb23/VTimeLLM" -> "wjun0830/CGDETR" ["e"=1]
"huangb23/VTimeLLM" -> "gyxxyg/TRACE"
"huangb23/VTimeLLM" -> "rese1f/MovieChat"
"huangb23/VTimeLLM" -> "TimeMarker-LLM/TimeMarker"
"huangb23/VTimeLLM" -> "wjun0830/QD-DETR" ["e"=1]
"huangb23/VTimeLLM" -> "minghangz/TFVTG"
"huangb23/VTimeLLM" -> "magic-research/PLLaVA"
"lzw-lzw/GroundingGPT" -> "huangb23/VTimeLLM"
"lzw-lzw/GroundingGPT" -> "mbzuai-oryx/groundingLMM"
"lzw-lzw/GroundingGPT" -> "NExT-ChatV/NExT-Chat"
"lzw-lzw/GroundingGPT" -> "UX-Decoder/LLaVA-Grounding" ["e"=1]
"lzw-lzw/GroundingGPT" -> "X2FD/LVIS-INSTRUCT4V"
"lzw-lzw/GroundingGPT" -> "gyxxyg/VTG-LLM"
"lzw-lzw/GroundingGPT" -> "RenShuhuai-Andy/TimeChat"
"lzw-lzw/GroundingGPT" -> "zamling/PSALM"
"mengtong0110/InferDPT" -> "Fish-and-Sheep/Text-Fluoroscopy"
"mengtong0110/InferDPT" -> "coriverchen/Robust_Steganography"
"Dongping-Chen/MLLM-Judge" -> "Flossiee/HonestyLLM"
"qinghuannn/PAMFN" -> "ZhouKanglei/HGCN_AQA"
"nttmdlab-nlp/InstructDoc" -> "ucaslcl/Fox"
"nttmdlab-nlp/InstructDoc" -> "LukeForeverYoung/UReader" ["e"=1]
"OpenGVLab/MMT-Bench" -> "OpenGVLab/MMIU"
"OpenGVLab/MMT-Bench" -> "KainingYing/CTVIS" ["e"=1]
"OpenGVLab/MMT-Bench" -> "OpenGVLab/PhyBench"
"lntzm/MESM" -> "minghangz/TFVTG"
"mbzuai-oryx/MobiLlama" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"mbzuai-oryx/MobiLlama" -> "mbzuai-oryx/LlamaV-o1" ["e"=1]
"THUDM/CogCoM" -> "njucckevin/MM-Self-Improve"
"xb534/SED" -> "yongliu20/SCAN" ["e"=1]
"YUCHEN005/RobustGER" -> "YUCHEN005/STAR-Adapt"
"YUCHEN005/RobustGER" -> "Hypotheses-Paradise/Hypo2Trans"
"YUCHEN005/RobustGER" -> "YUCHEN005/GenTranslate"
"RammusLeo/DPMesh" -> "EternalEvan/DPMesh"
"RammusLeo/DPMesh" -> "AndyTang15/FLAG3Dv2"
"RammusLeo/DPMesh" -> "AndyTang15/FLAG3D"
"RenShuhuai-Andy/TimeChat" -> "huangb23/VTimeLLM"
"RenShuhuai-Andy/TimeChat" -> "gyxxyg/VTG-LLM"
"RenShuhuai-Andy/TimeChat" -> "rese1f/MovieChat"
"RenShuhuai-Andy/TimeChat" -> "boheumd/MA-LMM"
"RenShuhuai-Andy/TimeChat" -> "EvolvingLMMs-Lab/LongVA"
"RenShuhuai-Andy/TimeChat" -> "Yui010206/SeViLA"
"RenShuhuai-Andy/TimeChat" -> "magic-research/PLLaVA"
"RenShuhuai-Andy/TimeChat" -> "TencentARC/ST-LLM"
"RenShuhuai-Andy/TimeChat" -> "showlab/UniVTG" ["e"=1]
"RenShuhuai-Andy/TimeChat" -> "mbzuai-oryx/Video-ChatGPT"
"RenShuhuai-Andy/TimeChat" -> "mbzuai-oryx/Video-LLaVA"
"RenShuhuai-Andy/TimeChat" -> "RupertLuo/Valley"
"RenShuhuai-Andy/TimeChat" -> "yellow-binary-tree/HawkEye"
"RenShuhuai-Andy/TimeChat" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"RenShuhuai-Andy/TimeChat" -> "OpenGVLab/VideoChat-Flash"
"opendatalab/HA-DPO" -> "luka-group/mDPO"
"yuezih/less-is-more" -> "yuezih/SMILE"
"yuezih/less-is-more" -> "Lackel/AGLA"
"Liuziyu77/RAR" -> "Liuziyu77/MMDU" ["e"=1]
"Srijith-rkr/Whispering-LLaMA" -> "Hypotheses-Paradise/Hypo2Trans" ["e"=1]
"Srijith-rkr/Whispering-LLaMA" -> "YUCHEN005/RobustGER" ["e"=1]
"tpgh24/ag4masses" -> "felixludos/alphageometry"
"tpgh24/ag4masses" -> "foldl/AlphaGeometryRE"
"ModelTC/TFMQ-DM" -> "ChangyuanWang17/APQ-DM" ["e"=1]
"khuangaf/Awesome-Chart-Understanding" -> "vis-nlp/UniChart"
"khuangaf/Awesome-Chart-Understanding" -> "khuangaf/CHOCOLATE"
"khuangaf/Awesome-Chart-Understanding" -> "FuxiaoLiu/MMC"
"khuangaf/Awesome-Chart-Understanding" -> "OpenGVLab/ChartAst"
"khuangaf/Awesome-Chart-Understanding" -> "IDEA-FinAI/ChartBench"
"khuangaf/Awesome-Chart-Understanding" -> "SpursGoZmy/Table-LLaVA" ["e"=1]
"khuangaf/Awesome-Chart-Understanding" -> "vis-nlp/ChartQA"
"khuangaf/Awesome-Chart-Understanding" -> "Alpha-Innovator/ChartVLM"
"HZQ950419/Math-LLaVA" -> "ZrrSkywalker/MAVIS"
"HZQ950419/Math-LLaVA" -> "pengshuai-rin/MultiMath"
"Ucas-HaoranWei/Vary-family" -> "Ucas-HaoranWei/Vary-tiny-600k"
"KID-22/LLM-IR-Bias-Fairness-Survey" -> "XuChen0427/FairDiverse"
"KID-22/LLM-IR-Bias-Fairness-Survey" -> "KID-22/Cocktail"
"KID-22/LLM-IR-Bias-Fairness-Survey" -> "Ethan00Si/KuaiSAR"
"42Shawn/LLaVA-PruMerge" -> "pkunlp-icler/FastV"
"42Shawn/LLaVA-PruMerge" -> "Gumpest/SparseVLMs"
"42Shawn/LLaVA-PruMerge" -> "Yxxxb/VoCo-LLaMA"
"42Shawn/LLaVA-PruMerge" -> "ichbill/LTDD"
"dongyh20/Chain-of-Spot" -> "liuzuyan/ElasticCache"
"yongliu20/SCAN" -> "shiyi-zh0408/NAE_CVPR2024"
"yongliu20/SCAN" -> "Yxxxb/LAVT-RS"
"yongliu20/SCAN" -> "SuleBai/SC-CLIP"
"yongliu20/SCAN" -> "zhang9302002/Flash-VStream"
"yongliu20/SCAN" -> "yongliu20/UniLSeg"
"yongliu20/SCAN" -> "slonetime/EBSeg"
"HaozheZhao/MIC_tool" -> "kkk-an/COFFTEA"
"HankYe/Once-for-Both" -> "Alpha-Innovator/TrustGeoGen"
"HankYe/Once-for-Both" -> "HankYe/AdaptiveDiffusion"
"shiyi-zh0408/NAE_CVPR2024" -> "AndyTang15/FLAG3Dv2"
"shiyi-zh0408/NAE_CVPR2024" -> "Yxxxb/LAVT-RS"
"shiyi-zh0408/NAE_CVPR2024" -> "zhang9302002/Flash-VStream"
"shiyi-zh0408/NAE_CVPR2024" -> "shiyi-zh0408/LOGO"
"shiyi-zh0408/NAE_CVPR2024" -> "SuleBai/SC-CLIP"
"shiyi-zh0408/NAE_CVPR2024" -> "yongliu20/SCAN"
"shiyi-zh0408/NAE_CVPR2024" -> "AndyTang15/FLAG3D"
"AndyTang15/FLAG3Dv2" -> "AndyTang15/FLAG3D"
"AndyTang15/FLAG3Dv2" -> "EternalEvan/DPMesh"
"AndyTang15/FLAG3Dv2" -> "SuleBai/SC-CLIP"
"AndyTang15/FLAG3Dv2" -> "Yxxxb/LAVT-RS"
"teknium1/transformers-gptq-quant" -> "CarperAI/treasure_trove"
"junyangwang0410/AMBER" -> "RUCAIBox/POPE"
"junyangwang0410/AMBER" -> "tianyi-lab/HallusionBench"
"junyangwang0410/AMBER" -> "AoiDragon/POPE"
"junyangwang0410/AMBER" -> "yfzhang114/LLaVA-Align"
"junyangwang0410/AMBER" -> "wenhuang2000/VHTest"
"junyangwang0410/AMBER" -> "YiyangZhou/CSR"
"junyangwang0410/AMBER" -> "YiyangZhou/LURE"
"junyangwang0410/AMBER" -> "FuxiaoLiu/LRV-Instruction"
"junyangwang0410/AMBER" -> "hendryx-scale/mhal-detect"
"agentic-learning-ai-lab/lifelong-memory" -> "kkahatapitiya/LangRepo"
"kkahatapitiya/LangRepo" -> "kahnchana/mvu"
"jonathan-roberts1/charting-new-territories" -> "NJU-LHRS/ScoreRS"
"jonathan-roberts1/charting-new-territories" -> "xiong-zhitong/GeoLB-SigLIP"
"jonathan-roberts1/charting-new-territories" -> "jonathan-roberts1/GPT4GEO"
"kahnchana/mvu" -> "kkahatapitiya/LangRepo"
"kahnchana/mvu" -> "kahnchana/clippy"
"khuangaf/CHOCOLATE" -> "khuangaf/ZeroFEC"
"zhengbw0324/LC-Rec" -> "TengShi-RUC/UniSAR"
"zhengbw0324/LC-Rec" -> "lyingCS/Controllable-Multi-Objective-Reranking"
"zhengbw0324/LC-Rec" -> "lyingCS/UOEP"
"alibaba/conv-llava" -> "LeapLabTHU/DAT-Jittor" ["e"=1]
"KID-22/Source-Bias" -> "KID-22/Cocktail"
"TengShi-RUC/UniSAR" -> "lyingCS/Controllable-Multi-Objective-Reranking"
"TengShi-RUC/UniSAR" -> "zhengbw0324/LC-Rec"
"TengShi-RUC/UniSAR" -> "lyingCS/UOEP"
"Yxxxb/LAVT-RS" -> "zhang9302002/Flash-VStream"
"Yxxxb/LAVT-RS" -> "AndyTang15/FLAG3Dv2"
"Yxxxb/LAVT-RS" -> "AndyTang15/FLAG3D"
"Yxxxb/LAVT-RS" -> "shiyi-zh0408/NAE_CVPR2024"
"KID-22/Cocktail" -> "KID-22/Source-Bias"
"deepseek-ai/DeepSeek-Coder-V2" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"deepseek-ai/Janus" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"deepseek-ai/Janus" -> "OpenBMB/MiniCPM-o" ["e"=1]
"deepseek-ai/Janus" -> "haotian-liu/LLaVA" ["e"=1]
"LMM101/Awesome-Multimodal-Next-Token-Prediction" -> "pkunlp-icler/FastV" ["e"=1]
"meta-llama/llama-models" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"DAMO-NLP-SG/VideoLLaMA2" -> "DAMO-NLP-SG/VideoLLaMA3"
"DAMO-NLP-SG/VideoLLaMA2" -> "DAMO-NLP-SG/Video-LLaMA"
"DAMO-NLP-SG/VideoLLaMA2" -> "PKU-YuanGroup/Video-LLaVA"
"DAMO-NLP-SG/VideoLLaMA2" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"DAMO-NLP-SG/VideoLLaMA2" -> "mbzuai-oryx/Video-ChatGPT"
"DAMO-NLP-SG/VideoLLaMA2" -> "LLaVA-VL/LLaVA-NeXT"
"DAMO-NLP-SG/VideoLLaMA2" -> "magic-research/PLLaVA"
"DAMO-NLP-SG/VideoLLaMA2" -> "OpenGVLab/InternVideo"
"DAMO-NLP-SG/VideoLLaMA2" -> "EvolvingLMMs-Lab/LongVA"
"DAMO-NLP-SG/VideoLLaMA2" -> "RenShuhuai-Andy/TimeChat"
"DAMO-NLP-SG/VideoLLaMA2" -> "MME-Benchmarks/Video-MME"
"DAMO-NLP-SG/VideoLLaMA2" -> "rese1f/MovieChat"
"DAMO-NLP-SG/VideoLLaMA2" -> "PKU-YuanGroup/Chat-UniVi" ["e"=1]
"DAMO-NLP-SG/VideoLLaMA2" -> "OpenGVLab/Ask-Anything"
"DAMO-NLP-SG/VideoLLaMA2" -> "mbzuai-oryx/VideoGPT-plus"
"THUDM/CodeGeeX4" -> "THUDM/CogVLM2" ["e"=1]
"Ucas-HaoranWei/GOT-OCR2.0" -> "OpenGVLab/InternVL" ["e"=1]
"Ucas-HaoranWei/GOT-OCR2.0" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"Ucas-HaoranWei/GOT-OCR2.0" -> "OpenBMB/MiniCPM-o" ["e"=1]
"Ucas-HaoranWei/GOT-OCR2.0" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"Ucas-HaoranWei/GOT-OCR2.0" -> "Ucas-HaoranWei/Vary" ["e"=1]
"baaivision/Emu3" -> "cambrian-mllm/cambrian" ["e"=1]
"allenai/molmo" -> "2U1/Molmo-Finetune"
"allenai/molmo" -> "MMMU-Benchmark/MMMU"
"allenai/molmo" -> "allenai/unified-io-2" ["e"=1]
"allenai/molmo" -> "facebookresearch/perception_models" ["e"=1]
"allenai/molmo" -> "xiaoachen98/Open-LLaVA-NeXT" ["e"=1]
"ShiArthur03/ShiArthur03" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"mustafaaljadery/llama3v" -> "siddrrsh/ambientGPT" ["e"=1]
"mustafaaljadery/llama3v" -> "mbzuai-oryx/LLaVA-pp"
"QwenLM/Qwen2.5-VL" -> "OpenGVLab/InternVL"
"QwenLM/Qwen2.5-VL" -> "QwenLM/Qwen-VL"
"QwenLM/Qwen2.5-VL" -> "QwenLM/Qwen3" ["e"=1]
"QwenLM/Qwen2.5-VL" -> "LLaVA-VL/LLaVA-NeXT"
"QwenLM/Qwen2.5-VL" -> "om-ai-lab/VLM-R1"
"QwenLM/Qwen2.5-VL" -> "haotian-liu/LLaVA"
"QwenLM/Qwen2.5-VL" -> "modelscope/ms-swift" ["e"=1]
"QwenLM/Qwen2.5-VL" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"QwenLM/Qwen2.5-VL" -> "OpenBMB/MiniCPM-o"
"QwenLM/Qwen2.5-VL" -> "Deep-Agent/R1-V"
"QwenLM/Qwen2.5-VL" -> "deepseek-ai/Janus" ["e"=1]
"QwenLM/Qwen2.5-VL" -> "salesforce/LAVIS" ["e"=1]
"QwenLM/Qwen2.5-VL" -> "QwenLM/Qwen" ["e"=1]
"QwenLM/Qwen2.5-VL" -> "QwenLM/Qwen-Agent" ["e"=1]
"QwenLM/Qwen2.5-VL" -> "open-compass/VLMEvalKit"
"facebookresearch/sam2" -> "haotian-liu/LLaVA" ["e"=1]
"jingyaogong/minimind-v" -> "om-ai-lab/VLM-R1" ["e"=1]
"THUDM/GLM-4" -> "THUDM/CogVLM2" ["e"=1]
"THUDM/GLM-4" -> "OpenGVLab/InternVL" ["e"=1]
"THUDM/GLM-4" -> "THUDM/CogVLM" ["e"=1]
"facebookresearch/MobileLLM" -> "Meituan-AutoML/MobileVLM" ["e"=1]
"IVG-SZ/Flash-VStream" -> "IVGSZ/Flash-VStream"
"IVG-SZ/Flash-VStream" -> "AndyTang15/FLAG3Dv2"
"IVGSZ/Flash-VStream" -> "IVG-SZ/Flash-VStream"
"IVGSZ/Flash-VStream" -> "showlab/videollm-online"
"IVGSZ/Flash-VStream" -> "shiyi-zh0408/NAE_CVPR2024"
"IVGSZ/Flash-VStream" -> "Yxxxb/VoCo-LLaMA"
"IVGSZ/Flash-VStream" -> "zhang9302002/Flash-VStream"
"IVGSZ/Flash-VStream" -> "Tengbo-Yu/AnyBimanual"
"IVGSZ/Flash-VStream" -> "Mark12Ding/Dispider"
"IVGSZ/Flash-VStream" -> "Vision-CAIR/LongVU"
"IVGSZ/Flash-VStream" -> "SuleBai/SC-CLIP"
"IVGSZ/Flash-VStream" -> "hmxiong/StreamChat"
"IVGSZ/Flash-VStream" -> "ChangyuanWang17/QVLM"
"IVGSZ/Flash-VStream" -> "Jixuan-Fan/Momentum-GS" ["e"=1]
"IVGSZ/Flash-VStream" -> "AndyTang15/FLAG3Dv2"
"IVGSZ/Flash-VStream" -> "Yxxxb/LAVT-RS"
"IVGSZ/Flash-VStream" -> "yaolinli/TimeChat-Online"
"jingyaogong/minimind" -> "OpenBMB/MiniCPM-o" ["e"=1]
"hijkzzz/Awesome-LLM-Strawberry" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"YueFan1014/VideoAgent" -> "Ziyang412/VideoTree"
"YueFan1014/VideoAgent" -> "wxh1996/VideoAgent"
"YueFan1014/VideoAgent" -> "Leon1207/Video-RAG-master"
"YueFan1014/VideoAgent" -> "bytedance/tarsier"
"IDEA-Research/Grounded-SAM-2" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"AIDC-AI/Marco-o1" -> "PKU-YuanGroup/LLaVA-CoT" ["e"=1]
"PKU-YuanGroup/LLaVA-CoT" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"PKU-YuanGroup/LLaVA-CoT" -> "Deep-Agent/R1-V"
"PKU-YuanGroup/LLaVA-CoT" -> "LLaVA-VL/LLaVA-NeXT"
"PKU-YuanGroup/LLaVA-CoT" -> "open-compass/VLMEvalKit"
"PKU-YuanGroup/LLaVA-CoT" -> "om-ai-lab/VLM-R1"
"PKU-YuanGroup/LLaVA-CoT" -> "TideDra/lmm-r1"
"PKU-YuanGroup/LLaVA-CoT" -> "Osilly/Vision-R1"
"PKU-YuanGroup/LLaVA-CoT" -> "Liuziyu77/Visual-RFT"
"PKU-YuanGroup/LLaVA-CoT" -> "PKU-YuanGroup/Video-LLaVA"
"PKU-YuanGroup/LLaVA-CoT" -> "PKU-YuanGroup/MoE-LLaVA"
"PKU-YuanGroup/LLaVA-CoT" -> "baaivision/Emu3" ["e"=1]
"PKU-YuanGroup/LLaVA-CoT" -> "hiyouga/EasyR1" ["e"=1]
"PKU-YuanGroup/LLaVA-CoT" -> "AIDC-AI/Marco-o1" ["e"=1]
"PKU-YuanGroup/LLaVA-CoT" -> "mbzuai-oryx/LlamaV-o1"
"PKU-YuanGroup/LLaVA-CoT" -> "ModalMinds/MM-EUREKA"
"cambrian-mllm/cambrian" -> "LLaVA-VL/LLaVA-NeXT"
"cambrian-mllm/cambrian" -> "open-compass/VLMEvalKit"
"cambrian-mllm/cambrian" -> "facebookresearch/chameleon" ["e"=1]
"cambrian-mllm/cambrian" -> "baaivision/Emu3" ["e"=1]
"cambrian-mllm/cambrian" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"cambrian-mllm/cambrian" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"cambrian-mllm/cambrian" -> "FoundationVision/LlamaGen" ["e"=1]
"cambrian-mllm/cambrian" -> "NVlabs/VILA"
"cambrian-mllm/cambrian" -> "InternLM/InternLM-XComposer"
"cambrian-mllm/cambrian" -> "OpenGVLab/InternVL"
"cambrian-mllm/cambrian" -> "baaivision/Emu" ["e"=1]
"cambrian-mllm/cambrian" -> "dvlab-research/MGM"
"cambrian-mllm/cambrian" -> "showlab/Show-o" ["e"=1]
"cambrian-mllm/cambrian" -> "tsb0601/MMVP"
"cambrian-mllm/cambrian" -> "Liuziyu77/Visual-RFT"
"microsoft/Magma" -> "Liuziyu77/Visual-RFT" ["e"=1]
"microsoft/Magma" -> "om-ai-lab/VLM-R1" ["e"=1]
"microsoft/Magma" -> "NVlabs/VILA" ["e"=1]
"microsoft/Magma" -> "microsoft/SoM" ["e"=1]
"microsoft/Magma" -> "Deep-Agent/R1-V" ["e"=1]
"hzwer/WritingAIPaper" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "daixiangzi/Awesome-Token-Compress"
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "luogen1996/LLaVA-HR"
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "pkunlp-icler/FastV"
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "lxa9867/Awesome-Autoregressive-Visual-Generation" ["e"=1]
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "swordlidev/Evaluation-Multimodal-LLMs-Survey"
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "shufangxun/LLaVA-MoD" ["e"=1]
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "Wang-Xiaodong1899/CVPR25-MLLM-Paper-List"
"swordlidev/Efficient-Multimodal-LLMs-Survey" -> "showlab/Awesome-Unified-Multimodal-Models" ["e"=1]
"HJYao00/DenseConnector" -> "Yxxxb/VoCo-LLaMA"
"HJYao00/DenseConnector" -> "mrwu-mac/ControlMLLM"
"HJYao00/DenseConnector" -> "HJYao00/Side4Video"
"VITA-MLLM/VITA" -> "MME-Benchmarks/Video-MME" ["e"=1]
"VITA-MLLM/VITA" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"VITA-MLLM/VITA" -> "DAMO-NLP-SG/VideoLLaMA2" ["e"=1]
"ywh187/FitPrune" -> "LaVi-Lab/AIM"
"NVlabs/EAGLE" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"NVlabs/EAGLE" -> "open-compass/VLMEvalKit"
"NVlabs/EAGLE" -> "baaivision/EVE" ["e"=1]
"NVlabs/EAGLE" -> "xiaoachen98/Open-LLaVA-NeXT" ["e"=1]
"NVlabs/EAGLE" -> "cambrian-mllm/cambrian"
"NVlabs/EAGLE" -> "thunlp/LLaVA-UHD"
"NVlabs/EAGLE" -> "Windsander/ADI-Stable-Diffusion" ["e"=1]
"NVlabs/EAGLE" -> "ByungKwanLee/MoAI" ["e"=1]
"NVlabs/EAGLE" -> "tsb0601/MMVP"
"NVlabs/EAGLE" -> "FoundationVision/Groma" ["e"=1]
"NVlabs/EAGLE" -> "mbzuai-oryx/groundingLMM"
"NVlabs/EAGLE" -> "LLaVA-VL/LLaVA-NeXT"
"NVlabs/EAGLE" -> "HJYao00/DenseConnector"
"NVlabs/EAGLE" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"NVlabs/EAGLE" -> "showlab/Show-o" ["e"=1]
"dvlab-research/Step-DPO" -> "dvlab-research/VisionZip" ["e"=1]
"test-time-training/ttt-lm-pytorch" -> "cambrian-mllm/cambrian" ["e"=1]
"SimpleBerry/LLaMA-O1" -> "TideDra/lmm-r1" ["e"=1]
"16131zzzzzzzz/EveryoneNobel" -> "JusticeFighterDance/JusticeFighter110" ["e"=1]
"LongHZ140516/awesome-framework-gallery" -> "yaotingwangofficial/Awesome-MCoT" ["e"=1]
"facebookresearch/chameleon" -> "cambrian-mllm/cambrian" ["e"=1]
"facebookresearch/chameleon" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"facebookresearch/chameleon" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"Mark12Ding/SAM2Long" -> "Davion-Liu/Awesome-Robustness-in-Information-Retrieval" ["e"=1]
"pengboci/GraphRAG-Survey" -> "ChangyuanWang17/APQ-DM" ["e"=1]
"ZrrSkywalker/MAVIS" -> "HZQ950419/Math-LLaVA"
"ZrrSkywalker/MAVIS" -> "ZrrSkywalker/MathVerse"
"ZrrSkywalker/MAVIS" -> "pengshuai-rin/MultiMath"
"Tencent/Tencent-Hunyuan-Large" -> "MoonshotAI/Kimi-VL" ["e"=1]
"JusticeFighterDance/JusticeFighter110" -> "william-sto/JusticeNeverTooLate"
"JusticeFighterDance/JusticeFighter110" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"JusticeFighterDance/JusticeFighter110" -> "TideDra/lmm-r1"
"JusticeFighterDance/JusticeFighter110" -> "ByteDance-Seed/Triton-distributed" ["e"=1]
"Theia-4869/FasterVLM" -> "Gumpest/SparseVLMs"
"Theia-4869/FasterVLM" -> "KD-TAO/DyCoke"
"mlfoundations/MINT-1T" -> "OpenGVLab/OmniCorpus"
"mlfoundations/MINT-1T" -> "MMMU-Benchmark/MMMU"
"mlfoundations/MINT-1T" -> "cambrian-mllm/cambrian"
"mlfoundations/MINT-1T" -> "baaivision/DenseFusion"
"mlfoundations/MINT-1T" -> "BAAI-DCAI/Bunny"
"mlfoundations/MINT-1T" -> "facebookresearch/chameleon" ["e"=1]
"mlfoundations/MINT-1T" -> "UCSC-VLAA/Recap-DataComp-1B" ["e"=1]
"mlfoundations/MINT-1T" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"mlfoundations/MINT-1T" -> "zzxslp/SoM-LLaVA"
"mlfoundations/MINT-1T" -> "DAMO-NLP-SG/multimodal_textbook" ["e"=1]
"mlfoundations/MINT-1T" -> "open-compass/VLMEvalKit"
"mlfoundations/MINT-1T" -> "RUCAIBox/POPE"
"mlfoundations/MINT-1T" -> "snap-research/Panda-70M" ["e"=1]
"mlfoundations/MINT-1T" -> "JUNJIE99/MLVU"
"daixiangzi/Awesome-Token-Compress" -> "pkunlp-icler/FastV"
"daixiangzi/Awesome-Token-Compress" -> "Theia-4869/FasterVLM"
"daixiangzi/Awesome-Token-Compress" -> "Gumpest/SparseVLMs"
"daixiangzi/Awesome-Token-Compress" -> "dvlab-research/VisionZip"
"daixiangzi/Awesome-Token-Compress" -> "42Shawn/LLaVA-PruMerge"
"daixiangzi/Awesome-Token-Compress" -> "CircleRadon/TokenPacker" ["e"=1]
"daixiangzi/Awesome-Token-Compress" -> "Cooperx521/PyramidDrop"
"daixiangzi/Awesome-Token-Compress" -> "ZLKong/awesome-token-compression-reduction"
"daixiangzi/Awesome-Token-Compress" -> "swordlidev/Efficient-Multimodal-LLMs-Survey"
"daixiangzi/Awesome-Token-Compress" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"daixiangzi/Awesome-Token-Compress" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"daixiangzi/Awesome-Token-Compress" -> "luogen1996/LLaVA-HR"
"daixiangzi/Awesome-Token-Compress" -> "ModalMinds/MM-EUREKA"
"daixiangzi/Awesome-Token-Compress" -> "xuyang-liu16/Awesome-Token-level-Model-Compression" ["e"=1]
"daixiangzi/Awesome-Token-Compress" -> "Vision-CAIR/LongVU"
"aburkov/theLMbook" -> "Deep-Agent/R1-V" ["e"=1]
"byjlw/video-analyzer" -> "VectorSpaceLab/Video-XL" ["e"=1]
"TideDra/zotero-arxiv-daily" -> "JoeLeelyf/customize-arxiv-daily" ["e"=1]
"baaivision/DIVA" -> "showlab/Awesome-Unified-Multimodal-Models" ["e"=1]
"baaivision/DIVA" -> "tsb0601/MMVP"
"baaivision/DIVA" -> "baaivision/EVE" ["e"=1]
"ZiyuGuo99/SAM2Point" -> "ZrrSkywalker/MathVerse" ["e"=1]
"Yushi-Hu/VisualSketchpad" -> "FanqingM/R1-Multimodal-Journey"
"Vision-CAIR/LongVU" -> "VectorSpaceLab/Video-XL"
"Vision-CAIR/LongVU" -> "egoschema/EgoSchema"
"Vision-CAIR/LongVU" -> "OpenGVLab/VideoChat-Flash"
"Vision-CAIR/LongVU" -> "EvolvingLMMs-Lab/LongVA"
"Vision-CAIR/LongVU" -> "IVGSZ/Flash-VStream"
"Vision-CAIR/LongVU" -> "longvideobench/LongVideoBench"
"Vision-CAIR/LongVU" -> "rese1f/MovieChat"
"Vision-CAIR/LongVU" -> "Ziyang412/VideoTree"
"Vision-CAIR/LongVU" -> "bigai-nlco/VideoLLaMB"
"Vision-CAIR/LongVU" -> "JUNJIE99/MLVU"
"Vision-CAIR/LongVU" -> "TimeMarker-LLM/TimeMarker"
"Vision-CAIR/LongVU" -> "llyx97/TempCompass"
"Vision-CAIR/LongVU" -> "boheumd/MA-LMM"
"Vision-CAIR/LongVU" -> "daixiangzi/Awesome-Token-Compress"
"Vision-CAIR/LongVU" -> "MME-Benchmarks/Video-MME"
"dvlab-research/Lyra" -> "Falling-dow/Unsupervised-Image-Enhancement-with-CNN-and-GAN" ["e"=1]
"dvlab-research/Lyra" -> "Davion-Liu/Awesome-Robustness-in-Information-Retrieval"
"showlab/videollm-online" -> "IVGSZ/Flash-VStream"
"showlab/videollm-online" -> "Mark12Ding/Dispider"
"showlab/videollm-online" -> "EvolvingLMMs-Lab/LongVA"
"showlab/videollm-online" -> "THUNLP-MT/StreamingBench" ["e"=1]
"showlab/videollm-online" -> "OpenGVLab/VideoChat-Flash"
"showlab/videollm-online" -> "hmxiong/StreamChat"
"showlab/videollm-online" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"showlab/videollm-online" -> "RenShuhuai-Andy/TimeChat"
"showlab/videollm-online" -> "boheumd/MA-LMM"
"showlab/videollm-online" -> "MME-Benchmarks/Video-MME"
"showlab/videollm-online" -> "Vision-CAIR/LongVU"
"showlab/videollm-online" -> "JoeLeelyf/OVO-Bench"
"showlab/videollm-online" -> "DAMO-NLP-SG/VideoLLaMA2"
"showlab/videollm-online" -> "TencentARC/ST-LLM"
"showlab/videollm-online" -> "showlab/livecc"
"zjunlp/OneGen" -> "zjunlp/Deco"
"gordonhu608/MQT-LLaVA" -> "42Shawn/LLaVA-PruMerge"
"Yxxxb/VoCo-LLaMA" -> "shiyi-zh0408/NAE_CVPR2024"
"Yxxxb/VoCo-LLaMA" -> "AndyTang15/FLAG3Dv2"
"Yxxxb/VoCo-LLaMA" -> "Yxxxb/LAVT-RS"
"Yxxxb/VoCo-LLaMA" -> "SuleBai/SC-CLIP"
"Yxxxb/VoCo-LLaMA" -> "Tengbo-Yu/AnyBimanual"
"Yxxxb/VoCo-LLaMA" -> "IVGSZ/Flash-VStream"
"Yxxxb/VoCo-LLaMA" -> "EternalEvan/DPMesh"
"Yxxxb/VoCo-LLaMA" -> "Gumpest/SparseVLMs"
"Yxxxb/VoCo-LLaMA" -> "GuanxingLu/vlarl"
"Yxxxb/VoCo-LLaMA" -> "42Shawn/LLaVA-PruMerge"
"Yxxxb/VoCo-LLaMA" -> "HJYao00/DenseConnector"
"CircleRadon/TokenPacker" -> "daixiangzi/Awesome-Token-Compress" ["e"=1]
"MCG-NJU/p-MoD" -> "MCG-NJU/CaReBench"
"zhangfaen/finetune-Qwen2-VL" -> "2U1/Qwen2-VL-Finetune"
"zhangfaen/finetune-Qwen2-VL" -> "wjbmattingly/qwen2-vl-finetune-huggingface"
"zhangfaen/finetune-Qwen2-VL" -> "zhangfaen/finetune-Qwen2.5-VL"
"zhangfaen/finetune-Qwen2-VL" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"zhangfaen/finetune-Qwen2-VL" -> "Fancy-MLLM/R1-Onevision"
"zhangfaen/finetune-Qwen2-VL" -> "sandy1990418/Finetune-Qwen2.5-VL"
"zhangfaen/finetune-Qwen2-VL" -> "OpenGVLab/VideoChat-Flash"
"Ziyang412/VideoTree" -> "CeeZh/LLoVi"
"Ziyang412/VideoTree" -> "wxh1996/VideoAgent"
"Ziyang412/VideoTree" -> "kkahatapitiya/LangRepo"
"Ziyang412/VideoTree" -> "Yui010206/SeViLA"
"MME-Benchmarks/Video-MME" -> "VITA-MLLM/Woodpecker"
"MME-Benchmarks/Video-MME" -> "EvolvingLMMs-Lab/LongVA"
"MME-Benchmarks/Video-MME" -> "shenyunhang/APE" ["e"=1]
"MME-Benchmarks/Video-MME" -> "JUNJIE99/MLVU"
"MME-Benchmarks/Video-MME" -> "VITA-MLLM/Long-VITA"
"MME-Benchmarks/Video-MME" -> "OpenGVLab/VideoChat-Flash"
"MME-Benchmarks/Video-MME" -> "Ziyang412/VideoTree"
"MME-Benchmarks/Video-MME" -> "TencentARC/ST-LLM"
"MME-Benchmarks/Video-MME" -> "VITA-MLLM/VITA" ["e"=1]
"MME-Benchmarks/Video-MME" -> "RenShuhuai-Andy/TimeChat"
"MME-Benchmarks/Video-MME" -> "magic-research/PLLaVA"
"MME-Benchmarks/Video-MME" -> "zhourax/VEGA"
"MME-Benchmarks/Video-MME" -> "DAMO-NLP-SG/VideoLLaMA2"
"MME-Benchmarks/Video-MME" -> "rese1f/MovieChat"
"MME-Benchmarks/Video-MME" -> "longvideobench/LongVideoBench"
"mrwu-mac/ControlMLLM" -> "Cooperx521/PyramidDrop"
"mrwu-mac/ControlMLLM" -> "HJYao00/DenseConnector"
"mrwu-mac/ControlMLLM" -> "saccharomycetes/mllms_know"
"mrwu-mac/ControlMLLM" -> "wusize/F-LMM"
"mrwu-mac/ControlMLLM" -> "LALBJ/PAI"
"illuin-tech/colpali" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"EvolvingLMMs-Lab/LongVA" -> "VectorSpaceLab/Video-XL"
"EvolvingLMMs-Lab/LongVA" -> "JUNJIE99/MLVU"
"EvolvingLMMs-Lab/LongVA" -> "MME-Benchmarks/Video-MME"
"EvolvingLMMs-Lab/LongVA" -> "RifleZhang/LLaVA-Hound-DPO"
"EvolvingLMMs-Lab/LongVA" -> "FreedomIntelligence/LongLLaVA"
"EvolvingLMMs-Lab/LongVA" -> "RenShuhuai-Andy/TimeChat"
"EvolvingLMMs-Lab/LongVA" -> "Ziyang412/VideoTree"
"EvolvingLMMs-Lab/LongVA" -> "magic-research/PLLaVA"
"EvolvingLMMs-Lab/LongVA" -> "longvideobench/LongVideoBench"
"EvolvingLMMs-Lab/LongVA" -> "Vision-CAIR/LongVU"
"EvolvingLMMs-Lab/LongVA" -> "TencentARC/ST-LLM"
"EvolvingLMMs-Lab/LongVA" -> "OpenGVLab/VideoChat-Flash"
"EvolvingLMMs-Lab/LongVA" -> "gyxxyg/VTG-LLM"
"EvolvingLMMs-Lab/LongVA" -> "xiaoachen98/Open-LLaVA-NeXT" ["e"=1]
"EvolvingLMMs-Lab/LongVA" -> "EvolvingLMMs-Lab/EgoLife"
"bytedance/tarsier" -> "magic-research/PLLaVA"
"bytedance/tarsier" -> "Ziyang412/VideoTree"
"bytedance/tarsier" -> "mbzuai-oryx/VideoGPT-plus"
"bytedance/tarsier" -> "OpenGVLab/VideoChat-Flash"
"bytedance/tarsier" -> "MME-Benchmarks/Video-MME"
"bytedance/tarsier" -> "RifleZhang/LLaVA-Hound-DPO"
"bytedance/tarsier" -> "DAMO-NLP-SG/VideoLLaMA3"
"bytedance/tarsier" -> "EvolvingLMMs-Lab/LongVA"
"bytedance/tarsier" -> "IVGSZ/Flash-VStream"
"bytedance/tarsier" -> "QQ-MM/Video-CCAM"
"bytedance/tarsier" -> "VectorSpaceLab/Video-XL"
"bytedance/tarsier" -> "Vision-CAIR/LongVU"
"bytedance/tarsier" -> "YueFan1014/VideoAgent"
"bytedance/tarsier" -> "TimeMarker-LLM/TimeMarker"
"bytedance/tarsier" -> "egoschema/EgoSchema"
"wxh1996/VideoAgent" -> "Ziyang412/VideoTree"
"zjysteven/lmms-finetune" -> "TinyLLaVA/TinyLLaVA_Factory"
"zjysteven/lmms-finetune" -> "xiaoachen98/Open-LLaVA-NeXT" ["e"=1]
"zjysteven/lmms-finetune" -> "bfshi/scaling_on_scales"
"zjysteven/lmms-finetune" -> "Coobiw/MPP-LLaVA"
"VectorSpaceLab/Video-XL" -> "JUNJIE99/MLVU"
"VectorSpaceLab/Video-XL" -> "EvolvingLMMs-Lab/LongVA"
"VectorSpaceLab/Video-XL" -> "Vision-CAIR/LongVU"
"VectorSpaceLab/Video-XL" -> "OpenGVLab/VideoChat-Flash"
"VectorSpaceLab/Video-XL" -> "MME-Benchmarks/Video-MME"
"VectorSpaceLab/Video-XL" -> "rese1f/MovieChat"
"VectorSpaceLab/Video-XL" -> "yeliudev/VideoMind"
"VectorSpaceLab/Video-XL" -> "FreedomIntelligence/LongLLaVA"
"VectorSpaceLab/Video-XL" -> "IVGSZ/Flash-VStream"
"VectorSpaceLab/Video-XL" -> "bytedance/tarsier"
"VectorSpaceLab/Video-XL" -> "ttengwang/Awesome_Long_Form_Video_Understanding"
"VectorSpaceLab/Video-XL" -> "DAMO-NLP-SG/VideoLLaMA3"
"VectorSpaceLab/Video-XL" -> "Ziyang412/VideoTree"
"VectorSpaceLab/Video-XL" -> "RenShuhuai-Andy/TimeChat"
"VectorSpaceLab/Video-XL" -> "TimeMarker-LLM/TimeMarker"
"mc-lan/Text4Seg" -> "wusize/F-LMM"
"mc-lan/Text4Seg" -> "berkeley-hipie/segllm"
"mc-lan/Text4Seg" -> "xuliu-cyber/RSUniVLM"
"Leon1207/Video-RAG-master" -> "MAC-AutoML/QuoTA"
"Leon1207/Video-RAG-master" -> "VITA-MLLM/Sparrow"
"Leon1207/Video-RAG-master" -> "VITA-MLLM/Long-VITA"
"Leon1207/Video-RAG-master" -> "Kwai-YuanQi/MM-RLHF"
"Leon1207/Video-RAG-master" -> "Ziyang412/VideoTree"
"Leon1207/Video-RAG-master" -> "VITA-MLLM/VITA-Audio"
"Leon1207/Video-RAG-master" -> "yongliang-wu/NumPro" ["e"=1]
"AIDC-AI/Ovis" -> "open-compass/VLMEvalKit"
"AIDC-AI/Ovis" -> "showlab/Show-o" ["e"=1]
"AIDC-AI/Ovis" -> "bfshi/scaling_on_scales"
"AIDC-AI/Ovis" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"AIDC-AI/Ovis" -> "Oryx-mllm/Oryx"
"AIDC-AI/Ovis" -> "FanqingM/R1-Multimodal-Journey"
"AIDC-AI/Ovis" -> "THUDM/CogVLM2"
"AIDC-AI/Ovis" -> "LLaVA-VL/LLaVA-NeXT"
"AIDC-AI/Ovis" -> "Liuziyu77/Visual-RFT"
"AIDC-AI/Ovis" -> "cambrian-mllm/cambrian"
"AIDC-AI/Ovis" -> "InternLM/InternLM-XComposer"
"AIDC-AI/Ovis" -> "dongyh20/Insight-V"
"AIDC-AI/Ovis" -> "AIDC-AI/Parrot" ["e"=1]
"AIDC-AI/Ovis" -> "VITA-MLLM/VITA" ["e"=1]
"AIDC-AI/Ovis" -> "ModalMinds/MM-EUREKA"
"Bujiazi/ByTheWay" -> "beichenzbc/BoostStep"
"gyxxyg/VTG-LLM" -> "gyxxyg/TRACE"
"gyxxyg/VTG-LLM" -> "yellow-binary-tree/HawkEye"
"gyxxyg/VTG-LLM" -> "sudo-Boris/mr-Blip" ["e"=1]
"gyxxyg/VTG-LLM" -> "huangb23/VTimeLLM"
"gyxxyg/VTG-LLM" -> "minghangz/TFVTG"
"gyxxyg/VTG-LLM" -> "NVlabs/LITA"
"gyxxyg/VTG-LLM" -> "Becomebright/GroundVQA" ["e"=1]
"gyxxyg/VTG-LLM" -> "RenShuhuai-Andy/TimeChat"
"gyxxyg/VTG-LLM" -> "TimeMarker-LLM/TimeMarker"
"gyxxyg/VTG-LLM" -> "WHB139426/Grounded-Video-LLM"
"rhymes-ai/Aria" -> "rhymes-ai/Allegro" ["e"=1]
"rhymes-ai/Aria" -> "kijai/ComfyUI-MochiWrapper" ["e"=1]
"rhymes-ai/Aria" -> "MME-Benchmarks/Video-MME"
"rhymes-ai/Aria" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"rhymes-ai/Aria" -> "EvolvingLMMs-Lab/LongVA"
"rhymes-ai/Aria" -> "LLaVA-VL/LLaVA-NeXT"
"rhymes-ai/Aria" -> "facebookresearch/chameleon" ["e"=1]
"rhymes-ai/Aria" -> "bytedance/tarsier"
"rhymes-ai/Aria" -> "PKU-YuanGroup/LLaVA-CoT"
"rhymes-ai/Aria" -> "NVlabs/VILA"
"rhymes-ai/Aria" -> "longvideobench/LongVideoBench"
"rhymes-ai/Aria" -> "apple/ml-aim" ["e"=1]
"rhymes-ai/Aria" -> "DAMO-NLP-SG/VideoLLaMA2"
"rhymes-ai/Aria" -> "DAMO-NLP-SG/multimodal_textbook" ["e"=1]
"rhymes-ai/Aria" -> "EvolvingLMMs-Lab/lmms-eval" ["e"=1]
"FreedomIntelligence/LongLLaVA" -> "EvolvingLMMs-Lab/LongVA"
"yfzhang114/SliME" -> "MME-Benchmarks/MME-RealWorld"
"yfzhang114/SliME" -> "ParadoxZW/LLaVA-UHD-Better"
"yfzhang114/SliME" -> "Kwai-YuanQi/MM-RLHF"
"SpursGoZmy/Table-LLaVA" -> "khuangaf/Awesome-Chart-Understanding" ["e"=1]
"Alpha-Innovator/StructEqTable-Deploy" -> "HankYe/AdaptiveDiffusion" ["e"=1]
"Alpha-Innovator/StructEqTable-Deploy" -> "Alpha-Innovator/DocGenome" ["e"=1]
"gyxxyg/TRACE" -> "gyxxyg/VTG-LLM"
"gyxxyg/TRACE" -> "yongliang-wu/NumPro" ["e"=1]
"THUDM/LVBench" -> "Share14/ShareGemini"
"rese1f/aurora" -> "Espere-1119-Song/Video-MMLU"
"rese1f/aurora" -> "Share14/ShareGemini"
"ChangyuanWang17/QVLM" -> "ChangyuanWang17/APQ-DM"
"ChangyuanWang17/QVLM" -> "AndyTang15/FLAG3Dv2"
"ChangyuanWang17/QVLM" -> "Tengbo-Yu/AnyBimanual"
"ChangyuanWang17/QVLM" -> "Yxxxb/LAVT-RS"
"ChangyuanWang17/QVLM" -> "shiyi-zh0408/NAE_CVPR2024"
"ChangyuanWang17/QVLM" -> "EternalEvan/DPMesh"
"ChangyuanWang17/QVLM" -> "zhang9302002/Flash-VStream"
"Tengbo-Yu/AnyBimanual" -> "markusgrotz/peract_bimanual" ["e"=1]
"Tengbo-Yu/AnyBimanual" -> "GuanxingLu/vlarl"
"Tengbo-Yu/AnyBimanual" -> "AndyTang15/FLAG3Dv2"
"Tengbo-Yu/AnyBimanual" -> "ManiCM-fast/ManiCM" ["e"=1]
"zjysteven/VLM-Visualizer" -> "zhangbaijin/From-Redundancy-to-Relevance"
"zjysteven/VLM-Visualizer" -> "IntelLabs/lvlm-interpret"
"zjysteven/VLM-Visualizer" -> "saccharomycetes/mllms_know"
"zjysteven/VLM-Visualizer" -> "junyangwang0410/Attention-LLaVA"
"dongyh20/Insight-V" -> "RUCAIBox/Virgo"
"dongyh20/Insight-V" -> "RLHF-V/RLHF-V"
"dongyh20/Insight-V" -> "yihedeng9/STIC"
"dongyh20/Insight-V" -> "deepcs233/Visual-CoT"
"dongyh20/Insight-V" -> "FanqingM/R1-Multimodal-Journey"
"dongyh20/Insight-V" -> "mbzuai-oryx/LlamaV-o1"
"dongyh20/Insight-V" -> "dongyh20/Chain-of-Spot"
"dongyh20/Insight-V" -> "jungao1106/ICoT"
"dongyh20/Insight-V" -> "njucckevin/MM-Self-Improve"
"OpenGVLab/PhyGenBench" -> "OpenGVLab/MMIU" ["e"=1]
"gls0425/LinVT" -> "SCZwangxiao/video-FlexReduc"
"YUCHEN005/STAR-Adapt" -> "YUCHEN005/GenTranslate"
"YUCHEN005/STAR-Adapt" -> "YUCHEN005/RobustGER"
"njucckevin/MM-Self-Improve" -> "Liac-li/MM-self-improve-qwen2vl"
"2U1/Qwen2-VL-Finetune" -> "zhangfaen/finetune-Qwen2-VL"
"2U1/Qwen2-VL-Finetune" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"2U1/Qwen2-VL-Finetune" -> "Liuziyu77/Visual-RFT"
"2U1/Qwen2-VL-Finetune" -> "sandy1990418/Finetune-Qwen2.5-VL"
"2U1/Qwen2-VL-Finetune" -> "LLaVA-VL/LLaVA-NeXT"
"2U1/Qwen2-VL-Finetune" -> "Fancy-MLLM/R1-Onevision"
"2U1/Qwen2-VL-Finetune" -> "QwenLM/Qwen2.5-VL"
"2U1/Qwen2-VL-Finetune" -> "FanqingM/R1-Multimodal-Journey"
"2U1/Qwen2-VL-Finetune" -> "hiyouga/EasyR1" ["e"=1]
"2U1/Qwen2-VL-Finetune" -> "2U1/Llama3.2-Vision-Finetune"
"2U1/Qwen2-VL-Finetune" -> "zhangfaen/finetune-Qwen2.5-VL"
"2U1/Qwen2-VL-Finetune" -> "open-compass/VLMEvalKit"
"2U1/Qwen2-VL-Finetune" -> "TideDra/lmm-r1"
"2U1/Qwen2-VL-Finetune" -> "om-ai-lab/VLM-R1"
"2U1/Qwen2-VL-Finetune" -> "OpenGVLab/VideoChat-Flash"
"Alpha-Innovator/Chimera" -> "Alpha-Innovator/AdaptiveDiffusion"
"Alpha-Innovator/Chimera" -> "HankYe/AdaptiveDiffusion"
"Alpha-Innovator/Chimera" -> "Alpha-Innovator/OmniCaptioner"
"Alpha-Innovator/Chimera" -> "ch3cook-fdu/Vote2Cap-DETR"
"Alpha-Innovator/Chimera" -> "Alpha-Innovator/SurveyForge"
"ManiCM-fast/ManiCM" -> "Tengbo-Yu/AnyBimanual" ["e"=1]
"opendatalab/DocLayout-YOLO" -> "X-PLUG/mPLUG-DocOwl" ["e"=1]
"Alpha-Innovator/AdaptiveDiffusion" -> "Alpha-Innovator/Chimera"
"Alpha-Innovator/AdaptiveDiffusion" -> "HankYe/AdaptiveDiffusion"
"Alpha-Innovator/AdaptiveDiffusion" -> "Alpha-Innovator/SurveyForge"
"Alpha-Innovator/AdaptiveDiffusion" -> "Alpha-Innovator/DocGenome"
"Alpha-Innovator/AdaptiveDiffusion" -> "Alpha-Innovator/Dolphin"
"zli12321/Vision-Language-Models-Overview" -> "zli12321/qa_metrics"
"zli12321/Vision-Language-Models-Overview" -> "zli12321/VideoHallu"
"opendatalab/OmniDocBench" -> "Alpha-Innovator/DocGenome" ["e"=1]
"opendatalab/OmniDocBench" -> "ucaslcl/Fox" ["e"=1]
"markusgrotz/peract_bimanual" -> "Tengbo-Yu/AnyBimanual" ["e"=1]
"XJF2332/GOT-OCR-2-GUI" -> "1694439208/GOT-OCR-Inference"
"baaivision/EVE" -> "baaivision/DenseFusion" ["e"=1]
"baaivision/EVE" -> "thunlp/LLaVA-UHD" ["e"=1]
"shengliu66/VTI" -> "ustc-hyin/ClearSight"
"shengliu66/VTI" -> "TianyunYoung/Hallucination-Attribution"
"zhang9302002/Flash-VStream" -> "Yxxxb/LAVT-RS"
"zhang9302002/Flash-VStream" -> "AndyTang15/FLAG3Dv2"
"zhang9302002/Flash-VStream" -> "shiyi-zh0408/NAE_CVPR2024"
"Oryx-mllm/Oryx" -> "Ola-Omni/Ola"
"Oryx-mllm/Oryx" -> "shiml20/FlowTurbo"
"Oryx-mllm/Oryx" -> "dongyh20/Insight-V"
"dvlab-research/VisionZip" -> "pkunlp-icler/FastV"
"dvlab-research/VisionZip" -> "Gumpest/SparseVLMs"
"dvlab-research/VisionZip" -> "daixiangzi/Awesome-Token-Compress"
"dvlab-research/VisionZip" -> "dvlab-research/Lyra"
"dvlab-research/VisionZip" -> "Theia-4869/FasterVLM"
"dvlab-research/VisionZip" -> "Yangsenqiao/ULDA"
"dvlab-research/VisionZip" -> "42Shawn/LLaVA-PruMerge"
"dvlab-research/VisionZip" -> "dvlab-research/Step-DPO" ["e"=1]
"dvlab-research/VisionZip" -> "Cooperx521/PyramidDrop"
"dvlab-research/VisionZip" -> "Yxxxb/VoCo-LLaMA"
"dvlab-research/VisionZip" -> "saccharomycetes/mllms_know"
"dvlab-research/VisionZip" -> "ictnlp/LLaVA-Mini" ["e"=1]
"dvlab-research/VisionZip" -> "KD-TAO/DyCoke"
"dvlab-research/VisionZip" -> "dvlab-research/LLaMA-VID"
"microsoft/LLM2CLIP" -> "baaivision/DIVA" ["e"=1]
"OpenGVLab/OmniCorpus" -> "OpenGVLab/MM-NIAH"
"OpenGVLab/OmniCorpus" -> "mlfoundations/MINT-1T"
"OpenGVLab/OmniCorpus" -> "BAAI-DCAI/Visual-Instruction-Tuning"
"OpenGVLab/OmniCorpus" -> "OpenGVLab/all-seeing"
"OpenGVLab/OmniCorpus" -> "thunlp/LLaVA-UHD"
"OpenGVLab/OmniCorpus" -> "baaivision/DenseFusion"
"OpenGVLab/OmniCorpus" -> "baaivision/EVE" ["e"=1]
"OpenGVLab/OmniCorpus" -> "FreedomIntelligence/ALLaVA"
"OpenGVLab/OmniCorpus" -> "VisionXLab/STAR-MMRotate" ["e"=1]
"OpenGVLab/OmniCorpus" -> "X2FD/LVIS-INSTRUCT4V"
"NiuTrans/Vision-LLM-Alignment" -> "wangclnlp/DeepSpeed-Chat-Extension"
"NiuTrans/Vision-LLM-Alignment" -> "TideDra/VL-RLHF"
"NiuTrans/Vision-LLM-Alignment" -> "NiuTrans/LaMaTE"
"CaraJ7/MMSearch" -> "MME-Benchmarks/MME-CoT" ["e"=1]
"CaraJ7/MMSearch" -> "CaraJ7/CoMat" ["e"=1]
"CaraJ7/MMSearch" -> "CaraJ7/T2I-R1" ["e"=1]
"CaraJ7/MMSearch" -> "ZrrSkywalker/MathVerse"
"CaraJ7/MMSearch" -> "ZrrSkywalker/MAVIS"
"CaraJ7/MMSearch" -> "EvolvingLMMs-Lab/multimodal-search-r1"
"IntelLabs/lvlm-interpret" -> "nickjiang2378/vl-interp"
"VITA-MLLM/Freeze-Omni" -> "VITA-MLLM/VITA-Audio" ["e"=1]
"VITA-MLLM/Freeze-Omni" -> "VITA-MLLM/Long-VITA" ["e"=1]
"SUSTechBruce/LOOK-M" -> "NastyMarcus/A-Survey-of-Efficient-Diffusion-Models"
"SUSTechBruce/LOOK-M" -> "Cooperx521/PyramidDrop"
"Shengcao-Cao/groundLMM" -> "wusize/F-LMM"
"WHB139426/Grounded-Video-LLM" -> "TimeMarker-LLM/TimeMarker"
"WHB139426/Grounded-Video-LLM" -> "DCDmllm/Momentor"
"WHB139426/Grounded-Video-LLM" -> "minghangz/TFVTG"
"WHB139426/Grounded-Video-LLM" -> "gyxxyg/VTG-LLM"
"rccchoudhury/rlt" -> "Vchitect/FasterCache" ["e"=1]
"rccchoudhury/rlt" -> "contrastive/FreeVideoLLM"
"rccchoudhury/rlt" -> "gls0425/LinVT"
"JiazuoYu/PathWeave" -> "hmxiong/StreamChat"
"slonetime/EBSeg" -> "letitiabanana/PnP-OVSS"
"EdoardoBotta/RQ-VAE-Recommender" -> "zhengbw0324/LC-Rec" ["e"=1]
"UMass-Embodied-AGI/FlexAttention" -> "hasanar1f/HiRED"
"kang-wu/SkySensePlusPlus" -> "alipay/POA"
"kang-wu/SkySensePlusPlus" -> "likyoo/awesome-MLLM-for-image-segmentation"
"YiyangZhou/CSR" -> "YiyangZhou/POVID"
"YiyangZhou/CSR" -> "yihedeng9/STIC"
"YiyangZhou/CSR" -> "YuxiXie/V-DPO"
"2U1/Phi3-Vision-Finetune" -> "GaiZhenbiao/Phi3V-Finetuning"
"2U1/Phi3-Vision-Finetune" -> "2U1/Llama3.2-Vision-Finetune"
"2U1/Llama3.2-Vision-Finetune" -> "2U1/Phi3-Vision-Finetune"
"2U1/Llama3.2-Vision-Finetune" -> "2U1/Pixtral-Finetune"
"intsig-textin/parsex-frontend" -> "intsig-textin/markdown_tester"
"intsig-textin/parsex-frontend" -> "intsig-textin/parsex-sdk"
"intsig-textin/markdown_tester" -> "intsig-textin/parsex-sdk"
"intsig-textin/markdown_tester" -> "lutongyv/Textin_Tester"
"intsig-textin/markdown_tester" -> "intsig-textin/parsex-frontend"
"intsig-textin/markdown_tester" -> "ucaslcl/Fox"
"EleutherAI/delphi" -> "EvolvingLMMs-Lab/multimodal-sae" ["e"=1]
"Alpha-Innovator/DocGenome" -> "Alpha-Innovator/AdaptiveDiffusion"
"Alpha-Innovator/DocGenome" -> "Alpha-Innovator/SurveyForge"
"Alpha-Innovator/DocGenome" -> "jiaweizzhao/InRank" ["e"=1]
"Alpha-Innovator/DocGenome" -> "gulucaptain/DynamiCtrl" ["e"=1]
"Alpha-Innovator/DocGenome" -> "Alpha-Innovator/Chimera"
"Run542968/Awesome-3D-Human-Motion-Generation" -> "iSEE-Laboratory/EgoExo-Fitness"
"Run542968/Awesome-3D-Human-Motion-Generation" -> "Lyman-Smoker/Awesome-AQA"
"Gumpest/SparseVLMs" -> "Theia-4869/FasterVLM"
"Gumpest/SparseVLMs" -> "LaVi-Lab/AIM"
"Gumpest/SparseVLMs" -> "liuting20/MustDrop" ["e"=1]
"Gumpest/SparseVLMs" -> "Cooperx521/PyramidDrop"
"Gumpest/SparseVLMs" -> "foundation-multimodal-models/ConBench" ["e"=1]
"Gumpest/SparseVLMs" -> "pkunlp-icler/FastV"
"Gumpest/SparseVLMs" -> "xuyang-liu16/GlobalCom2" ["e"=1]
"Gumpest/SparseVLMs" -> "42Shawn/LLaVA-PruMerge"
"Cooperx521/PyramidDrop" -> "Liuziyu77/MIA-DPO"
"Cooperx521/PyramidDrop" -> "Gumpest/SparseVLMs"
"Cooperx521/PyramidDrop" -> "hasanar1f/HiRED"
"Cooperx521/PyramidDrop" -> "shikiw/Modality-Integration-Rate"
"Cooperx521/PyramidDrop" -> "ywh187/FitPrune"
"Cooperx521/PyramidDrop" -> "Wiselnn570/VideoRoPE"
"Cooperx521/PyramidDrop" -> "SUSTechBruce/LOOK-M"
"longvideobench/LongVideoBench" -> "Q-Future/Co-Instruct" ["e"=1]
"longvideobench/LongVideoBench" -> "JUNJIE99/MLVU"
"MME-Benchmarks/MME-RealWorld" -> "yfzhang114/SliME"
"MME-Benchmarks/MME-RealWorld" -> "Kwai-YuanQi/MM-RLHF"
"sudo-Boris/mr-Blip" -> "gyxxyg/VTG-LLM" ["e"=1]
"mbzuai-oryx/VideoGPT-plus" -> "mbzuai-oryx/Video-ChatGPT"
"mbzuai-oryx/VideoGPT-plus" -> "mbzuai-oryx/Video-LLaVA"
"mbzuai-oryx/VideoGPT-plus" -> "bytedance/tarsier"
"mbzuai-oryx/VideoGPT-plus" -> "Ziyang412/VideoTree"
"mbzuai-oryx/VideoGPT-plus" -> "Amshaker/Mobile-VideoGPT" ["e"=1]
"mbzuai-oryx/VideoGPT-plus" -> "ziplab/LongVLM"
"mbzuai-oryx/VideoGPT-plus" -> "magic-research/PLLaVA"
"mbzuai-oryx/VideoGPT-plus" -> "DAMO-NLP-SG/VideoLLaMA2"
"mbzuai-oryx/VideoGPT-plus" -> "RenShuhuai-Andy/TimeChat"
"mbzuai-oryx/VideoGPT-plus" -> "boheumd/MA-LMM"
"mbzuai-oryx/VideoGPT-plus" -> "mbzuai-oryx/LlamaV-o1"
"mbzuai-oryx/VideoGPT-plus" -> "DCDmllm/Momentor"
"mbzuai-oryx/VideoGPT-plus" -> "Vision-CAIR/LongVU"
"mbzuai-oryx/VideoGPT-plus" -> "mbzuai-oryx/ClimateGPT" ["e"=1]
"JUNJIE99/MLVU" -> "VectorSpaceLab/Video-XL"
"JUNJIE99/MLVU" -> "longvideobench/LongVideoBench"
"JUNJIE99/MLVU" -> "EvolvingLMMs-Lab/LongVA"
"JUNJIE99/MLVU" -> "MME-Benchmarks/Video-MME"
"JUNJIE99/MLVU" -> "joez17/VideoNIAH"
"JUNJIE99/MLVU" -> "THUDM/LVBench"
"JUNJIE99/MLVU" -> "SCZwangxiao/video-FlexReduc"
"JUNJIE99/MLVU" -> "egoschema/EgoSchema"
"JUNJIE99/MLVU" -> "OpenGVLab/VideoChat-Flash"
"TimeMarker-LLM/TimeMarker" -> "WHB139426/Grounded-Video-LLM"
"TimeMarker-LLM/TimeMarker" -> "DCDmllm/Momentor"
"apple/ml-slowfast-llava" -> "imagegridworth/IG-VLM"
"apple/ml-slowfast-llava" -> "contrastive/FreeVideoLLM"
"apple/ml-slowfast-llava" -> "huofushuo/C2KD" ["e"=1]
"HowieHwong/UniGen" -> "Flossiee/HonestyLLM"
"InfiMM/Awesome-Multimodal-LLM-for-Math-STEM" -> "pengshuai-rin/MultiMath"
"Jixuan-Fan/Momentum-GS" -> "shiyi-zh0408/NAE_CVPR2024" ["e"=1]
"Jixuan-Fan/Momentum-GS" -> "AndyTang15/FLAG3Dv2" ["e"=1]
"shikiw/Modality-Integration-Rate" -> "Cooperx521/PyramidDrop"
"shikiw/Modality-Integration-Rate" -> "YUCHEN005/UNA-GAN"
"shikiw/Modality-Integration-Rate" -> "mengtong0110/InferDPT"
"shikiw/Modality-Integration-Rate" -> "YUCHEN005/RATS-Channel-A-Speech-Data"
"shikiw/Modality-Integration-Rate" -> "YUCHEN005/UniVPM"
"shikiw/Modality-Integration-Rate" -> "YUCHEN005/Gradient-Remedy"
"shikiw/Modality-Integration-Rate" -> "YUCHEN005/MIR-GAN"
"xjtupanda/Sparrow" -> "VITA-MLLM/Sparrow"
"xjtupanda/Sparrow" -> "zhourax/VEGA"
"yihedeng9/STIC" -> "YiyangZhou/CSR"
"yihedeng9/STIC" -> "YiyangZhou/POVID"
"JoeLeelyf/OVO-Bench" -> "hmxiong/StreamChat"
"KD-TAO/DyCoke" -> "KD-TAO/VidKV"
"KD-TAO/DyCoke" -> "Visual-AI/PruneVid"
"Liuziyu77/MMDU" -> "songweii/DualToken"
"Liuziyu77/MMDU" -> "Liuziyu77/MIA-DPO"
"Liuziyu77/MMDU" -> "Bujiazi/HiFlow"
"Liuziyu77/MIA-DPO" -> "beichenzbc/BoostStep"
"Liuziyu77/MIA-DPO" -> "Cooperx521/PyramidDrop"
"Liuziyu77/MIA-DPO" -> "SYuan03/MM-IFEngine"
"EvolvingLMMs-Lab/multimodal-sae" -> "nickjiang2378/vl-interp"
"OpenGVLab/MMIU" -> "OpenGVLab/Multitask-Model-Selector"
"OpenGVLab/MMIU" -> "lyingCS/Controllable-Multi-Objective-Reranking"
"OpenGVLab/MMIU" -> "lyingCS/UOEP"
"OpenGVLab/MMIU" -> "E-qin/GEAR"
"ucaslcl/Fox" -> "LingyvKong/OneChart"
"ucaslcl/Fox" -> "nttmdlab-nlp/InstructDoc"
"ucaslcl/Fox" -> "Ucas-HaoranWei/Vary-tiny-600k"
"ucaslcl/Fox" -> "intsig-textin/markdown_tester"
"SuleBai/SC-CLIP" -> "AndyTang15/FLAG3Dv2"
"SuleBai/SC-CLIP" -> "AndyTang15/FLAG3D"
"SuleBai/SC-CLIP" -> "zhang9302002/Flash-VStream"
"SuleBai/SC-CLIP" -> "shiyi-zh0408/NAE_CVPR2024"
"SuleBai/SC-CLIP" -> "Yxxxb/LAVT-RS"
"minghangz/TFVTG" -> "lntzm/MESM"
"tdsone/extract-line-chart-data" -> "pengyu965/ChartDete"
"DreamMr/HR-Bench" -> "DreamMr/RAP"
"berkeley-hipie/segllm" -> "Shengcao-Cao/groundLMM"
"berkeley-hipie/segllm" -> "wusize/F-LMM"
"berkeley-hipie/segllm" -> "mc-lan/Text4Seg"
"NastyMarcus/A-Survey-of-Efficient-Diffusion-Models" -> "AIoT-MLSys-Lab/Famba-V"
"alipay/POA" -> "kang-wu/SkySensePlusPlus"
"intsig-textin/parsex-sdk" -> "intsig-textin/chatdoc"
"ichbill/DQAS" -> "ichbill/LTDD"
"iSEE-Laboratory/EgoExo-Fitness" -> "Run542968/Awesome-3D-Human-Motion-Generation"
"iSEE-Laboratory/EgoExo-Fitness" -> "Lyman-Smoker/Awesome-AQA"
"iSEE-Laboratory/EgoExo-Fitness" -> "iSEE-Laboratory/Continual-AQA"
"ichbill/LTDD" -> "ichbill/DQAS"
"1100111GTH/XG-RAG" -> "modelscope/mcp-central"
"HankYe/AdaptiveDiffusion" -> "HankYe/Once-for-Both"
"HankYe/AdaptiveDiffusion" -> "Alpha-Innovator/TrustGeoGen"
"MiniMax-AI/MiniMax-01" -> "MoonshotAI/Kimi-VL" ["e"=1]
"Jiayi-Pan/TinyZero" -> "Deep-Agent/R1-V" ["e"=1]
"hkust-nlp/simpleRL-reason" -> "Deep-Agent/R1-V" ["e"=1]
"zzli2022/Awesome-System2-Reasoning-LLM" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"zzli2022/Awesome-System2-Reasoning-LLM" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"zzli2022/Awesome-System2-Reasoning-LLM" -> "TideDra/lmm-r1" ["e"=1]
"om-ai-lab/VLM-R1" -> "Deep-Agent/R1-V"
"om-ai-lab/VLM-R1" -> "Liuziyu77/Visual-RFT"
"om-ai-lab/VLM-R1" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"om-ai-lab/VLM-R1" -> "hiyouga/EasyR1" ["e"=1]
"om-ai-lab/VLM-R1" -> "QwenLM/Qwen2.5-VL"
"om-ai-lab/VLM-R1" -> "OpenGVLab/InternVL"
"om-ai-lab/VLM-R1" -> "LLaVA-VL/LLaVA-NeXT"
"om-ai-lab/VLM-R1" -> "open-compass/VLMEvalKit"
"om-ai-lab/VLM-R1" -> "volcengine/verl" ["e"=1]
"om-ai-lab/VLM-R1" -> "TideDra/lmm-r1"
"om-ai-lab/VLM-R1" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"om-ai-lab/VLM-R1" -> "modelscope/ms-swift" ["e"=1]
"om-ai-lab/VLM-R1" -> "OpenRLHF/OpenRLHF" ["e"=1]
"om-ai-lab/VLM-R1" -> "BradyFU/Awesome-Multimodal-Large-Language-Models"
"om-ai-lab/VLM-R1" -> "hkust-nlp/simpleRL-reason" ["e"=1]
"simplescaling/s1" -> "Deep-Agent/R1-V" ["e"=1]
"MoonshotAI/Kimi-k1.5" -> "Deep-Agent/R1-V" ["e"=1]
"tulerfeng/Video-R1" -> "Wang-Xiaodong1899/Open-R1-Video"
"tulerfeng/Video-R1" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"tulerfeng/Video-R1" -> "www-Ye/TimeZero"
"tulerfeng/Video-R1" -> "OpenGVLab/VideoChat-R1"
"tulerfeng/Video-R1" -> "turningpoint-ai/VisualThinker-R1-Zero"
"tulerfeng/Video-R1" -> "Liuziyu77/Visual-RFT"
"tulerfeng/Video-R1" -> "dvlab-research/Seg-Zero"
"tulerfeng/Video-R1" -> "TideDra/lmm-r1"
"tulerfeng/Video-R1" -> "TencentARC/SEED-Bench-R1" ["e"=1]
"tulerfeng/Video-R1" -> "yaotingwangofficial/Awesome-MCoT"
"tulerfeng/Video-R1" -> "Fancy-MLLM/R1-Onevision"
"tulerfeng/Video-R1" -> "ModalMinds/MM-EUREKA"
"tulerfeng/Video-R1" -> "appletea233/Temporal-R1"
"tulerfeng/Video-R1" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"tulerfeng/Video-R1" -> "Osilly/Vision-R1"
"HKUDS/VideoRAG" -> "Leon1207/Video-RAG-master" ["e"=1]
"HKUDS/VideoRAG" -> "yeliudev/VideoMind" ["e"=1]
"FanqingM/R1-Multimodal-Journey" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"FanqingM/R1-Multimodal-Journey" -> "ModalMinds/MM-EUREKA"
"FanqingM/R1-Multimodal-Journey" -> "TideDra/lmm-r1"
"FanqingM/R1-Multimodal-Journey" -> "turningpoint-ai/VisualThinker-R1-Zero"
"FanqingM/R1-Multimodal-Journey" -> "Wang-Xiaodong1899/Open-R1-Video"
"FanqingM/R1-Multimodal-Journey" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"FanqingM/R1-Multimodal-Journey" -> "Osilly/Vision-R1"
"FanqingM/R1-Multimodal-Journey" -> "Fancy-MLLM/R1-Onevision"
"FanqingM/R1-Multimodal-Journey" -> "RLHF-V/RLHF-V"
"FanqingM/R1-Multimodal-Journey" -> "OpenRLHF/OpenRLHF-M"
"FanqingM/R1-Multimodal-Journey" -> "dongyh20/Insight-V"
"FanqingM/R1-Multimodal-Journey" -> "modelscope/awesome-deep-reasoning"
"FanqingM/R1-Multimodal-Journey" -> "hiyouga/EasyR1" ["e"=1]
"FanqingM/R1-Multimodal-Journey" -> "LengSicong/MMR1"
"FanqingM/R1-Multimodal-Journey" -> "Deep-Agent/R1-V"
"deepseek-ai/DeepSeek-VL2" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"deepseek-ai/DeepSeek-VL2" -> "OpenGVLab/InternVL" ["e"=1]
"deepseek-ai/DeepSeek-VL2" -> "om-ai-lab/VLM-R1" ["e"=1]
"deepseek-ai/DeepSeek-VL2" -> "LLaVA-VL/LLaVA-NeXT" ["e"=1]
"ArcInstitute/evo2" -> "om-ai-lab/VLM-R1" ["e"=1]
"hiyouga/EasyR1" -> "Deep-Agent/R1-V" ["e"=1]
"hiyouga/EasyR1" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"hiyouga/EasyR1" -> "Liuziyu77/Visual-RFT" ["e"=1]
"hiyouga/EasyR1" -> "om-ai-lab/VLM-R1" ["e"=1]
"hiyouga/EasyR1" -> "TideDra/lmm-r1" ["e"=1]
"hiyouga/EasyR1" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"hiyouga/EasyR1" -> "Osilly/Vision-R1" ["e"=1]
"hiyouga/EasyR1" -> "ModalMinds/MM-EUREKA" ["e"=1]
"hiyouga/EasyR1" -> "open-compass/VLMEvalKit" ["e"=1]
"computerhistory/AlexNet-Source-Code" -> "Liuziyu77/Visual-RFT" ["e"=1]
"saccharomycetes/mllms_know" -> "mrwu-mac/ControlMLLM"
"saccharomycetes/mllms_know" -> "zjunlp/Deco"
"saccharomycetes/mllms_know" -> "yu-rp/VisualPerceptionToken" ["e"=1]
"saccharomycetes/mllms_know" -> "DreamMr/HR-Bench"
"saccharomycetes/mllms_know" -> "zjysteven/VLM-Visualizer"
"Wang-Xiaodong1899/CVPR25-MLLM-Paper-List" -> "Wang-Xiaodong1899/Open-R1-Video"
"QwenLM/Qwen2.5-Omni" -> "QwenLM/Qwen2.5-VL" ["e"=1]
"QwenLM/Qwen2.5-Omni" -> "HumanMLLM/R1-Omni" ["e"=1]
"QwenLM/Qwen2.5-Omni" -> "OpenGVLab/InternVL" ["e"=1]
"ictnlp/LLaMA-Omni2" -> "VITA-MLLM/VITA-Audio" ["e"=1]
"VITA-MLLM/VITA-Audio" -> "VITA-MLLM/Sparrow"
"VITA-MLLM/VITA-Audio" -> "MAC-AutoML/QuoTA"
"VITA-MLLM/VITA-Audio" -> "VITA-MLLM/Long-VITA"
"VITA-MLLM/VITA-Audio" -> "zhourax/VEGA"
"VITA-MLLM/VITA-Audio" -> "yfzhang114/r1_reward"
"Kwai-YuanQi/MM-RLHF" -> "VITA-MLLM/Long-VITA"
"Kwai-YuanQi/MM-RLHF" -> "VITA-MLLM/Sparrow"
"Kwai-YuanQi/MM-RLHF" -> "yfzhang114/r1_reward"
"Kwai-YuanQi/MM-RLHF" -> "MME-Benchmarks/MME-Unify"
"Kwai-YuanQi/MM-RLHF" -> "MME-Benchmarks/MME-RealWorld"
"Kwai-YuanQi/MM-RLHF" -> "MAC-AutoML/QuoTA"
"Kwai-YuanQi/MM-RLHF" -> "VITA-MLLM/VITA-Audio"
"Kwai-YuanQi/MM-RLHF" -> "Leon1207/Video-RAG-master"
"Kwai-YuanQi/MM-RLHF" -> "yfzhang114/SliME"
"Deep-Agent/R1-V" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"Deep-Agent/R1-V" -> "om-ai-lab/VLM-R1"
"Deep-Agent/R1-V" -> "hiyouga/EasyR1" ["e"=1]
"Deep-Agent/R1-V" -> "Liuziyu77/Visual-RFT"
"Deep-Agent/R1-V" -> "TideDra/lmm-r1"
"Deep-Agent/R1-V" -> "open-compass/VLMEvalKit"
"Deep-Agent/R1-V" -> "volcengine/verl" ["e"=1]
"Deep-Agent/R1-V" -> "hkust-nlp/simpleRL-reason" ["e"=1]
"Deep-Agent/R1-V" -> "LLaVA-VL/LLaVA-NeXT"
"Deep-Agent/R1-V" -> "ModalMinds/MM-EUREKA"
"Deep-Agent/R1-V" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"Deep-Agent/R1-V" -> "OpenRLHF/OpenRLHF" ["e"=1]
"Deep-Agent/R1-V" -> "Open-Reasoner-Zero/Open-Reasoner-Zero" ["e"=1]
"Deep-Agent/R1-V" -> "turningpoint-ai/VisualThinker-R1-Zero"
"Deep-Agent/R1-V" -> "QwenLM/Qwen2.5-VL"
"dhcode-cpp/X-R1" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"dhcode-cpp/X-R1" -> "turningpoint-ai/VisualThinker-R1-Zero" ["e"=1]
"dhcode-cpp/X-R1" -> "Fancy-MLLM/R1-Onevision" ["e"=1]
"ModalMinds/MM-EUREKA" -> "FanqingM/R1-Multimodal-Journey"
"ModalMinds/MM-EUREKA" -> "TideDra/lmm-r1"
"ModalMinds/MM-EUREKA" -> "Osilly/Vision-R1"
"ModalMinds/MM-EUREKA" -> "turningpoint-ai/VisualThinker-R1-Zero"
"ModalMinds/MM-EUREKA" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"ModalMinds/MM-EUREKA" -> "Fancy-MLLM/R1-Onevision"
"ModalMinds/MM-EUREKA" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"ModalMinds/MM-EUREKA" -> "Liuziyu77/Visual-RFT"
"ModalMinds/MM-EUREKA" -> "Wang-Xiaodong1899/Open-R1-Video"
"ModalMinds/MM-EUREKA" -> "Deep-Agent/R1-V"
"ModalMinds/MM-EUREKA" -> "hiyouga/EasyR1" ["e"=1]
"ModalMinds/MM-EUREKA" -> "yaotingwangofficial/Awesome-MCoT"
"ModalMinds/MM-EUREKA" -> "open-compass/VLMEvalKit"
"ModalMinds/MM-EUREKA" -> "tulerfeng/Video-R1"
"ModalMinds/MM-EUREKA" -> "OpenRLHF/OpenRLHF-M"
"CodeGoat24/UnifiedReward" -> "Wiselnn570/VideoRoPE" ["e"=1]
"CodeGoat24/UnifiedReward" -> "SYuan03/MM-IFEngine" ["e"=1]
"CodeGoat24/UnifiedReward" -> "Liuziyu77/MMDU" ["e"=1]
"NVlabs/describe-anything" -> "MoonshotAI/Kimi-VL" ["e"=1]
"NVlabs/describe-anything" -> "DAMO-NLP-SG/VideoLLaMA3" ["e"=1]
"ali-vilab/UniAnimate-DiT" -> "zhuang2002/Cobra" ["e"=1]
"facebookresearch/perception_models" -> "NVlabs/EAGLE" ["e"=1]
"Tencent/HunyuanCustom" -> "shiyi-zh0408/FlexiAct" ["e"=1]
"MoonshotAI/MoBA" -> "MoonshotAI/Kimi-VL" ["e"=1]
"facebookresearch/large_concept_model" -> "PKU-YuanGroup/LLaVA-CoT" ["e"=1]
"zhuang2002/Cobra" -> "shiyi-zh0408/FlexiAct"
"RUCAIBox/Slow_Thinking_with_LLMs" -> "RUCAIBox/Virgo" ["e"=1]
"MoonshotAI/Moonlight" -> "MoonshotAI/Kimi-VL" ["e"=1]
"openai/openai-realtime-agents" -> "OpenBMB/MiniCPM-o" ["e"=1]
"Ola-Omni/Ola" -> "Oryx-mllm/Oryx"
"Ola-Omni/Ola" -> "MoonshotAI/Kimi-VL"
"Ola-Omni/Ola" -> "dongyh20/Insight-V"
"Ola-Omni/Ola" -> "dongyh20/Chain-of-Spot"
"mbzuai-oryx/Awesome-LLM-Post-training" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"mbzuai-oryx/Awesome-LLM-Post-training" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"XiaomiMiMo/MiMo" -> "MoonshotAI/Kimi-VL" ["e"=1]
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "bruno686/Awesome-RL-based-LLM-Reasoning" ["e"=1]
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "TideDra/lmm-r1"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "ModalMinds/MM-EUREKA"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "Osilly/Vision-R1"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "yaotingwangofficial/Awesome-MCoT"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "FanqingM/R1-Multimodal-Journey"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "Liuziyu77/Visual-RFT"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "hiyouga/EasyR1" ["e"=1]
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "turningpoint-ai/VisualThinker-R1-Zero"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "Fancy-MLLM/R1-Onevision"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "open-compass/VLMEvalKit"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "Wang-Xiaodong1899/Open-R1-Video"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "Deep-Agent/R1-V"
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" -> "dvlab-research/Seg-Zero"
"SCZwangxiao/video-FlexReduc" -> "gls0425/LinVT"
"modelscope/MCPBench" -> "modelscope/mcp-central"
"modelscope/MCPBench" -> "modelscope/DiffSynth-Engine"
"Liuziyu77/Visual-RFT" -> "om-ai-lab/VLM-R1"
"Liuziyu77/Visual-RFT" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"Liuziyu77/Visual-RFT" -> "Deep-Agent/R1-V"
"Liuziyu77/Visual-RFT" -> "hiyouga/EasyR1" ["e"=1]
"Liuziyu77/Visual-RFT" -> "Osilly/Vision-R1"
"Liuziyu77/Visual-RFT" -> "TideDra/lmm-r1"
"Liuziyu77/Visual-RFT" -> "ModalMinds/MM-EUREKA"
"Liuziyu77/Visual-RFT" -> "turningpoint-ai/VisualThinker-R1-Zero"
"Liuziyu77/Visual-RFT" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"Liuziyu77/Visual-RFT" -> "Fancy-MLLM/R1-Onevision"
"Liuziyu77/Visual-RFT" -> "tulerfeng/Video-R1"
"Liuziyu77/Visual-RFT" -> "open-compass/VLMEvalKit"
"Liuziyu77/Visual-RFT" -> "dvlab-research/Seg-Zero"
"Liuziyu77/Visual-RFT" -> "LLaVA-VL/LLaVA-NeXT"
"Liuziyu77/Visual-RFT" -> "Wang-Xiaodong1899/Open-R1-Video"
"TideDra/lmm-r1" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"TideDra/lmm-r1" -> "FanqingM/R1-Multimodal-Journey"
"TideDra/lmm-r1" -> "ModalMinds/MM-EUREKA"
"TideDra/lmm-r1" -> "Fancy-MLLM/R1-Onevision"
"TideDra/lmm-r1" -> "turningpoint-ai/VisualThinker-R1-Zero"
"TideDra/lmm-r1" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"TideDra/lmm-r1" -> "OpenRLHF/OpenRLHF-M"
"TideDra/lmm-r1" -> "Osilly/Vision-R1"
"TideDra/lmm-r1" -> "Wang-Xiaodong1899/Open-R1-Video"
"TideDra/lmm-r1" -> "Deep-Agent/R1-V"
"TideDra/lmm-r1" -> "hiyouga/EasyR1" ["e"=1]
"TideDra/lmm-r1" -> "Liuziyu77/Visual-RFT"
"TideDra/lmm-r1" -> "yaotingwangofficial/Awesome-MCoT"
"TideDra/lmm-r1" -> "Open-Reasoner-Zero/Open-Reasoner-Zero" ["e"=1]
"TideDra/lmm-r1" -> "tulerfeng/Video-R1"
"vision-x-nyu/thinking-in-space" -> "cambrian-mllm/cambrian" ["e"=1]
"vision-x-nyu/thinking-in-space" -> "deepcs233/Visual-CoT" ["e"=1]
"vision-x-nyu/thinking-in-space" -> "tsb0601/MMVP" ["e"=1]
"vision-x-nyu/thinking-in-space" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"bruno686/Awesome-RL-based-LLM-Reasoning" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"bruno686/Awesome-RL-based-LLM-Reasoning" -> "ModalMinds/MM-EUREKA" ["e"=1]
"bruno686/Awesome-RL-based-LLM-Reasoning" -> "TideDra/lmm-r1" ["e"=1]
"bruno686/Awesome-RL-based-LLM-Reasoning" -> "turningpoint-ai/VisualThinker-R1-Zero" ["e"=1]
"bruno686/Awesome-RL-based-LLM-Reasoning" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"turningpoint-ai/VisualThinker-R1-Zero" -> "ModalMinds/MM-EUREKA"
"turningpoint-ai/VisualThinker-R1-Zero" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"turningpoint-ai/VisualThinker-R1-Zero" -> "TideDra/lmm-r1"
"turningpoint-ai/VisualThinker-R1-Zero" -> "Osilly/Vision-R1"
"turningpoint-ai/VisualThinker-R1-Zero" -> "Fancy-MLLM/R1-Onevision"
"turningpoint-ai/VisualThinker-R1-Zero" -> "FanqingM/R1-Multimodal-Journey"
"turningpoint-ai/VisualThinker-R1-Zero" -> "Wang-Xiaodong1899/Open-R1-Video"
"turningpoint-ai/VisualThinker-R1-Zero" -> "Liuziyu77/Visual-RFT"
"turningpoint-ai/VisualThinker-R1-Zero" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"turningpoint-ai/VisualThinker-R1-Zero" -> "tulerfeng/Video-R1"
"turningpoint-ai/VisualThinker-R1-Zero" -> "Deep-Agent/R1-V"
"turningpoint-ai/VisualThinker-R1-Zero" -> "hiyouga/EasyR1" ["e"=1]
"turningpoint-ai/VisualThinker-R1-Zero" -> "MoonshotAI/Kimi-VL"
"turningpoint-ai/VisualThinker-R1-Zero" -> "deepcs233/Visual-CoT"
"turningpoint-ai/VisualThinker-R1-Zero" -> "LengSicong/MMR1"
"HumanMLLM/R1-Omni" -> "HumanMLLM/HumanOmni"
"HumanMLLM/R1-Omni" -> "Liuziyu77/Visual-RFT"
"HumanMLLM/R1-Omni" -> "Fancy-MLLM/R1-Onevision"
"HumanMLLM/R1-Omni" -> "Osilly/Vision-R1"
"HumanMLLM/R1-Omni" -> "QwenLM/Qwen2.5-Omni" ["e"=1]
"HumanMLLM/R1-Omni" -> "Deep-Agent/R1-V"
"HumanMLLM/R1-Omni" -> "tulerfeng/Video-R1"
"HumanMLLM/R1-Omni" -> "turningpoint-ai/VisualThinker-R1-Zero"
"HumanMLLM/R1-Omni" -> "ZebangCheng/Emotion-LLaMA" ["e"=1]
"HumanMLLM/R1-Omni" -> "zeroQiaoba/AffectGPT" ["e"=1]
"HumanMLLM/R1-Omni" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"HumanMLLM/R1-Omni" -> "Wang-Xiaodong1899/Open-R1-Video"
"HumanMLLM/R1-Omni" -> "ModalMinds/MM-EUREKA"
"HumanMLLM/R1-Omni" -> "TideDra/lmm-r1"
"HumanMLLM/R1-Omni" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"Osilly/Vision-R1" -> "Fancy-MLLM/R1-Onevision"
"Osilly/Vision-R1" -> "ModalMinds/MM-EUREKA"
"Osilly/Vision-R1" -> "turningpoint-ai/VisualThinker-R1-Zero"
"Osilly/Vision-R1" -> "TideDra/lmm-r1"
"Osilly/Vision-R1" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"Osilly/Vision-R1" -> "Liuziyu77/Visual-RFT"
"Osilly/Vision-R1" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"Osilly/Vision-R1" -> "LengSicong/MMR1"
"Osilly/Vision-R1" -> "FanqingM/R1-Multimodal-Journey"
"Osilly/Vision-R1" -> "hiyouga/EasyR1" ["e"=1]
"Osilly/Vision-R1" -> "Wang-Xiaodong1899/Open-R1-Video"
"Osilly/Vision-R1" -> "tulerfeng/Video-R1"
"Osilly/Vision-R1" -> "UCSC-VLAA/VLAA-Thinking"
"Osilly/Vision-R1" -> "Deep-Agent/R1-V"
"Osilly/Vision-R1" -> "yaotingwangofficial/Awesome-MCoT"
"dvlab-research/Seg-Zero" -> "tulerfeng/Video-R1"
"dvlab-research/Seg-Zero" -> "Liuziyu77/Visual-RFT"
"dvlab-research/Seg-Zero" -> "mc-lan/Awesome-MLLM-Segmentation"
"dvlab-research/Seg-Zero" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"dvlab-research/Seg-Zero" -> "cilinyan/VISA" ["e"=1]
"dvlab-research/Seg-Zero" -> "Fancy-MLLM/R1-Onevision"
"dvlab-research/Seg-Zero" -> "magic-research/Sa2VA" ["e"=1]
"dvlab-research/Seg-Zero" -> "MaverickRen/PixelLM"
"dvlab-research/Seg-Zero" -> "FanqingM/R1-Multimodal-Journey"
"dvlab-research/Seg-Zero" -> "Osilly/Vision-R1"
"dvlab-research/Seg-Zero" -> "showlab/VideoLISA" ["e"=1]
"dvlab-research/Seg-Zero" -> "TideDra/lmm-r1"
"dvlab-research/Seg-Zero" -> "zamling/PSALM"
"dvlab-research/Seg-Zero" -> "hiyouga/EasyR1" ["e"=1]
"dvlab-research/Seg-Zero" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"BytedTsinghua-SIA/DAPO" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"BytedTsinghua-SIA/DAPO" -> "ModalMinds/MM-EUREKA" ["e"=1]
"SYuan03/MM-IFEngine" -> "PhoenixZ810/OmniAlign-V" ["e"=1]
"SYuan03/MM-IFEngine" -> "Sunleader1997/transflow" ["e"=1]
"SYuan03/MM-IFEngine" -> "liyown/nextjs_stream_demo" ["e"=1]
"yaotingwangofficial/Awesome-MCoT" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"yaotingwangofficial/Awesome-MCoT" -> "Fancy-MLLM/R1-Onevision"
"yaotingwangofficial/Awesome-MCoT" -> "TideDra/lmm-r1"
"yaotingwangofficial/Awesome-MCoT" -> "tulerfeng/Video-R1"
"yaotingwangofficial/Awesome-MCoT" -> "ModalMinds/MM-EUREKA"
"yaotingwangofficial/Awesome-MCoT" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"yaotingwangofficial/Awesome-MCoT" -> "Osilly/Vision-R1"
"yaotingwangofficial/Awesome-MCoT" -> "Liuziyu77/Visual-RFT"
"yaotingwangofficial/Awesome-MCoT" -> "LightChen233/Awesome-Long-Chain-of-Thought-Reasoning" ["e"=1]
"yaotingwangofficial/Awesome-MCoT" -> "hiyouga/EasyR1" ["e"=1]
"yaotingwangofficial/Awesome-MCoT" -> "FanqingM/R1-Multimodal-Journey"
"yaotingwangofficial/Awesome-MCoT" -> "dvlab-research/Seg-Zero"
"yaotingwangofficial/Awesome-MCoT" -> "showlab/Awesome-Unified-Multimodal-Models" ["e"=1]
"yaotingwangofficial/Awesome-MCoT" -> "turningpoint-ai/VisualThinker-R1-Zero"
"yaotingwangofficial/Awesome-MCoT" -> "daixiangzi/Awesome-Token-Compress"
"DAMO-NLP-SG/VideoLLaMA3" -> "DAMO-NLP-SG/VideoLLaMA2"
"DAMO-NLP-SG/VideoLLaMA3" -> "OpenGVLab/VideoChat-Flash"
"DAMO-NLP-SG/VideoLLaMA3" -> "tulerfeng/Video-R1"
"DAMO-NLP-SG/VideoLLaMA3" -> "bytedance/tarsier"
"DAMO-NLP-SG/VideoLLaMA3" -> "MME-Benchmarks/Video-MME"
"DAMO-NLP-SG/VideoLLaMA3" -> "DAMO-NLP-SG/VideoRefer" ["e"=1]
"DAMO-NLP-SG/VideoLLaMA3" -> "VectorSpaceLab/Video-XL"
"DAMO-NLP-SG/VideoLLaMA3" -> "OpenGVLab/InternVideo"
"DAMO-NLP-SG/VideoLLaMA3" -> "magic-research/Sa2VA" ["e"=1]
"DAMO-NLP-SG/VideoLLaMA3" -> "yunlong10/Awesome-LLMs-for-Video-Understanding"
"DAMO-NLP-SG/VideoLLaMA3" -> "EvolvingLMMs-Lab/LongVA"
"DAMO-NLP-SG/VideoLLaMA3" -> "magic-research/PLLaVA"
"DAMO-NLP-SG/VideoLLaMA3" -> "LLaVA-VL/LLaVA-NeXT"
"DAMO-NLP-SG/VideoLLaMA3" -> "NVlabs/VILA"
"DAMO-NLP-SG/VideoLLaMA3" -> "VITA-MLLM/VITA" ["e"=1]
"TencentARC/ColorFlow" -> "zhuang2002/Cobra" ["e"=1]
"TencentARC/ColorFlow" -> "shiyi-zh0408/FlexiAct" ["e"=1]
"EvolvingLMMs-Lab/open-r1-multimodal" -> "Deep-Agent/R1-V"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "FanqingM/R1-Multimodal-Journey"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "TideDra/lmm-r1"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "Liuziyu77/Visual-RFT"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "ModalMinds/MM-EUREKA"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "hiyouga/EasyR1" ["e"=1]
"EvolvingLMMs-Lab/open-r1-multimodal" -> "turningpoint-ai/VisualThinker-R1-Zero"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "om-ai-lab/VLM-R1"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "Fancy-MLLM/R1-Onevision"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "Wang-Xiaodong1899/Open-R1-Video"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "Osilly/Vision-R1"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "tulerfeng/Video-R1"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "open-compass/VLMEvalKit"
"EvolvingLMMs-Lab/open-r1-multimodal" -> "LLaVA-VL/LLaVA-NeXT"
"Alpha-Innovator/SurveyForge" -> "Alpha-Innovator/AdaptiveDiffusion"
"Alpha-Innovator/SurveyForge" -> "Alpha-Innovator/DocGenome"
"Alpha-Innovator/SurveyForge" -> "Alpha-Innovator/Chimera"
"beichenzbc/BoostStep" -> "Bujiazi/ByTheWay"
"bcmi/Light-A-Video" -> "Bujiazi/ByTheWay" ["e"=1]
"bcmi/Light-A-Video" -> "Wiselnn570/VideoRoPE" ["e"=1]
"bcmi/Light-A-Video" -> "Bujiazi/HiFlow" ["e"=1]
"Wiselnn570/VideoRoPE" -> "beichenzbc/BoostStep"
"Wiselnn570/VideoRoPE" -> "SYuan03/MM-IFEngine"
"Wiselnn570/VideoRoPE" -> "Cooperx521/PyramidDrop"
"Wiselnn570/VideoRoPE" -> "Bujiazi/ByTheWay"
"Wiselnn570/VideoRoPE" -> "Mark12Ding/Dispider"
"Wiselnn570/VideoRoPE" -> "Bujiazi/HiFlow"
"Wiselnn570/VideoRoPE" -> "shikiw/Modality-Integration-Rate"
"Wiselnn570/VideoRoPE" -> "Liuziyu77/MIA-DPO"
"VITA-MLLM/Long-VITA" -> "Kwai-YuanQi/MM-RLHF"
"VITA-MLLM/Long-VITA" -> "VITA-MLLM/Sparrow"
"VITA-MLLM/Long-VITA" -> "VITA-MLLM/VITA-Audio"
"VITA-MLLM/Long-VITA" -> "MAC-AutoML/QuoTA"
"baichuan-inc/Baichuan-Omni-1.5" -> "fengzi258/Ocean-R1" ["e"=1]
"TsinghuaC3I/Awesome-RL-Reasoning-Recipes" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"TsinghuaC3I/Awesome-RL-Reasoning-Recipes" -> "TideDra/lmm-r1" ["e"=1]
"TsinghuaC3I/Awesome-RL-Reasoning-Recipes" -> "Osilly/Vision-R1" ["e"=1]
"TsinghuaC3I/Awesome-RL-Reasoning-Recipes" -> "EvolvingLMMs-Lab/open-r1-multimodal" ["e"=1]
"Alpha-Innovator/OmniCaptioner" -> "Alpha-Innovator/Chimera"
"Alpha-Innovator/OmniCaptioner" -> "ch3cook-fdu/Vote2Cap-DETR"
"Alpha-Innovator/OmniCaptioner" -> "HankYe/AdaptiveDiffusion"
"Alpha-Innovator/OmniCaptioner" -> "Alpha-Innovator/AdaptiveDiffusion"
"Video-R1/Awesome-Multimodal-Reasoning" -> "MCG-NJU/CaReBench"
"LightChen233/Awesome-Long-Chain-of-Thought-Reasoning" -> "yaotingwangofficial/Awesome-MCoT" ["e"=1]
"shiyi-zh0408/FlexiAct" -> "AndyTang15/FLAG3Dv2"
"shiyi-zh0408/FlexiAct" -> "shiyi-zh0408/NAE_CVPR2024"
"shiyi-zh0408/FlexiAct" -> "zhuang2002/Cobra"
"shiyi-zh0408/FlexiAct" -> "Xilluill/KV-Edit" ["e"=1]
"shiyi-zh0408/FlexiAct" -> "ChangyuanWang17/QVLM"
"Fancy-MLLM/R1-Onevision" -> "Osilly/Vision-R1"
"Fancy-MLLM/R1-Onevision" -> "TideDra/lmm-r1"
"Fancy-MLLM/R1-Onevision" -> "ModalMinds/MM-EUREKA"
"Fancy-MLLM/R1-Onevision" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"Fancy-MLLM/R1-Onevision" -> "turningpoint-ai/VisualThinker-R1-Zero"
"Fancy-MLLM/R1-Onevision" -> "Wang-Xiaodong1899/Open-R1-Video"
"Fancy-MLLM/R1-Onevision" -> "yaotingwangofficial/Awesome-MCoT"
"Fancy-MLLM/R1-Onevision" -> "FanqingM/R1-Multimodal-Journey"
"Fancy-MLLM/R1-Onevision" -> "Liuziyu77/Visual-RFT"
"Fancy-MLLM/R1-Onevision" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"Fancy-MLLM/R1-Onevision" -> "tulerfeng/Video-R1"
"Fancy-MLLM/R1-Onevision" -> "Deep-Agent/R1-V"
"Fancy-MLLM/R1-Onevision" -> "hiyouga/EasyR1" ["e"=1]
"Fancy-MLLM/R1-Onevision" -> "dvlab-research/Seg-Zero"
"Fancy-MLLM/R1-Onevision" -> "yuyq96/R1-Vision"
"Xilluill/KV-Edit" -> "shiyi-zh0408/FlexiAct" ["e"=1]
"mbzuai-oryx/LlamaV-o1" -> "dongyh20/Insight-V"
"mbzuai-oryx/LlamaV-o1" -> "mbzuai-oryx/CVRR-Evaluation-Suite" ["e"=1]
"mbzuai-oryx/LlamaV-o1" -> "VIROBO-15/XM-GAN" ["e"=1]
"mbzuai-oryx/LlamaV-o1" -> "Wang-Xiaodong1899/Open-R1-Video"
"mbzuai-oryx/LlamaV-o1" -> "mbzuai-oryx/VideoGPT-plus"
"mbzuai-oryx/LlamaV-o1" -> "ModalMinds/MM-EUREKA"
"mbzuai-oryx/LlamaV-o1" -> "mbzuai-oryx/ClimateGPT" ["e"=1]
"mbzuai-oryx/LlamaV-o1" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"ant-research/AniDoc" -> "zhuang2002/Cobra" ["e"=1]
"mc-lan/Awesome-MLLM-Segmentation" -> "likyoo/awesome-MLLM-for-image-segmentation"
"MoonshotAI/Kimi-VL" -> "ByteDance-Seed/Seed-Thinking-v1.5" ["e"=1]
"MoonshotAI/Kimi-VL" -> "turningpoint-ai/VisualThinker-R1-Zero"
"MoonshotAI/Kimi-VL" -> "TideDra/lmm-r1"
"MoonshotAI/Kimi-VL" -> "Osilly/Vision-R1"
"MoonshotAI/Kimi-VL" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"MoonshotAI/Kimi-VL" -> "MoonshotAI/Kimina-Prover-Preview" ["e"=1]
"MoonshotAI/Kimi-VL" -> "FanqingM/R1-Multimodal-Journey"
"MoonshotAI/Kimi-VL" -> "MoonshotAI/Moonlight" ["e"=1]
"MoonshotAI/Kimi-VL" -> "Ola-Omni/Ola"
"MoonshotAI/Kimi-VL" -> "MoonshotAI/MoBA" ["e"=1]
"MoonshotAI/Kimi-VL" -> "Fancy-MLLM/R1-Onevision"
"MoonshotAI/Kimi-VL" -> "Liuziyu77/Visual-RFT"
"MoonshotAI/Kimi-VL" -> "hiyouga/EasyR1" ["e"=1]
"MoonshotAI/Kimi-VL" -> "ByteFlow-AI/TokenFlow" ["e"=1]
"MoonshotAI/Kimi-VL" -> "open-compass/VLMEvalKit"
"ByteDance-Seed/Seed-Thinking-v1.5" -> "MoonshotAI/Kimi-VL" ["e"=1]
"modelscope/DiffSynth-Engine" -> "modelscope/MCPBench"
"modelscope/DiffSynth-Engine" -> "modelscope/mcp-central"
"OpenRLHF/OpenRLHF-M" -> "OpenRLHF/OpenRLHF-M"
"OpenRLHF/OpenRLHF-M" -> "TideDra/lmm-r1"
"OpenRLHF/OpenRLHF-M" -> "LengSicong/MMR1"
"modelscope/awesome-deep-reasoning" -> "FanqingM/R1-Multimodal-Journey"
"modelscope/awesome-deep-reasoning" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"modelscope/awesome-deep-reasoning" -> "modelscope/mcp-central"
"modelscope/awesome-deep-reasoning" -> "Osilly/Vision-R1"
"modelscope/awesome-deep-reasoning" -> "Hongcheng-Gao/Awesome-Long2short-on-LRMs" ["e"=1]
"modelscope/awesome-deep-reasoning" -> "RUCAIBox/Slow_Thinking_with_LLMs" ["e"=1]
"modelscope/awesome-deep-reasoning" -> "yfzhang114/Awesome-Multimodal-Large-Language-Models"
"Wang-Xiaodong1899/Open-R1-Video" -> "tulerfeng/Video-R1"
"Wang-Xiaodong1899/Open-R1-Video" -> "TideDra/lmm-r1"
"Wang-Xiaodong1899/Open-R1-Video" -> "EvolvingLMMs-Lab/open-r1-multimodal"
"Wang-Xiaodong1899/Open-R1-Video" -> "Fancy-MLLM/R1-Onevision"
"Wang-Xiaodong1899/Open-R1-Video" -> "FanqingM/R1-Multimodal-Journey"
"Wang-Xiaodong1899/Open-R1-Video" -> "turningpoint-ai/VisualThinker-R1-Zero"
"Wang-Xiaodong1899/Open-R1-Video" -> "ModalMinds/MM-EUREKA"
"Wang-Xiaodong1899/Open-R1-Video" -> "TencentARC/SEED-Bench-R1" ["e"=1]
"Wang-Xiaodong1899/Open-R1-Video" -> "Wang-Xiaodong1899/CVPR25-MLLM-Paper-List"
"Wang-Xiaodong1899/Open-R1-Video" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs"
"Wang-Xiaodong1899/Open-R1-Video" -> "OpenGVLab/VideoChat-R1"
"Wang-Xiaodong1899/Open-R1-Video" -> "OpenGVLab/VideoChat-Flash"
"Wang-Xiaodong1899/Open-R1-Video" -> "Osilly/Vision-R1"
"Wang-Xiaodong1899/Open-R1-Video" -> "Liuziyu77/Visual-RFT"
"Wang-Xiaodong1899/Open-R1-Video" -> "Hui-design/R1-Video-fixbug"
"bloomberg/scatteract" -> "soap117/DeepRule"
"MAC-AutoML/QuoTA" -> "VITA-MLLM/Sparrow"
"MAC-AutoML/QuoTA" -> "VITA-MLLM/VITA-Audio"
"MAC-AutoML/QuoTA" -> "zhourax/VEGA"
"MAC-AutoML/QuoTA" -> "Kwai-YuanQi/MM-RLHF"
"MAC-AutoML/QuoTA" -> "Leon1207/Video-RAG-master"
"MAC-AutoML/QuoTA" -> "VITA-MLLM/Long-VITA"
"MAC-AutoML/QuoTA" -> "ggg0919/cantor"
"MAC-AutoML/QuoTA" -> "xjtupanda/Sparrow"
"VITA-MLLM/Sparrow" -> "MME-Benchmarks/MME-Unify"
"HumanMLLM/HumanOmni" -> "HumanMLLM/R1-Omni"
"HumanMLLM/HumanOmni" -> "HumanMLLM/Omni-Emotion"
"HumanMLLM/HumanOmni" -> "zeroQiaoba/AffectGPT" ["e"=1]
"songweii/DualToken" -> "fengzi258/Ocean-R1"
"OpenGVLab/VideoChat-R1" -> "www-Ye/TimeZero"
"OpenGVLab/VideoChat-R1" -> "TencentARC/SEED-Bench-R1" ["e"=1]
"OpenGVLab/VideoChat-R1" -> "MCG-NJU/VideoChat-Online"
"OpenGVLab/VideoChat-R1" -> "OpenGVLab/FluxViT"
"OpenGVLab/VideoChat-R1" -> "MCG-NJU/p-MoD"
"TencentARC/VideoPainter" -> "shiyi-zh0408/FlexiAct" ["e"=1]
"modelscope/Nexus-Gen" -> "modelscope/mcp-central"
"foldl/AlphaGeometryRE" -> "tpgh24/ag4masses"
"fengzi258/Ocean-R1" -> "songweii/DualToken"
"GeoGPT-Research-Project/GeoGPT" -> "likyoo/awesome-MLLM-for-image-segmentation"
"Aleafy/RelightVid" -> "Bujiazi/ByTheWay" ["e"=1]
"DAMO-NLP-SG/multimodal_textbook" -> "LengSicong/MMR1" ["e"=1]
"LengSicong/MMR1" -> "Osilly/Vision-R1"
"LengSicong/MMR1" -> "OpenRLHF/OpenRLHF-M"
"LengSicong/MMR1" -> "DAMO-NLP-SG/multimodal_textbook" ["e"=1]
"LengSicong/MMR1" -> "John-AI-Lab/NoisyRollout" ["e"=1]
"Mark12Ding/Dispider" -> "yellow-binary-tree/MMDuet"
"Mark12Ding/Dispider" -> "Wiselnn570/VideoRoPE"
"Mark12Ding/Dispider" -> "JoeLeelyf/OVO-Bench"
"Mark12Ding/Dispider" -> "THUNLP-MT/StreamingBench" ["e"=1]
"Mark12Ding/Dispider" -> "xinding-sys/StreamMind"
"Mark12Ding/Dispider" -> "shikiw/Modality-Integration-Rate"
"Mark12Ding/Dispider" -> "Bujiazi/HiFlow"
"showlab/livecc" -> "bigai-nlco/VideoLLaMB"
"yeliudev/VideoMind" -> "TimeMarker-LLM/TimeMarker"
"yeliudev/VideoMind" -> "www-Ye/TimeZero"
"maitrix-org/Voila" -> "VITA-MLLM/VITA-Audio" ["e"=1]
"0russwest0/Agent-R1" -> "TideDra/lmm-r1" ["e"=1]
"0russwest0/Agent-R1" -> "Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["e"=1]
"Ucas-HaoranWei/Slow-Perception" -> "dle666/R-CoT"
"Ucas-HaoranWei/Slow-Perception" -> "Ucas-HaoranWei/Vary-family"
"Ucas-HaoranWei/Slow-Perception" -> "felixludos/alphageometry"
"Ucas-HaoranWei/Slow-Perception" -> "Ucas-HaoranWei/Vary-tiny-600k"
"Ucas-HaoranWei/Slow-Perception" -> "mingliangzhang2018/PGDP" ["e"=1]
"yayafengzi/LMM-HiMTok" -> "xuliu-cyber/RSUniVLM"
"yayafengzi/LMM-HiMTok" -> "NJU-LHRS/ScoreRS"
"TrustGen/TrustEval-toolkit" -> "HowieHwong/UniGen" ["e"=1]
"Bujiazi/HiFlow" -> "Bujiazi/ByTheWay"
"Bujiazi/HiFlow" -> "beichenzbc/BoostStep"
"Bujiazi/HiFlow" -> "SYuan03/MM-IFEngine"
"JoeLeelyf/customize-arxiv-daily" -> "SYuan03/MM-IFEngine"
"ChrisDong-THU/GaussianToken" -> "Yxxxb/LAVT-RS"
"ChrisDong-THU/GaussianToken" -> "VoyageWang/IteRPrimE"
"ChrisDong-THU/GaussianToken" -> "zhang9302002/Flash-VStream"
"www-Ye/TimeZero" -> "appletea233/Temporal-R1"
"www-Ye/TimeZero" -> "OpenGVLab/VideoChat-R1"
"www-Ye/TimeZero" -> "TencentARC/SEED-Bench-R1" ["e"=1]
"www-Ye/TimeZero" -> "yellow-binary-tree/HawkEye"
"modelscope/mcp-central" -> "1100111GTH/XG-RAG"
"GuanxingLu/vlarl" -> "Tengbo-Yu/AnyBimanual"
"GuanxingLu/vlarl" -> "shiyi-zh0408/NAE_CVPR2024"
"NJU-LHRS/ScoreRS" -> "xiong-zhitong/GeoLB-SigLIP"
"NJU-LHRS/ScoreRS" -> "jonathan-roberts1/charting-new-territories"
"MME-Benchmarks/MME-Unify" -> "VITA-MLLM/Sparrow"
"MCG-NJU/VideoChat-Online" -> "yaolinli/TimeChat-Online"
"yaolinli/TimeChat-Online" -> "MCG-NJU/VideoChat-Online"
"jonathan-roberts1/zerobench" -> "jonathan-roberts1/SciFIBench"
"intsig-textin/chatdoc_stack" -> "intsig-textin/chatdoc"
"intsig-textin/chatdoc_stack" -> "intsig-textin/parsex-sdk"
"yfzhang114/r1_reward" -> "Kwai-YuanQi/MM-RLHF"
"yfzhang114/r1_reward" -> "VITA-MLLM/Sparrow"
"yfzhang114/r1_reward" -> "MME-Benchmarks/MME-Unify"
"yfzhang114/r1_reward" -> "VITA-MLLM/VITA-Audio"
"Alpha-Innovator/Dolphin" -> "Alpha-Innovator/TrustGeoGen"
"likyoo/awesome-MLLM-for-image-segmentation" -> "kang-wu/SkySensePlusPlus"
"hmxiong/StreamChat" -> "JiazuoYu/PathWeave"
"intsig-textin/chatdoc" -> "intsig-textin/chatdoc_stack"
"intsig-textin/chatdoc" -> "intsig-textin/parsex-sdk"
"xiong-zhitong/GeoLB-SigLIP" -> "NJU-LHRS/ScoreRS"
"MCG-NJU/CaReBench" -> "MCG-NJU/NeuralSolver" ["e"=1]
"MCG-NJU/CaReBench" -> "HELLORPG/CV-Framework"
"microsoft/unilm" ["l"="38.785,-0.95", "c"=39]
"haotian-liu/LLaVA" ["l"="47.375,29.865"]
"BradyFU/Awesome-Multimodal-Large-Language-Models" ["l"="47.423,29.9"]
"mattbierbaum/arxiv-public-datasets" ["l"="-34.118,16.248", "c"=996]
"tingyaohsu/SciCap" ["l"="47.078,30.365"]
"pliang279/awesome-multimodal-ml" ["l"="48.592,32.051", "c"=300]
"dmlc/decord" ["l"="47.928,33.807", "c"=168]
"OpenGVLab/InternVideo" ["l"="47.549,30.045"]
"315386775/DeepLearing-Interview-Awesome-2024" ["l"="38.708,-2.14", "c"=202]
"davidmrau/mixture-of-experts" ["l"="38.625,-0.46", "c"=39]
"PKU-YuanGroup/MoE-LLaVA" ["l"="47.445,30.071"]
"hassony2/torch_videovision" ["l"="47.85,33.86", "c"=168]
"yuxumin/CoRe" ["l"="47.944,30.225"]
"xujinglin/FineDiving" ["l"="47.96,30.225"]
"baiyang4/aqa_tpt" ["l"="47.95,30.218"]
"nzl-thu/MUSDL" ["l"="47.978,30.238"]
"MarkMoHR/Awesome-Referring-Image-Segmentation" ["l"="48.925,31.903", "c"=300]
"dvlab-research/LISA" ["l"="47.454,30.124"]
"coin-dataset/annotation-tool" ["l"="48.001,30.233"]
"425776024/VideoLabeling" ["l"="48.031,30.233"]
"pomonam/AttentionCluster" ["l"="48.037,30.266"]
"hazeld/rank-aware-attention-network" ["l"="48.011,30.251"]
"Maluuba/FigureQA" ["l"="47.03,30.459"]
"kushalkafle/DVQA_dataset" ["l"="47.044,30.456"]
"vmichals/FigureQA-baseline" ["l"="47.031,30.475"]
"LisaAnne/Hallucination" ["l"="47.322,30.322"]
"AoiDragon/POPE" ["l"="47.318,30.307"]
"RUCAIBox/POPE" ["l"="47.342,30.283"]
"BillChan226/HALC" ["l"="47.332,30.307"]
"ParitoshParmar/MTL-AQA" ["l"="47.96,30.236"]
"ZhouKanglei/Awesome-AQA" ["l"="47.944,30.212"]
"Shunli-Wang/TSA-Net" ["l"="47.97,30.22"]
"shiyi-zh0408/LOGO" ["l"="47.898,30.222"]
"ParitoshParmar/Fitness-AQA" ["l"="47.967,30.212"]
"kushalkafle/PReFIL" ["l"="47.037,30.444"]
"NiteshMethani/PlotQA" ["l"="47.054,30.43"]
"NielsRogge/Transformers-Tutorials" ["l"="47.772,26.319", "c"=323]
"hyunwoongko/transformer" ["l"="53.299,25.715", "c"=172]
"Breakthrough/PySceneDetect" ["l"="44.147,20.072", "c"=20]
"DAMO-NLP-SG/Video-LLaMA" ["l"="47.525,30.006"]
"IBM/action-recognition-pytorch" ["l"="47.838,33.805", "c"=168]
"Chuhanxx/Temporal_Query_Networks" ["l"="48.106,30.266"]
"soCzech/TransNetV2" ["l"="47.898,33.305", "c"=373]
"RenShuhuai-Andy/TimeChat" ["l"="47.583,30.113"]
"JasonObeid/Chart2Text" ["l"="47.06,30.359"]
"vis-nlp/Chart-to-text" ["l"="47.077,30.343"]
"gongliym/data2text-transformer" ["l"="47.022,30.372"]
"soap117/DeepRule" ["l"="47.043,30.378"]
"pranonrahman/ChartSumm" ["l"="47.074,30.331"]
"vis-nlp/ChartQA" ["l"="47.101,30.344"]
"xtaci/gaio" ["l"="3.498,-8.421", "c"=0]
"dvlab-research/Lyra" ["l"="47.75,30.301"]
"Cvrane/ChartReader" ["l"="47.029,30.405"]
"SDOlivia/FineGym" ["l"="48.071,30.26"]
"coriverchen/Robust_Steganography" ["l"="47.787,30.441"]
"mengtong0110/InferDPT" ["l"="47.788,30.417"]
"openai/CLIP" ["l"="50.642,29.547", "c"=83]
"mlfoundations/open_clip" ["l"="48.974,30.216", "c"=191]
"ccfddl/ccf-deadlines" ["l"="-3.963,23.586", "c"=827]
"microsoft/LoRA" ["l"="39.84,0.5", "c"=7]
"luo3300612/Visualizer" ["l"="50.875,29.609", "c"=83]
"openai/grok" ["l"="39.287,-0.62", "c"=39]
"LargeWorldModel/LWM" ["l"="47.44,29.845"]
"google-research/scenic" ["l"="48.018,33.755", "c"=168]
"xiaobai1217/Awesome-Video-Datasets" ["l"="47.974,33.748", "c"=168]
"yunlong10/Awesome-LLMs-for-Video-Understanding" ["l"="47.523,30.059"]
"mbzuai-oryx/Video-ChatGPT" ["l"="47.532,30.076"]
"SwinTransformer/Video-Swin-Transformer" ["l"="48.024,33.802", "c"=168]
"m-bain/webvid" ["l"="33.61,31.323", "c"=109]
"cvdfoundation/kinetics-dataset" ["l"="48.004,33.839", "c"=168]
"DirtyHarryLYL/Transformer-in-Vision" ["l"="50.85,29.7", "c"=83]
"DirtyHarryLYL/LLM-in-Vision" ["l"="47.427,30.163"]
"FuxiaoLiu/VisualNews-Repository" ["l"="52.581,26.749", "c"=1052]
"FuxiaoLiu/DocumentCLIP" ["l"="47.226,30.293"]
"FuxiaoLiu/Twitter-Video-dataset" ["l"="47.197,30.299"]
"TheJaeLal/LineFormer" ["l"="46.982,30.422"]
"bloomberg/scatteract" ["l"="47.002,30.395"]
"Lyman-Smoker/Awesome-AQA" ["l"="47.993,30.206"]
"xuangch/CVPR22_GDLT" ["l"="47.936,30.218"]
"Luciferbobo/DAE-AQA" ["l"="47.956,30.207"]
"doc-doc/NExT-QA" ["l"="47.875,33.08", "c"=373]
"egoschema/EgoSchema" ["l"="47.635,30.141"]
"archiki/Robust-E2E-ASR" ["l"="47.869,30.455"]
"YUCHEN005/DPSL-ASR" ["l"="47.852,30.431"]
"bliunlpr/Robust_e2e_gan" ["l"="47.882,30.478"]
"ZhouKanglei/HGCN_AQA" ["l"="47.963,30.195"]
"THUDM/SwissArmyTransformer" ["l"="39.242,-2.131", "c"=202]
"THUDM/CogVLM" ["l"="47.384,29.926"]
"mli/paper-reading" ["l"="50.604,29.465", "c"=83]
"salesforce/BLIP" ["l"="48.991,30.249", "c"=191]
"QwenLM/Qwen-VL" ["l"="47.411,29.954"]
"OFA-Sys/OFA" ["l"="49.008,30.282", "c"=191]
"X-PLUG/mPLUG-Owl" ["l"="47.444,29.996"]
"isl-org/lang-seg" ["l"="48.77,30.285", "c"=191]
"MLNLP-World/Paper-Writing-Tips" ["l"="-3.951,23.526", "c"=827]
"microsoft/GLIP" ["l"="48.866,30.251", "c"=191]
"MCG-NJU/VideoMAE" ["l"="48.045,33.812", "c"=168]
"deepdoctection/deepdoctection" ["l"="46.288,6.092", "c"=571]
"X-PLUG/mPLUG-DocOwl" ["l"="47.309,29.975"]
"cmhungsteve/Awesome-Transformer-Attention" ["l"="50.774,29.648", "c"=83]
"diff-usion/Awesome-Diffusion-Models" ["l"="45.856,31.577", "c"=605]
"open-mmlab/mmengine" ["l"="50.421,29.903", "c"=83]
"open-mmlab/Multimodal-GPT" ["l"="47.499,30.148"]
"facebookresearch/Mask2Former" ["l"="48.83,30.207", "c"=191]
"kahnchana/svt" ["l"="47.886,30.068"]
"kahnchana/clippy" ["l"="47.841,30.083"]
"google-research/big_vision" ["l"="48.957,30.256", "c"=191]
"LLaVA-VL/LLaVA-NeXT" ["l"="47.414,30.029"]
"cambrian-mllm/cambrian" ["l"="47.383,30.081"]
"showlab/all-in-one" ["l"="47.833,32.976", "c"=373]
"Yui010206/SeViLA" ["l"="47.62,30.115"]
"codecaution/Awesome-Mixture-of-Experts-Papers" ["l"="38.648,-0.468", "c"=39]
"swordlidev/Efficient-Multimodal-LLMs-Survey" ["l"="47.518,30.217"]
"lyc8503/2021-nju-software-engineering-textbook" ["l"="-4.995,19.529", "c"=564]
"SYuan03/MM-IFEngine" ["l"="47.752,30.36"]
"timojl/clipseg" ["l"="48.781,30.205", "c"=191]
"tingxueronghua/ChartLlama-code" ["l"="47.107,30.279"]
"vis-nlp/UniChart" ["l"="47.112,30.306"]
"google-research/pix2struct" ["l"="46.322,5.965", "c"=571]
"mitvis/vistext" ["l"="47.085,30.322"]
"FuxiaoLiu/MMC" ["l"="47.168,30.298"]
"zengxingchen/ChartQA-MLLM" ["l"="-5.59,-41.828", "c"=333]
"OpenGVLab/ChartAst" ["l"="47.044,30.306"]
"princeton-nlp/CharXiv" ["l"="47.077,30.392"]
"levymsn/CQA-CRCT" ["l"="47.102,30.368"]
"vis-nlp/ChartGemma" ["l"="47.101,30.386"]
"XueFuzhao/awesome-mixture-of-experts" ["l"="38.639,-0.441", "c"=39]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["l"="50.313,38.23", "c"=684]
"google-research/pix2seq" ["l"="48.825,30.345", "c"=191]
"OpenGVLab/VisionLLM" ["l"="47.46,30.162"]
"ZwwWayne/K-Net" ["l"="50.751,30.441", "c"=83]
"jshilong/GPT4RoI" ["l"="47.44,30.215"]
"facebookresearch/multimodal" ["l"="48.947,30.29", "c"=191]
"microsoft/XPretrain" ["l"="47.83,32.958", "c"=373]
"rese1f/MovieChat" ["l"="47.572,30.105"]
"Thinklab-SJTU/pygmtools" ["l"="51.014,26.523", "c"=490]
"Alpha-Innovator/ChartVLM" ["l"="47.066,30.242"]
"vis-nlp/ChartInstruct" ["l"="47.101,30.327"]
"Soldelli/MAD" ["l"="47.972,33.147", "c"=373]
"lntzm/MESM" ["l"="47.681,30.049"]
"VRU-NExT/VideoQA" ["l"="47.854,33.09", "c"=373]
"jshilong/DDQ" ["l"="48.679,30.342", "c"=191]
"neillu23/CDiffuSE" ["l"="36.697,4.2", "c"=128]
"YUCHEN005/NASE" ["l"="47.841,30.452"]
"YUCHEN005/RATS-Channel-A-Speech-Data" ["l"="47.835,30.419"]
"YUCHEN005/Gradient-Remedy" ["l"="47.818,30.411"]
"YUCHEN005/UNA-GAN" ["l"="47.832,30.408"]
"LibRerank-Community/LibRerank" ["l"="59.844,23.899", "c"=235]
"lyingCS/Controllable-Multi-Objective-Reranking" ["l"="47.009,30.321"]
"Ethan00Si/Instrumental-variables-for-recommendation" ["l"="46.884,30.357"]
"Ethan00Si/KuaiSAR" ["l"="46.863,30.365"]
"Ethan00Si/SESREC-SIGIR-2023" ["l"="46.907,30.35"]
"qinghuannn/PAMFN" ["l"="47.977,30.18"]
"mlfoundations/open_flamingo" ["l"="49.028,30.256", "c"=191]
"amazon-science/mm-cot" ["l"="49.134,30.261", "c"=191]
"lupantech/ScienceQA" ["l"="47.248,30.316"]
"huggingface/peft" ["l"="40.04,0.52", "c"=7]
"baaivision/Painter" ["l"="48.864,30.163", "c"=191]
"omriav/blended-latent-diffusion" ["l"="33.311,31.346", "c"=109]
"microsoft/GenerativeImage2Text" ["l"="48.772,31.949", "c"=300]
"JialianW/GRiT" ["l"="47.45,30.28"]
"yuweihao/MM-Vet" ["l"="47.314,30.258"]
"dair-ai/ML-Papers-of-the-Week" ["l"="47.787,26.477", "c"=323]
"microsoft/i-Code" ["l"="47.583,29.952"]
"microsoft/UDOP" ["l"="46.327,6.043", "c"=571]
"NExT-GPT/NExT-GPT" ["l"="47.491,29.926"]
"facebookresearch/ImageBind" ["l"="49.068,30.103", "c"=191]
"thu-ml/unidiffuser" ["l"="45.765,31.528", "c"=605]
"SHI-Labs/Versatile-Diffusion" ["l"="33.385,31.435", "c"=109]
"AlibabaResearch/AdvancedLiterateMachinery" ["l"="46.369,6.029", "c"=571]
"baaivision/Emu" ["l"="46.521,30.662", "c"=367]
"lucidrains/make-a-video-pytorch" ["l"="33.657,31.288", "c"=109]
"YingqingHe/LVDM" ["l"="33.653,31.329", "c"=109]
"AILab-CVC/VideoCrafter" ["l"="33.589,31.197", "c"=109]
"salesforce/LAVIS" ["l"="49.008,30.194", "c"=191]
"showlab/Show-o" ["l"="46.437,30.648", "c"=367]
"gligen/GLIGEN" ["l"="33.379,31.323", "c"=109]
"wenhuchen/TheoremQA" ["l"="37.55,-0.353", "c"=126]
"lupantech/chameleon-llm" ["l"="36.795,-2.439", "c"=797]
"lupantech/MathVista" ["l"="47.22,30.34"]
"ylsung/VL_adapter" ["l"="50.197,38.201", "c"=684]
"FuxiaoLiu/LRV-Instruction" ["l"="47.339,30.264"]
"lupantech/dl4math" ["l"="37.487,-0.344", "c"=126]
"MMMU-Benchmark/MMMU" ["l"="47.26,30.272"]
"mandyyyyii/scibench" ["l"="37.599,-0.355", "c"=126]
"lupantech/IconQA" ["l"="56.566,28.826", "c"=310]
"Timothyxxx/Chain-of-ThoughtsPapers" ["l"="36.759,-2.46", "c"=797]
"LightChen233/M3CoT" ["l"="47.227,30.36"]
"modelscope/modelscope" ["l"="38.989,-1.789", "c"=202]
"clovaai/donut" ["l"="46.254,6.071", "c"=571]
"OpenGVLab/InternVL" ["l"="47.362,29.955"]
"ShoufaChen/DiffusionDet" ["l"="48.83,30.299", "c"=191]
"microsoft/X-Decoder" ["l"="48.821,30.224", "c"=191]
"breezedeus/Pix2Text" ["l"="46.617,5.821", "c"=571]
"Ucas-HaoranWei/Vary" ["l"="47.281,30.013"]
"OFA-Sys/Chinese-CLIP" ["l"="49.065,30.217", "c"=191]
"eliahuhorwitz/Academic-project-page-template" ["l"="-3.915,23.63", "c"=827]
"SALT-NLP/LLaVAR" ["l"="47.35,30.24"]
"Computer-Vision-in-the-Wild/CVinW_Readings" ["l"="47.472,30.142"]
"KaiyangZhou/CoOp" ["l"="50.344,38.24", "c"=684]
"IDEA-Research/awesome-detection-transformer" ["l"="48.75,30.304", "c"=191]
"VainF/Awesome-Anything" ["l"="48.929,30.125", "c"=191]
"jianzongwu/Awesome-Open-Vocabulary" ["l"="48.681,30.233", "c"=191]
"TheShadow29/awesome-grounding" ["l"="48.847,31.967", "c"=300]
"KMnP/vpt" ["l"="50.273,38.24", "c"=684]
"facebookresearch/LaViLa" ["l"="47.738,32.967", "c"=373]
"huangb23/VTimeLLM" ["l"="47.598,30.106"]
"OpenGVLab/Ask-Anything" ["l"="47.522,30.03"]
"OpenGVLab/VideoChat-Flash" ["l"="47.552,30.13"]
"OpenGVLab/VideoMAEv2" ["l"="48.083,33.864", "c"=168]
"OpenGVLab/unmasked_teacher" ["l"="47.778,32.957", "c"=373]
"PKU-YuanGroup/Video-LLaVA" ["l"="47.477,30.025"]
"OpenGVLab/VideoMamba" ["l"="49.175,34.184", "c"=556]
"DAMO-NLP-SG/VideoLLaMA2" ["l"="47.553,30.072"]
"ArrowLuo/CLIP4Clip" ["l"="47.909,32.98", "c"=373]
"magic-research/PLLaVA" ["l"="47.569,30.119"]
"showlab/Image2Paragraph" ["l"="49.024,30.224", "c"=191]
"davidnvq/grit" ["l"="48.639,31.933", "c"=300]
"allenai/unified-io-inference" ["l"="47.462,30.349"]
"showlab/EgoVLP" ["l"="47.697,32.969", "c"=373]
"shikras/shikra" ["l"="47.418,30.229"]
"X2FD/LVIS-INSTRUCT4V" ["l"="47.43,30.278"]
"ttengwang/Awesome_Long_Form_Video_Understanding" ["l"="47.576,30.135"]
"Ziyang412/VideoTree" ["l"="47.624,30.144"]
"boheumd/MA-LMM" ["l"="47.596,30.128"]
"md-mohaiminul/VideoRecap" ["l"="47.654,30.138"]
"MME-Benchmarks/Video-MME" ["l"="47.549,30.151"]
"daixiangzi/Awesome-Token-Compress" ["l"="47.547,30.207"]
"Wang-Xiaodong1899/Open-R1-Video" ["l"="47.373,30.13"]
"JUNJIE99/MLVU" ["l"="47.587,30.164"]
"EvolvingLMMs-Lab/LongVA" ["l"="47.562,30.158"]
"j-min/HiREST" ["l"="47.969,33.095", "c"=373]
"QQ-MM/Video-CCAM" ["l"="47.614,30.171"]
"baaivision/EVA" ["l"="48.906,30.24", "c"=191]
"Oneirocom/Magick" ["l"="36.673,25.571", "c"=1665]
"SkunkworksAI/hydra-moe" ["l"="47.46,30.439"]
"iejMac/video2dataset" ["l"="33.639,31.396", "c"=109]
"facebookresearch/ToMe" ["l"="48.99,33.157", "c"=401]
"allenai/unified-io-2" ["l"="46.539,30.684", "c"=367]
"TencentARC/GVT" ["l"="47.467,30.392"]
"ch3cook-fdu/Vote2Cap-DETR" ["l"="46.905,30.238"]
"Open3DA/LL3DA" ["l"="65.134,11.739", "c"=203]
"HankYe/AdaptiveDiffusion" ["l"="46.966,30.242"]
"eliahuhorwitz/Conffusion" ["l"="38.643,2.623", "c"=54]
"OpenGVLab/UniFormerV2" ["l"="48.1,33.834", "c"=168]
"Yangyi-Chen/Multimodal-AND-Large-Language-Models" ["l"="47.335,30.191"]
"showlab/Awesome-MLLM-Hallucination" ["l"="47.329,30.274"]
"DAMO-NLP-SG/VCD" ["l"="47.352,30.311"]
"Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs" ["l"="47.336,30.127"]
"xieyuquanxx/awesome-Large-MultiModal-Hallucination" ["l"="47.345,30.3"]
"HillZhang1999/llm-hallucination-survey" ["l"="37.656,-6.917", "c"=766]
"shikiw/OPERA" ["l"="47.362,30.288"]
"Fancy-MLLM/R1-Onevision" ["l"="47.322,30.115"]
"TideDra/lmm-r1" ["l"="47.316,30.101"]
"SinclairCoder/Instruction-Tuning-Papers" ["l"="36.756,-2.518", "c"=797]
"ChaofanTao/Autoregressive-Models-in-Vision-Survey" ["l"="46.437,30.673", "c"=367]
"Osilly/Vision-R1" ["l"="47.312,30.127"]
"open-compass/VLMEvalKit" ["l"="47.359,30.06"]
"EvolvingLMMs-Lab/open-r1-multimodal" ["l"="47.343,30.101"]
"antoyang/FrozenBiLM" ["l"="47.853,32.988", "c"=373]
"google-deepmind/perception_test" ["l"="47.642,30.179"]
"PKU-YuanGroup/Video-Bench" ["l"="64.241,3.9", "c"=49]
"YUCHEN005/Unified-Enhance-Separation" ["l"="47.831,30.432"]
"HELLORPG/CV-Framework" ["l"="47.635,30.355"]
"MCG-NJU/CaReBench" ["l"="47.625,30.34"]
"AndyTang15/FLAG3D" ["l"="47.79,30.222"]
"AndyTang15/FLAG3Dv2" ["l"="47.781,30.214"]
"iSEE-Laboratory/Continual-AQA" ["l"="48.021,30.191"]
"Run542968/Awesome-3D-Human-Motion-Generation" ["l"="48.007,30.193"]
"iSEE-Laboratory/EgoExo-Fitness" ["l"="48.017,30.204"]
"facebookresearch/segment-anything" ["l"="50.506,29.519", "c"=83]
"IDEA-Research/Grounded-Segment-Anything" ["l"="48.915,30.081", "c"=191]
"UX-Decoder/Semantic-SAM" ["l"="48.814,30.127", "c"=191]
"aiwaves-cn/agents" ["l"="36.726,-2.215", "c"=797]
"Alpha-VLLM/LLaMA2-Accessory" ["l"="47.478,29.969"]
"OpenGVLab/LLaMA-Adapter" ["l"="39.851,0.706", "c"=7]
"InternLM/InternLM-XComposer" ["l"="47.384,30.005"]
"EvolvingLMMs-Lab/Otter" ["l"="50.99,2.78", "c"=85]
"Alpha-VLLM/Lumina-T2X" ["l"="33.459,31.175", "c"=109]
"meta-llama/llama-cookbook" ["l"="40.401,0.438", "c"=7]
"meta-llama/codellama" ["l"="40.146,0.454", "c"=7]
"liguodongiot/llm-action" ["l"="38.757,-2.008", "c"=202]
"Vision-CAIR/MiniGPT-4" ["l"="39.832,0.405", "c"=7]
"RUCAIBox/LLMSurvey" ["l"="38.837,-2.036", "c"=202]
"open-compass/opencompass" ["l"="38.902,-2.019", "c"=202]
"IDEA-Research/GroundingDINO" ["l"="48.845,30.11", "c"=191]
"lm-sys/FastChat" ["l"="40.074,0.427", "c"=7]
"vllm-project/vllm" ["l"="40.265,0.397", "c"=7]
"hiyouga/LLaMA-Factory" ["l"="40.278,0.218", "c"=7]
"Dao-AILab/flash-attention" ["l"="38.876,-0.736", "c"=39]
"open-mmlab/mmfewshot" ["l"="49.426,29.431", "c"=1525]
"InternLM/InternLM-techreport" ["l"="38.794,-1.827", "c"=202]
"open-mmlab/mmeval" ["l"="49.474,29.465", "c"=1525]
"open-mmlab/playground" ["l"="48.845,30.051", "c"=191]
"jshilong/GroupRCNN" ["l"="49.461,29.481", "c"=1525]
"InternLM/lmdeploy" ["l"="38.95,-0.593", "c"=39]
"InternLM/xtuner" ["l"="38.848,-1.888", "c"=202]
"modelscope/ms-swift" ["l"="38.867,-1.929", "c"=202]
"QwenLM/Qwen2.5-VL" ["l"="47.318,29.932"]
"pengsida/learning_research" ["l"="-4.009,23.579", "c"=827]
"QwenLM/Qwen" ["l"="38.989,-1.849", "c"=202]
"RiseInRose/MiniGPT-4-ZH" ["l"="39.276,-2.083", "c"=202]
"Stability-AI/generative-models" ["l"="38.355,0.95", "c"=54]
"UbiquitousLearning/mllm" ["l"="38.941,-0.237", "c"=39]
"Meituan-AutoML/MobileVLM" ["l"="47.409,30.083"]
"mistralai/mistral-inference" ["l"="38.79,-0.727", "c"=39]
"google/magika" ["l"="-13.239,-7.636", "c"=86]
"facebookresearch/nougat" ["l"="40.569,0.31", "c"=7]
"KoljaB/RealtimeSTT" ["l"="40.486,3.318", "c"=908]
"OpenBMB/MiniCPM-o" ["l"="47.272,29.853"]
"Hannibal046/Awesome-LLM" ["l"="40.2,0.476", "c"=7]
"InvincibleWyq/ChatVID" ["l"="47.807,30.208"]
"SuleBai/SC-CLIP" ["l"="47.773,30.219"]
"OpenGVLab/InternGPT" ["l"="40.967,-3.607", "c"=146]
"mbzuai-oryx/groundingLMM" ["l"="47.465,30.182"]
"UX-Decoder/Segment-Everything-Everywhere-All-At-Once" ["l"="48.865,30.127", "c"=191]
"Liuziyu77/Visual-RFT" ["l"="47.336,30.087"]
"NVlabs/ODISE" ["l"="48.752,30.251", "c"=191]
"InternLM/InternLM" ["l"="38.916,-1.913", "c"=202]
"kaixindelele/ChatPaper" ["l"="-4.127,23.596", "c"=827]
"microsoft/MM-REACT" ["l"="47.433,30.256"]
"allenai/visprog" ["l"="47.462,30.218"]
"OptimalScale/DetGPT" ["l"="47.468,30.2"]
"allenai/mmc4" ["l"="49.046,30.301", "c"=191]
"cvlab-columbia/viper" ["l"="49.056,30.267", "c"=191]
"ashkamath/mdetr" ["l"="48.833,31.941", "c"=300]
"Vision-CAIR/ChatCaptioner" ["l"="49.086,30.262", "c"=191]
"penghao-wu/vstar" ["l"="47.403,30.173"]
"facebookresearch/seamless_communication" ["l"="38.571,1.463", "c"=54]
"Mooler0410/LLMsPracticalGuide" ["l"="40.014,0.629", "c"=7]
"THUDM/VisualGLM-6B" ["l"="39.161,-2.048", "c"=202]
"ctlllll/LLM-ToolMaker" ["l"="38.024,-1.604", "c"=1218]
"lyuchenyang/Macaw-LLM" ["l"="47.559,29.998"]
"WooooDyy/LLM-Agent-Paper-List" ["l"="36.678,-2.286", "c"=797]
"longyuewangdcu/Chinese-Llama-2" ["l"="50.676,3.032", "c"=85]
"yxuansu/PandaGPT" ["l"="47.481,30.097"]
"wxjiao/ParroT" ["l"="53.962,25.021", "c"=492]
"bytedance/SALMONN" ["l"="38.502,2.031", "c"=54]
"Text-to-Audio/Make-An-Audio" ["l"="50.744,2.954", "c"=85]
"PeiranLi0930/TorchProject" ["l"="50.651,2.959", "c"=85]
"thunlp/WebCPM" ["l"="50.705,2.93", "c"=85]
"Dicklesworthstone/llm_aided_ocr" ["l"="41.038,0.191", "c"=7]
"spcl/graph-of-thoughts" ["l"="36.648,-2.245", "c"=797]
"williamyang1991/Rerender_A_Video" ["l"="33.635,31.168", "c"=109]
"OpenMOSS/AnyGPT" ["l"="38.484,2.042", "c"=54]
"dvlab-research/LLaMA-VID" ["l"="47.518,30.09"]
"PKU-YuanGroup/LanguageBind" ["l"="47.504,30.049"]
"junyangwang0410/AMBER" ["l"="47.316,30.289"]
"AILab-CVC/SEED" ["l"="46.508,30.691", "c"=367]
"AILab-CVC/SEED-Bench" ["l"="47.282,30.295"]
"facebookresearch/dinov2" ["l"="48.971,30.114", "c"=191]
"ttengwang/Caption-Anything" ["l"="48.949,30.157", "c"=191]
"EvolvingLMMs-Lab/lmms-eval" ["l"="51.076,2.828", "c"=85]
"modal-labs/quillman" ["l"="41.609,1.553", "c"=7]
"lxe/llavavision" ["l"="47.539,30.471"]
"CASIA-IVA-Lab/AnomalyGPT" ["l"="53.251,14.178", "c"=669]
"zeroQiaoba/AffectGPT" ["l"="56.513,28.11", "c"=940]
"luogen1996/LaVIN" ["l"="47.399,30.214"]
"OpenGVLab/LAMM" ["l"="47.389,30.235"]
"0nutation/SpeechGPT" ["l"="38.507,1.994", "c"=54]
"HenryHZY/Awesome-Multimodal-LLM" ["l"="47.409,30.263"]
"csuhan/OneLLM" ["l"="47.478,30.055"]
"LinkSoul-AI/LLaSM" ["l"="38.455,2.106", "c"=54]
"OFA-Sys/ONE-PEACE" ["l"="48.896,30.217", "c"=191]
"abacaj/code-eval" ["l"="36.209,-0.078", "c"=315]
"AblateIt/finetune-study" ["l"="47.46,30.541"]
"THUDM/CogVLM2" ["l"="47.355,29.987"]
"THUDM/ChatGLM3" ["l"="39.017,-1.891", "c"=202]
"THUDM/GLM-4" ["l"="39.032,-1.848", "c"=202]
"xinyu1205/recognize-anything" ["l"="48.905,30.145", "c"=191]
"invictus717/MetaTransformer" ["l"="47.494,30.005"]
"baaivision/Uni3D" ["l"="65.179,11.71", "c"=203]
"Yuliang-Liu/MultimodalOCR" ["l"="46.656,7.414", "c"=148]
"nttmdlab-nlp/InstructDoc" ["l"="47.077,30.136"]
"QwenLM/Qwen-Agent" ["l"="40.896,0.064", "c"=7]
"InternLM/lagent" ["l"="38.82,-1.827", "c"=202]
"Yuliang-Liu/Monkey" ["l"="51.19,2.719", "c"=85]
"awaisrauf/Awesome-CV-Foundational-Models" ["l"="50.424,38.221", "c"=684]
"mbzuai-oryx/Video-LLaVA" ["l"="47.543,30.119"]
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" ["l"="50.339,38.212", "c"=684]
"mbzuai-oryx/VideoGPT-plus" ["l"="47.556,30.111"]
"PKU-YuanGroup/Chat-UniVi" ["l"="-54.539,-12.838", "c"=252]
"Vision-CAIR/MiniGPT4-video" ["l"="47.553,30.093"]
"facebookresearch/MetaCLIP" ["l"="48.898,30.287", "c"=191]
"tsb0601/MMVP" ["l"="47.341,30.208"]
"Jamie-Stirling/RetNet" ["l"="49.022,32.886", "c"=401]
"voidism/DoLa" ["l"="37.648,-6.864", "c"=766]
"YiyangZhou/LURE" ["l"="47.331,30.293"]
"YiyangZhou/CSR" ["l"="47.289,30.267"]
"YiyangZhou/POVID" ["l"="47.309,30.238"]
"tianyi-lab/HallusionBench" ["l"="47.306,30.283"]
"Purshow/Awesome-LVLM-Hallucination" ["l"="47.342,30.337"]
"Ucas-HaoranWei/GOT-OCR2.0" ["l"="40.783,-0.011", "c"=7]
"LukeForeverYoung/UReader" ["l"="46.352,5.921", "c"=571]
"opendatalab/DocLayout-YOLO" ["l"="46.421,5.954", "c"=571]
"illuin-tech/colpali" ["l"="41.303,0.709", "c"=7]
"hikopensource/DAVAR-Lab-OCR" ["l"="46.413,6.081", "c"=571]
"OpenBMB/VisCPM" ["l"="50.732,2.904", "c"=85]
"RLHF-V/RLAIF-V" ["l"="47.305,30.225"]
"cientgu/InstructDiffusion" ["l"="33.206,31.412", "c"=109]
"SkyworkAI/Vitron" ["l"="47.494,30.173"]
"lxtGH/OMG-Seg" ["l"="51.008,2.867", "c"=85]
"yuhangzang/ContextDET" ["l"="47.491,30.206"]
"YUCHEN005/GILA" ["l"="47.849,30.408"]
"YUCHEN005/MIR-GAN" ["l"="47.835,30.397"]
"YUCHEN005/UniVPM" ["l"="47.82,30.392"]
"EvolvingLMMs-Lab/RelateAnything" ["l"="50.117,38.365", "c"=684]
"fabawi/ImageBind-LoRA" ["l"="52.975,14.35", "c"=669]
"snap-research/Panda-70M" ["l"="33.584,31.343", "c"=109]
"TXH-mercury/VALOR" ["l"="47.778,32.932", "c"=373]
"TXH-mercury/VAST" ["l"="47.793,32.925", "c"=373]
"microsoft/LLaVA-Med" ["l"="62.359,37.614", "c"=284]
"HaozheZhao/MIC" ["l"="47.396,30.311"]
"tsujuifu/pytorch_mgie" ["l"="47.43,29.677"]
"apple/ml-mgie" ["l"="47.423,29.727"]
"TencentARC/SmartEdit" ["l"="33.215,31.317", "c"=109]
"Vision-CAIR/LongVU" ["l"="47.607,30.157"]
"VectorSpaceLab/Video-XL" ["l"="47.596,30.143"]
"DCDmllm/Momentor" ["l"="47.624,30.09"]
"modelscope/motionagent" ["l"="46.949,30.263"]
"modelscope/DiffSynth-Engine" ["l"="47.012,30.217"]
"jy0205/LaVIT" ["l"="46.494,30.675", "c"=367]
"LinkSoul-AI/Chinese-LLaVA" ["l"="39.299,-2.015", "c"=202]
"pkunlp-icler/MIC" ["l"="47.394,30.362"]
"HaozheZhao/MIC_tool" ["l"="47.38,30.356"]
"pkunlp-icler/PCA-EVAL" ["l"="37.077,-1.18", "c"=795]
"VT-NLP/MultiInstruct" ["l"="47.375,30.313"]
"DCDmllm/Cheetah" ["l"="47.386,30.394"]
"RLHF-V/RLHF-V" ["l"="47.316,30.212"]
"OpenGVLab/Multi-Modality-Arena" ["l"="47.363,30.271"]
"FreedomIntelligence/HuatuoGPT-Vision" ["l"="62.364,37.678", "c"=284]
"xiaoman-zhang/PMC-VQA" ["l"="62.331,37.605", "c"=284]
"OpenGVLab/MMT-Bench" ["l"="47.182,30.317"]
"BAAI-DCAI/M3D" ["l"="62.447,37.637", "c"=284]
"RunpeiDong/DreamLLM" ["l"="46.515,30.707", "c"=367]
"OpenGVLab/all-seeing" ["l"="47.38,30.222"]
"BAAI-DCAI/Visual-Instruction-Tuning" ["l"="47.329,30.233"]
"FreedomIntelligence/ALLaVA" ["l"="47.359,30.204"]
"OpenGVLab/OmniCorpus" ["l"="47.353,30.225"]
"shikras/d-cube" ["l"="48.549,30.21", "c"=191]
"FoundationVision/Groma" ["l"="50.752,3.019", "c"=85]
"magic-research/bubogpt" ["l"="47.411,30.288"]
"AILab-CVC/Animate-A-Story" ["l"="33.317,31.109", "c"=109]
"WisconsinAIVision/ViP-LLaVA" ["l"="47.4,30.245"]
"VITA-MLLM/Woodpecker" ["l"="47.384,30.268"]
"shenyunhang/APE" ["l"="48.647,30.18", "c"=191]
"llava-rlhf/LLaVA-RLHF" ["l"="47.332,30.247"]
"xjtupanda/Sparrow" ["l"="47.505,30.279"]
"VITA-MLLM/VITA" ["l"="38.446,1.902", "c"=54]
"MrYxJ/calculate-flops.pytorch" ["l"="51.029,29.87", "c"=83]
"mlpc-ucsd/BLIVA" ["l"="47.226,30.226"]
"ziplab/PTQD" ["l"="38.934,0.075", "c"=39]
"ChangyuanWang17/APQ-DM" ["l"="47.82,30.253"]
"liltom-eth/llama2-webui" ["l"="40.974,-3.885", "c"=146]
"jondurbin/airoboros" ["l"="42.563,-2.031", "c"=1097]
"kohjingyu/gill" ["l"="46.547,30.714", "c"=367]
"Share14/ShareGemini" ["l"="47.701,30.113"]
"bigai-nlco/VideoLLaMB" ["l"="47.698,30.151"]
"facebookresearch/VLPart" ["l"="48.75,30.278", "c"=191]
"jshilong/FisherPruning" ["l"="49.463,29.505", "c"=1525]
"hyungkwonko/chart-llm" ["l"="-5.542,-41.781", "c"=333]
"eric-ai-lab/MiniGPT-5" ["l"="46.568,30.723", "c"=367]
"LLaVA-VL/LLaVA-Plus-Codebase" ["l"="47.468,30.239"]
"NVlabs/prismer" ["l"="48.976,30.277", "c"=191]
"X-PLUG/ChatPLUG" ["l"="47.711,32.831", "c"=373]
"showlab/VLog" ["l"="49.116,30.228", "c"=191]
"berkeley-hipie/HIPIE" ["l"="48.703,30.247", "c"=191]
"aimagelab/freeda" ["l"="47.927,30.282"]
"hydrallm/llama-moe-v1" ["l"="47.46,30.504"]
"r-three/phatgoose" ["l"="47.451,30.477"]
"taylorai/galactic" ["l"="38.489,-0.366", "c"=39]
"Cohere-Labs-Community/parameter-efficient-moe" ["l"="38.492,-0.299", "c"=39]
"liuqidong07/MOELoRA-peft" ["l"="38.439,-0.251", "c"=39]
"SkunkworksAI/BakLLaVA" ["l"="47.471,30.311"]
"ytongbai/LVM" ["l"="46.493,30.613", "c"=367]
"42Shawn/PTQ4DM" ["l"="38.936,0.056", "c"=39]
"Ma-Lab-Berkeley/CRATE" ["l"="53.179,29.958", "c"=547]
"BAAI-DCAI/DataOptim" ["l"="47.246,30.253"]
"EdinburghNLP/awesome-hallucination-detection" ["l"="37.618,-6.92", "c"=766]
"VPGTrans/VPGTrans" ["l"="47.488,30.272"]
"NExT-ChatV/NExT-Chat" ["l"="47.529,30.23"]
"vincentlux/Awesome-Multimodal-LLM" ["l"="47.424,30.329"]
"thunlp/LLaVA-UHD" ["l"="47.38,30.202"]
"YuchenLiu98/COMM" ["l"="47.234,30.275"]
"sail-sg/lorahub" ["l"="38.39,-0.282", "c"=39]
"mathllm/MathCoder" ["l"="37.462,-0.358", "c"=126]
"mathllm/MATH-V" ["l"="47.2,30.384"]
"apitube/docs" ["l"="47.441,30.638"]
"mevbotcrypto/mev-bot" ["l"="47.454,30.651"]
"opendatalab/WanJuan1.0" ["l"="49.607,29.532", "c"=1525]
"Yuqifan1117/HalluciDoctor" ["l"="47.304,30.316"]
"LuckyyySTA/Awesome-LLM-hallucination" ["l"="37.64,-6.906", "c"=766]
"open-compass/MMBench" ["l"="47.294,30.252"]
"uncbiag/Awesome-Foundation-Models" ["l"="50.381,38.189", "c"=684]
"RupertLuo/Valley" ["l"="47.652,30.103"]
"bytedance/Shot2Story" ["l"="47.722,30.082"]
"TimeMarker-LLM/TimeMarker" ["l"="47.631,30.109"]
"Atomic-man007/Awesome_Multimodel_LLM" ["l"="33.171,31.217", "c"=109]
"FreedomIntelligence/MLLM-Bench" ["l"="47.139,30.275"]
"lyingCS/UOEP" ["l"="47.01,30.329"]
"OpenGVLab/Multitask-Model-Selector" ["l"="47.023,30.332"]
"antoyang/VidChapters" ["l"="47.765,33.039", "c"=373]
"DCDmllm/MorphTokens" ["l"="47.359,30.459"]
"DCDmllm/AnyEdit" ["l"="33.109,31.259", "c"=109]
"IDEA-Research/OpenSeeD" ["l"="48.733,30.218", "c"=191]
"microsoft/RegionCLIP" ["l"="48.631,30.276", "c"=191]
"witnessai/Awesome-Open-Vocabulary-Object-Detection" ["l"="48.608,30.245", "c"=191]
"Saiyan-World/grounded-segment-any-parts" ["l"="48.712,30.23", "c"=191]
"TideDra/VL-RLHF" ["l"="47.263,30.23"]
"RifleZhang/LLaVA-Hound-DPO" ["l"="47.428,30.187"]
"FanqingM/R1-Multimodal-Journey" ["l"="47.308,30.139"]
"vlf-silkie/VLFeedback" ["l"="47.281,30.239"]
"luogen1996/LLaVA-HR" ["l"="47.44,30.228"]
"luogen1996/RepAdapter" ["l"="50.203,38.237", "c"=684]
"luogen1996/SimREC" ["l"="49.12,31.875", "c"=300]
"seanzhuh/SeqTR" ["l"="49.029,31.903", "c"=300]
"IranQin/MP5" ["l"="41.004,-4.541", "c"=146]
"embodied-generalist/embodied-generalist" ["l"="65.084,11.742", "c"=203]
"ChenYi99/EgoPlan" ["l"="59.571,16.531", "c"=234]
"palchenli/VL-Instruction-Tuning" ["l"="47.263,30.362"]
"OpenGVLab/MM-Interleaved" ["l"="46.561,30.699", "c"=367]
"doc-doc/NExT-GQA" ["l"="47.85,33.05", "c"=373]
"minghangz/cpl" ["l"="48.047,33.108", "c"=373]
"mlvlab/Flipped-VQA" ["l"="-35.077,23.278", "c"=630]
"klauscc/VindLU" ["l"="47.796,32.981", "c"=373]
"showlab/mist" ["l"="47.823,33.104", "c"=373]
"jayleicn/singularity" ["l"="47.846,32.965", "c"=373]
"rainym00d/LLM4RS" ["l"="59.258,22.901", "c"=778]
"KID-22/LLM-IR-Bias-Fairness-Survey" ["l"="46.82,30.378"]
"Paranioar/UniPT" ["l"="32.793,31.114", "c"=109]
"JiazuoYu/PathWeave" ["l"="47.742,30.18"]
"IMCCretrieval/MomentDiff" ["l"="47.987,33.093", "c"=373]
"reactorsh/ambrosia" ["l"="47.451,30.612"]
"thunlp/Muffin" ["l"="47.289,30.212"]
"TengShi-RUC/UniSAR" ["l"="46.982,30.326"]
"rucliujn/JDsearch" ["l"="-2.534,8.893", "c"=1024]
"patrick-tssn/Awesome-Colorful-LLM" ["l"="47.77,30.142"]
"yuezih/Movie101" ["l"="47.377,30.521"]
"yuezih/SMILE" ["l"="47.382,30.485"]
"Hypotheses-Paradise/Hypo2Trans" ["l"="47.878,30.428"]
"YUCHEN005/RobustGER" ["l"="47.913,30.453"]
"CarperAI/treasure_trove" ["l"="47.475,30.575"]
"Alignment-Lab-AI/datagen" ["l"="47.452,30.565"]
"shiyi-zh0408/NAE_CVPR2024" ["l"="47.796,30.217"]
"MasterAI-EAM/GraphMaster" ["l"="46.956,30.426"]
"pengyu965/ChartDete" ["l"="46.959,30.443"]
"salesforce/HIVE" ["l"="33.175,31.384", "c"=109]
"KD-TAO/VidKV" ["l"="47.693,30.301"]
"jonathan-roberts1/GPT4GEO" ["l"="47.638,30.612"]
"jonathan-roberts1/charting-new-territories" ["l"="47.63,30.587"]
"jonathan-roberts1/SciFIBench" ["l"="47.65,30.638"]
"tdsone/extract-line-chart-data" ["l"="46.941,30.458"]
"Alpha-Innovator/SimChart9K" ["l"="47.068,30.272"]
"OpenGVLab/MMIU" ["l"="47.046,30.324"]
"kahnchana/mvu" ["l"="47.803,30.097"]
"khuangaf/ZeroFEC" ["l"="47.017,30.35"]
"khuangaf/CHOCOLATE" ["l"="47.037,30.337"]
"zhengbw0324/LC-Rec" ["l"="46.99,30.335"]
"E-qin/GEAR" ["l"="47.022,30.315"]
"jaeill/CVPR23-VNE" ["l"="47.733,30.011"]
"SNU-DRL/Attribution-ECG" ["l"="47.706,30.035"]
"Alpha-Innovator/TrustGeoGen" ["l"="47,30.256"]
"TencentQQGYLab/AppAgent" ["l"="40.729,0.325", "c"=7]
"FujiwaraChoki/MoneyPrinter" ["l"="40.638,-0.235", "c"=7]
"ml-explore/mlx" ["l"="40.274,0.591", "c"=7]
"apple/ml-ferret" ["l"="47.375,29.761"]
"OpenBMB/MiniCPM" ["l"="51.395,2.802", "c"=85]
"cumulo-autumn/StreamDiffusion" ["l"="33.639,30.968", "c"=109]
"ml-explore/mlx-examples" ["l"="27.528,-21.061", "c"=577]
"apple/corenet" ["l"="39.202,-0.546", "c"=39]
"stanfordnlp/dspy" ["l"="40.726,0.466", "c"=7]
"letta-ai/letta" ["l"="40.762,0.35", "c"=7]
"apple/ml-stable-diffusion" ["l"="27.398,-20.985", "c"=577]
"ShishirPatil/gorilla" ["l"="41.126,-3.931", "c"=146]
"QwenLM/Qwen3" ["l"="38.923,-1.766", "c"=202]
"karpathy/minbpe" ["l"="40.261,0.682", "c"=7]
"facebookresearch/DiT" ["l"="45.849,31.545", "c"=605]
"PKU-YuanGroup/Open-Sora-Plan" ["l"="33.524,31.053", "c"=109]
"Stability-AI/StableCascade" ["l"="33.544,30.971", "c"=109]
"hpcaitech/Open-Sora" ["l"="33.39,32.96", "c"=81]
"dvlab-research/MGM" ["l"="47.407,29.986"]
"facebookresearch/jepa" ["l"="49.119,30.148", "c"=191]
"mit-han-lab/streaming-llm" ["l"="38.853,-0.645", "c"=39]
"sgl-project/sglang" ["l"="38.933,-0.652", "c"=39]
"meta-llama/llama3" ["l"="40.155,0.388", "c"=7]
"om-ai-lab/VLM-R1" ["l"="47.323,30.014"]
"Deep-Agent/R1-V" ["l"="47.321,30.059"]
"hiyouga/EasyR1" ["l"="37.16,-0.534", "c"=126]
"ModalMinds/MM-EUREKA" ["l"="47.343,30.116"]
"fishaudio/fish-speech" ["l"="38.438,1.562", "c"=54]
"unslothai/unsloth" ["l"="40.533,0.197", "c"=7]
"stanford-oval/storm" ["l"="40.862,0.041", "c"=7]
"opendatalab/MinerU" ["l"="40.716,-0.19", "c"=7]
"FoundationAgents/MetaGPT" ["l"="40.43,0.067", "c"=7]
"2noise/ChatTTS" ["l"="38.426,1.48", "c"=54]
"google-deepmind/gemma" ["l"="38.97,-0.712", "c"=39]
"Genesis-Embodied-AI/Genesis" ["l"="59.161,16.789", "c"=234]
"google/gemma_pytorch" ["l"="38.988,-0.572", "c"=39]
"NVlabs/VILA" ["l"="47.441,30.034"]
"mit-han-lab/llm-awq" ["l"="38.87,-0.434", "c"=39]
"openvla/openvla" ["l"="59.342,16.633", "c"=234]
"baaivision/Emu3" ["l"="46.418,30.643", "c"=367]
"GoogleCloudPlatform/localllm" ["l"="40.581,-0.463", "c"=7]
"metavoiceio/metavoice-src" ["l"="38.601,1.733", "c"=54]
"YangLing0818/RPG-DiffusionMaster" ["l"="33.405,31.259", "c"=109]
"levihsu/OOTDiffusion" ["l"="33.575,30.956", "c"=109]
"Doubiiu/DynamiCrafter" ["l"="33.511,31.173", "c"=109]
"LLaVA-VL/LLaVA-Interactive-Demo" ["l"="47.502,30.311"]
"oumi-ai/oumi" ["l"="40.978,-0.074", "c"=7]
"FoundationVision/VAR" ["l"="51.232,2.921", "c"=85]
"wnlen/clash-for-linux" ["l"="-46.186,15.131", "c"=93]
"state-spaces/mamba" ["l"="49.202,34.162", "c"=556]
"TinyLLaVA/TinyLLaVA_Factory" ["l"="47.406,30.116"]
"BAAI-DCAI/Bunny" ["l"="47.39,30.104"]
"PKU-YuanGroup/LLaVA-CoT" ["l"="47.386,30.057"]
"01-ai/Yi" ["l"="39.031,-1.943", "c"=202]
"qinghew/StableIdentity" ["l"="32.89,31.156", "c"=109]
"hmxiong/StreamChat" ["l"="47.712,30.182"]
"I-S00N/I-S00N" ["l"="-45.605,-32.096", "c"=53]
"LetheSec/HuggingFace-Download-Accelerator" ["l"="37.086,-0.63", "c"=126]
"wdndev/llm_interview_note" ["l"="38.723,-2.075", "c"=202]
"khanrc/honeybee" ["l"="-5.315,-23.192", "c"=164]
"QwenLM/Qwen2.5-Coder" ["l"="36.309,-0.244", "c"=315]
"vikhyat/moondream" ["l"="40.74,0.374", "c"=7]
"AILab-CVC/UniRepLKNet" ["l"="49.061,32.948", "c"=401]
"BAAI-Agents/Cradle" ["l"="36.745,-1.61", "c"=795]
"IVGSZ/Flash-VStream" ["l"="47.693,30.192"]
"ziplab/LongVLM" ["l"="47.631,30.125"]
"TencentARC/ST-LLM" ["l"="47.608,30.132"]
"xk-huang/segment-caption-anything" ["l"="47.801,30.234"]
"EternalEvan/DPMesh" ["l"="47.771,30.23"]
"Tengbo-Yu/AnyBimanual" ["l"="47.753,30.222"]
"GuanxingLu/vlarl" ["l"="47.76,30.239"]
"Yxxxb/VoCo-LLaMA" ["l"="47.702,30.234"]
"shiyi-zh0408/FlexiAct" ["l"="47.832,30.213"]
"GuanxingLu/ManiGaussian" ["l"="59.551,16.646", "c"=234]
"zhang9302002/Flash-VStream" ["l"="47.779,30.206"]
"yongliu20/SCAN" ["l"="47.817,30.224"]
"databricks/dbrx" ["l"="38.828,-0.62", "c"=39]
"deepseek-ai/DeepSeek-VL" ["l"="39.042,-0.82", "c"=39]
"enoch3712/ExtractThinker" ["l"="41.326,0.363", "c"=7]
"Fuzzy-Search/realtime-bakllava" ["l"="47.506,30.431"]
"cognitivecomputations/laserRMT" ["l"="38.471,-0.19", "c"=39]
"DLYuanGod/TinyGPT-V" ["l"="47.424,30.061"]
"xmoanvaf/llava-phi" ["l"="47.417,30.103"]
"TRI-ML/prismatic-vlms" ["l"="59.418,16.641", "c"=234]
"chongzhou96/EdgeSAM" ["l"="48.723,29.949", "c"=191]
"YouHuang67/mamba-code-explained" ["l"="53.109,30.404", "c"=155]
"mbzuai-oryx/LLaVA-pp" ["l"="47.428,30.125"]
"zjysteven/lmms-finetune" ["l"="47.39,30.154"]
"xiaoachen98/Open-LLaVA-NeXT" ["l"="50.725,3.192", "c"=85]
"bfshi/scaling_on_scales" ["l"="47.42,30.207"]
"alibaba/conv-llava" ["l"="47.437,30.304"]
"HJYao00/DenseConnector" ["l"="47.583,30.244"]
"baaivision/EVE" ["l"="46.476,30.715", "c"=367]
"SHI-Labs/VCoder" ["l"="47.472,30.286"]
"aixcoder-plugin/aiXcoder-7B" ["l"="40.604,-0.313", "c"=7]
"AILab-CVC/YOLO-World" ["l"="48.743,30.053", "c"=191]
"baaivision/tokenize-anything" ["l"="48.655,30.158", "c"=191]
"InternLM/Tutorial" ["l"="38.779,-1.881", "c"=202]
"FoundationVision/GLEE" ["l"="48.677,30.134", "c"=191]
"gokayfem/awesome-vlm-architectures" ["l"="47.373,30.036"]
"gokayfem/ComfyUI_VLM_nodes" ["l"="32.663,32.815", "c"=81]
"jingyi0000/VLM_survey" ["l"="50.954,2.79", "c"=85]
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" ["l"="50.383,38.257", "c"=684]
"DAMO-NLP-SG/VideoLLaMA3" ["l"="47.523,30.108"]
"friedrichor/Awesome-Multimodal-Papers" ["l"="48.543,29.881", "c"=191]
"zli12321/Vision-Language-Models-Overview" ["l"="47.231,29.966"]
"NVlabs/VILA-archive" ["l"="47.258,30.073"]
"lqtrung1998/mwp_ReFT" ["l"="37.297,-0.47", "c"=126]
"adamcohenhillel/ADeus" ["l"="41.131,0.292", "c"=7]
"roboflow/awesome-openai-vision-api-experiments" ["l"="48.644,29.895", "c"=191]
"microsoft/SoM" ["l"="47.404,30.19"]
"AviSoori1x/makeMoE" ["l"="38.595,-0.41", "c"=39]
"Ucas-HaoranWei/Vary-toy" ["l"="47.285,30.125"]
"LingyvKong/OneChart" ["l"="47.152,30.148"]
"oh-my-ocr/text_renderer" ["l"="46.459,7.308", "c"=148]
"Tele-AI/Telechat" ["l"="38.966,-1.947", "c"=202]
"likejazz/llama3.np" ["l"="-5.257,-23.246", "c"=164]
"mustafaaljadery/llama3v" ["l"="47.236,30.05"]
"NishilBalar/Awesome-LVLM-Hallucination" ["l"="47.336,30.326"]
"LALBJ/PAI" ["l"="47.393,30.338"]
"yfzhang114/LLaVA-Align" ["l"="47.312,30.338"]
"Ziwei-Zheng/Nullu" ["l"="49.316,32.993", "c"=401]
"zjunlp/Deco" ["l"="47.495,30.375"]
"shengliu66/VTI" ["l"="47.353,30.36"]
"InternLM/HuixiangDou" ["l"="38.823,-1.869", "c"=202]
"gptscript-ai/gptscript" ["l"="40.934,0.247", "c"=7]
"SunzeY/AlphaCLIP" ["l"="48.796,30.337", "c"=191]
"yfzhang114/Awesome-Multimodal-Large-Language-Models" ["l"="47.35,30.165"]
"Kwai-YuanQi/MM-RLHF" ["l"="47.509,30.26"]
"yfzhang114/SliME" ["l"="47.447,30.245"]
"rongyaofang/GoT" ["l"="46.399,30.723", "c"=367]
"MME-Benchmarks/MME-RealWorld" ["l"="47.465,30.257"]
"modelscope/awesome-deep-reasoning" ["l"="47.26,30.152"]
"showlab/Awesome-Unified-Multimodal-Models" ["l"="46.419,30.688", "c"=367]
"zzli2022/Awesome-System2-Reasoning-LLM" ["l"="37.192,-0.543", "c"=126]
"microsoft/PhiCookBook" ["l"="-44.212,6.224", "c"=1131]
"baaivision/CapsFusion" ["l"="46.532,30.74", "c"=367]
"yihedeng9/STIC" ["l"="47.284,30.226"]
"luka-group/mDPO" ["l"="47.263,30.193"]
"scofield7419/Video-of-Thought" ["l"="47.777,32.115", "c"=1070]
"scofield7419/THOR-ISA" ["l"="51.344,-0.544", "c"=487]
"MaverickRen/PixelLM" ["l"="47.492,30.241"]
"AILab-CVC/SEED-X" ["l"="46.466,30.681", "c"=367]
"mit-han-lab/vila-u" ["l"="46.447,30.695", "c"=367]
"apple/ml-4m" ["l"="46.544,30.64", "c"=367]
"RUCAIBox/Virgo" ["l"="47.288,30.189"]
"ParadoxZW/LLaVA-UHD-Better" ["l"="47.414,30.247"]
"pkunlp-icler/FastV" ["l"="47.583,30.26"]
"NVlabs/LITA" ["l"="47.614,30.079"]
"gyxxyg/VTG-LLM" ["l"="47.614,30.099"]
"WHB139426/Grounded-Video-LLM" ["l"="47.647,30.083"]
"www-Ye/TimeZero" ["l"="47.528,30.132"]
"gyxxyg/TRACE" ["l"="47.634,30.07"]
"yellow-binary-tree/HawkEye" ["l"="47.596,30.094"]
"MILVLG/imp" ["l"="47.369,30.095"]
"OSU-NLP-Group/SeeAct" ["l"="36.785,-1.484", "c"=795]
"Flossiee/HonestyLLM" ["l"="47.528,29.955"]
"dvmazur/mixtral-offloading" ["l"="38.805,-0.477", "c"=39]
"RUCAIBox/LC-Rec" ["l"="59.256,22.875", "c"=778]
"Zhen-Tan-dmml/LLM4Annotation" ["l"="37.187,-0.429", "c"=126]
"letitiabanana/PnP-OVSS" ["l"="47.9,30.263"]
"slonetime/EBSeg" ["l"="47.875,30.25"]
"Xinjie-Q/GaussianImage" ["l"="-39.51,21.64", "c"=806]
"ChrisDong-THU/GaussianToken" ["l"="47.81,30.186"]
"ddupont808/GPT-4V-Act" ["l"="36.813,-1.531", "c"=795]
"NVlabs/RADIO" ["l"="64.088,2.918", "c"=49]
"apple/ml-aim" ["l"="46.514,30.627", "c"=367]
"apple/ml-mobileclip" ["l"="48.747,30.132", "c"=191]
"modelscope/evalscope" ["l"="39.032,-0.573", "c"=39]
"VikParuchuri/texify" ["l"="46.561,5.889", "c"=571]
"Ucas-HaoranWei/Vary-family" ["l"="47.168,30.119"]
"Ucas-HaoranWei/Vary-tiny-600k" ["l"="47.152,30.125"]
"Coobiw/MPP-LLaVA" ["l"="47.375,30.166"]
"ucaslcl/Fox" ["l"="47.115,30.133"]
"LeapLabTHU/GSVA" ["l"="49.299,32.932", "c"=401]
"zamling/PSALM" ["l"="47.498,30.224"]
"NJU-LHRS/LHRS-Bot" ["l"="41.723,25.141", "c"=525]
"xuliu-cyber/RSUniVLM" ["l"="47.573,30.433"]
"mira-space/MiraData" ["l"="33.596,31.403", "c"=109]
"MMStar-Benchmark/MMStar" ["l"="38.964,0.533", "c"=39]
"zzxslp/SoM-LLaVA" ["l"="47.312,30.19"]
"huangwl18/VoxPoser" ["l"="59.456,16.583", "c"=234]
"huangwl18/ReKep" ["l"="59.371,16.593", "c"=234]
"IDEA-Research/Grounding-DINO-1.5-API" ["l"="48.72,30.116", "c"=191]
"bytedance/tarsier" ["l"="47.579,30.148"]
"imagegridworth/IG-VLM" ["l"="47.667,30.078"]
"UX-Decoder/LLaVA-Grounding" ["l"="-55.506,-13.083", "c"=910]
"Meituan-AutoML/Lenna" ["l"="47.604,30.239"]
"lzw-lzw/GroundingGPT" ["l"="47.536,30.187"]
"IDEA-FinAI/ChartMoE" ["l"="47.438,30.195"]
"OpenGVLab/PonderV2" ["l"="65.724,11.155", "c"=1265]
"congvvc/HyperSeg" ["l"="33.362,31.758", "c"=109]
"berkeley-hipie/segllm" ["l"="47.532,30.328"]
"mc-lan/Text4Seg" ["l"="47.538,30.349"]
"shikiw/Awesome-MLLM-Hallucination" ["l"="38.283,-0.098", "c"=39]
"42Shawn/LLaVA-PruMerge" ["l"="47.651,30.251"]
"Gumpest/SparseVLMs" ["l"="47.638,30.269"]
"Cooperx521/PyramidDrop" ["l"="47.674,30.31"]
"dvlab-research/VisionZip" ["l"="47.628,30.253"]
"Theia-4869/FasterVLM" ["l"="47.616,30.266"]
"SUSTechBruce/LOOK-M" ["l"="47.652,30.326"]
"lzhxmu/VTW" ["l"="47.604,30.296"]
"Beckschen/LLaVolta" ["l"="47.614,30.31"]
"ywh187/FitPrune" ["l"="47.637,30.299"]
"mrwu-mac/ControlMLLM" ["l"="47.577,30.316"]
"liuting20/MustDrop" ["l"="39.096,0.153", "c"=39]
"yuezih/less-is-more" ["l"="47.387,30.433"]
"CircleRadon/Osprey" ["l"="64.134,11.38", "c"=61]
"HarborYuan/ovsam" ["l"="48.712,30.143", "c"=191]
"magic-research/Sa2VA" ["l"="50.832,3.039", "c"=85]
"deepcs233/Visual-CoT" ["l"="47.312,30.165"]
"dongyh20/Insight-V" ["l"="47.275,30.163"]
"RupertLuo/VoCoT" ["l"="47.244,30.175"]
"turningpoint-ai/VisualThinker-R1-Zero" ["l"="47.304,30.113"]
"chancharikmitra/CCoT" ["l"="46.285,5.799", "c"=571]
"ggg0919/cantor" ["l"="47.57,30.283"]
"zhourax/VEGA" ["l"="47.549,30.244"]
"MAC-AutoML/QuoTA" ["l"="47.545,30.267"]
"beichenzbc/Long-CLIP" ["l"="48.896,30.398", "c"=191]
"baaivision/DIVA" ["l"="47.261,30.211"]
"GAIR-NLP/MathPile" ["l"="37.527,-0.432", "c"=126]
"FreedomIntelligence/Apollo" ["l"="47.05,30.274"]
"FreedomIntelligence/ApolloMoE" ["l"="47.013,30.277"]
"FreedomIntelligence/CMB" ["l"="55.54,27.501", "c"=476]
"wangcunxiang/LLM-Factuality-Survey" ["l"="37.64,-6.93", "c"=766]
"OpenGVLab/video-mamba-suite" ["l"="48.112,33.987", "c"=168]
"Deaddawn/MovieLLM-code" ["l"="47.637,30.029"]
"sming256/OpenTAD" ["l"="48.068,33.977", "c"=168]
"gls0425/LinVT" ["l"="47.686,30.094"]
"findalexli/SciGraphQA" ["l"="51.477,-0.583", "c"=487]
"jun0wanan/awesome-large-multimodal-agents" ["l"="36.789,-1.398", "c"=795]
"yaotingwangofficial/Awesome-MCoT" ["l"="47.341,30.143"]
"Ucas-HaoranWei/Slow-Perception" ["l"="47.101,30.103"]
"1694439208/GOT-OCR-Inference" ["l"="47.067,30.114"]
"TencentARC/ViT-Lens" ["l"="33.425,31.75", "c"=109]
"ZrrSkywalker/I2P-MAE" ["l"="65.294,11.693", "c"=203]
"salesforce/ULIP" ["l"="65.237,11.688", "c"=203]
"EvolvingLMMs-Lab/EgoLife" ["l"="47.529,30.253"]
"EvolvingLMMs-Lab/multimodal-search-r1" ["l"="47.416,30.364"]
"EvolvingLMMs-Lab/Aero-1" ["l"="50.071,38.423", "c"=684]
"ZrrSkywalker/MathVerse" ["l"="47.224,30.392"]
"HZQ950419/Math-LLaVA" ["l"="47.204,30.409"]
"pipilurj/G-LLaVA" ["l"="64.964,12.242", "c"=203]
"ZrrSkywalker/MAVIS" ["l"="47.226,30.414"]
"liangyn22/MCUFormer" ["l"="47.858,30.274"]
"jzhzhang/NaVid-VLN-CE" ["l"="60.26,17.623", "c"=363]
"ChangyuanWang17/QVLM" ["l"="47.784,30.229"]
"OpenGVLab/PhyGenBench" ["l"="63.76,3.449", "c"=49]
"CeeZh/LLoVi" ["l"="47.701,30.13"]
"agentic-learning-ai-lab/lifelong-memory" ["l"="47.735,30.116"]
"jongwoopark7978/LVNet" ["l"="47.742,30.13"]
"opendatalab/HA-DPO" ["l"="47.237,30.204"]
"cocktailpeanut/mirror" ["l"="47.504,30.467"]
"OneInterface/chat-with-page" ["l"="47.516,30.497"]
"EternalEvan/FlowIE" ["l"="47.79,30.203"]
"YuzheZhang-1999/DiffTSR" ["l"="-35.203,21.164", "c"=127]
"YUCHEN005/GenTranslate" ["l"="47.933,30.448"]
"YUCHEN005/STAR-Adapt" ["l"="47.943,30.47"]
"baaivision/DenseFusion" ["l"="47.366,30.247"]
"Thinklab-SJTU/ML4CO-Kit" ["l"="51.035,26.507", "c"=490]
"Alpha-Innovator/SurveyForge" ["l"="46.962,30.224"]
"wusize/F-LMM" ["l"="47.552,30.326"]
"Shengcao-Cao/groundLMM" ["l"="47.559,30.348"]
"MICV-yonsei/CASS" ["l"="47.569,30.361"]
"RammusLeo/DPMesh" ["l"="47.786,30.242"]
"yuanzhoulvpi2017/vscode_debug_transformers" ["l"="47.322,30.4"]
"yuanzhoulvpi2017/zero_nlp" ["l"="39.013,-2.112", "c"=202]
"yongliu20/UniLSeg" ["l"="47.832,30.236"]
"NiuTrans/Vision-LLM-Alignment" ["l"="47.199,30.242"]
"njucckevin/MM-Self-Improve" ["l"="47.194,30.204"]
"shikiw/DAM-VP" ["l"="38.305,-0.105", "c"=39]
"yonseivnl/vlm-rlaif" ["l"="33.12,30.885", "c"=109]
"RUCAIBox/ComVint" ["l"="47.255,30.392"]
"llyx97/TempCompass" ["l"="47.669,30.155"]
"Becomebright/GroundVQA" ["l"="33.06,30.919", "c"=109]
"BAAI-DCAI/Training-Data-Synthesis" ["l"="47.199,30.269"]
"mertyg/vision-language-models-are-bows" ["l"="38.248,-0.115", "c"=39]
"bronyayang/Law_of_Vision_Representation_in_MLLMs" ["l"="46.347,30.725", "c"=367]
"DCDmllm/WorldGPT" ["l"="47.341,30.503"]
"mbzuai-oryx/VideoGLaMM" ["l"="50.492,38.311", "c"=684]
"hananshafi/llmblueprint" ["l"="50.534,38.33", "c"=684]
"showlab/VideoLISA" ["l"="33.367,31.698", "c"=109]
"mbzuai-oryx/ClimateGPT" ["l"="48.882,33.03", "c"=401]
"zli12321/qa_metrics" ["l"="47.187,29.944"]
"zli12321/VideoHallu" ["l"="47.202,29.953"]
"mistralai/megablocks-public" ["l"="38.669,-0.458", "c"=39]
"YingqingHe/Awesome-LLMs-meet-Multimodal-Generation" ["l"="46.379,30.698", "c"=367]
"yongchaoz/diffusion_inversion" ["l"="47.16,30.278"]
"JiazuoYu/MoE-Adapters4CL" ["l"="34.045,31.848", "c"=520]
"UX-Decoder/DINOv" ["l"="48.702,30.159", "c"=191]
"RL4VLM/RL4VLM" ["l"="36.734,-1.36", "c"=795]
"heshuting555/DsHmp" ["l"="47.633,34.539", "c"=1004]
"congvvc/LaSagnA" ["l"="33.344,31.774", "c"=109]
"apple/ml-slowfast-llava" ["l"="47.726,30.048"]
"sudo-Boris/mr-Blip" ["l"="48.04,33.152", "c"=373]
"wjun0830/CGDETR" ["l"="48.016,33.11", "c"=373]
"wjun0830/QD-DETR" ["l"="48.009,33.121", "c"=373]
"minghangz/TFVTG" ["l"="47.654,30.066"]
"Fish-and-Sheep/Text-Fluoroscopy" ["l"="47.803,30.436"]
"Dongping-Chen/MLLM-Judge" ["l"="47.58,29.908"]
"KainingYing/CTVIS" ["l"="50.693,30.683", "c"=83]
"OpenGVLab/PhyBench" ["l"="47.156,30.333"]
"mbzuai-oryx/MobiLlama" ["l"="39.001,-0.027", "c"=39]
"mbzuai-oryx/LlamaV-o1" ["l"="47.392,30.128"]
"THUDM/CogCoM" ["l"="47.125,30.213"]
"xb534/SED" ["l"="48.662,30.304", "c"=191]
"showlab/UniVTG" ["l"="60.48,17.581", "c"=363]
"Lackel/AGLA" ["l"="47.387,30.464"]
"Liuziyu77/RAR" ["l"="38.975,0.562", "c"=39]
"Liuziyu77/MMDU" ["l"="47.791,30.35"]
"Srijith-rkr/Whispering-LLaMA" ["l"="38.303,2.275", "c"=54]
"tpgh24/ag4masses" ["l"="46.988,30.057"]
"felixludos/alphageometry" ["l"="47.037,30.079"]
"foldl/AlphaGeometryRE" ["l"="46.965,30.048"]
"ModelTC/TFMQ-DM" ["l"="38.917,0.056", "c"=39]
"khuangaf/Awesome-Chart-Understanding" ["l"="47.08,30.302"]
"IDEA-FinAI/ChartBench" ["l"="47.057,30.296"]
"SpursGoZmy/Table-LLaVA" ["l"="46.293,24.819", "c"=1262]
"pengshuai-rin/MultiMath" ["l"="47.2,30.444"]
"XuChen0427/FairDiverse" ["l"="46.806,30.393"]
"KID-22/Cocktail" ["l"="46.791,30.383"]
"ichbill/LTDD" ["l"="47.684,30.274"]
"dongyh20/Chain-of-Spot" ["l"="47.18,30.159"]
"liuzuyan/ElasticCache" ["l"="47.131,30.169"]
"Yxxxb/LAVT-RS" ["l"="47.767,30.211"]
"kkk-an/COFFTEA" ["l"="47.372,30.381"]
"HankYe/Once-for-Both" ["l"="46.982,30.251"]
"teknium1/transformers-gptq-quant" ["l"="47.487,30.599"]
"wenhuang2000/VHTest" ["l"="47.287,30.32"]
"hendryx-scale/mhal-detect" ["l"="47.288,30.335"]
"kkahatapitiya/LangRepo" ["l"="47.749,30.11"]
"NJU-LHRS/ScoreRS" ["l"="47.62,30.559"]
"xiong-zhitong/GeoLB-SigLIP" ["l"="47.624,30.574"]
"LeapLabTHU/DAT-Jittor" ["l"="49.285,32.958", "c"=401]
"KID-22/Source-Bias" ["l"="46.772,30.387"]
"deepseek-ai/DeepSeek-Coder-V2" ["l"="39.05,-0.879", "c"=39]
"deepseek-ai/Janus" ["l"="40.156,-0.309", "c"=7]
"LMM101/Awesome-Multimodal-Next-Token-Prediction" ["l"="46.449,30.718", "c"=367]
"meta-llama/llama-models" ["l"="40.506,0.452", "c"=7]
"THUDM/CodeGeeX4" ["l"="39.142,-1.773", "c"=202]
"allenai/molmo" ["l"="47.175,30.359"]
"2U1/Molmo-Finetune" ["l"="47.151,30.394"]
"facebookresearch/perception_models" ["l"="64.107,3.01", "c"=49]
"ShiArthur03/ShiArthur03" ["l"="-4.134,23.691", "c"=827]
"siddrrsh/ambientGPT" ["l"="27.635,-20.94", "c"=577]
"facebookresearch/sam2" ["l"="48.908,29.989", "c"=191]
"jingyaogong/minimind-v" ["l"="38.736,-2.042", "c"=202]
"facebookresearch/MobileLLM" ["l"="38.877,-0.223", "c"=39]
"IVG-SZ/Flash-VStream" ["l"="47.736,30.201"]
"showlab/videollm-online" ["l"="47.64,30.159"]
"Mark12Ding/Dispider" ["l"="47.732,30.273"]
"Jixuan-Fan/Momentum-GS" ["l"="63.92,3.395", "c"=49]
"yaolinli/TimeChat-Online" ["l"="47.657,30.209"]
"jingyaogong/minimind" ["l"="38.662,-1.9", "c"=202]
"hijkzzz/Awesome-LLM-Strawberry" ["l"="37.136,-0.438", "c"=126]
"YueFan1014/VideoAgent" ["l"="47.62,30.189"]
"wxh1996/VideoAgent" ["l"="47.661,30.174"]
"Leon1207/Video-RAG-master" ["l"="47.566,30.238"]
"IDEA-Research/Grounded-SAM-2" ["l"="48.784,30.026", "c"=191]
"AIDC-AI/Marco-o1" ["l"="37.253,-0.466", "c"=126]
"facebookresearch/chameleon" ["l"="46.478,30.635", "c"=367]
"FoundationVision/LlamaGen" ["l"="46.44,30.619", "c"=367]
"microsoft/Magma" ["l"="59.306,16.546", "c"=234]
"hzwer/WritingAIPaper" ["l"="-3.966,23.557", "c"=827]
"lxa9867/Awesome-Autoregressive-Visual-Generation" ["l"="46.41,30.677", "c"=367]
"swordlidev/Evaluation-Multimodal-LLMs-Survey" ["l"="47.559,30.294"]
"shufangxun/LLaVA-MoD" ["l"="48.873,34.285", "c"=556]
"Wang-Xiaodong1899/CVPR25-MLLM-Paper-List" ["l"="47.44,30.174"]
"HJYao00/Side4Video" ["l"="47.616,30.282"]
"LaVi-Lab/AIM" ["l"="47.655,30.299"]
"NVlabs/EAGLE" ["l"="47.413,30.144"]
"Windsander/ADI-Stable-Diffusion" ["l"="50.812,2.921", "c"=85]
"ByungKwanLee/MoAI" ["l"="40.622,5.155", "c"=1285]
"dvlab-research/Step-DPO" ["l"="37.324,-0.361", "c"=126]
"test-time-training/ttt-lm-pytorch" ["l"="49.201,34.129", "c"=556]
"SimpleBerry/LLaMA-O1" ["l"="37.275,-0.448", "c"=126]
"16131zzzzzzzz/EveryoneNobel" ["l"="-3.998,23.622", "c"=827]
"JusticeFighterDance/JusticeFighter110" ["l"="47.202,30.051"]
"LongHZ140516/awesome-framework-gallery" ["l"="59.636,9.855", "c"=274]
"Mark12Ding/SAM2Long" ["l"="50.78,3.05", "c"=85]
"Davion-Liu/Awesome-Robustness-in-Information-Retrieval" ["l"="47.818,30.322"]
"pengboci/GraphRAG-Survey" ["l"="53.707,15.447", "c"=504]
"Tencent/Tencent-Hunyuan-Large" ["l"="37.014,-0.513", "c"=126]
"MoonshotAI/Kimi-VL" ["l"="47.261,30.105"]
"william-sto/JusticeNeverTooLate" ["l"="47.156,30.029"]
"ByteDance-Seed/Triton-distributed" ["l"="39.013,-0.386", "c"=39]
"KD-TAO/DyCoke" ["l"="47.665,30.288"]
"mlfoundations/MINT-1T" ["l"="47.362,30.186"]
"UCSC-VLAA/Recap-DataComp-1B" ["l"="49.046,30.556", "c"=191]
"DAMO-NLP-SG/multimodal_textbook" ["l"="64.083,11.461", "c"=61]
"CircleRadon/TokenPacker" ["l"="64.16,11.396", "c"=61]
"ZLKong/awesome-token-compression-reduction" ["l"="47.59,30.219"]
"xuyang-liu16/Awesome-Token-level-Model-Compression" ["l"="39.077,0.116", "c"=39]
"aburkov/theLMbook" ["l"="37.14,-0.588", "c"=126]
"byjlw/video-analyzer" ["l"="32.423,30.369", "c"=297]
"TideDra/zotero-arxiv-daily" ["l"="-4.052,23.583", "c"=827]
"JoeLeelyf/customize-arxiv-daily" ["l"="47.77,30.395"]
"ZiyuGuo99/SAM2Point" ["l"="65.124,11.71", "c"=203]
"Yushi-Hu/VisualSketchpad" ["l"="47.207,30.159"]
"longvideobench/LongVideoBench" ["l"="47.569,30.173"]
"Falling-dow/Unsupervised-Image-Enhancement-with-CNN-and-GAN" ["l"="50.8,3.125", "c"=85]
"THUNLP-MT/StreamingBench" ["l"="-55.462,-10.768", "c"=845]
"JoeLeelyf/OVO-Bench" ["l"="47.706,30.212"]
"showlab/livecc" ["l"="47.723,30.155"]
"zjunlp/OneGen" ["l"="47.529,30.412"]
"gordonhu608/MQT-LLaVA" ["l"="47.703,30.268"]
"MCG-NJU/p-MoD" ["l"="47.589,30.277"]
"zhangfaen/finetune-Qwen2-VL" ["l"="47.299,30.05"]
"2U1/Qwen2-VL-Finetune" ["l"="47.342,30.038"]
"wjbmattingly/qwen2-vl-finetune-huggingface" ["l"="47.229,30.017"]
"zhangfaen/finetune-Qwen2.5-VL" ["l"="47.284,30.036"]
"sandy1990418/Finetune-Qwen2.5-VL" ["l"="47.264,30.034"]
"VITA-MLLM/Long-VITA" ["l"="47.553,30.255"]
"saccharomycetes/mllms_know" ["l"="47.601,30.37"]
"FreedomIntelligence/LongLLaVA" ["l"="47.597,30.187"]
"yeliudev/VideoMind" ["l"="47.647,30.12"]
"VITA-MLLM/Sparrow" ["l"="47.535,30.275"]
"VITA-MLLM/VITA-Audio" ["l"="47.555,30.274"]
"yongliang-wu/NumPro" ["l"="38.473,-7.025", "c"=448]
"AIDC-AI/Ovis" ["l"="47.308,30.081"]
"Oryx-mllm/Oryx" ["l"="47.197,30.108"]
"AIDC-AI/Parrot" ["l"="34.313,31.923", "c"=520]
"Bujiazi/ByTheWay" ["l"="47.764,30.342"]
"beichenzbc/BoostStep" ["l"="47.749,30.342"]
"rhymes-ai/Aria" ["l"="47.496,30.113"]
"rhymes-ai/Allegro" ["l"="50.916,2.927", "c"=85]
"kijai/ComfyUI-MochiWrapper" ["l"="32.948,32.934", "c"=81]
"Alpha-Innovator/StructEqTable-Deploy" ["l"="46.451,5.959", "c"=571]
"Alpha-Innovator/DocGenome" ["l"="46.934,30.213"]
"THUDM/LVBench" ["l"="47.681,30.139"]
"rese1f/aurora" ["l"="47.773,30.089"]
"Espere-1119-Song/Video-MMLU" ["l"="47.803,30.077"]
"markusgrotz/peract_bimanual" ["l"="59.551,16.62", "c"=234]
"ManiCM-fast/ManiCM" ["l"="59.471,16.681", "c"=234]
"zjysteven/VLM-Visualizer" ["l"="47.644,30.438"]
"zhangbaijin/From-Redundancy-to-Relevance" ["l"="47.658,30.475"]
"IntelLabs/lvlm-interpret" ["l"="47.684,30.499"]
"junyangwang0410/Attention-LLaVA" ["l"="47.665,30.455"]
"jungao1106/ICoT" ["l"="47.223,30.176"]
"SCZwangxiao/video-FlexReduc" ["l"="47.674,30.117"]
"Liac-li/MM-self-improve-qwen2vl" ["l"="47.162,30.209"]
"2U1/Llama3.2-Vision-Finetune" ["l"="47.481,29.88"]
"Alpha-Innovator/Chimera" ["l"="46.929,30.231"]
"Alpha-Innovator/AdaptiveDiffusion" ["l"="46.945,30.234"]
"Alpha-Innovator/OmniCaptioner" ["l"="46.921,30.249"]
"Alpha-Innovator/Dolphin" ["l"="46.968,30.252"]
"opendatalab/OmniDocBench" ["l"="46.442,5.941", "c"=571]
"XJF2332/GOT-OCR-2-GUI" ["l"="47.01,30.093"]
"ustc-hyin/ClearSight" ["l"="47.352,30.392"]
"TianyunYoung/Hallucination-Attribution" ["l"="47.341,30.384"]
"Ola-Omni/Ola" ["l"="47.206,30.128"]
"shiml20/FlowTurbo" ["l"="47.145,30.088"]
"Yangsenqiao/ULDA" ["l"="47.664,30.267"]
"ictnlp/LLaVA-Mini" ["l"="-55.153,-10.684", "c"=845]
"microsoft/LLM2CLIP" ["l"="48.879,30.422", "c"=191]
"OpenGVLab/MM-NIAH" ["l"="47.267,30.253"]
"VisionXLab/STAR-MMRotate" ["l"="53.279,31.922", "c"=731]
"wangclnlp/DeepSpeed-Chat-Extension" ["l"="47.166,30.253"]
"NiuTrans/LaMaTE" ["l"="47.163,30.24"]
"CaraJ7/MMSearch" ["l"="47.284,30.421"]
"MME-Benchmarks/MME-CoT" ["l"="46.34,30.773", "c"=367]
"CaraJ7/CoMat" ["l"="33.47,31.548", "c"=109]
"CaraJ7/T2I-R1" ["l"="46.369,30.74", "c"=367]
"nickjiang2378/vl-interp" ["l"="47.71,30.541"]
"VITA-MLLM/Freeze-Omni" ["l"="38.405,2.046", "c"=54]
"NastyMarcus/A-Survey-of-Efficient-Diffusion-Models" ["l"="47.676,30.368"]
"rccchoudhury/rlt" ["l"="47.764,30.052"]
"Vchitect/FasterCache" ["l"="39.097,-0.046", "c"=39]
"contrastive/FreeVideoLLM" ["l"="47.77,30.029"]
"EdoardoBotta/RQ-VAE-Recommender" ["l"="59.27,22.819", "c"=778]
"UMass-Embodied-AGI/FlexAttention" ["l"="47.718,30.384"]
"hasanar1f/HiRED" ["l"="47.701,30.355"]
"kang-wu/SkySensePlusPlus" ["l"="47.576,29.88"]
"alipay/POA" ["l"="47.597,29.866"]
"likyoo/awesome-MLLM-for-image-segmentation" ["l"="47.547,29.901"]
"YuxiXie/V-DPO" ["l"="47.244,30.295"]
"2U1/Phi3-Vision-Finetune" ["l"="47.511,29.835"]
"GaiZhenbiao/Phi3V-Finetuning" ["l"="47.532,29.797"]
"2U1/Pixtral-Finetune" ["l"="47.504,29.855"]
"intsig-textin/parsex-frontend" ["l"="46.996,30.13"]
"intsig-textin/markdown_tester" ["l"="47.034,30.128"]
"intsig-textin/parsex-sdk" ["l"="46.978,30.12"]
"lutongyv/Textin_Tester" ["l"="47.009,30.117"]
"EleutherAI/delphi" ["l"="37.862,-6.941", "c"=766]
"EvolvingLMMs-Lab/multimodal-sae" ["l"="47.73,30.57"]
"jiaweizzhao/InRank" ["l"="65.145,3.409", "c"=49]
"gulucaptain/DynamiCtrl" ["l"="65.1,3.396", "c"=49]
"foundation-multimodal-models/ConBench" ["l"="53.527,33.364", "c"=1263]
"xuyang-liu16/GlobalCom2" ["l"="39.096,0.137", "c"=39]
"Liuziyu77/MIA-DPO" ["l"="47.726,30.339"]
"shikiw/Modality-Integration-Rate" ["l"="47.774,30.367"]
"Wiselnn570/VideoRoPE" ["l"="47.735,30.323"]
"Q-Future/Co-Instruct" ["l"="46.914,31.412", "c"=580]
"Amshaker/Mobile-VideoGPT" ["l"="48.869,33.013", "c"=401]
"joez17/VideoNIAH" ["l"="47.637,30.202"]
"huofushuo/C2KD" ["l"="38.316,-0.122", "c"=39]
"HowieHwong/UniGen" ["l"="47.559,29.919"]
"InfiMM/Awesome-Multimodal-LLM-for-Math-STEM" ["l"="47.183,30.481"]
"Visual-AI/PruneVid" ["l"="47.693,30.32"]
"songweii/DualToken" ["l"="47.839,30.362"]
"Bujiazi/HiFlow" ["l"="47.762,30.328"]
"DreamMr/HR-Bench" ["l"="47.614,30.404"]
"DreamMr/RAP" ["l"="47.62,30.425"]
"AIoT-MLSys-Lab/Famba-V" ["l"="47.692,30.398"]
"intsig-textin/chatdoc" ["l"="46.958,30.117"]
"ichbill/DQAS" ["l"="47.701,30.286"]
"1100111GTH/XG-RAG" ["l"="47.071,30.191"]
"modelscope/mcp-central" ["l"="47.09,30.187"]
"MiniMax-AI/MiniMax-01" ["l"="37.065,-0.545", "c"=126]
"Jiayi-Pan/TinyZero" ["l"="37.1,-0.574", "c"=126]
"hkust-nlp/simpleRL-reason" ["l"="37.177,-0.469", "c"=126]
"volcengine/verl" ["l"="37.143,-0.472", "c"=126]
"OpenRLHF/OpenRLHF" ["l"="37.161,-0.425", "c"=126]
"simplescaling/s1" ["l"="37.103,-0.537", "c"=126]
"MoonshotAI/Kimi-k1.5" ["l"="37.085,-0.504", "c"=126]
"tulerfeng/Video-R1" ["l"="47.365,30.117"]
"OpenGVLab/VideoChat-R1" ["l"="47.515,30.177"]
"dvlab-research/Seg-Zero" ["l"="47.362,30.136"]
"TencentARC/SEED-Bench-R1" ["l"="46.391,30.796", "c"=367]
"appletea233/Temporal-R1" ["l"="47.445,30.141"]
"HKUDS/VideoRAG" ["l"="59.48,22.953", "c"=778]
"OpenRLHF/OpenRLHF-M" ["l"="47.284,30.111"]
"LengSicong/MMR1" ["l"="47.263,30.13"]
"deepseek-ai/DeepSeek-VL2" ["l"="39.076,-0.814", "c"=39]
"ArcInstitute/evo2" ["l"="24.382,13.444", "c"=281]
"computerhistory/AlexNet-Source-Code" ["l"="39.165,-0.577", "c"=39]
"yu-rp/VisualPerceptionToken" ["l"="38.971,0.409", "c"=39]
"QwenLM/Qwen2.5-Omni" ["l"="38.436,1.868", "c"=54]
"HumanMLLM/R1-Omni" ["l"="47.284,30.092"]
"ictnlp/LLaMA-Omni2" ["l"="38.331,2.212", "c"=54]
"yfzhang114/r1_reward" ["l"="47.538,30.29"]
"MME-Benchmarks/MME-Unify" ["l"="47.525,30.295"]
"Open-Reasoner-Zero/Open-Reasoner-Zero" ["l"="37.209,-0.473", "c"=126]
"dhcode-cpp/X-R1" ["l"="37.164,-0.486", "c"=126]
"CodeGoat24/UnifiedReward" ["l"="33.557,31.514", "c"=109]
"NVlabs/describe-anything" ["l"="64.089,2.969", "c"=49]
"ali-vilab/UniAnimate-DiT" ["l"="33.063,33.138", "c"=81]
"zhuang2002/Cobra" ["l"="47.871,30.204"]
"Tencent/HunyuanCustom" ["l"="33.019,33.127", "c"=81]
"MoonshotAI/MoBA" ["l"="39.065,-0.332", "c"=39]
"facebookresearch/large_concept_model" ["l"="37.317,-0.585", "c"=126]
"RUCAIBox/Slow_Thinking_with_LLMs" ["l"="37.259,-0.484", "c"=126]
"MoonshotAI/Moonlight" ["l"="39.002,-0.403", "c"=39]
"openai/openai-realtime-agents" ["l"="41.198,0.13", "c"=7]
"mbzuai-oryx/Awesome-LLM-Post-training" ["l"="37.13,-0.517", "c"=126]
"XiaomiMiMo/MiMo" ["l"="37.168,-0.572", "c"=126]
"bruno686/Awesome-RL-based-LLM-Reasoning" ["l"="37.207,-0.592", "c"=126]
"modelscope/MCPBench" ["l"="47.03,30.201"]
"vision-x-nyu/thinking-in-space" ["l"="59.641,16.728", "c"=234]
"HumanMLLM/HumanOmni" ["l"="47.21,30.078"]
"ZebangCheng/Emotion-LLaMA" ["l"="56.488,28.114", "c"=940]
"UCSC-VLAA/VLAA-Thinking" ["l"="47.231,30.133"]
"mc-lan/Awesome-MLLM-Segmentation" ["l"="47.468,29.995"]
"cilinyan/VISA" ["l"="33.364,31.739", "c"=109]
"BytedTsinghua-SIA/DAPO" ["l"="37.201,-0.516", "c"=126]
"PhoenixZ810/OmniAlign-V" ["l"="-55.528,-10.757", "c"=845]
"Sunleader1997/transflow" ["l"="-55.323,-10.594", "c"=845]
"liyown/nextjs_stream_demo" ["l"="-55.343,-10.704", "c"=845]
"LightChen233/Awesome-Long-Chain-of-Thought-Reasoning" ["l"="37.201,-0.642", "c"=126]
"DAMO-NLP-SG/VideoRefer" ["l"="64.157,11.426", "c"=61]
"TencentARC/ColorFlow" ["l"="-35.231,20.156", "c"=1031]
"bcmi/Light-A-Video" ["l"="33.06,33.051", "c"=81]
"baichuan-inc/Baichuan-Omni-1.5" ["l"="38.239,2.212", "c"=54]
"fengzi258/Ocean-R1" ["l"="47.862,30.367"]
"TsinghuaC3I/Awesome-RL-Reasoning-Recipes" ["l"="37.19,-0.562", "c"=126]
"Video-R1/Awesome-Multimodal-Reasoning" ["l"="47.647,30.381"]
"Xilluill/KV-Edit" ["l"="33.164,31.232", "c"=109]
"yuyq96/R1-Vision" ["l"="47.238,30.098"]
"mbzuai-oryx/CVRR-Evaluation-Suite" ["l"="50.472,38.302", "c"=684]
"VIROBO-15/XM-GAN" ["l"="39.553,-7.487", "c"=232]
"ant-research/AniDoc" ["l"="-35.204,20.17", "c"=1031]
"ByteDance-Seed/Seed-Thinking-v1.5" ["l"="37.217,-0.551", "c"=126]
"MoonshotAI/Kimina-Prover-Preview" ["l"="-21.258,-19.151", "c"=752]
"ByteFlow-AI/TokenFlow" ["l"="46.433,30.686", "c"=367]
"Hongcheng-Gao/Awesome-Long2short-on-LRMs" ["l"="37.175,-0.649", "c"=126]
"Hui-design/R1-Video-fixbug" ["l"="47.332,30.157"]
"HumanMLLM/Omni-Emotion" ["l"="47.168,30.066"]
"MCG-NJU/VideoChat-Online" ["l"="47.613,30.208"]
"OpenGVLab/FluxViT" ["l"="47.562,30.195"]
"TencentARC/VideoPainter" ["l"="33.184,31.264", "c"=109]
"modelscope/Nexus-Gen" ["l"="47.053,30.181"]
"GeoGPT-Research-Project/GeoGPT" ["l"="47.58,29.855"]
"Aleafy/RelightVid" ["l"="33.1,32.978", "c"=81]
"John-AI-Lab/NoisyRollout" ["l"="36.747,-0.307", "c"=126]
"yellow-binary-tree/MMDuet" ["l"="47.773,30.285"]
"xinding-sys/StreamMind" ["l"="47.759,30.277"]
"maitrix-org/Voila" ["l"="38.272,2.231", "c"=54]
"0russwest0/Agent-R1" ["l"="37.262,-0.562", "c"=126]
"dle666/R-CoT" ["l"="47.051,30.065"]
"mingliangzhang2018/PGDP" ["l"="64.93,12.299", "c"=203]
"yayafengzi/LMM-HiMTok" ["l"="47.598,30.503"]
"TrustGen/TrustEval-toolkit" ["l"="38.222,-7.388", "c"=448]
"VoyageWang/IteRPrimE" ["l"="47.837,30.175"]
"jonathan-roberts1/zerobench" ["l"="47.658,30.662"]
"intsig-textin/chatdoc_stack" ["l"="46.942,30.114"]
"MCG-NJU/NeuralSolver" ["l"="46.297,30.764", "c"=367]
}