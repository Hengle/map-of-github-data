digraph G {
"facebookresearch/LAMA" -> "facebookresearch/KILT" ["e"=1]
"facebookresearch/LAMA" -> "facebookresearch/DPR" ["e"=1]
"facebookresearch/LAMA" -> "jzbjyb/LPAQA"
"facebookresearch/LAMA" -> "facebookresearch/XLM" ["e"=1]
"facebookresearch/LAMA" -> "facebookresearch/BLINK" ["e"=1]
"facebookresearch/LAMA" -> "ucinlp/autoprompt"
"facebookresearch/LAMA" -> "thunlp/ERNIE" ["e"=1]
"facebookresearch/LAMA" -> "atcbosselut/comet-commonsense" ["e"=1]
"facebookresearch/LAMA" -> "THUDM/P-tuning"
"facebookresearch/LAMA" -> "google-research/electra" ["e"=1]
"facebookresearch/LAMA" -> "facebookresearch/GENRE" ["e"=1]
"facebookresearch/LAMA" -> "princeton-nlp/LM-BFF"
"facebookresearch/LAMA" -> "nyu-mll/jiant" ["e"=1]
"facebookresearch/LAMA" -> "namisan/mt-dnn" ["e"=1]
"facebookresearch/LAMA" -> "danqi/acl2020-openqa-tutorial" ["e"=1]
"lixin4ever/Conference-Acceptance-Rate" -> "thunlp/PromptPapers" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "thunlp/PromptPapers" ["e"=1]
"strawberrypie/bert_adapter" -> "cs-mshah/Adapter-Bert"
"google-research/adapter-bert" -> "AsaCooperStickland/Bert-n-Pals"
"google-research/adapter-bert" -> "adapter-hub/adapters"
"google-research/adapter-bert" -> "strawberrypie/bert_adapter"
"google-research/adapter-bert" -> "jxhe/unify-parameter-efficient-tuning"
"google-research/adapter-bert" -> "XiangLi1999/PrefixTuning"
"google-research/adapter-bert" -> "clarkkev/attention-analysis" ["e"=1]
"google-research/adapter-bert" -> "facebookresearch/LAMA"
"google-research/adapter-bert" -> "microsoft/K-Adapter" ["e"=1]
"google-research/adapter-bert" -> "THUDM/P-tuning"
"google-research/adapter-bert" -> "princeton-nlp/LM-BFF"
"atcbosselut/comet-commonsense" -> "facebookresearch/LAMA" ["e"=1]
"thunlp/TAADpapers" -> "thunlp/PromptPapers" ["e"=1]
"g1910/HyperNetworks" -> "rabeehk/hyperformer" ["e"=1]
"commonsense/conceptnet5" -> "facebookresearch/LAMA" ["e"=1]
"adapter-hub/adapters" -> "jxhe/unify-parameter-efficient-tuning"
"adapter-hub/adapters" -> "thunlp/OpenDelta"
"adapter-hub/adapters" -> "XiangLi1999/PrefixTuning"
"adapter-hub/adapters" -> "thunlp/OpenPrompt"
"adapter-hub/adapters" -> "bigscience-workshop/promptsource" ["e"=1]
"adapter-hub/adapters" -> "huggingface/peft" ["e"=1]
"adapter-hub/adapters" -> "AGI-Edgerunners/LLM-Adapters"
"adapter-hub/adapters" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"adapter-hub/adapters" -> "thunlp/PromptPapers"
"adapter-hub/adapters" -> "makcedward/nlpaug" ["e"=1]
"adapter-hub/adapters" -> "THUDM/P-tuning-v2" ["e"=1]
"adapter-hub/adapters" -> "allenai/RL4LMs" ["e"=1]
"adapter-hub/adapters" -> "CarperAI/trlx" ["e"=1]
"adapter-hub/adapters" -> "timoschick/pet"
"adapter-hub/adapters" -> "princeton-nlp/SimCSE" ["e"=1]
"microsoft/DeBERTa" -> "timoschick/pet" ["e"=1]
"facebookresearch/BLINK" -> "facebookresearch/LAMA" ["e"=1]
"MLNLP-World/Top-AI-Conferences-Paper-with-Code" -> "thunlp/PromptPapers" ["e"=1]
"asheeshcric/awesome-contrastive-self-supervised-learning" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"marcotcr/checklist" -> "timoschick/pet" ["e"=1]
"uber-research/PPLM" -> "XiangLi1999/PrefixTuning" ["e"=1]
"NiuTrans/ABigSurvey" -> "thunlp/PromptPapers" ["e"=1]
"thunlp/PLMpapers" -> "thunlp/PromptPapers" ["e"=1]
"graph4ai/graph4nlp" -> "thunlp/PromptPapers" ["e"=1]
"xuyige/BERT4doc-Classification" -> "princeton-nlp/LM-BFF" ["e"=1]
"KaiyangZhou/mixstyle-release" -> "KaiyangZhou/on-device-dg" ["e"=1]
"google-research/big_transfer" -> "google-research/task_adaptation" ["e"=1]
"KaiyangZhou/Dassl.pytorch" -> "KaiyangZhou/CoOp" ["e"=1]
"KaiyangZhou/Dassl.pytorch" -> "gaopengcuhk/CLIP-Adapter" ["e"=1]
"KaiyangZhou/Dassl.pytorch" -> "muzairkhattak/multimodal-prompt-learning" ["e"=1]
"KaiyangZhou/Dassl.pytorch" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"KaiyangZhou/Dassl.pytorch" -> "KMnP/vpt" ["e"=1]
"timoschick/pet" -> "princeton-nlp/LM-BFF"
"timoschick/pet" -> "THUDM/P-tuning"
"timoschick/pet" -> "timoschick/fewglue"
"timoschick/pet" -> "rrmenon10/ADAPET"
"timoschick/pet" -> "XiangLi1999/PrefixTuning"
"timoschick/pet" -> "thunlp/OpenPrompt"
"timoschick/pet" -> "thunlp/PromptPapers"
"timoschick/pet" -> "princeton-nlp/SimCSE" ["e"=1]
"timoschick/pet" -> "ucinlp/autoprompt"
"timoschick/pet" -> "marcotcr/checklist" ["e"=1]
"timoschick/pet" -> "THUDM/P-tuning-v2" ["e"=1]
"timoschick/pet" -> "makcedward/nlpaug" ["e"=1]
"timoschick/pet" -> "google-research/multilingual-t5" ["e"=1]
"timoschick/pet" -> "bigscience-workshop/promptsource" ["e"=1]
"timoschick/pet" -> "tonyzhaozh/few-shot-learning"
"renatoviolin/next_word_prediction" -> "timoschick/pet" ["e"=1]
"facebookresearch/KILT" -> "facebookresearch/LAMA" ["e"=1]
"Eric-Wallace/universal-triggers" -> "ucinlp/autoprompt" ["e"=1]
"sangminwoo/awesome-vision-and-language" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"allenai/dont-stop-pretraining" -> "princeton-nlp/LM-BFF" ["e"=1]
"ucinlp/autoprompt" -> "princeton-nlp/LM-BFF"
"ucinlp/autoprompt" -> "THUDM/P-tuning"
"ucinlp/autoprompt" -> "XiangLi1999/PrefixTuning"
"ucinlp/autoprompt" -> "jzbjyb/LPAQA"
"ucinlp/autoprompt" -> "timoschick/pet"
"ucinlp/autoprompt" -> "Eric-Wallace/universal-triggers" ["e"=1]
"ucinlp/autoprompt" -> "princeton-nlp/OptiPrompt"
"ucinlp/autoprompt" -> "facebookresearch/LAMA"
"ucinlp/autoprompt" -> "thunlp/OpenPrompt"
"ucinlp/autoprompt" -> "tonyzhaozh/few-shot-learning"
"ucinlp/autoprompt" -> "kipgparker/soft-prompt-tuning"
"ucinlp/autoprompt" -> "thunlp/PromptPapers"
"ucinlp/autoprompt" -> "keirp/automatic_prompt_engineer" ["e"=1]
"ucinlp/autoprompt" -> "thunlp/OpenDelta"
"ucinlp/autoprompt" -> "google-research-datasets/KELM-corpus"
"bhoov/exbert" -> "facebookresearch/LAMA" ["e"=1]
"google-research/task_adaptation" -> "dongzelian/SSF"
"google-research/task_adaptation" -> "ZhangYuanhan-AI/NOAH"
"google-research/task_adaptation" -> "hjbahng/visual_prompting"
"google-research/task_adaptation" -> "KMnP/vpt"
"google-research/task_adaptation" -> "JieShibo/PETL-ViT"
"google-research/task_adaptation" -> "google-research/head2toe"
"yyhhenry/tank" -> "pufanyi/syphus"
"jzbjyb/LPAQA" -> "princeton-nlp/OptiPrompt"
"timoschick/fewglue" -> "timoschick/pet"
"jacobgil/vit-explain" -> "KMnP/vpt" ["e"=1]
"haltakov/natural-language-image-search" -> "yzhuoning/Awesome-CLIP" ["e"=1]
"princeton-nlp/SimCSE" -> "thunlp/OpenPrompt" ["e"=1]
"princeton-nlp/SimCSE" -> "thunlp/PromptPapers" ["e"=1]
"google-research/multilingual-t5" -> "timoschick/pet" ["e"=1]
"yuchenlin/rebiber" -> "thunlp/PromptPapers" ["e"=1]
"salesforce/ALBEF" -> "KaiyangZhou/CoOp" ["e"=1]
"luo3300612/Visualizer" -> "KaiyangZhou/CoOp" ["e"=1]
"luo3300612/Visualizer" -> "KMnP/vpt" ["e"=1]
"thunlp/PromptPapers" -> "thunlp/OpenPrompt"
"thunlp/PromptPapers" -> "Timothyxxx/Chain-of-ThoughtsPapers" ["e"=1]
"thunlp/PromptPapers" -> "princeton-nlp/SimCSE" ["e"=1]
"thunlp/PromptPapers" -> "thunlp/PLMpapers" ["e"=1]
"thunlp/PromptPapers" -> "XiangLi1999/PrefixTuning"
"thunlp/PromptPapers" -> "THUDM/P-tuning-v2" ["e"=1]
"thunlp/PromptPapers" -> "km1994/nlp_paper_study" ["e"=1]
"thunlp/PromptPapers" -> "THUDM/P-tuning"
"thunlp/PromptPapers" -> "bigscience-workshop/promptsource" ["e"=1]
"thunlp/PromptPapers" -> "thunlp/OpenDelta"
"thunlp/PromptPapers" -> "MLNLP-World/Paper-Writing-Tips" ["e"=1]
"thunlp/PromptPapers" -> "timoschick/pet"
"thunlp/PromptPapers" -> "yizhongw/self-instruct" ["e"=1]
"thunlp/PromptPapers" -> "princeton-nlp/LM-BFF"
"thunlp/PromptPapers" -> "CLUEbenchmark/CLUE" ["e"=1]
"sagizty/Insight" -> "sagizty/MAE"
"sagizty/Insight" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"sagizty/Insight" -> "sagizty/NTUS_application"
"sagizty/Insight" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/Insight" -> "sagizty/sagizty"
"sagizty/Insight" -> "sagizty/VPT"
"sagizty/Insight" -> "lsqqqq/notifyemail"
"google/BIG-bench" -> "thunlp/PromptPapers" ["e"=1]
"bigscience-workshop/promptsource" -> "thunlp/OpenPrompt" ["e"=1]
"bigscience-workshop/promptsource" -> "thunlp/PromptPapers" ["e"=1]
"bigscience-workshop/promptsource" -> "adapter-hub/adapters" ["e"=1]
"yangjianxin1/CPM" -> "TsinghuaAI/CPM" ["e"=1]
"yangjianxin1/CPM" -> "TsinghuaAI/CPM-2-Pretrain" ["e"=1]
"yangjianxin1/CPM" -> "BAAI-WuDao/Chinese-Transformer-XL" ["e"=1]
"yangjianxin1/CPM" -> "OpenBMB/BMInf" ["e"=1]
"lsqqqq/notifyemail" -> "sagizty/MAE"
"lsqqqq/notifyemail" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"XiangLi1999/PrefixTuning" -> "THUDM/P-tuning"
"XiangLi1999/PrefixTuning" -> "kipgparker/soft-prompt-tuning"
"XiangLi1999/PrefixTuning" -> "THUDM/P-tuning-v2" ["e"=1]
"XiangLi1999/PrefixTuning" -> "google-research/prompt-tuning"
"XiangLi1999/PrefixTuning" -> "princeton-nlp/LM-BFF"
"XiangLi1999/PrefixTuning" -> "thunlp/PromptPapers"
"XiangLi1999/PrefixTuning" -> "thunlp/OpenPrompt"
"XiangLi1999/PrefixTuning" -> "xlang-ai/UnifiedSKG" ["e"=1]
"XiangLi1999/PrefixTuning" -> "timoschick/pet"
"XiangLi1999/PrefixTuning" -> "jxhe/unify-parameter-efficient-tuning"
"XiangLi1999/PrefixTuning" -> "ucinlp/autoprompt"
"XiangLi1999/PrefixTuning" -> "mkshing/Prompt-Tuning"
"XiangLi1999/PrefixTuning" -> "jordiclive/ControlPrefixes"
"XiangLi1999/PrefixTuning" -> "adapter-hub/adapters"
"XiangLi1999/PrefixTuning" -> "allenai/natural-instructions" ["e"=1]
"RUCAIBox/TextBox" -> "XiangLi1999/PrefixTuning" ["e"=1]
"deepdialog/CPM-LM-TF2" -> "jm12138/CPM-Generate-Pytorch" ["e"=1]
"deepdialog/CPM-LM-TF2" -> "TsinghuaAI/CPM-KG" ["e"=1]
"deepdialog/CPM-LM-TF2" -> "OpenBMB/BMInf" ["e"=1]
"deepdialog/CPM-LM-TF2" -> "TsinghuaAI/CPM-2-Pretrain" ["e"=1]
"TsinghuaAI/CPM-2-Pretrain" -> "TsinghuaAI/CPM-2-Finetune"
"TsinghuaAI/CPM-2-Pretrain" -> "TsinghuaAI/CPM-1-Finetune"
"TsinghuaAI/CPM-2-Pretrain" -> "TsinghuaAI/CPM"
"TsinghuaAI/CPM-2-Pretrain" -> "BAAI-WuDao/P-tuning"
"OpenBMB/BMInf" -> "OpenBMB/BMCook"
"OpenBMB/BMInf" -> "OpenBMB/CPM-Live"
"OpenBMB/BMInf" -> "OpenBMB/BMTrain"
"OpenBMB/BMInf" -> "OpenBMB/BMList"
"OpenBMB/BMInf" -> "OpenBMB/ModelCenter"
"OpenBMB/BMInf" -> "TsinghuaAI/CPM"
"OpenBMB/BMInf" -> "TsinghuaAI/CPM-1-Generate" ["e"=1]
"OpenBMB/BMInf" -> "TsinghuaAI/CPM-2-Pretrain"
"OpenBMB/BMInf" -> "TsinghuaAI/CPM-2-Finetune"
"OpenBMB/BMInf" -> "deepdialog/CPM-LM-TF2" ["e"=1]
"OpenBMB/BMInf" -> "thunlp/OpenDelta"
"OpenBMB/BMInf" -> "thu-coai/EVA" ["e"=1]
"OpenBMB/BMInf" -> "CLUEbenchmark/pCLUE" ["e"=1]
"OpenBMB/BMInf" -> "yangjianxin1/CPM" ["e"=1]
"OpenBMB/BMInf" -> "hpcaitech/EnergonAI" ["e"=1]
"dandelin/ViLT" -> "KaiyangZhou/CoOp" ["e"=1]
"TsinghuaAI/CPM-1-Generate" -> "OpenBMB/BMInf" ["e"=1]
"hila-chefer/Transformer-MM-Explainability" -> "KaiyangZhou/CoOp" ["e"=1]
"Zasder3/train-CLIP" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"Zasder3/train-CLIP" -> "KaiyangZhou/CoOp" ["e"=1]
"Zasder3/train-CLIP" -> "gaopengcuhk/CLIP-Adapter" ["e"=1]
"bojone/P-tuning" -> "THUDM/P-tuning" ["e"=1]
"allenai/natural-instructions" -> "XiangLi1999/PrefixTuning" ["e"=1]
"CLUEbenchmark/FewCLUE" -> "rrmenon10/ADAPET" ["e"=1]
"yumingj/Talk-to-Edit" -> "ZhangYuanhan-AI/OmniBenchmark" ["e"=1]
"yumingj/Talk-to-Edit" -> "hongfz16/Garment4D" ["e"=1]
"yumingj/Talk-to-Edit" -> "Jiahao000/ORL" ["e"=1]
"GEM-benchmark/NL-Augmenter" -> "timoschick/pet" ["e"=1]
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/MAE"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/NTUS_application"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/sagizty"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/Insight"
"sagizty/Multi-Stage-Hybrid-Transformer" -> "sagizty/VPT"
"google-research-datasets/KELM-corpus" -> "wenhuchen/KGPT" ["e"=1]
"j-min/VL-T5" -> "ylsung/VL_adapter" ["e"=1]
"jxhe/unify-parameter-efficient-tuning" -> "ylsung/Ladder-Side-Tuning"
"jxhe/unify-parameter-efficient-tuning" -> "ylsung/VL_adapter"
"jxhe/unify-parameter-efficient-tuning" -> "ShoufaChen/AdaptFormer"
"jxhe/unify-parameter-efficient-tuning" -> "adapter-hub/adapters"
"jxhe/unify-parameter-efficient-tuning" -> "JieShibo/PETL-ViT"
"jxhe/unify-parameter-efficient-tuning" -> "XiangLi1999/PrefixTuning"
"jxhe/unify-parameter-efficient-tuning" -> "thunlp/OpenDelta"
"jxhe/unify-parameter-efficient-tuning" -> "dongzelian/SSF"
"jxhe/unify-parameter-efficient-tuning" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"jxhe/unify-parameter-efficient-tuning" -> "KMnP/vpt"
"jxhe/unify-parameter-efficient-tuning" -> "AkariAsai/ATTEMPT"
"jxhe/unify-parameter-efficient-tuning" -> "r-three/t-few"
"jxhe/unify-parameter-efficient-tuning" -> "AGI-Edgerunners/LLM-Adapters"
"jxhe/unify-parameter-efficient-tuning" -> "ZhangYuanhan-AI/NOAH"
"jxhe/unify-parameter-efficient-tuning" -> "fuzihaofzh/AnalyzeParameterEfficientFinetune"
"moein-shariatnia/OpenAI-CLIP" -> "yzhuoning/Awesome-CLIP" ["e"=1]
"huggingface/awesome-huggingface" -> "adapter-hub/adapters" ["e"=1]
"grandchicken/1708SEM_ISIM" -> "sagizty/MAE"
"grandchicken/1708SEM_ISIM" -> "sagizty/NTUS_application"
"grandchicken/1708SEM_ISIM" -> "sagizty/Parallel-Hybrid-Transformer"
"grandchicken/1708SEM_ISIM" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"kipgparker/soft-prompt-tuning" -> "mkshing/Prompt-Tuning"
"kipgparker/soft-prompt-tuning" -> "corolla-johnson/mkultra"
"kipgparker/soft-prompt-tuning" -> "XiangLi1999/PrefixTuning"
"kipgparker/soft-prompt-tuning" -> "google-research/prompt-tuning"
"kipgparker/soft-prompt-tuning" -> "Zeng-WH/Prompt-Tuning"
"kipgparker/soft-prompt-tuning" -> "qhduan/mt5-soft-prompt-tuning"
"kipgparker/soft-prompt-tuning" -> "exelents/soft-prompt-tuning"
"kipgparker/soft-prompt-tuning" -> "THUDM/P-tuning"
"kipgparker/soft-prompt-tuning" -> "txsun1997/Black-Box-Tuning" ["e"=1]
"kipgparker/soft-prompt-tuning" -> "hiaoxui/soft-prompts"
"kipgparker/soft-prompt-tuning" -> "thu-coai/PPT"
"kipgparker/soft-prompt-tuning" -> "thunlp/PTR"
"kipgparker/soft-prompt-tuning" -> "jordiclive/ControlPrefixes"
"kipgparker/soft-prompt-tuning" -> "AkariAsai/ATTEMPT"
"kipgparker/soft-prompt-tuning" -> "ucinlp/autoprompt"
"IntelLabs/academic-budget-bert" -> "princeton-nlp/DinkyTrain"
"princeton-nlp/LM-BFF" -> "ucinlp/autoprompt"
"princeton-nlp/LM-BFF" -> "timoschick/pet"
"princeton-nlp/LM-BFF" -> "XiangLi1999/PrefixTuning"
"princeton-nlp/LM-BFF" -> "THUDM/P-tuning"
"princeton-nlp/LM-BFF" -> "princeton-nlp/OptiPrompt"
"princeton-nlp/LM-BFF" -> "rrmenon10/ADAPET"
"princeton-nlp/LM-BFF" -> "tonyzhaozh/few-shot-learning"
"princeton-nlp/LM-BFF" -> "princeton-nlp/DensePhrases" ["e"=1]
"princeton-nlp/LM-BFF" -> "thunlp/PromptPapers"
"princeton-nlp/LM-BFF" -> "yym6472/ConSERT" ["e"=1]
"princeton-nlp/LM-BFF" -> "princeton-nlp/SimCSE" ["e"=1]
"princeton-nlp/LM-BFF" -> "zjunlp/DART" ["e"=1]
"princeton-nlp/LM-BFF" -> "thunlp/OpenPrompt"
"princeton-nlp/LM-BFF" -> "facebookresearch/DPR" ["e"=1]
"princeton-nlp/LM-BFF" -> "microsoft/ANCE" ["e"=1]
"DirtyHarryLYL/Transformer-in-Vision" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"princeton-nlp/DensePhrases" -> "princeton-nlp/LM-BFF" ["e"=1]
"THUDM/P-tuning" -> "THUDM/P-tuning-v2" ["e"=1]
"THUDM/P-tuning" -> "XiangLi1999/PrefixTuning"
"THUDM/P-tuning" -> "bojone/P-tuning" ["e"=1]
"THUDM/P-tuning" -> "timoschick/pet"
"THUDM/P-tuning" -> "princeton-nlp/LM-BFF"
"THUDM/P-tuning" -> "kipgparker/soft-prompt-tuning"
"THUDM/P-tuning" -> "ucinlp/autoprompt"
"THUDM/P-tuning" -> "thunlp/PromptPapers"
"THUDM/P-tuning" -> "google-research/prompt-tuning"
"THUDM/P-tuning" -> "THUDM/GLM" ["e"=1]
"THUDM/P-tuning" -> "thunlp/OpenPrompt"
"THUDM/P-tuning" -> "princeton-nlp/SimCSE" ["e"=1]
"THUDM/P-tuning" -> "bojone/Pattern-Exploiting-Training" ["e"=1]
"THUDM/P-tuning" -> "facebookresearch/LAMA"
"THUDM/P-tuning" -> "thunlp/OpenDelta"
"tonyzhaozh/few-shot-learning" -> "peterwestuw/surface-form-competition"
"tonyzhaozh/few-shot-learning" -> "princeton-nlp/LM-BFF"
"tonyzhaozh/few-shot-learning" -> "ethanjperez/true_few_shot"
"tonyzhaozh/few-shot-learning" -> "Alrope123/rethinking-demonstrations" ["e"=1]
"tonyzhaozh/few-shot-learning" -> "ucinlp/autoprompt"
"tonyzhaozh/few-shot-learning" -> "timoschick/pet"
"tonyzhaozh/few-shot-learning" -> "urvashik/knnlm" ["e"=1]
"TsinghuaAI/CPM-KG" -> "TsinghuaAI/CPM-1-Finetune"
"thunlp/PTR" -> "zjunlp/KnowPrompt"
"thunlp/PTR" -> "wzhouad/RE_improved_baseline" ["e"=1]
"thunlp/PTR" -> "thunlp/KnowledgeablePromptTuning"
"thu-coai/EVA" -> "BAAI-WuDao/EVA" ["e"=1]
"thu-coai/EVA" -> "TsinghuaAI/CPM" ["e"=1]
"BAAI-WuDao/Model" -> "BAAI-WuDao/CogView"
"yym6472/ConSERT" -> "princeton-nlp/LM-BFF" ["e"=1]
"BAAI-WuDao/CogView" -> "BAAI-WuDao/P-tuning"
"BAAI-WuDao/CogView" -> "BAAI-WuDao/CPM"
"TsinghuaAI/CPM" -> "TsinghuaAI/CPM-2-Finetune"
"TsinghuaAI/CPM" -> "TsinghuaAI/CPM-2-Pretrain"
"TsinghuaAI/CPM" -> "TsinghuaAI/CPM-1-Pretrain"
"TsinghuaAI/CPM" -> "OpenBMB/BMInf"
"TsinghuaAI/CPM" -> "thu-coai/EVA" ["e"=1]
"sagizty/Parallel-Hybrid-Transformer" -> "sagizty/NTUS_application"
"sagizty/Parallel-Hybrid-Transformer" -> "sagizty/MAE"
"sagizty/Parallel-Hybrid-Transformer" -> "sagizty/sagizty"
"corolla-johnson/mkultra" -> "mkshing/Prompt-Tuning"
"corolla-johnson/mkultra" -> "kipgparker/soft-prompt-tuning"
"corolla-johnson/mkultra" -> "exelents/soft-prompt-tuning"
"pfliu-nlp/NLPedia-Pretrain" -> "thunlp/PromptPapers" ["e"=1]
"pfliu-nlp/NLPedia-Pretrain" -> "THUDM/P-tuning" ["e"=1]
"rabeehk/hyperformer" -> "AkariAsai/ATTEMPT"
"rabeehk/hyperformer" -> "rabeehk/compacter"
"rabeehk/hyperformer" -> "McGill-NLP/polytropon"
"rabeehk/hyperformer" -> "ylsung/VL_adapter"
"Muzammal-Naseer/IPViT" -> "jameelhassan/PromptAlign" ["e"=1]
"rrmenon10/ADAPET" -> "ethanjperez/true_few_shot"
"TsinghuaAI/CPM-2-Finetune" -> "TsinghuaAI/CPM-2-Pretrain"
"TsinghuaAI/CPM-2-Finetune" -> "TsinghuaAI/CPM"
"thunlp/Prompt-Transferability" -> "RUCAIBox/Transfer-Prompts-for-Text-Generation"
"ethanjperez/true_few_shot" -> "rrmenon10/ADAPET"
"TsinghuaAI/CPM-1-Finetune" -> "jm12138/CPM-Generate-Pytorch"
"TsinghuaAI/CPM-1-Finetune" -> "BAAI-WuDao/EVA"
"TsinghuaAI/CPM-1-Finetune" -> "TsinghuaAI/CPM-KG"
"TsinghuaAI/CPM-1-Finetune" -> "TsinghuaAI/TDS"
"princeton-nlp/OptiPrompt" -> "princeton-nlp/EntityQuestions" ["e"=1]
"princeton-nlp/OptiPrompt" -> "princeton-nlp/DinkyTrain"
"princeton-nlp/OptiPrompt" -> "princeton-nlp/rationale-robustness"
"TsinghuaAI/CPM-1-Pretrain" -> "TsinghuaAI/TDS"
"TsinghuaAI/CPM-1-Pretrain" -> "TsinghuaAI/CPM-1-Distill"
"jm12138/CPM-Generate-Pytorch" -> "TsinghuaAI/CPM-1-Finetune"
"BAAI-WuDao/Chinese-Transformer-XL" -> "BAAI-WuDao/GLM"
"BAAI-WuDao/Chinese-Transformer-XL" -> "BAAI-WuDao/CPM"
"BAAI-WuDao/GLM" -> "BAAI-WuDao/Chinese-Transformer-XL"
"BAAI-WuDao/CPM" -> "BAAI-WuDao/Chinese-Transformer-XL"
"TsinghuaAI/TDS" -> "TsinghuaAI/CPM-1-Pretrain"
"thunlp/OpenPrompt" -> "thunlp/PromptPapers"
"thunlp/OpenPrompt" -> "thunlp/OpenDelta"
"thunlp/OpenPrompt" -> "bigscience-workshop/promptsource" ["e"=1]
"thunlp/OpenPrompt" -> "princeton-nlp/SimCSE" ["e"=1]
"thunlp/OpenPrompt" -> "THUDM/P-tuning-v2" ["e"=1]
"thunlp/OpenPrompt" -> "XiangLi1999/PrefixTuning"
"thunlp/OpenPrompt" -> "timoschick/pet"
"thunlp/OpenPrompt" -> "Timothyxxx/Chain-of-ThoughtsPapers" ["e"=1]
"thunlp/OpenPrompt" -> "CLUEbenchmark/CLUE" ["e"=1]
"thunlp/OpenPrompt" -> "LianjiaTech/BELLE" ["e"=1]
"thunlp/OpenPrompt" -> "THUDM/P-tuning"
"thunlp/OpenPrompt" -> "km1994/NLP-Interview-Notes" ["e"=1]
"thunlp/OpenPrompt" -> "km1994/nlp_paper_study" ["e"=1]
"thunlp/OpenPrompt" -> "dbiir/UER-py" ["e"=1]
"thunlp/OpenPrompt" -> "adapter-hub/adapters"
"THUDM/SwissArmyTransformer" -> "THUDM/P-tuning" ["e"=1]
"google-research/t5x" -> "google-research/prompt-tuning" ["e"=1]
"lucidrains/PaLM-pytorch" -> "princeton-nlp/LM-BFF" ["e"=1]
"OFA-Sys/OFA" -> "KaiyangZhou/CoOp" ["e"=1]
"isl-org/lang-seg" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"KaiyangZhou/CoOp" -> "muzairkhattak/multimodal-prompt-learning"
"KaiyangZhou/CoOp" -> "KMnP/vpt"
"KaiyangZhou/CoOp" -> "KaiyangZhou/Dassl.pytorch" ["e"=1]
"KaiyangZhou/CoOp" -> "gaopengcuhk/Tip-Adapter"
"KaiyangZhou/CoOp" -> "gaopengcuhk/CLIP-Adapter"
"KaiyangZhou/CoOp" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"KaiyangZhou/CoOp" -> "raoyongming/DenseCLIP" ["e"=1]
"KaiyangZhou/CoOp" -> "microsoft/GLIP" ["e"=1]
"KaiyangZhou/CoOp" -> "salesforce/ALBEF" ["e"=1]
"KaiyangZhou/CoOp" -> "muzairkhattak/PromptSRC"
"KaiyangZhou/CoOp" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs"
"KaiyangZhou/CoOp" -> "mlfoundations/open_clip" ["e"=1]
"KaiyangZhou/CoOp" -> "yzhuoning/Awesome-CLIP"
"KaiyangZhou/CoOp" -> "salesforce/BLIP" ["e"=1]
"KaiyangZhou/CoOp" -> "salesforce/LAVIS" ["e"=1]
"THUDM/P-tuning-v2" -> "THUDM/P-tuning" ["e"=1]
"THUDM/P-tuning-v2" -> "XiangLi1999/PrefixTuning" ["e"=1]
"THUDM/P-tuning-v2" -> "thunlp/OpenPrompt" ["e"=1]
"THUDM/P-tuning-v2" -> "thunlp/PromptPapers" ["e"=1]
"acl-org/acl-style-files" -> "XiangLi1999/PrefixTuning" ["e"=1]
"acl-org/acl-style-files" -> "thunlp/PromptPapers" ["e"=1]
"MLNLP-World/Paper-Writing-Tips" -> "thunlp/PromptPapers" ["e"=1]
"microsoft/GLIP" -> "KaiyangZhou/CoOp" ["e"=1]
"zhoubolei/bolei_awesome_posters" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"zhoubolei/bolei_awesome_posters" -> "KaiyangZhou/CoOp" ["e"=1]
"lucidrains/CoCa-pytorch" -> "KaiyangZhou/CoOp" ["e"=1]
"huggingface/evaluate" -> "adapter-hub/adapters" ["e"=1]
"Timothyxxx/Chain-of-ThoughtsPapers" -> "thunlp/PromptPapers" ["e"=1]
"Timothyxxx/Chain-of-ThoughtsPapers" -> "thunlp/OpenPrompt" ["e"=1]
"microsoft/RegionCLIP" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"lucidrains/RETRO-pytorch" -> "princeton-nlp/LM-BFF" ["e"=1]
"lucidrains/RETRO-pytorch" -> "r-three/t-few" ["e"=1]
"bigscience-workshop/t-zero" -> "r-three/t-few" ["e"=1]
"bigscience-workshop/t-zero" -> "AkariAsai/ATTEMPT" ["e"=1]
"sagizty/VPT" -> "sagizty/MAE"
"sagizty/VPT" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"sagizty/VPT" -> "sagizty/NTUS_application"
"sagizty/VPT" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/VPT" -> "sagizty/sagizty"
"sagizty/VPT" -> "sagizty/Insight"
"sagizty/VPT" -> "ZhangYuanhan-AI/NOAH"
"facebookresearch/SLIP" -> "KaiyangZhou/CoOp" ["e"=1]
"XiangLi1999/Diffusion-LM" -> "XiangLi1999/PrefixTuning" ["e"=1]
"rmokady/CLIP_prefix_caption" -> "KaiyangZhou/CoOp" ["e"=1]
"lucidrains/x-clip" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"zjunlp/PromptKG" -> "zjunlp/KnowPrompt" ["e"=1]
"Yutong-Zhou-cv/Awesome-Multimodality" -> "Yutong-Zhou-cv/Awesome-Transformer-in-CV"
"Yutong-Zhou-cv/Awesome-Multimodality" -> "wangxiao5791509/MultiModal_BigModels_Survey"
"thunlp/OpenDelta" -> "thunlp/OpenPrompt"
"thunlp/OpenDelta" -> "thunlp/DeltaPapers"
"thunlp/OpenDelta" -> "OpenBMB/BMTrain"
"thunlp/OpenDelta" -> "jxhe/unify-parameter-efficient-tuning"
"thunlp/OpenDelta" -> "thunlp/PromptPapers"
"thunlp/OpenDelta" -> "OpenBMB/CPM-Live"
"thunlp/OpenDelta" -> "adapter-hub/adapters"
"thunlp/OpenDelta" -> "txsun1997/LMaaS-Papers" ["e"=1]
"thunlp/OpenDelta" -> "txsun1997/Black-Box-Tuning" ["e"=1]
"thunlp/OpenDelta" -> "XiangLi1999/PrefixTuning"
"thunlp/OpenDelta" -> "OpenBMB/BMInf"
"thunlp/OpenDelta" -> "OpenBMB/ModelCenter"
"thunlp/OpenDelta" -> "THUDM/P-tuning"
"thunlp/OpenDelta" -> "princeton-nlp/LM-BFF"
"thunlp/OpenDelta" -> "thunlp/UltraChat" ["e"=1]
"MLNLP-World/AI-Paper-Collector" -> "thunlp/PromptPapers" ["e"=1]
"MLNLP-World/AI-Paper-Collector" -> "thunlp/OpenDelta" ["e"=1]
"czczup/ViT-Adapter" -> "ShoufaChen/AdaptFormer" ["e"=1]
"czczup/ViT-Adapter" -> "KMnP/vpt" ["e"=1]
"tim-learn/awesome-test-time-adaptation" -> "azshue/TPT" ["e"=1]
"alibaba/EasyNLP" -> "thunlp/OpenPrompt" ["e"=1]
"hustvl/MIMDet" -> "ShoufaChen/AdaptFormer" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"yzhuoning/Awesome-CLIP" -> "KaiyangZhou/CoOp"
"yzhuoning/Awesome-CLIP" -> "raoyongming/DenseCLIP" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "gaopengcuhk/CLIP-Adapter"
"yzhuoning/Awesome-CLIP" -> "Sense-GVT/DeCLIP" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "microsoft/GLIP" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "dk-liang/Awesome-Visual-Transformer" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "jingyi0000/VLM_survey" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "ShoufaChen/DiffusionDet" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "mlfoundations/open_clip" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "facebookresearch/SLIP" ["e"=1]
"yzhuoning/Awesome-CLIP" -> "gaopengcuhk/Tip-Adapter"
"yzhuoning/Awesome-CLIP" -> "TheShadow29/awesome-grounding" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "KMnP/vpt"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "muzairkhattak/multimodal-prompt-learning"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "KaiyangZhou/CoOp"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "yzhuoning/Awesome-CLIP"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "YuejiangLIU/awesome-source-free-test-time-adaptation" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "hjbahng/visual_prompting"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "gaopengcuhk/Tip-Adapter"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "DirtyHarryLYL/LLM-in-Vision" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "google-research/l2p" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "amirbar/visual_prompting" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "MarkMoHR/Awesome-Referring-Image-Segmentation" ["e"=1]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "ZhangYuanhan-AI/NOAH"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "JindongGu/Awesome-Prompting-on-Vision-Language-Model"
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers" ["e"=1]
"gaopengcuhk/Tip-Adapter" -> "gaopengcuhk/CLIP-Adapter"
"gaopengcuhk/Tip-Adapter" -> "OpenGVLab/CaFo"
"gaopengcuhk/Tip-Adapter" -> "KaiyangZhou/CoOp"
"gaopengcuhk/Tip-Adapter" -> "yangyangyang127/APE"
"gaopengcuhk/Tip-Adapter" -> "muzairkhattak/multimodal-prompt-learning"
"gaopengcuhk/Tip-Adapter" -> "linzhiqiu/cross_modal_adaptation"
"gaopengcuhk/Tip-Adapter" -> "raoyongming/DenseCLIP" ["e"=1]
"gaopengcuhk/Tip-Adapter" -> "sarahpratt/CuPL"
"gaopengcuhk/Tip-Adapter" -> "KMnP/vpt"
"gaopengcuhk/Tip-Adapter" -> "muzairkhattak/PromptSRC"
"gaopengcuhk/Tip-Adapter" -> "vishaal27/SuS-X"
"gaopengcuhk/Tip-Adapter" -> "CHENGY12/PLOT"
"gaopengcuhk/Tip-Adapter" -> "mrflogs/ICLR24" ["e"=1]
"gaopengcuhk/Tip-Adapter" -> "KaiyangZhou/Dassl.pytorch" ["e"=1]
"gaopengcuhk/Tip-Adapter" -> "ZiyuGuo99/CALIP"
"Langboat/Mengzi" -> "OpenBMB/CPM-Live" ["e"=1]
"mmaaz60/mvits_for_class_agnostic_od" -> "asif-hanif/vafa" ["e"=1]
"mmaaz60/mvits_for_class_agnostic_od" -> "Muhammad-Huzaifaa/ObjectCompose" ["e"=1]
"txsun1997/Black-Box-Tuning" -> "kipgparker/soft-prompt-tuning" ["e"=1]
"txsun1997/Black-Box-Tuning" -> "thunlp/Prompt-Transferability" ["e"=1]
"ShoufaChen/AdaptFormer" -> "dongzelian/SSF"
"ShoufaChen/AdaptFormer" -> "JieShibo/PETL-ViT"
"ShoufaChen/AdaptFormer" -> "KMnP/vpt"
"ShoufaChen/AdaptFormer" -> "ZhangYuanhan-AI/NOAH"
"ShoufaChen/AdaptFormer" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"ShoufaChen/AdaptFormer" -> "jxhe/unify-parameter-efficient-tuning"
"ShoufaChen/AdaptFormer" -> "hustvl/MIMDet" ["e"=1]
"ShoufaChen/AdaptFormer" -> "hjbahng/visual_prompting"
"ShoufaChen/AdaptFormer" -> "czczup/ViT-Adapter" ["e"=1]
"ShoufaChen/AdaptFormer" -> "Leiyi-Hu/mona"
"ShoufaChen/AdaptFormer" -> "luogen1996/RepAdapter"
"ShoufaChen/AdaptFormer" -> "synbol/Awesome-Parameter-Efficient-Transfer-Learning" ["e"=1]
"ShoufaChen/AdaptFormer" -> "gaopengcuhk/Tip-Adapter"
"Sense-GVT/DeCLIP" -> "yzhuoning/Awesome-CLIP" ["e"=1]
"Sense-GVT/DeCLIP" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"zdou0830/METER" -> "ylsung/VL_adapter" ["e"=1]
"raoyongming/DenseCLIP" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"raoyongming/DenseCLIP" -> "KaiyangZhou/CoOp" ["e"=1]
"raoyongming/DenseCLIP" -> "gaopengcuhk/CLIP-Adapter" ["e"=1]
"ylsung/VL_adapter" -> "ylsung/Ladder-Side-Tuning"
"ylsung/VL_adapter" -> "j-min/VL-T5" ["e"=1]
"ylsung/VL_adapter" -> "LeapLabTHU/Cross-Modal-Adapter" ["e"=1]
"ylsung/VL_adapter" -> "dongzelian/SSF"
"r-three/t-few" -> "bigscience-workshop/t-zero" ["e"=1]
"r-three/t-few" -> "AkariAsai/ATTEMPT"
"r-three/t-few" -> "microsoft/AdaMix"
"r-three/t-few" -> "ylsung/Ladder-Side-Tuning"
"r-three/t-few" -> "craffel/llm-seminar"
"r-three/t-few" -> "google-research/prompt-tuning"
"r-three/t-few" -> "jxhe/unify-parameter-efficient-tuning"
"r-three/t-few" -> "allenai/acl2022-zerofewshot-tutorial" ["e"=1]
"r-three/t-few" -> "txsun1997/Black-Box-Tuning" ["e"=1]
"r-three/t-few" -> "clinicalml/TabLLM" ["e"=1]
"r-three/t-few" -> "Shark-NLP/OpenICL" ["e"=1]
"r-three/t-few" -> "ylsung/VL_adapter"
"r-three/t-few" -> "tonyzhaozh/few-shot-learning"
"r-three/t-few" -> "facebookresearch/tart" ["e"=1]
"r-three/t-few" -> "thunlp/Prompt-Transferability"
"ylsung/Ladder-Side-Tuning" -> "ylsung/VL_adapter"
"ylsung/Ladder-Side-Tuning" -> "jxhe/unify-parameter-efficient-tuning"
"ylsung/Ladder-Side-Tuning" -> "dongzelian/SSF"
"ylsung/Ladder-Side-Tuning" -> "Paranioar/UniPT" ["e"=1]
"McGill-NLP/polytropon" -> "microsoft/mttl"
"mlfoundations/wise-ft" -> "gaopengcuhk/CLIP-Adapter" ["e"=1]
"mlfoundations/wise-ft" -> "LightDXY/FT-CLIP" ["e"=1]
"mlfoundations/wise-ft" -> "KaiyangZhou/CoOp" ["e"=1]
"mlfoundations/wise-ft" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"thunlp/KnowledgeablePromptTuning" -> "wjn1996/TransPrompt" ["e"=1]
"thunlp/KnowledgeablePromptTuning" -> "zjunlp/KnowPrompt"
"thunlp/KnowledgeablePromptTuning" -> "thunlp/PTR"
"thunlp/KnowledgeablePromptTuning" -> "thu-coai/PPT"
"thunlp/KnowledgeablePromptTuning" -> "kongds/Prompt-BERT" ["e"=1]
"declare-lab/RelationPrompt" -> "dinobby/ZS-BERT"
"declare-lab/RelationPrompt" -> "ssnvxia/OneRel" ["e"=1]
"xlang-ai/UnifiedSKG" -> "XiangLi1999/PrefixTuning" ["e"=1]
"sallymmx/ActionCLIP" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"wangxiao5791509/MultiModal_BigModels_Survey" -> "muzairkhattak/multimodal-prompt-learning"
"wangxiao5791509/MultiModal_BigModels_Survey" -> "Yutong-Zhou-cv/Awesome-Multimodality"
"gaopengcuhk/CLIP-Adapter" -> "gaopengcuhk/Tip-Adapter"
"gaopengcuhk/CLIP-Adapter" -> "KaiyangZhou/CoOp"
"gaopengcuhk/CLIP-Adapter" -> "OpenGVLab/CaFo"
"gaopengcuhk/CLIP-Adapter" -> "muzairkhattak/multimodal-prompt-learning"
"gaopengcuhk/CLIP-Adapter" -> "KaiyangZhou/Dassl.pytorch" ["e"=1]
"gaopengcuhk/CLIP-Adapter" -> "raoyongming/DenseCLIP" ["e"=1]
"gaopengcuhk/CLIP-Adapter" -> "linzhiqiu/cross_modal_adaptation"
"gaopengcuhk/CLIP-Adapter" -> "KMnP/vpt"
"gaopengcuhk/CLIP-Adapter" -> "yangyangyang127/APE"
"gaopengcuhk/CLIP-Adapter" -> "ylsung/VL_adapter"
"gaopengcuhk/CLIP-Adapter" -> "sarahpratt/CuPL"
"gaopengcuhk/CLIP-Adapter" -> "mlfoundations/wise-ft" ["e"=1]
"gaopengcuhk/CLIP-Adapter" -> "BeierZhu/Prompt-align"
"gaopengcuhk/CLIP-Adapter" -> "muzairkhattak/PromptSRC"
"gaopengcuhk/CLIP-Adapter" -> "CHENGY12/PLOT"
"tonyhuang2022/UPL" -> "BatsResearch/menghini-neurips23-code" ["e"=1]
"tonyhuang2022/UPL" -> "yuhangzang/UPT"
"tonyhuang2022/UPL" -> "korawat-tanwisuth/POUF"
"tonyhuang2022/UPL" -> "CEWu/PTNL"
"hongfz16/Garment4D" -> "hongfz16/HCMoCo"
"hpcaitech/EnergonAI" -> "OpenBMB/BMInf" ["e"=1]
"princeton-nlp/EntityQuestions" -> "princeton-nlp/OptiPrompt" ["e"=1]
"OpenBMB/CPM-Live" -> "OpenBMB/BMTrain"
"OpenBMB/CPM-Live" -> "OpenBMB/ModelCenter"
"OpenBMB/CPM-Live" -> "OpenBMB/BMCook"
"OpenBMB/CPM-Live" -> "OpenBMB/BMList"
"OpenBMB/CPM-Live" -> "OpenBMB/BMInf"
"OpenBMB/CPM-Live" -> "thunlp/OpenDelta"
"OpenBMB/CPM-Live" -> "thu-coai/EVA" ["e"=1]
"OpenBMB/CPM-Live" -> "thunlp/WebCPM" ["e"=1]
"OpenBMB/CPM-Live" -> "OpenBMB/CPM-Bee" ["e"=1]
"OpenBMB/CPM-Live" -> "Langboat/Mengzi" ["e"=1]
"kongds/Prompt-BERT" -> "thunlp/KnowledgeablePromptTuning" ["e"=1]
"kongds/Prompt-BERT" -> "princeton-nlp/LM-BFF" ["e"=1]
"google-research/prompt-tuning" -> "XiangLi1999/PrefixTuning"
"google-research/prompt-tuning" -> "kipgparker/soft-prompt-tuning"
"google-research/prompt-tuning" -> "mkshing/Prompt-Tuning"
"google-research/prompt-tuning" -> "THUDM/P-tuning"
"google-research/prompt-tuning" -> "THUDM/P-tuning-v2" ["e"=1]
"google-research/prompt-tuning" -> "thu-coai/PPT"
"google-research/prompt-tuning" -> "r-three/t-few"
"google-research/prompt-tuning" -> "AkariAsai/ATTEMPT"
"google-research/prompt-tuning" -> "jxhe/unify-parameter-efficient-tuning"
"google-research/prompt-tuning" -> "bigscience-workshop/t-zero" ["e"=1]
"google-research/prompt-tuning" -> "thunlp/Prompt-Transferability"
"google-research/prompt-tuning" -> "princeton-nlp/LM-BFF"
"google-research/prompt-tuning" -> "corolla-johnson/mkultra"
"google-research/prompt-tuning" -> "thunlp/OpenPrompt"
"google-research/prompt-tuning" -> "google-research/t5x" ["e"=1]
"OpenBMB/BMTrain" -> "OpenBMB/ModelCenter"
"OpenBMB/BMTrain" -> "OpenBMB/BMCook"
"OpenBMB/BMTrain" -> "OpenBMB/CPM-Live"
"OpenBMB/BMTrain" -> "OpenBMB/BMInf"
"OpenBMB/BMTrain" -> "OpenBMB/BMList"
"OpenBMB/BMTrain" -> "thunlp/OpenDelta"
"OpenBMB/ModelCenter" -> "OpenBMB/BMTrain"
"OpenBMB/ModelCenter" -> "OpenBMB/BMCook"
"OpenBMB/ModelCenter" -> "OpenBMB/CPM-Live"
"OpenBMB/ModelCenter" -> "OpenBMB/BMList"
"OpenBMB/ModelCenter" -> "OpenBMB/BMInf"
"princeton-nlp/DinkyTrain" -> "princeton-nlp/OptiPrompt"
"OpenGVLab/gv-benchmark" -> "ZhangYuanhan-AI/Bamboo" ["e"=1]
"YuejiangLIU/awesome-source-free-test-time-adaptation" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"hjbahng/visual_prompting" -> "KMnP/vpt"
"hjbahng/visual_prompting" -> "changdaeoh/BlackVIP"
"hjbahng/visual_prompting" -> "shikiw/DAM-VP" ["e"=1]
"hjbahng/visual_prompting" -> "amirbar/visual_prompting" ["e"=1]
"hjbahng/visual_prompting" -> "UCSC-VLAA/EVP"
"hjbahng/visual_prompting" -> "OPTML-Group/ILM-VP"
"hjbahng/visual_prompting" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"hjbahng/visual_prompting" -> "muzairkhattak/multimodal-prompt-learning"
"hjbahng/visual_prompting" -> "dongzelian/SSF"
"hjbahng/visual_prompting" -> "ZhangYuanhan-AI/NOAH"
"hjbahng/visual_prompting" -> "gaopengcuhk/Tip-Adapter"
"hjbahng/visual_prompting" -> "ShoufaChen/AdaptFormer"
"hjbahng/visual_prompting" -> "sagizty/VPT"
"hjbahng/visual_prompting" -> "yuhangzang/UPT"
"hjbahng/visual_prompting" -> "azshue/TPT"
"ZhangYuanhan-AI/Bamboo" -> "ZhangYuanhan-AI/OmniBenchmark"
"ZhangYuanhan-AI/Bamboo" -> "KaiyangZhou/on-device-dg"
"ZhangYuanhan-AI/Bamboo" -> "hongfz16/HCMoCo"
"ZhangYuanhan-AI/Bamboo" -> "hongfz16/Garment4D"
"AkariAsai/ATTEMPT" -> "ShiZhengyan/DePT"
"jiawei-ren/BalancedMSE" -> "hongfz16/HCMoCo" ["e"=1]
"jiawei-ren/BalancedMSE" -> "ZhangYuanhan-AI/OmniBenchmark" ["e"=1]
"sagizty/NTUS_application" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/NTUS_application" -> "sagizty/MAE"
"sagizty/NTUS_application" -> "sagizty/sagizty"
"sagizty/MAE" -> "sagizty/NTUS_application"
"sagizty/MAE" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/MAE" -> "sagizty/sagizty"
"sagizty/MAE" -> "sagizty/Multi-Stage-Hybrid-Transformer"
"thunlp/DeltaPapers" -> "thunlp/OpenDelta"
"thunlp/DeltaPapers" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"thunlp/DeltaPapers" -> "txsun1997/Black-Box-Tuning" ["e"=1]
"thunlp/DeltaPapers" -> "cambridgeltl/autopeft"
"thunlp/DeltaPapers" -> "calpt/awesome-adapter-resources"
"jordiclive/ControlPrefixes" -> "EagleW/Stage-wise-Fine-tuning"
"jordiclive/ControlPrefixes" -> "RUCAIBox/Transfer-Prompts-for-Text-Generation"
"zjunlp/KnowPrompt" -> "zjunlp/PromptKG" ["e"=1]
"zjunlp/KnowPrompt" -> "rtmaww/EntLM" ["e"=1]
"zjunlp/KnowPrompt" -> "thunlp/KnowledgeablePromptTuning"
"zjunlp/KnowPrompt" -> "thunlp/PTR"
"zjunlp/KnowPrompt" -> "declare-lab/RelationPrompt"
"zjunlp/KnowPrompt" -> "thu-coai/PPT"
"ju-chen/Efficient-Prompt" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"hongfz16/HCMoCo" -> "hongfz16/Garment4D"
"Arnav0400/ViT-Slim" -> "VITA-Group/UVC" ["e"=1]
"Arnav0400/ViT-Slim" -> "ZhangYuanhan-AI/NOAH"
"Arnav0400/ViT-Slim" -> "ziplab/SPT"
"Arnav0400/ViT-Slim" -> "dongzelian/SSF"
"Arnav0400/ViT-Slim" -> "luogen1996/RepAdapter"
"Arnav0400/ViT-Slim" -> "VITA-Group/SViTE" ["e"=1]
"mkshing/Prompt-Tuning" -> "kipgparker/soft-prompt-tuning"
"mkshing/Prompt-Tuning" -> "corolla-johnson/mkultra"
"mkshing/Prompt-Tuning" -> "Zeng-WH/Prompt-Tuning"
"mkshing/Prompt-Tuning" -> "google-research/prompt-tuning"
"OpenBMB/BMCook" -> "OpenBMB/ModelCenter"
"OpenBMB/BMCook" -> "OpenBMB/BMTrain"
"OpenBMB/BMCook" -> "OpenBMB/BMInf"
"OpenBMB/BMCook" -> "OpenBMB/BMList"
"OpenBMB/BMCook" -> "OpenBMB/CPM-Live"
"Zeng-WH/Prompt-Tuning" -> "mkshing/Prompt-Tuning"
"Jiahao000/ORL" -> "Jingkang50/EgoLife"
"BeierZhu/Prompt-align" -> "CHENGY12/PLOT"
"BeierZhu/Prompt-align" -> "BeierZhu/xERM"
"BeierZhu/Prompt-align" -> "muzairkhattak/PromptSRC"
"BeierZhu/Prompt-align" -> "yxymessi/H2E-Framework"
"rtmaww/EntLM" -> "zjunlp/KnowPrompt" ["e"=1]
"ShiZhengyan/StepGame" -> "ZhengxiangShi/LearnToAsk"
"ShiZhengyan/StepGame" -> "amzn/pretraining-or-self-training"
"ShiZhengyan/StepGame" -> "ShiZhengyan/PowerfulPromptFT"
"ShiZhengyan/StepGame" -> "Fangjun-Li/SpatialLM-StepGame"
"ShiZhengyan/StepGame" -> "ZhengxiangShi/SelfContrastiveLearningRecSys"
"lupantech/ScienceQA" -> "ylsung/VL_adapter" ["e"=1]
"promptslab/Promptify" -> "thunlp/OpenPrompt" ["e"=1]
"ShoufaChen/DiffusionDet" -> "KaiyangZhou/CoOp" ["e"=1]
"JunweiLiang/awesome_lists" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"promptslab/Awesome-Prompt-Engineering" -> "thunlp/OpenPrompt" ["e"=1]
"promptslab/Awesome-Prompt-Engineering" -> "thunlp/PromptPapers" ["e"=1]
"txsun1997/MOSS" -> "princeton-nlp/LM-BFF" ["e"=1]
"trigaten/Learn_Prompting" -> "thunlp/OpenPrompt" ["e"=1]
"mmaaz60/EdgeNeXt" -> "asif-hanif/vafa" ["e"=1]
"mmaaz60/EdgeNeXt" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"mmaaz60/EdgeNeXt" -> "Muhammad-Huzaifaa/ObjectCompose" ["e"=1]
"mmaaz60/EdgeNeXt" -> "asif-hanif/baple" ["e"=1]
"hanoonaR/object-centric-ovd" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"hanoonaR/object-centric-ovd" -> "Muhammad-Huzaifaa/ObjectCompose" ["e"=1]
"hanoonaR/object-centric-ovd" -> "mbzuai-oryx/CVRR-Evaluation-Suite" ["e"=1]
"hanoonaR/object-centric-ovd" -> "asif-hanif/vafa" ["e"=1]
"muzairkhattak/multimodal-prompt-learning" -> "muzairkhattak/PromptSRC"
"muzairkhattak/multimodal-prompt-learning" -> "KaiyangZhou/CoOp"
"muzairkhattak/multimodal-prompt-learning" -> "KMnP/vpt"
"muzairkhattak/multimodal-prompt-learning" -> "gaopengcuhk/Tip-Adapter"
"muzairkhattak/multimodal-prompt-learning" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"muzairkhattak/multimodal-prompt-learning" -> "zhengli97/PromptKD"
"muzairkhattak/multimodal-prompt-learning" -> "gaopengcuhk/CLIP-Adapter"
"muzairkhattak/multimodal-prompt-learning" -> "muzairkhattak/ViFi-CLIP"
"muzairkhattak/multimodal-prompt-learning" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs"
"muzairkhattak/multimodal-prompt-learning" -> "CHENGY12/PLOT"
"muzairkhattak/multimodal-prompt-learning" -> "muzairkhattak/ProText"
"muzairkhattak/multimodal-prompt-learning" -> "KaiyangZhou/Dassl.pytorch" ["e"=1]
"muzairkhattak/multimodal-prompt-learning" -> "jameelhassan/PromptAlign"
"muzairkhattak/multimodal-prompt-learning" -> "YiLunLee/missing_aware_prompts" ["e"=1]
"muzairkhattak/multimodal-prompt-learning" -> "azshue/TPT"
"muzairkhattak/ViFi-CLIP" -> "TalalWasim/Vita-CLIP" ["e"=1]
"muzairkhattak/ViFi-CLIP" -> "mbzuai-oryx/CVRR-Evaluation-Suite"
"muzairkhattak/ViFi-CLIP" -> "muzairkhattak/multimodal-prompt-learning"
"muzairkhattak/ViFi-CLIP" -> "ju-chen/Efficient-Prompt" ["e"=1]
"muzairkhattak/ViFi-CLIP" -> "sallymmx/ActionCLIP" ["e"=1]
"muzairkhattak/ViFi-CLIP" -> "whwu95/BIKE" ["e"=1]
"muzairkhattak/ViFi-CLIP" -> "jameelhassan/PromptAlign"
"muzairkhattak/ViFi-CLIP" -> "hanoonaR/object-centric-ovd" ["e"=1]
"muzairkhattak/ViFi-CLIP" -> "muzairkhattak/PromptSRC"
"muzairkhattak/ViFi-CLIP" -> "asif-hanif/vafa"
"muzairkhattak/ViFi-CLIP" -> "OpenGVLab/efficient-video-recognition" ["e"=1]
"muzairkhattak/ViFi-CLIP" -> "wengzejia1/Open-VCLIP" ["e"=1]
"muzairkhattak/ViFi-CLIP" -> "muzairkhattak/ProText"
"muzairkhattak/ViFi-CLIP" -> "mbzuai-oryx/VideoGLaMM"
"muzairkhattak/ViFi-CLIP" -> "TalalWasim/Video-FocalNets"
"linzhiqiu/cross_modal_adaptation" -> "OpenGVLab/CaFo"
"linzhiqiu/cross_modal_adaptation" -> "gaopengcuhk/Tip-Adapter"
"linzhiqiu/cross_modal_adaptation" -> "yangyangyang127/APE"
"linzhiqiu/cross_modal_adaptation" -> "gaopengcuhk/CLIP-Adapter"
"linzhiqiu/cross_modal_adaptation" -> "skingorz/FD-Align"
"linzhiqiu/cross_modal_adaptation" -> "muzairkhattak/multimodal-prompt-learning"
"linzhiqiu/cross_modal_adaptation" -> "sarahpratt/CuPL"
"linzhiqiu/cross_modal_adaptation" -> "mrflogs/SHIP" ["e"=1]
"linzhiqiu/cross_modal_adaptation" -> "KaiyangZhou/CoOp"
"linzhiqiu/cross_modal_adaptation" -> "vishaal27/SuS-X"
"linzhiqiu/cross_modal_adaptation" -> "YiLunLee/missing_aware_prompts" ["e"=1]
"linzhiqiu/cross_modal_adaptation" -> "ArmanAfrasiyabi/SetFeat-fs" ["e"=1]
"linzhiqiu/cross_modal_adaptation" -> "muzairkhattak/PromptSRC"
"linzhiqiu/cross_modal_adaptation" -> "sachit-menon/classify_by_description_release"
"linzhiqiu/cross_modal_adaptation" -> "LightDXY/FT-CLIP"
"amazon-science/auto-cot" -> "thunlp/PromptPapers" ["e"=1]
"amazon-science/auto-cot" -> "thunlp/OpenPrompt" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "KaiyangZhou/CoOp" ["e"=1]
"Computer-Vision-in-the-Wild/CVinW_Readings" -> "KMnP/vpt" ["e"=1]
"microsoft/prompt-engine" -> "thunlp/OpenPrompt" ["e"=1]
"MLNLP-World/Paper-Picture-Writing-Code" -> "thunlp/PromptPapers" ["e"=1]
"KMnP/vpt" -> "KaiyangZhou/CoOp"
"KMnP/vpt" -> "muzairkhattak/multimodal-prompt-learning"
"KMnP/vpt" -> "hjbahng/visual_prompting"
"KMnP/vpt" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"KMnP/vpt" -> "ShoufaChen/AdaptFormer"
"KMnP/vpt" -> "dongzelian/SSF"
"KMnP/vpt" -> "gaopengcuhk/Tip-Adapter"
"KMnP/vpt" -> "sagizty/VPT"
"KMnP/vpt" -> "ZhangYuanhan-AI/NOAH"
"KMnP/vpt" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"KMnP/vpt" -> "JieShibo/PETL-ViT"
"KMnP/vpt" -> "gaopengcuhk/CLIP-Adapter"
"KMnP/vpt" -> "KaiyangZhou/Dassl.pytorch" ["e"=1]
"KMnP/vpt" -> "microsoft/GLIP" ["e"=1]
"KMnP/vpt" -> "tim-learn/awesome-test-time-adaptation" ["e"=1]
"sauradip/STALE" -> "benedettaliberatori/T3AL" ["e"=1]
"dqxiu/ICL_PaperList" -> "thunlp/PromptPapers" ["e"=1]
"azshue/TPT" -> "jameelhassan/PromptAlign"
"azshue/TPT" -> "mr-eggplant/SAR" ["e"=1]
"azshue/TPT" -> "kdiAAA/TDA"
"azshue/TPT" -> "chunmeifeng/DiffTPT"
"azshue/TPT" -> "mzhaoshuai/RLCF"
"azshue/TPT" -> "zhangce01/DPE-CLIP"
"azshue/TPT" -> "muzairkhattak/PromptSRC"
"azshue/TPT" -> "CHENGY12/PLOT"
"azshue/TPT" -> "mrflogs/SHIP" ["e"=1]
"azshue/TPT" -> "ShuvenduRoy/CoPrompt"
"azshue/TPT" -> "DequanWang/tent" ["e"=1]
"azshue/TPT" -> "qinenergy/cotta" ["e"=1]
"azshue/TPT" -> "BeierZhu/Prompt-align"
"azshue/TPT" -> "FarinaMatteo/zero"
"azshue/TPT" -> "Bala93/CLIPCalib"
"sachit-menon/classify_by_description_release" -> "ExplainableML/WaffleCLIP"
"sachit-menon/classify_by_description_release" -> "sarahpratt/CuPL"
"sachit-menon/classify_by_description_release" -> "mertyg/vision-language-models-are-bows" ["e"=1]
"keirp/automatic_prompt_engineer" -> "ucinlp/autoprompt" ["e"=1]
"clinicalml/TabLLM" -> "r-three/t-few" ["e"=1]
"52CV/CVPR-2023-Papers" -> "KMnP/vpt" ["e"=1]
"dongzelian/SSF" -> "JieShibo/PETL-ViT"
"dongzelian/SSF" -> "ShoufaChen/AdaptFormer"
"dongzelian/SSF" -> "ZhangYuanhan-AI/NOAH"
"dongzelian/SSF" -> "ziplab/SPT"
"dongzelian/SSF" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"dongzelian/SSF" -> "DavidYanAnDe/ARC"
"dongzelian/SSF" -> "KMnP/vpt"
"dongzelian/SSF" -> "ylsung/VL_adapter"
"dongzelian/SSF" -> "luogen1996/RepAdapter"
"Jingkang50/OpenPSG" -> "EvolvingLMMs-Lab/RelateAnything" ["e"=1]
"SEU-COIN/LLMPapers" -> "thunlp/OpenDelta" ["e"=1]
"JieShibo/PETL-ViT" -> "ZhangYuanhan-AI/NOAH"
"JieShibo/PETL-ViT" -> "dongzelian/SSF"
"JieShibo/PETL-ViT" -> "ShoufaChen/AdaptFormer"
"JieShibo/PETL-ViT" -> "ylsung/VL_adapter"
"JieShibo/PETL-ViT" -> "ChengHan111/E2VPT"
"JieShibo/PETL-ViT" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"JieShibo/PETL-ViT" -> "linziyi96/st-adapter" ["e"=1]
"JieShibo/PETL-ViT" -> "KMnP/vpt"
"sarahpratt/CuPL" -> "vishaal27/SuS-X"
"sarahpratt/CuPL" -> "ExplainableML/WaffleCLIP"
"sarahpratt/CuPL" -> "yangyangyang127/APE"
"sarahpratt/CuPL" -> "sachit-menon/classify_by_description_release"
"sarahpratt/CuPL" -> "OpenGVLab/CaFo"
"sarahpratt/CuPL" -> "ZiyuGuo99/CALIP"
"sarahpratt/CuPL" -> "zhangce01/DualAdapter"
"sarahpratt/CuPL" -> "gaopengcuhk/Tip-Adapter"
"vishaal27/SuS-X" -> "ZiyuGuo99/CALIP"
"vishaal27/SuS-X" -> "yangyangyang127/APE"
"vishaal27/SuS-X" -> "sarahpratt/CuPL"
"vishaal27/SuS-X" -> "OpenGVLab/CaFo"
"htyao89/KgCoOp" -> "bbbdylan/proda"
"txsun1997/LMaaS-Papers" -> "thunlp/OpenDelta" ["e"=1]
"txsun1997/LMaaS-Papers" -> "thunlp/PromptPapers" ["e"=1]
"ZhangYuanhan-AI/NOAH" -> "JieShibo/PETL-ViT"
"ZhangYuanhan-AI/NOAH" -> "dongzelian/SSF"
"ZhangYuanhan-AI/NOAH" -> "ZhangYuanhan-AI/OmniBenchmark"
"ZhangYuanhan-AI/NOAH" -> "sagizty/VPT"
"ZhangYuanhan-AI/NOAH" -> "ziplab/SPT"
"ZhangYuanhan-AI/NOAH" -> "luogen1996/RepAdapter"
"ZhangYuanhan-AI/NOAH" -> "KaiyangZhou/on-device-dg"
"ZhangYuanhan-AI/NOAH" -> "ShoufaChen/AdaptFormer"
"ZhangYuanhan-AI/NOAH" -> "Arnav0400/ViT-Slim"
"ZhangYuanhan-AI/NOAH" -> "KMnP/vpt"
"ZhangYuanhan-AI/NOAH" -> "Jiahao000/ORL"
"saiboxx/chexray-diffusion" -> "BioMedIA-MBZUAI/XReal" ["e"=1]
"HazyResearch/ama_prompting" -> "tonyzhaozh/few-shot-learning" ["e"=1]
"thunlp/BMCourse" -> "OpenBMB/ModelCenter"
"thunlp/BMCourse" -> "OpenBMB/BMInf"
"OpenGVLab/efficient-video-recognition" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"ZiyuGuo99/CALIP" -> "vishaal27/SuS-X"
"ZiyuGuo99/CALIP" -> "yangyangyang127/APE"
"ZiyuGuo99/CALIP" -> "Ivan-Tang-3D/ViewRefer3D" ["e"=1]
"ZiyuGuo99/CALIP" -> "OpenGVLab/CaFo"
"taoyang1122/adapt-image-models" -> "JieShibo/PETL-ViT" ["e"=1]
"taoyang1122/adapt-image-models" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"hananshafi/vits-for-small-scale-datasets" -> "Muhammad-Huzaifaa/ObjectCompose" ["e"=1]
"OpenBMB/BMList" -> "OpenBMB/BMCook"
"OpenBMB/BMList" -> "OpenBMB/CPM-Live"
"OpenBMB/BMList" -> "OpenBMB/ModelCenter"
"OpenBMB/BMList" -> "OpenBMB/BMInf"
"OpenBMB/BMList" -> "OpenBMB/BMTrain"
"mariodoebler/test-time-adaptation" -> "azshue/TPT" ["e"=1]
"mr-eggplant/SAR" -> "azshue/TPT" ["e"=1]
"mr-eggplant/SAR" -> "kdiAAA/TDA" ["e"=1]
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "dongzelian/SSF"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "synbol/Awesome-Parameter-Efficient-Transfer-Learning" ["e"=1]
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "ShoufaChen/AdaptFormer"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "JieShibo/PETL-ViT"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "thunlp/DeltaPapers"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "KMnP/vpt"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "ylsung/VL_adapter"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "calpt/awesome-adapter-resources"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "WillDreamer/Aurora"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "jxhe/unify-parameter-efficient-tuning"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "YuejiangLIU/awesome-source-free-test-time-adaptation" ["e"=1]
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "luogen1996/RepAdapter"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "muzairkhattak/multimodal-prompt-learning"
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" -> "ZhangYuanhan-AI/NOAH"
"FrozenBurning/Text2Light" -> "hongfz16/Garment4D" ["e"=1]
"Computer-Vision-in-the-Wild/DataDownload" -> "Computer-Vision-in-the-Wild/Elevater_Toolkit_IC"
"CHENGY12/PLOT" -> "BeierZhu/Prompt-align"
"CHENGY12/PLOT" -> "htyao89/KgCoOp"
"CHENGY12/PLOT" -> "azshue/TPT"
"CHENGY12/PLOT" -> "BatsResearch/csp" ["e"=1]
"CHENGY12/PLOT" -> "ShuvenduRoy/CoPrompt"
"CHENGY12/PLOT" -> "muzairkhattak/multimodal-prompt-learning"
"amirbar/visual_prompting" -> "hjbahng/visual_prompting" ["e"=1]
"amirbar/visual_prompting" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"eric-ai-lab/PEViT" -> "Computer-Vision-in-the-Wild/Elevater_Toolkit_IC"
"sagizty/sagizty" -> "sagizty/NTUS_application"
"sagizty/sagizty" -> "sagizty/Parallel-Hybrid-Transformer"
"sagizty/sagizty" -> "sagizty/MAE"
"luogen1996/RepAdapter" -> "ZhangYuanhan-AI/NOAH"
"luogen1996/RepAdapter" -> "DavidYanAnDe/ARC"
"KaihuaTang/Generalized-Long-Tailed-Benchmarks.pytorch" -> "yxymessi/H2E-Framework" ["e"=1]
"craffel/llm-seminar" -> "r-three/t-few"
"craffel/llm-seminar" -> "r-three/git-theta"
"LightDXY/FT-CLIP" -> "sarahpratt/CuPL"
"microsoft/mttl" -> "McGill-NLP/polytropon"
"Computer-Vision-in-the-Wild/Elevater_Toolkit_IC" -> "Computer-Vision-in-the-Wild/DataDownload"
"Computer-Vision-in-the-Wild/Elevater_Toolkit_IC" -> "ZhangYuanhan-AI/OmniBenchmark"
"ZhangYuanhan-AI/OmniBenchmark" -> "Jiahao000/ORL"
"ZhangYuanhan-AI/OmniBenchmark" -> "KaiyangZhou/on-device-dg"
"ZhangYuanhan-AI/OmniBenchmark" -> "ZhangYuanhan-AI/Bamboo"
"ZhangYuanhan-AI/OmniBenchmark" -> "Jingkang50/EgoLife"
"muzairkhattak/PromptSRC" -> "muzairkhattak/multimodal-prompt-learning"
"muzairkhattak/PromptSRC" -> "muzairkhattak/ProText"
"muzairkhattak/PromptSRC" -> "ShuvenduRoy/CoPrompt"
"muzairkhattak/PromptSRC" -> "zhengli97/PromptKD"
"muzairkhattak/PromptSRC" -> "jameelhassan/PromptAlign"
"muzairkhattak/PromptSRC" -> "TalalWasim/Video-FocalNets"
"muzairkhattak/PromptSRC" -> "BeierZhu/Prompt-align"
"muzairkhattak/PromptSRC" -> "Koorye/DePT"
"muzairkhattak/PromptSRC" -> "azshue/TPT"
"muzairkhattak/PromptSRC" -> "mrflogs/SHIP" ["e"=1]
"muzairkhattak/PromptSRC" -> "ZjjConan/VLM-MultiModalAdapter"
"muzairkhattak/PromptSRC" -> "tonyhuang2022/UPL"
"muzairkhattak/PromptSRC" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs"
"muzairkhattak/PromptSRC" -> "ThomasWangY/2024-AAAI-HPT"
"muzairkhattak/PromptSRC" -> "CHENGY12/PLOT"
"diffusion-classifier/diffusion-classifier" -> "ZhangYuanhan-AI/NOAH" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "uncbiag/Awesome-Foundation-Models"
"awaisrauf/Awesome-CV-Foundational-Models" -> "mbzuai-oryx/groundingLMM" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "DirtyHarryLYL/LLM-in-Vision" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "asif-hanif/vafa"
"awaisrauf/Awesome-CV-Foundational-Models" -> "muzairkhattak/PromptSRC"
"awaisrauf/Awesome-CV-Foundational-Models" -> "muzairkhattak/ViFi-CLIP"
"awaisrauf/Awesome-CV-Foundational-Models" -> "mbzuai-oryx/Video-LLaVA" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "xmindflow/Awesome-Foundation-Models-in-Medical-Imaging" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "yzhuoning/Awesome-CLIP"
"awaisrauf/Awesome-CV-Foundational-Models" -> "jianzongwu/Awesome-Open-Vocabulary" ["e"=1]
"awaisrauf/Awesome-CV-Foundational-Models" -> "mbzuai-oryx/ClimateGPT" ["e"=1]
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs"
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "DirtyHarryLYL/LLM-in-Vision" ["e"=1]
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "KaiyangZhou/CoOp"
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "jingyi0000/VLM_survey" ["e"=1]
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "uncbiag/Awesome-Foundation-Models"
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "xmed-lab/CLIP_Surgery" ["e"=1]
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" -> "muzairkhattak/multimodal-prompt-learning"
"Shark-NLP/OpenICL" -> "r-three/t-few" ["e"=1]
"cs-mshah/Adapter-Bert" -> "strawberrypie/bert_adapter"
"fahadshamshad/Clip2Protect" -> "mbzuai-oryx/CVRR-Evaluation-Suite" ["e"=1]
"fahadshamshad/Clip2Protect" -> "HashmatShadab/MambaRobustness" ["e"=1]
"tomhartke/knowledge-graph-from-GPT" -> "zjunlp/KnowPrompt" ["e"=1]
"huytransformer/Awesome-Out-Of-Distribution-Detection" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" ["e"=1]
"OpenBMB/VisCPM" -> "OpenBMB/BMTrain" ["e"=1]
"Jun-CEN/SegmentAnyRGBD" -> "EvolvingLMMs-Lab/RelateAnything" ["e"=1]
"EvolvingLMMs-Lab/RelateAnything" -> "Jingkang50/OpenPSG" ["e"=1]
"EvolvingLMMs-Lab/RelateAnything" -> "Nicous20/FunQA"
"EvolvingLMMs-Lab/RelateAnything" -> "hongfz16/HCMoCo"
"EvolvingLMMs-Lab/RelateAnything" -> "hongfz16/Garment4D"
"EvolvingLMMs-Lab/RelateAnything" -> "kaleido-lab/dolphin"
"EvolvingLMMs-Lab/RelateAnything" -> "ZhangYuanhan-AI/Bamboo"
"EvolvingLMMs-Lab/RelateAnything" -> "Jun-CEN/SegmentAnyRGBD" ["e"=1]
"EvolvingLMMs-Lab/RelateAnything" -> "Kenneth-Wong/MMSceneGraph" ["e"=1]
"EvolvingLMMs-Lab/RelateAnything" -> "JialianW/GRiT" ["e"=1]
"EvolvingLMMs-Lab/RelateAnything" -> "ZhangYuanhan-AI/OmniBenchmark"
"EvolvingLMMs-Lab/RelateAnything" -> "omniobject3d/OmniObject3D" ["e"=1]
"EvolvingLMMs-Lab/RelateAnything" -> "jiawei-ren/diffmimic" ["e"=1]
"EvolvingLMMs-Lab/RelateAnything" -> "Jiahao000/ORL"
"TonyLianLong/LLM-groundedDiffusion" -> "tsunghan-wu/SLD" ["e"=1]
"DmitryRyumin/ICCV-2023-Papers" -> "KMnP/vpt" ["e"=1]
"JamesQFreeman/LoRA-ViT" -> "ShoufaChen/AdaptFormer" ["e"=1]
"JamesQFreeman/LoRA-ViT" -> "KMnP/vpt" ["e"=1]
"JamesQFreeman/LoRA-ViT" -> "eric-ai-lab/PEViT" ["e"=1]
"showlab/Image2Paragraph" -> "EvolvingLMMs-Lab/RelateAnything" ["e"=1]
"cliangyu/Cola" -> "pufanyi/syphus"
"Nicous20/FunQA" -> "pufanyi/syphus"
"Nicous20/FunQA" -> "Jingkang50/EgoLife"
"Amshaker/SwiftFormer" -> "asif-hanif/vafa" ["e"=1]
"YiLunLee/missing_aware_prompts" -> "muzairkhattak/multimodal-prompt-learning" ["e"=1]
"OpenGVLab/CaFo" -> "gaopengcuhk/Tip-Adapter"
"OpenGVLab/CaFo" -> "yangyangyang127/APE"
"OpenGVLab/CaFo" -> "gaopengcuhk/CLIP-Adapter"
"OpenGVLab/CaFo" -> "vishaal27/SuS-X"
"OpenGVLab/CaFo" -> "ZiyuGuo99/CALIP"
"OpenGVLab/CaFo" -> "linzhiqiu/cross_modal_adaptation"
"OpenGVLab/CaFo" -> "sarahpratt/CuPL"
"OpenGVLab/CaFo" -> "mrflogs/ICLR24" ["e"=1]
"OpenGVLab/CaFo" -> "ZrrSkywalker/CaFo"
"OpenGVLab/CaFo" -> "CHENGY12/PLOT"
"OpenGVLab/CaFo" -> "geekyutao/TaskRes"
"OpenGVLab/CaFo" -> "skingorz/FD-Align"
"OpenGVLab/CaFo" -> "Tsingularity/FRN" ["e"=1]
"OpenGVLab/CaFo" -> "ShuvenduRoy/CoPrompt"
"OpenGVLab/CaFo" -> "jusiro/CLAP" ["e"=1]
"mbzuai-oryx/ClimateGPT" -> "asif-hanif/vafa" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"DirtyHarryLYL/LLM-in-Vision" -> "JindongGu/Awesome-Prompting-on-Vision-Language-Model" ["e"=1]
"Ma-Lab-Berkeley/CRATE" -> "KMnP/vpt" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "OpenGVLab/LLaMA-Adapter" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "adapter-hub/adapters"
"AGI-Edgerunners/LLM-Adapters" -> "jxhe/unify-parameter-efficient-tuning"
"AGI-Edgerunners/LLM-Adapters" -> "NVlabs/DoRA" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "AetherCortex/Llama-X" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "HillZhang1999/llm-hallucination-survey" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "SinclairCoder/Instruction-Tuning-Papers" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "GanjinZero/RRHF" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "thunlp/OpenDelta"
"AGI-Edgerunners/LLM-Adapters" -> "PhoebusSi/Alpaca-CoT" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "FranxYao/chain-of-thought-hub" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "zjunlp/EasyEdit" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "Instruction-Tuning-with-GPT-4/GPT-4-LLM" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "Timothyxxx/Chain-of-ThoughtsPapers" ["e"=1]
"AGI-Edgerunners/LLM-Adapters" -> "allenai/open-instruct" ["e"=1]
"altndrr/vic" -> "altndrr/lmms-owc"
"altndrr/vic" -> "tdemin16/multi-lane"
"altndrr/vic" -> "FarinaMatteo/zero"
"AGI-Edgerunners/Plan-and-Solve-Prompting" -> "AGI-Edgerunners/LLM-Adapters" ["e"=1]
"xmed-lab/CLIP_Surgery" -> "OpenGVLab/CaFo" ["e"=1]
"xmed-lab/CLIP_Surgery" -> "azshue/TPT" ["e"=1]
"xmed-lab/CLIP_Surgery" -> "muzairkhattak/PromptSRC" ["e"=1]
"TalalWasim/Vita-CLIP" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"kaleido-lab/dolphin" -> "Nicous20/FunQA"
"kaleido-lab/dolphin" -> "Jiahao000/ORL"
"kaleido-lab/dolphin" -> "EvolvingLMMs-Lab/RelateAnything"
"uncbiag/Awesome-Foundation-Models" -> "awaisrauf/Awesome-CV-Foundational-Models"
"uncbiag/Awesome-Foundation-Models" -> "JindongGu/Awesome-Prompting-on-Vision-Language-Model"
"uncbiag/Awesome-Foundation-Models" -> "Jianing-Qiu/Awesome-Healthcare-Foundation-Models" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "EdisonLeeeee/Awesome-Masked-Autoencoders" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "jingyi0000/VLM_survey" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "jianzongwu/Awesome-Open-Vocabulary" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "liliu-avril/Awesome-Segment-Anything" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "Hedlen/awesome-segment-anything" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "Computer-Vision-in-the-Wild/CVinW_Readings" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "richard-peng-xia/awesome-multimodal-in-medical-imaging" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"uncbiag/Awesome-Foundation-Models" -> "NVlabs/RADIO" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "robotics-survey/Awesome-Robotics-Foundation-Models" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "xmindflow/Awesome-Foundation-Models-in-Medical-Imaging" ["e"=1]
"uncbiag/Awesome-Foundation-Models" -> "yzhuoning/Awesome-CLIP"
"ryongithub/GatedPromptTuning" -> "WangYZ1608/Self-Prompt-Tuning"
"ChengHan111/E2VPT" -> "ryongithub/GatedPromptTuning"
"luogen1996/LaVIN" -> "luogen1996/RepAdapter" ["e"=1]
"yangyangyang127/APE" -> "ZiyuGuo99/CALIP"
"yangyangyang127/APE" -> "vishaal27/SuS-X"
"yangyangyang127/APE" -> "OpenGVLab/CaFo"
"yangyangyang127/APE" -> "gaopengcuhk/Tip-Adapter"
"yangyangyang127/APE" -> "sarahpratt/CuPL"
"yangyangyang127/APE" -> "mlvlab/RPO"
"yangyangyang127/APE" -> "mrflogs/ICLR24" ["e"=1]
"ExplainableML/WaffleCLIP" -> "Zhiyuan-R/ChatGPT-Powered-Hierarchical-Comparisons-for-Image-Classification"
"FarinaMatteo/qmmf" -> "tdemin16/Continual-LayerNorm-Tuning"
"ShiZhengyan/DePT" -> "ShiZhengyan/PowerfulPromptFT"
"ShiZhengyan/DePT" -> "ZhengxiangShi/LearnToAsk"
"ShiZhengyan/DePT" -> "amzn/pretraining-or-self-training"
"ShiZhengyan/DePT" -> "ShiZhengyan/StepGame"
"ShiZhengyan/DePT" -> "ZhengxiangShi/SelfContrastiveLearningRecSys"
"ShiZhengyan/DePT" -> "ZhengxiangShi/InstructionModelling"
"calpt/awesome-adapter-resources" -> "UKPLab/adaptable-adapters"
"calpt/awesome-adapter-resources" -> "cambridgeltl/autopeft"
"calpt/awesome-adapter-resources" -> "jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning"
"Koorye/DePT" -> "Koorye/SkipTuning"
"Koorye/DePT" -> "muzairkhattak/PromptSRC"
"chunmeifeng/DiffTPT" -> "kdiAAA/TDA"
"chunmeifeng/DiffTPT" -> "chunmeifeng/FedIns"
"ShiZhengyan/PowerfulPromptFT" -> "ZhengxiangShi/LearnToAsk"
"ShiZhengyan/PowerfulPromptFT" -> "amzn/pretraining-or-self-training"
"ShiZhengyan/PowerfulPromptFT" -> "ShiZhengyan/StepGame"
"ShiZhengyan/PowerfulPromptFT" -> "ShiZhengyan/DePT"
"ziplab/SPT" -> "DavidYanAnDe/ARC"
"ziplab/SPT" -> "WangYZ1608/Self-Prompt-Tuning"
"WillDreamer/Aurora" -> "RERV/UniAdapter"
"WillDreamer/Aurora" -> "UniAdapter/UniAdapter"
"TalalWasim/Video-FocalNets" -> "muzairkhattak/PromptSRC"
"TalalWasim/Video-FocalNets" -> "asif-hanif/vafa"
"ZhengxiangShi/SelfContrastiveLearningRecSys" -> "ZhengxiangShi/LearnToAsk"
"amzn/pretraining-or-self-training" -> "ZhengxiangShi/LearnToAsk"
"amzn/pretraining-or-self-training" -> "ShiZhengyan/PowerfulPromptFT"
"asif-hanif/vafa" -> "asif-hanif/baple"
"asif-hanif/vafa" -> "Muhammad-Huzaifaa/ObjectCompose"
"Luodian/GenBench" -> "pufanyi/syphus"
"tdemin16/Continual-LayerNorm-Tuning" -> "FarinaMatteo/qmmf"
"pufanyi/syphus" -> "Jingkang50/EgoLife"
"gokayfem/awesome-vlm-architectures" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" ["e"=1]
"gokayfem/awesome-vlm-architectures" -> "JindongGu/Awesome-Prompting-on-Vision-Language-Model" ["e"=1]
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "zhengli97/PromptKD"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "muzairkhattak/multimodal-prompt-learning"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "muzairkhattak/PromptSRC"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "JindongGu/Awesome-Prompting-on-Vision-Language-Model"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "ZjjConan/VLM-MultiModalAdapter"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "KaiyangZhou/CoOp"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "gaopengcuhk/Tip-Adapter"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "ShuvenduRoy/CoPrompt"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "CHENGY12/PLOT"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "YBZh/DMN"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "tim-learn/awesome-test-time-adaptation" ["e"=1]
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "ThomasWangY/2024-AAAI-HPT"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "OpenGVLab/CaFo"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" -> "sarahpratt/CuPL"
"Razaimam45/TTL-Test-Time-Low-Rank-Adaptation" -> "umer-sheikh/bird-whisperer"
"muzairkhattak/ProText" -> "muzairkhattak/PromptSRC"
"muzairkhattak/ProText" -> "mbzuai-oryx/CVRR-Evaluation-Suite"
"muzairkhattak/ProText" -> "jameelhassan/PromptAlign"
"muzairkhattak/ProText" -> "asif-hanif/baple"
"SunzeY/AlphaCLIP" -> "gaopengcuhk/Tip-Adapter" ["e"=1]
"SunzeY/AlphaCLIP" -> "zhengli97/PromptKD" ["e"=1]
"SunzeY/AlphaCLIP" -> "KaiyangZhou/CoOp" ["e"=1]
"SunzeY/AlphaCLIP" -> "muzairkhattak/PromptSRC" ["e"=1]
"WangYZ1608/Self-Prompt-Tuning" -> "tommy-xq/SA2VP"
"zhengli97/PromptKD" -> "zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs"
"zhengli97/PromptKD" -> "muzairkhattak/PromptSRC"
"zhengli97/PromptKD" -> "muzairkhattak/multimodal-prompt-learning"
"zhengli97/PromptKD" -> "ShuvenduRoy/CoPrompt"
"zhengli97/PromptKD" -> "muzairkhattak/ProText"
"zhengli97/PromptKD" -> "CHENGY12/PLOT"
"zhengli97/PromptKD" -> "Koorye/DePT"
"zhengli97/PromptKD" -> "zhengli97/ATPrompt" ["e"=1]
"zhengli97/PromptKD" -> "ZjjConan/VLM-MultiModalAdapter"
"zhengli97/PromptKD" -> "OpenGVLab/CaFo"
"zhengli97/PromptKD" -> "tonyhuang2022/UPL"
"zhengli97/PromptKD" -> "KaiyangZhou/Dassl.pytorch" ["e"=1]
"zhengli97/PromptKD" -> "BeierZhu/Prompt-align"
"NVlabs/DoRA" -> "AGI-Edgerunners/LLM-Adapters" ["e"=1]
"sming256/OpenTAD" -> "benedettaliberatori/T3AL" ["e"=1]
"EvolvingLMMs-Lab/EgoLife" -> "EvolvingLMMs-Lab/Aero-1" ["e"=1]
"ShuvenduRoy/CoPrompt" -> "muzairkhattak/PromptSRC"
"ShuvenduRoy/CoPrompt" -> "zhaohengz/LLaMP"
"ShuvenduRoy/CoPrompt" -> "schowdhury671/APoLLo"
"SkalskiP/awesome-foundation-and-multimodal-models" -> "awaisrauf/Awesome-CV-Foundational-Models" ["e"=1]
"Picsart-AI-Research/OpenBias" -> "FarinaMatteo/qmmf"
"mbzuai-oryx/Video-LLaVA" -> "mbzuai-oryx/VideoGLaMM" ["e"=1]
"mbzuai-oryx/Video-LLaVA" -> "hananshafi/llmblueprint" ["e"=1]
"Leiyi-Hu/mona" -> "ShoufaChen/AdaptFormer"
"kdiAAA/TDA" -> "zhangce01/DPE-CLIP"
"kdiAAA/TDA" -> "YBZh/DMN"
"kdiAAA/TDA" -> "elaine-sui/TPS"
"kdiAAA/TDA" -> "FarinaMatteo/zero"
"kdiAAA/TDA" -> "chunmeifeng/DiffTPT"
"ZjjConan/VLM-MultiModalAdapter" -> "ShuvenduRoy/CoPrompt"
"mbzuai-oryx/MobiLlama" -> "asif-hanif/vafa" ["e"=1]
"tsunghan-wu/SLD" -> "hananshafi/llmblueprint"
"hananshafi/llmblueprint" -> "rohit901/VANE-Bench" ["e"=1]
"hananshafi/llmblueprint" -> "HashmatShadab/MambaRobustness"
"hananshafi/llmblueprint" -> "mbzuai-oryx/VideoGLaMM"
"hananshafi/llmblueprint" -> "Razaimam45/TTL-Test-Time-Low-Rank-Adaptation"
"hananshafi/llmblueprint" -> "Muhammad-Huzaifaa/ObjectCompose"
"DavidYanAnDe/ARC" -> "zstarN70/RLRR"
"elaine-sui/TPS" -> "zhangce01/DPE-CLIP"
"Muhammad-Huzaifaa/ObjectCompose" -> "HashmatShadab/MambaRobustness"
"benedettaliberatori/T3AL" -> "marco-garosi/ComCa"
"benedettaliberatori/T3AL" -> "tdemin16/multi-lane"
"benedettaliberatori/T3AL" -> "altndrr/lmms-owc"
"benedettaliberatori/T3AL" -> "francescotonini/al-gtd"
"YBZh/DMN" -> "YBZh/LAPT" ["e"=1]
"YBZh/DMN" -> "kdiAAA/TDA"
"YBZh/DMN" -> "zhangce01/DPE-CLIP"
"YBZh/DMN" -> "strongwolf/OpenSD" ["e"=1]
"mbzuai-oryx/CVRR-Evaluation-Suite" -> "HashmatShadab/MambaRobustness"
"BioMedIA-MBZUAI/MedPromptX" -> "BioMedIA-MBZUAI/XReal"
"BioMedIA-MBZUAI/MedPromptX" -> "asif-hanif/baple"
"BioMedIA-MBZUAI/MedPromptX" -> "umer-sheikh/bird-whisperer"
"BioMedIA-MBZUAI/MedPromptX" -> "Muhammad-Huzaifaa/ObjectCompose"
"jameelhassan/PromptAlign" -> "azshue/TPT"
"jameelhassan/PromptAlign" -> "muzairkhattak/ProText"
"jameelhassan/PromptAlign" -> "asif-hanif/baple"
"jameelhassan/PromptAlign" -> "mbzuai-oryx/CVRR-Evaluation-Suite"
"jameelhassan/PromptAlign" -> "akhtarvision/cal-detr" ["e"=1]
"zstarN70/RLRR" -> "DavidYanAnDe/ARC"
"FarinaMatteo/multiflow" -> "FarinaMatteo/qmmf"
"FarinaMatteo/multiflow" -> "altndrr/lmms-owc"
"Jingkang50/EgoLife" -> "pufanyi/syphus"
"tommy-xq/SA2VP" -> "WangYZ1608/Self-Prompt-Tuning"
"hee-suk-yoon/C-TPT" -> "Bala93/CLIPCalib"
"umer-sheikh/bird-whisperer" -> "Muhammad-Ibraheem-Siddiqui/PerSense"
"umer-sheikh/bird-whisperer" -> "asif-hanif/baple"
"umer-sheikh/bird-whisperer" -> "Razaimam45/TTL-Test-Time-Low-Rank-Adaptation"
"asif-hanif/baple" -> "umer-sheikh/bird-whisperer"
"asif-hanif/baple" -> "asif-hanif/vafa"
"asif-hanif/baple" -> "Razaimam45/TTL-Test-Time-Low-Rank-Adaptation"
"asif-hanif/baple" -> "asif-hanif/palm"
"zhangce01/DPE-CLIP" -> "elaine-sui/TPS"
"zhangce01/DPE-CLIP" -> "kdiAAA/TDA"
"FarinaMatteo/zero" -> "tdemin16/multi-lane"
"FarinaMatteo/zero" -> "altndrr/lmms-owc"
"FarinaMatteo/zero" -> "elaine-sui/TPS"
"Muhammad-Ibraheem-Siddiqui/PerSense" -> "umer-sheikh/bird-whisperer"
"mbzuai-oryx/VideoGLaMM" -> "mbzuai-oryx/CVRR-Evaluation-Suite"
"ayesha-ishaq/Open3DTrack" -> "umer-sheikh/bird-whisperer"
"francescotonini/al-gtd" -> "marco-garosi/ComCa"
"francescotonini/al-gtd" -> "tdemin16/multi-lane"
"francescotonini/al-gtd" -> "FarinaMatteo/qmmf"
"ZhengxiangShi/InstructionModelling" -> "ZhengxiangShi/LearnToAsk"
"tdemin16/multi-lane" -> "marco-garosi/ComCa"
"tdemin16/multi-lane" -> "francescotonini/al-gtd"
"HashmatShadab/MambaRobustness" -> "HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models"
"HashmatShadab/MambaRobustness" -> "fahadshamshad/deep-facial-privacy-prior"
"mbzuai-oryx/UniMed-CLIP" -> "Muhammad-Huzaifaa/ObjectCompose"
"mbzuai-oryx/UniMed-CLIP" -> "umer-sheikh/bird-whisperer"
"mbzuai-oryx/UniMed-CLIP" -> "ayesha-ishaq/Open3DTrack"
"EvolvingLMMs-Lab/VideoMMMU" -> "pufanyi/syphus"
"OpenBMB/UltraRAG" -> "OpenBMB/ModelCenter" ["e"=1]
"OpenBMB/UltraRAG" -> "OpenBMB/BMTrain" ["e"=1]
"mbzuai-oryx/LlamaV-o1" -> "mbzuai-oryx/CVRR-Evaluation-Suite" ["e"=1]
"altndrr/lmms-owc" -> "FarinaMatteo/qmmf"
"altndrr/lmms-owc" -> "marco-garosi/ComCa"
"altndrr/lmms-owc" -> "FarinaMatteo/multiflow"
"altndrr/lmms-owc" -> "tdemin16/multi-lane"
"EvolvingLMMs-Lab/Aero-1" -> "EvolvingLMMs-Lab/VideoMMMU"
"EvolvingLMMs-Lab/Aero-1" -> "pufanyi/syphus"
"marco-garosi/ComCa" -> "francescotonini/al-gtd"
"facebookresearch/LAMA" ["l"="49.998,38.071"]
"facebookresearch/KILT" ["l"="54.501,25.555", "c"=439]
"facebookresearch/DPR" ["l"="54.497,25.597", "c"=439]
"jzbjyb/LPAQA" ["l"="49.98,38.045"]
"facebookresearch/XLM" ["l"="53.036,25.657", "c"=172]
"facebookresearch/BLINK" ["l"="-0.394,-42.155", "c"=934]
"ucinlp/autoprompt" ["l"="50.032,38.065"]
"thunlp/ERNIE" ["l"="53.397,27.266", "c"=60]
"atcbosselut/comet-commonsense" ["l"="55.763,25.925", "c"=1119]
"THUDM/P-tuning" ["l"="50.066,38.077"]
"google-research/electra" ["l"="53.282,27.163", "c"=60]
"facebookresearch/GENRE" ["l"="-0.406,-42.173", "c"=934]
"princeton-nlp/LM-BFF" ["l"="50.039,38.082"]
"nyu-mll/jiant" ["l"="52.948,25.537", "c"=172]
"namisan/mt-dnn" ["l"="53.308,27.184", "c"=60]
"danqi/acl2020-openqa-tutorial" ["l"="54.495,25.53", "c"=439]
"lixin4ever/Conference-Acceptance-Rate" ["l"="-3.805,23.557", "c"=827]
"thunlp/PromptPapers" ["l"="50.059,38.052"]
"pliang279/awesome-multimodal-ml" ["l"="48.592,32.051", "c"=300]
"strawberrypie/bert_adapter" ["l"="50.017,38.152"]
"cs-mshah/Adapter-Bert" ["l"="49.998,38.162"]
"google-research/adapter-bert" ["l"="50.064,38.125"]
"AsaCooperStickland/Bert-n-Pals" ["l"="50.039,38.157"]
"adapter-hub/adapters" ["l"="50.123,38.113"]
"jxhe/unify-parameter-efficient-tuning" ["l"="50.167,38.161"]
"XiangLi1999/PrefixTuning" ["l"="50.086,38.092"]
"clarkkev/attention-analysis" ["l"="23.564,14.904", "c"=728]
"microsoft/K-Adapter" ["l"="-0.392,-42.235", "c"=934]
"thunlp/TAADpapers" ["l"="52.779,25.414", "c"=172]
"g1910/HyperNetworks" ["l"="34.082,32.172", "c"=520]
"rabeehk/hyperformer" ["l"="50.1,38.209"]
"commonsense/conceptnet5" ["l"="52.907,25.52", "c"=172]
"thunlp/OpenDelta" ["l"="50.146,38.073"]
"thunlp/OpenPrompt" ["l"="50.099,38.067"]
"bigscience-workshop/promptsource" ["l"="37.178,-0.097", "c"=126]
"huggingface/peft" ["l"="40.04,0.52", "c"=7]
"AGI-Edgerunners/LLM-Adapters" ["l"="50.173,38.121"]
"OpenGVLab/LLaMA-Adapter" ["l"="39.851,0.706", "c"=7]
"makcedward/nlpaug" ["l"="52.729,25.717", "c"=172]
"THUDM/P-tuning-v2" ["l"="39.159,-2.107", "c"=202]
"allenai/RL4LMs" ["l"="37.17,-0.197", "c"=126]
"CarperAI/trlx" ["l"="37.133,-0.204", "c"=126]
"timoschick/pet" ["l"="50.044,38.101"]
"princeton-nlp/SimCSE" ["l"="53.339,27.107", "c"=60]
"microsoft/DeBERTa" ["l"="53.231,27.124", "c"=60]
"MLNLP-World/Top-AI-Conferences-Paper-with-Code" ["l"="53.468,27.102", "c"=60]
"asheeshcric/awesome-contrastive-self-supervised-learning" ["l"="52.975,29.605", "c"=547]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["l"="50.313,38.23"]
"marcotcr/checklist" ["l"="52.788,25.588", "c"=172]
"uber-research/PPLM" ["l"="53.34,25.91", "c"=172]
"NiuTrans/ABigSurvey" ["l"="52.697,25.87", "c"=172]
"thunlp/PLMpapers" ["l"="53.319,27.158", "c"=60]
"graph4ai/graph4nlp" ["l"="52.745,16.131", "c"=100]
"xuyige/BERT4doc-Classification" ["l"="53.515,27.164", "c"=60]
"KaiyangZhou/mixstyle-release" ["l"="50.783,37.916", "c"=1182]
"KaiyangZhou/on-device-dg" ["l"="50.171,38.321"]
"google-research/big_transfer" ["l"="52.918,29.562", "c"=547]
"google-research/task_adaptation" ["l"="50.242,38.257"]
"KaiyangZhou/Dassl.pytorch" ["l"="50.818,37.918", "c"=1182]
"KaiyangZhou/CoOp" ["l"="50.344,38.24"]
"gaopengcuhk/CLIP-Adapter" ["l"="50.337,38.258"]
"muzairkhattak/multimodal-prompt-learning" ["l"="50.364,38.261"]
"gaopengcuhk/Tip-Adapter" ["l"="50.332,38.271"]
"KMnP/vpt" ["l"="50.273,38.24"]
"timoschick/fewglue" ["l"="50.005,38.126"]
"rrmenon10/ADAPET" ["l"="49.983,38.101"]
"google-research/multilingual-t5" ["l"="53.272,27.002", "c"=60]
"tonyzhaozh/few-shot-learning" ["l"="50.014,38.105"]
"renatoviolin/next_word_prediction" ["l"="52.702,25.427", "c"=172]
"Eric-Wallace/universal-triggers" ["l"="52.797,25.345", "c"=172]
"sangminwoo/awesome-vision-and-language" ["l"="48.789,31.976", "c"=300]
"allenai/dont-stop-pretraining" ["l"="53.345,27.068", "c"=60]
"princeton-nlp/OptiPrompt" ["l"="49.956,38.053"]
"kipgparker/soft-prompt-tuning" ["l"="50.086,38.043"]
"keirp/automatic_prompt_engineer" ["l"="36.859,-2.471", "c"=797]
"google-research-datasets/KELM-corpus" ["l"="49.986,38.017"]
"bhoov/exbert" ["l"="52.809,25.505", "c"=172]
"dongzelian/SSF" ["l"="50.217,38.222"]
"ZhangYuanhan-AI/NOAH" ["l"="50.218,38.256"]
"hjbahng/visual_prompting" ["l"="50.289,38.254"]
"JieShibo/PETL-ViT" ["l"="50.222,38.231"]
"google-research/head2toe" ["l"="50.207,38.288"]
"yyhhenry/tank" ["l"="50.051,38.404"]
"pufanyi/syphus" ["l"="50.066,38.396"]
"jacobgil/vit-explain" ["l"="50.883,29.629", "c"=83]
"haltakov/natural-language-image-search" ["l"="49.146,30.374", "c"=191]
"yzhuoning/Awesome-CLIP" ["l"="50.366,38.215"]
"yuchenlin/rebiber" ["l"="-3.863,23.517", "c"=827]
"salesforce/ALBEF" ["l"="48.761,31.992", "c"=300]
"luo3300612/Visualizer" ["l"="50.875,29.609", "c"=83]
"Timothyxxx/Chain-of-ThoughtsPapers" ["l"="36.759,-2.46", "c"=797]
"km1994/nlp_paper_study" ["l"="53.515,27.092", "c"=60]
"MLNLP-World/Paper-Writing-Tips" ["l"="-3.951,23.526", "c"=827]
"yizhongw/self-instruct" ["l"="39.042,-2.296", "c"=202]
"CLUEbenchmark/CLUE" ["l"="53.381,27.209", "c"=60]
"sagizty/Insight" ["l"="50.215,38.326"]
"sagizty/MAE" ["l"="50.229,38.338"]
"sagizty/Multi-Stage-Hybrid-Transformer" ["l"="50.231,38.327"]
"sagizty/NTUS_application" ["l"="50.246,38.329"]
"sagizty/Parallel-Hybrid-Transformer" ["l"="50.242,38.336"]
"sagizty/sagizty" ["l"="50.238,38.321"]
"sagizty/VPT" ["l"="50.238,38.301"]
"lsqqqq/notifyemail" ["l"="50.214,38.347"]
"google/BIG-bench" ["l"="37.205,-0.127", "c"=126]
"yangjianxin1/CPM" ["l"="53.292,27.119", "c"=60]
"TsinghuaAI/CPM" ["l"="50.239,37.959"]
"TsinghuaAI/CPM-2-Pretrain" ["l"="50.265,37.945"]
"BAAI-WuDao/Chinese-Transformer-XL" ["l"="50.39,37.846"]
"OpenBMB/BMInf" ["l"="50.215,38"]
"google-research/prompt-tuning" ["l"="50.11,38.089"]
"xlang-ai/UnifiedSKG" ["l"="37.899,-2.309", "c"=1210]
"mkshing/Prompt-Tuning" ["l"="50.112,38.046"]
"jordiclive/ControlPrefixes" ["l"="50.136,38.047"]
"allenai/natural-instructions" ["l"="37.211,-0.1", "c"=126]
"RUCAIBox/TextBox" ["l"="53.41,26.976", "c"=60]
"deepdialog/CPM-LM-TF2" ["l"="53.276,27.034", "c"=60]
"jm12138/CPM-Generate-Pytorch" ["l"="50.285,37.88"]
"TsinghuaAI/CPM-KG" ["l"="50.265,37.88"]
"TsinghuaAI/CPM-2-Finetune" ["l"="50.258,37.967"]
"TsinghuaAI/CPM-1-Finetune" ["l"="50.276,37.898"]
"BAAI-WuDao/P-tuning" ["l"="50.309,37.91"]
"OpenBMB/BMCook" ["l"="50.218,38.018"]
"OpenBMB/CPM-Live" ["l"="50.203,38.035"]
"OpenBMB/BMTrain" ["l"="50.181,38.025"]
"OpenBMB/BMList" ["l"="50.191,38.002"]
"OpenBMB/ModelCenter" ["l"="50.198,38.015"]
"TsinghuaAI/CPM-1-Generate" ["l"="53.313,27.122", "c"=60]
"thu-coai/EVA" ["l"="56.774,29.037", "c"=310]
"CLUEbenchmark/pCLUE" ["l"="39.171,-2.255", "c"=202]
"hpcaitech/EnergonAI" ["l"="38.988,-0.837", "c"=39]
"dandelin/ViLT" ["l"="48.751,32.016", "c"=300]
"hila-chefer/Transformer-MM-Explainability" ["l"="50.91,29.608", "c"=83]
"Zasder3/train-CLIP" ["l"="49.054,30.363", "c"=191]
"bojone/P-tuning" ["l"="53.315,26.951", "c"=60]
"CLUEbenchmark/FewCLUE" ["l"="53.381,27.058", "c"=60]
"yumingj/Talk-to-Edit" ["l"="43.461,30.676", "c"=318]
"ZhangYuanhan-AI/OmniBenchmark" ["l"="50.154,38.346"]
"hongfz16/Garment4D" ["l"="50.139,38.389"]
"Jiahao000/ORL" ["l"="50.134,38.333"]
"GEM-benchmark/NL-Augmenter" ["l"="52.667,25.643", "c"=172]
"wenhuchen/KGPT" ["l"="37.812,-2.183", "c"=1210]
"j-min/VL-T5" ["l"="47.84,32.913", "c"=373]
"ylsung/VL_adapter" ["l"="50.197,38.201"]
"ylsung/Ladder-Side-Tuning" ["l"="50.174,38.184"]
"ShoufaChen/AdaptFormer" ["l"="50.241,38.231"]
"jianghaojun/Awesome-Parameter-Efficient-Transfer-Learning" ["l"="50.242,38.206"]
"AkariAsai/ATTEMPT" ["l"="50.086,38.143"]
"r-three/t-few" ["l"="50.119,38.15"]
"fuzihaofzh/AnalyzeParameterEfficientFinetune" ["l"="50.146,38.177"]
"moein-shariatnia/OpenAI-CLIP" ["l"="49.017,30.363", "c"=191]
"huggingface/awesome-huggingface" ["l"="38.584,-0.838", "c"=39]
"grandchicken/1708SEM_ISIM" ["l"="50.239,38.351"]
"corolla-johnson/mkultra" ["l"="50.113,38.032"]
"Zeng-WH/Prompt-Tuning" ["l"="50.099,38.022"]
"qhduan/mt5-soft-prompt-tuning" ["l"="50.079,38.008"]
"exelents/soft-prompt-tuning" ["l"="50.108,38.007"]
"txsun1997/Black-Box-Tuning" ["l"="36.817,-2.603", "c"=797]
"hiaoxui/soft-prompts" ["l"="50.089,37.991"]
"thu-coai/PPT" ["l"="50.059,38.005"]
"thunlp/PTR" ["l"="50.042,37.98"]
"IntelLabs/academic-budget-bert" ["l"="49.861,38.028"]
"princeton-nlp/DinkyTrain" ["l"="49.909,38.042"]
"princeton-nlp/DensePhrases" ["l"="54.506,25.572", "c"=439]
"yym6472/ConSERT" ["l"="53.336,27.054", "c"=60]
"zjunlp/DART" ["l"="38.284,-7.837", "c"=1088]
"microsoft/ANCE" ["l"="54.475,25.7", "c"=439]
"DirtyHarryLYL/Transformer-in-Vision" ["l"="50.85,29.7", "c"=83]
"THUDM/GLM" ["l"="39.147,-2.076", "c"=202]
"bojone/Pattern-Exploiting-Training" ["l"="53.322,26.986", "c"=60]
"peterwestuw/surface-form-competition" ["l"="49.973,38.124"]
"ethanjperez/true_few_shot" ["l"="49.957,38.108"]
"Alrope123/rethinking-demonstrations" ["l"="36.98,-2.517", "c"=797]
"urvashik/knnlm" ["l"="36.965,-2.651", "c"=797]
"zjunlp/KnowPrompt" ["l"="50.025,37.958"]
"wzhouad/RE_improved_baseline" ["l"="55.538,26.866", "c"=464]
"thunlp/KnowledgeablePromptTuning" ["l"="50.048,37.961"]
"BAAI-WuDao/EVA" ["l"="50.302,37.887"]
"BAAI-WuDao/Model" ["l"="50.346,37.851"]
"BAAI-WuDao/CogView" ["l"="50.343,37.875"]
"BAAI-WuDao/CPM" ["l"="50.373,37.858"]
"TsinghuaAI/CPM-1-Pretrain" ["l"="50.242,37.921"]
"pfliu-nlp/NLPedia-Pretrain" ["l"="58.366,28.82", "c"=665]
"rabeehk/compacter" ["l"="50.076,38.235"]
"McGill-NLP/polytropon" ["l"="50.051,38.242"]
"Muzammal-Naseer/IPViT" ["l"="38.804,-7.422", "c"=232]
"jameelhassan/PromptAlign" ["l"="50.455,38.275"]
"thunlp/Prompt-Transferability" ["l"="50.153,38.101"]
"RUCAIBox/Transfer-Prompts-for-Text-Generation" ["l"="50.17,38.07"]
"TsinghuaAI/TDS" ["l"="50.255,37.906"]
"princeton-nlp/EntityQuestions" ["l"="54.477,25.647", "c"=439]
"princeton-nlp/rationale-robustness" ["l"="49.932,38.035"]
"TsinghuaAI/CPM-1-Distill" ["l"="50.231,37.904"]
"BAAI-WuDao/GLM" ["l"="50.404,37.835"]
"LianjiaTech/BELLE" ["l"="39.054,-2.075", "c"=202]
"km1994/NLP-Interview-Notes" ["l"="53.498,27.11", "c"=60]
"dbiir/UER-py" ["l"="53.376,27.187", "c"=60]
"THUDM/SwissArmyTransformer" ["l"="39.242,-2.131", "c"=202]
"google-research/t5x" ["l"="37.139,-0.072", "c"=126]
"lucidrains/PaLM-pytorch" ["l"="-5.186,-23.312", "c"=164]
"OFA-Sys/OFA" ["l"="49.008,30.282", "c"=191]
"isl-org/lang-seg" ["l"="48.77,30.285", "c"=191]
"raoyongming/DenseCLIP" ["l"="48.818,30.315", "c"=191]
"microsoft/GLIP" ["l"="48.866,30.251", "c"=191]
"muzairkhattak/PromptSRC" ["l"="50.408,38.267"]
"zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" ["l"="50.383,38.257"]
"mlfoundations/open_clip" ["l"="48.974,30.216", "c"=191]
"salesforce/BLIP" ["l"="48.991,30.249", "c"=191]
"salesforce/LAVIS" ["l"="49.008,30.194", "c"=191]
"acl-org/acl-style-files" ["l"="36.749,-2.579", "c"=797]
"zhoubolei/bolei_awesome_posters" ["l"="-3.929,23.549", "c"=827]
"lucidrains/CoCa-pytorch" ["l"="48.961,30.31", "c"=191]
"huggingface/evaluate" ["l"="38.644,-0.702", "c"=39]
"microsoft/RegionCLIP" ["l"="48.631,30.276", "c"=191]
"lucidrains/RETRO-pytorch" ["l"="-5.216,-23.353", "c"=164]
"bigscience-workshop/t-zero" ["l"="37.173,-0.038", "c"=126]
"facebookresearch/SLIP" ["l"="48.929,30.344", "c"=191]
"XiangLi1999/Diffusion-LM" ["l"="45.967,30.736", "c"=367]
"rmokady/CLIP_prefix_caption" ["l"="49.115,30.367", "c"=191]
"lucidrains/x-clip" ["l"="48.971,30.385", "c"=191]
"zjunlp/PromptKG" ["l"="53.865,15.187", "c"=504]
"Yutong-Zhou-cv/Awesome-Multimodality" ["l"="50.439,38.403"]
"Yutong-Zhou-cv/Awesome-Transformer-in-CV" ["l"="50.458,38.436"]
"wangxiao5791509/MultiModal_BigModels_Survey" ["l"="50.419,38.365"]
"thunlp/DeltaPapers" ["l"="50.215,38.139"]
"txsun1997/LMaaS-Papers" ["l"="36.778,-2.536", "c"=797]
"thunlp/UltraChat" ["l"="50.786,2.873", "c"=85]
"MLNLP-World/AI-Paper-Collector" ["l"="-3.913,23.473", "c"=827]
"czczup/ViT-Adapter" ["l"="48.832,30.241", "c"=191]
"tim-learn/awesome-test-time-adaptation" ["l"="50.888,37.933", "c"=1182]
"azshue/TPT" ["l"="50.443,38.248"]
"alibaba/EasyNLP" ["l"="53.391,27.129", "c"=60]
"hustvl/MIMDet" ["l"="52.886,29.378", "c"=547]
"Sense-GVT/DeCLIP" ["l"="48.883,30.342", "c"=191]
"ArrowLuo/CLIP4Clip" ["l"="47.909,32.98", "c"=373]
"dk-liang/Awesome-Visual-Transformer" ["l"="50.812,29.72", "c"=83]
"jingyi0000/VLM_survey" ["l"="50.954,2.79", "c"=85]
"ShoufaChen/DiffusionDet" ["l"="48.83,30.299", "c"=191]
"yuewang-cuhk/awesome-vision-language-pretraining-papers" ["l"="48.733,31.999", "c"=300]
"TheShadow29/awesome-grounding" ["l"="48.847,31.967", "c"=300]
"YuejiangLIU/awesome-source-free-test-time-adaptation" ["l"="50.903,37.922", "c"=1182]
"DirtyHarryLYL/LLM-in-Vision" ["l"="47.427,30.163", "c"=254]
"google-research/l2p" ["l"="34.085,31.922", "c"=520]
"amirbar/visual_prompting" ["l"="33.104,31.549", "c"=109]
"MarkMoHR/Awesome-Referring-Image-Segmentation" ["l"="48.925,31.903", "c"=300]
"JindongGu/Awesome-Prompting-on-Vision-Language-Model" ["l"="50.339,38.212"]
"OpenGVLab/CaFo" ["l"="50.366,38.284"]
"yangyangyang127/APE" ["l"="50.33,38.295"]
"linzhiqiu/cross_modal_adaptation" ["l"="50.35,38.288"]
"sarahpratt/CuPL" ["l"="50.354,38.309"]
"vishaal27/SuS-X" ["l"="50.344,38.3"]
"CHENGY12/PLOT" ["l"="50.398,38.252"]
"mrflogs/ICLR24" ["l"="38.326,0.076", "c"=39]
"ZiyuGuo99/CALIP" ["l"="50.334,38.308"]
"Langboat/Mengzi" ["l"="53.407,27.011", "c"=60]
"mmaaz60/mvits_for_class_agnostic_od" ["l"="48.479,30.325", "c"=191]
"asif-hanif/vafa" ["l"="50.483,38.279"]
"Muhammad-Huzaifaa/ObjectCompose" ["l"="50.527,38.312"]
"Leiyi-Hu/mona" ["l"="50.232,38.274"]
"luogen1996/RepAdapter" ["l"="50.203,38.237"]
"synbol/Awesome-Parameter-Efficient-Transfer-Learning" ["l"="-54.32,-11.476", "c"=843]
"zdou0830/METER" ["l"="48.784,32.002", "c"=300]
"LeapLabTHU/Cross-Modal-Adapter" ["l"="49.273,32.974", "c"=401]
"microsoft/AdaMix" ["l"="50.108,38.179"]
"craffel/llm-seminar" ["l"="50.075,38.18"]
"allenai/acl2022-zerofewshot-tutorial" ["l"="36.947,-2.563", "c"=797]
"clinicalml/TabLLM" ["l"="46.149,24.759", "c"=1262]
"Shark-NLP/OpenICL" ["l"="36.863,-2.508", "c"=797]
"facebookresearch/tart" ["l"="54.429,25.575", "c"=439]
"Paranioar/UniPT" ["l"="32.793,31.114", "c"=109]
"microsoft/mttl" ["l"="50.028,38.257"]
"mlfoundations/wise-ft" ["l"="48.956,30.334", "c"=191]
"LightDXY/FT-CLIP" ["l"="50.333,38.337"]
"wjn1996/TransPrompt" ["l"="38.256,-7.85", "c"=1088]
"kongds/Prompt-BERT" ["l"="53.301,26.899", "c"=60]
"declare-lab/RelationPrompt" ["l"="50,37.921"]
"dinobby/ZS-BERT" ["l"="49.984,37.898"]
"ssnvxia/OneRel" ["l"="55.477,26.588", "c"=464]
"sallymmx/ActionCLIP" ["l"="48.051,33.881", "c"=168]
"muzairkhattak/ViFi-CLIP" ["l"="50.438,38.277"]
"BeierZhu/Prompt-align" ["l"="50.417,38.242"]
"tonyhuang2022/UPL" ["l"="50.391,38.312"]
"BatsResearch/menghini-neurips23-code" ["l"="52.827,27.778", "c"=60]
"yuhangzang/UPT" ["l"="50.313,38.299"]
"korawat-tanwisuth/POUF" ["l"="50.4,38.341"]
"CEWu/PTNL" ["l"="50.415,38.335"]
"hongfz16/HCMoCo" ["l"="50.119,38.391"]
"thunlp/WebCPM" ["l"="50.705,2.93", "c"=85]
"OpenBMB/CPM-Bee" ["l"="51.127,2.846", "c"=85]
"OpenGVLab/gv-benchmark" ["l"="49.376,29.551", "c"=1525]
"ZhangYuanhan-AI/Bamboo" ["l"="50.147,38.365"]
"changdaeoh/BlackVIP" ["l"="50.29,38.205"]
"shikiw/DAM-VP" ["l"="38.305,-0.105", "c"=39]
"UCSC-VLAA/EVP" ["l"="50.282,38.22"]
"OPTML-Group/ILM-VP" ["l"="50.276,38.284"]
"ShiZhengyan/DePT" ["l"="49.973,38.187"]
"jiawei-ren/BalancedMSE" ["l"="51.332,30.501", "c"=83]
"cambridgeltl/autopeft" ["l"="50.246,38.141"]
"calpt/awesome-adapter-resources" ["l"="50.248,38.166"]
"EagleW/Stage-wise-Fine-tuning" ["l"="50.143,38.019"]
"rtmaww/EntLM" ["l"="53.805,27.643", "c"=60]
"ju-chen/Efficient-Prompt" ["l"="48.086,33.92", "c"=168]
"Arnav0400/ViT-Slim" ["l"="50.191,38.256"]
"VITA-Group/UVC" ["l"="49.09,33.256", "c"=401]
"ziplab/SPT" ["l"="50.174,38.25"]
"VITA-Group/SViTE" ["l"="49.083,33.218", "c"=401]
"Jingkang50/EgoLife" ["l"="50.1,38.37"]
"BeierZhu/xERM" ["l"="50.455,38.225"]
"yxymessi/H2E-Framework" ["l"="50.455,38.205"]
"ShiZhengyan/StepGame" ["l"="49.935,38.198"]
"ZhengxiangShi/LearnToAsk" ["l"="49.952,38.199"]
"amzn/pretraining-or-self-training" ["l"="49.953,38.19"]
"ShiZhengyan/PowerfulPromptFT" ["l"="49.94,38.184"]
"Fangjun-Li/SpatialLM-StepGame" ["l"="49.909,38.205"]
"ZhengxiangShi/SelfContrastiveLearningRecSys" ["l"="49.946,38.209"]
"lupantech/ScienceQA" ["l"="47.248,30.316", "c"=254]
"promptslab/Promptify" ["l"="43.938,0.65", "c"=135]
"JunweiLiang/awesome_lists" ["l"="-3.978,23.512", "c"=827]
"promptslab/Awesome-Prompt-Engineering" ["l"="44.019,0.648", "c"=135]
"txsun1997/MOSS" ["l"="36.802,-2.539", "c"=797]
"trigaten/Learn_Prompting" ["l"="43.944,0.78", "c"=135]
"mmaaz60/EdgeNeXt" ["l"="48.956,33.056", "c"=401]
"asif-hanif/baple" ["l"="50.51,38.291"]
"hanoonaR/object-centric-ovd" ["l"="48.546,30.31", "c"=191]
"mbzuai-oryx/CVRR-Evaluation-Suite" ["l"="50.472,38.302"]
"zhengli97/PromptKD" ["l"="50.397,38.279"]
"muzairkhattak/ProText" ["l"="50.435,38.289"]
"YiLunLee/missing_aware_prompts" ["l"="56.454,27.984", "c"=940]
"TalalWasim/Vita-CLIP" ["l"="48.146,33.888", "c"=168]
"whwu95/BIKE" ["l"="48.139,33.845", "c"=168]
"OpenGVLab/efficient-video-recognition" ["l"="48.127,33.857", "c"=168]
"wengzejia1/Open-VCLIP" ["l"="48.215,33.853", "c"=168]
"mbzuai-oryx/VideoGLaMM" ["l"="50.492,38.311"]
"TalalWasim/Video-FocalNets" ["l"="50.455,38.29"]
"skingorz/FD-Align" ["l"="50.375,38.306"]
"mrflogs/SHIP" ["l"="38.308,0.073", "c"=39]
"ArmanAfrasiyabi/SetFeat-fs" ["l"="57.839,19.142", "c"=433]
"sachit-menon/classify_by_description_release" ["l"="50.364,38.331"]
"amazon-science/auto-cot" ["l"="36.741,-2.4", "c"=797]
"Computer-Vision-in-the-Wild/CVinW_Readings" ["l"="47.472,30.142", "c"=254]
"microsoft/prompt-engine" ["l"="43.957,0.56", "c"=135]
"MLNLP-World/Paper-Picture-Writing-Code" ["l"="-3.991,23.494", "c"=827]
"sauradip/STALE" ["l"="48.085,33.941", "c"=168]
"benedettaliberatori/T3AL" ["l"="50.618,38.227"]
"dqxiu/ICL_PaperList" ["l"="36.777,-2.505", "c"=797]
"mr-eggplant/SAR" ["l"="50.937,37.942", "c"=1182]
"kdiAAA/TDA" ["l"="50.499,38.238"]
"chunmeifeng/DiffTPT" ["l"="50.502,38.223"]
"mzhaoshuai/RLCF" ["l"="50.479,38.212"]
"zhangce01/DPE-CLIP" ["l"="50.495,38.25"]
"ShuvenduRoy/CoPrompt" ["l"="50.412,38.287"]
"DequanWang/tent" ["l"="50.925,37.923", "c"=1182]
"qinenergy/cotta" ["l"="50.922,37.938", "c"=1182]
"FarinaMatteo/zero" ["l"="50.538,38.23"]
"Bala93/CLIPCalib" ["l"="50.479,38.226"]
"ExplainableML/WaffleCLIP" ["l"="50.363,38.355"]
"mertyg/vision-language-models-are-bows" ["l"="38.248,-0.115", "c"=39]
"52CV/CVPR-2023-Papers" ["l"="50.919,29.684", "c"=83]
"DavidYanAnDe/ARC" ["l"="50.177,38.235"]
"Jingkang50/OpenPSG" ["l"="47.641,32.075", "c"=1070]
"EvolvingLMMs-Lab/RelateAnything" ["l"="50.117,38.365"]
"SEU-COIN/LLMPapers" ["l"="36.811,-2.57", "c"=797]
"ChengHan111/E2VPT" ["l"="50.168,38.27"]
"linziyi96/st-adapter" ["l"="48.167,33.858", "c"=168]
"zhangce01/DualAdapter" ["l"="50.351,38.343"]
"htyao89/KgCoOp" ["l"="50.434,38.193"]
"bbbdylan/proda" ["l"="50.453,38.17"]
"saiboxx/chexray-diffusion" ["l"="61.625,36.623", "c"=178]
"BioMedIA-MBZUAI/XReal" ["l"="50.577,38.328"]
"HazyResearch/ama_prompting" ["l"="41.647,-3.792", "c"=146]
"thunlp/BMCourse" ["l"="50.204,37.969"]
"Ivan-Tang-3D/ViewRefer3D" ["l"="65.077,11.793", "c"=203]
"taoyang1122/adapt-image-models" ["l"="48.142,33.868", "c"=168]
"hananshafi/vits-for-small-scale-datasets" ["l"="49.262,33.298", "c"=401]
"mariodoebler/test-time-adaptation" ["l"="50.908,37.945", "c"=1182]
"WillDreamer/Aurora" ["l"="50.171,38.216"]
"FrozenBurning/Text2Light" ["l"="64.218,3.61", "c"=49]
"Computer-Vision-in-the-Wild/DataDownload" ["l"="50.177,38.418"]
"Computer-Vision-in-the-Wild/Elevater_Toolkit_IC" ["l"="50.166,38.403"]
"BatsResearch/csp" ["l"="52.82,27.754", "c"=60]
"eric-ai-lab/PEViT" ["l"="50.163,38.437"]
"KaihuaTang/Generalized-Long-Tailed-Benchmarks.pytorch" ["l"="51.242,30.509", "c"=83]
"r-three/git-theta" ["l"="50.032,38.205"]
"Koorye/DePT" ["l"="50.442,38.264"]
"ZjjConan/VLM-MultiModalAdapter" ["l"="50.419,38.278"]
"ThomasWangY/2024-AAAI-HPT" ["l"="50.424,38.255"]
"diffusion-classifier/diffusion-classifier" ["l"="33.18,31.467", "c"=109]
"awaisrauf/Awesome-CV-Foundational-Models" ["l"="50.424,38.221"]
"uncbiag/Awesome-Foundation-Models" ["l"="50.381,38.189"]
"mbzuai-oryx/groundingLMM" ["l"="47.465,30.182", "c"=254]
"mbzuai-oryx/Video-LLaVA" ["l"="47.543,30.119", "c"=254]
"xmindflow/Awesome-Foundation-Models-in-Medical-Imaging" ["l"="62.419,37.616", "c"=284]
"jianzongwu/Awesome-Open-Vocabulary" ["l"="48.681,30.233", "c"=191]
"mbzuai-oryx/ClimateGPT" ["l"="48.882,33.03", "c"=401]
"xmed-lab/CLIP_Surgery" ["l"="48.678,30.279", "c"=191]
"fahadshamshad/Clip2Protect" ["l"="39.386,-7.496", "c"=232]
"HashmatShadab/MambaRobustness" ["l"="50.51,38.329"]
"tomhartke/knowledge-graph-from-GPT" ["l"="53.828,15.149", "c"=504]
"huytransformer/Awesome-Out-Of-Distribution-Detection" ["l"="52.632,14.206", "c"=1208]
"OpenBMB/VisCPM" ["l"="50.732,2.904", "c"=85]
"Jun-CEN/SegmentAnyRGBD" ["l"="65.108,11.631", "c"=203]
"Nicous20/FunQA" ["l"="50.081,38.377"]
"kaleido-lab/dolphin" ["l"="50.092,38.354"]
"Kenneth-Wong/MMSceneGraph" ["l"="47.634,32.052", "c"=1070]
"JialianW/GRiT" ["l"="47.45,30.28", "c"=254]
"omniobject3d/OmniObject3D" ["l"="64.225,3.534", "c"=49]
"jiawei-ren/diffmimic" ["l"="30.446,28.686", "c"=94]
"TonyLianLong/LLM-groundedDiffusion" ["l"="33.314,31.472", "c"=109]
"tsunghan-wu/SLD" ["l"="50.57,38.358"]
"DmitryRyumin/ICCV-2023-Papers" ["l"="38.218,2.355", "c"=54]
"JamesQFreeman/LoRA-ViT" ["l"="62.538,37.798", "c"=284]
"showlab/Image2Paragraph" ["l"="49.024,30.224", "c"=191]
"cliangyu/Cola" ["l"="50.028,38.415"]
"Amshaker/SwiftFormer" ["l"="48.93,33.012", "c"=401]
"ZrrSkywalker/CaFo" ["l"="50.378,38.242"]
"geekyutao/TaskRes" ["l"="50.394,38.235"]
"Tsingularity/FRN" ["l"="57.863,19.25", "c"=433]
"jusiro/CLAP" ["l"="62.287,37.074", "c"=178]
"Ma-Lab-Berkeley/CRATE" ["l"="53.179,29.958", "c"=547]
"NVlabs/DoRA" ["l"="38.605,-0.136", "c"=39]
"AetherCortex/Llama-X" ["l"="39.103,-2.269", "c"=202]
"HillZhang1999/llm-hallucination-survey" ["l"="37.656,-6.917", "c"=766]
"SinclairCoder/Instruction-Tuning-Papers" ["l"="36.756,-2.518", "c"=797]
"GanjinZero/RRHF" ["l"="37.215,-0.204", "c"=126]
"PhoebusSi/Alpaca-CoT" ["l"="39.078,-2.202", "c"=202]
"FranxYao/chain-of-thought-hub" ["l"="37.241,-0.207", "c"=126]
"zjunlp/EasyEdit" ["l"="37.704,-6.998", "c"=766]
"Instruction-Tuning-with-GPT-4/GPT-4-LLM" ["l"="39.064,-2.245", "c"=202]
"allenai/open-instruct" ["l"="37.202,-0.321", "c"=126]
"altndrr/vic" ["l"="50.571,38.216"]
"altndrr/lmms-owc" ["l"="50.594,38.215"]
"tdemin16/multi-lane" ["l"="50.587,38.226"]
"AGI-Edgerunners/Plan-and-Solve-Prompting" ["l"="36.713,-2.427", "c"=797]
"Jianing-Qiu/Awesome-Healthcare-Foundation-Models" ["l"="62.379,37.633", "c"=284]
"EdisonLeeeee/Awesome-Masked-Autoencoders" ["l"="52.811,29.38", "c"=547]
"liliu-avril/Awesome-Segment-Anything" ["l"="48.774,30.116", "c"=191]
"Hedlen/awesome-segment-anything" ["l"="48.803,30.077", "c"=191]
"richard-peng-xia/awesome-multimodal-in-medical-imaging" ["l"="62.381,37.596", "c"=284]
"NVlabs/RADIO" ["l"="64.088,2.918", "c"=49]
"robotics-survey/Awesome-Robotics-Foundation-Models" ["l"="59.391,16.667", "c"=234]
"ryongithub/GatedPromptTuning" ["l"="50.142,38.286"]
"WangYZ1608/Self-Prompt-Tuning" ["l"="50.136,38.27"]
"luogen1996/LaVIN" ["l"="47.399,30.214", "c"=254]
"mlvlab/RPO" ["l"="50.306,38.328"]
"Zhiyuan-R/ChatGPT-Powered-Hierarchical-Comparisons-for-Image-Classification" ["l"="50.363,38.38"]
"FarinaMatteo/qmmf" ["l"="50.625,38.206"]
"tdemin16/Continual-LayerNorm-Tuning" ["l"="50.645,38.208"]
"ZhengxiangShi/InstructionModelling" ["l"="49.963,38.209"]
"UKPLab/adaptable-adapters" ["l"="50.272,38.15"]
"Koorye/SkipTuning" ["l"="50.478,38.264"]
"chunmeifeng/FedIns" ["l"="50.524,38.21"]
"RERV/UniAdapter" ["l"="50.133,38.231"]
"UniAdapter/UniAdapter" ["l"="50.144,38.216"]
"Luodian/GenBench" ["l"="50.039,38.393"]
"gokayfem/awesome-vlm-architectures" ["l"="47.373,30.036", "c"=254]
"YBZh/DMN" ["l"="50.47,38.245"]
"Razaimam45/TTL-Test-Time-Low-Rank-Adaptation" ["l"="50.536,38.304"]
"umer-sheikh/bird-whisperer" ["l"="50.551,38.296"]
"SunzeY/AlphaCLIP" ["l"="48.796,30.337", "c"=191]
"tommy-xq/SA2VP" ["l"="50.116,38.277"]
"zhengli97/ATPrompt" ["l"="47.747,35.556", "c"=695]
"sming256/OpenTAD" ["l"="48.068,33.977", "c"=168]
"EvolvingLMMs-Lab/EgoLife" ["l"="47.529,30.253", "c"=254]
"EvolvingLMMs-Lab/Aero-1" ["l"="50.071,38.423"]
"zhaohengz/LLaMP" ["l"="50.442,38.318"]
"schowdhury671/APoLLo" ["l"="50.425,38.315"]
"SkalskiP/awesome-foundation-and-multimodal-models" ["l"="48.61,29.938", "c"=191]
"Picsart-AI-Research/OpenBias" ["l"="50.649,38.193"]
"hananshafi/llmblueprint" ["l"="50.534,38.33"]
"elaine-sui/TPS" ["l"="50.519,38.243"]
"mbzuai-oryx/MobiLlama" ["l"="39.001,-0.027", "c"=39]
"rohit901/VANE-Bench" ["l"="48.509,30.383", "c"=191]
"zstarN70/RLRR" ["l"="50.156,38.24"]
"marco-garosi/ComCa" ["l"="50.604,38.229"]
"francescotonini/al-gtd" ["l"="50.608,38.217"]
"YBZh/LAPT" ["l"="-35.407,21.173", "c"=127]
"strongwolf/OpenSD" ["l"="-35.392,21.183", "c"=127]
"BioMedIA-MBZUAI/MedPromptX" ["l"="50.55,38.312"]
"akhtarvision/cal-detr" ["l"="39.493,-7.48", "c"=232]
"FarinaMatteo/multiflow" ["l"="50.608,38.201"]
"hee-suk-yoon/C-TPT" ["l"="50.503,38.205"]
"Muhammad-Ibraheem-Siddiqui/PerSense" ["l"="50.571,38.287"]
"asif-hanif/palm" ["l"="50.534,38.281"]
"ayesha-ishaq/Open3DTrack" ["l"="50.584,38.299"]
"HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models" ["l"="50.512,38.349"]
"fahadshamshad/deep-facial-privacy-prior" ["l"="50.526,38.348"]
"mbzuai-oryx/UniMed-CLIP" ["l"="50.571,38.311"]
"EvolvingLMMs-Lab/VideoMMMU" ["l"="50.054,38.419"]
"OpenBMB/UltraRAG" ["l"="-2.29,-33.862", "c"=30]
"mbzuai-oryx/LlamaV-o1" ["l"="47.392,30.128", "c"=254]
}