digraph G {
"vwxyzjn/cleanrl" -> "DLR-RM/stable-baselines3"
"vwxyzjn/cleanrl" -> "Farama-Foundation/Gymnasium"
"vwxyzjn/cleanrl" -> "thu-ml/tianshou"
"vwxyzjn/cleanrl" -> "tinkoff-ai/CORL" ["e"=1]
"vwxyzjn/cleanrl" -> "Farama-Foundation/PettingZoo"
"vwxyzjn/cleanrl" -> "pytorch/rl"
"vwxyzjn/cleanrl" -> "DLR-RM/rl-baselines3-zoo"
"vwxyzjn/cleanrl" -> "openai/spinningup"
"vwxyzjn/cleanrl" -> "AI4Finance-Foundation/ElegantRL"
"vwxyzjn/cleanrl" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"vwxyzjn/cleanrl" -> "google/brax" ["e"=1]
"vwxyzjn/cleanrl" -> "isaac-sim/IsaacGymEnvs" ["e"=1]
"vwxyzjn/cleanrl" -> "google-deepmind/acme"
"vwxyzjn/cleanrl" -> "openai/baselines"
"vwxyzjn/cleanrl" -> "Farama-Foundation/D4RL" ["e"=1]
"alex-petrenko/sample-factory" -> "sail-sg/envpool" ["e"=1]
"alex-petrenko/sample-factory" -> "google-research/rliable" ["e"=1]
"alex-petrenko/sample-factory" -> "RobertTLange/gymnax" ["e"=1]
"alex-petrenko/sample-factory" -> "google-research/seed_rl"
"alex-petrenko/sample-factory" -> "facebookresearch/torchbeast"
"alex-petrenko/sample-factory" -> "MichaelTMatthews/Craftax" ["e"=1]
"alex-petrenko/sample-factory" -> "alex-petrenko/megaverse"
"alex-petrenko/sample-factory" -> "YeWR/EfficientZero" ["e"=1]
"alex-petrenko/sample-factory" -> "luchris429/purejaxrl" ["e"=1]
"alex-petrenko/sample-factory" -> "instadeepai/jumanji" ["e"=1]
"alex-petrenko/sample-factory" -> "google/brax" ["e"=1]
"alex-petrenko/sample-factory" -> "google-deepmind/acme"
"alex-petrenko/sample-factory" -> "astooke/rlpyt"
"alex-petrenko/sample-factory" -> "Farama-Foundation/Miniworld"
"alex-petrenko/sample-factory" -> "openai/procgen"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/Pool_AI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/Enigma-Simulator"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/Google-Chrome-Dino-Game-AI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/PacNeat"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/Chess-AI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/PacmanGame"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/RubiksCubeAI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/2048-AI"
"Code-Bullet/SnakeFusion" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"google/dopamine" -> "google-deepmind/trfl"
"google/dopamine" -> "openai/baselines"
"google/dopamine" -> "dennybritz/reinforcement-learning"
"google/dopamine" -> "facebookresearch/ReAgent"
"google/dopamine" -> "keras-rl/keras-rl"
"google/dopamine" -> "tensorforce/tensorforce"
"google/dopamine" -> "hill-a/stable-baselines"
"google/dopamine" -> "openai/spinningup"
"google/dopamine" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"google/dopamine" -> "tensorflow/agents"
"google/dopamine" -> "rll/rllab"
"google/dopamine" -> "openai/gym"
"google/dopamine" -> "aikorea/awesome-rl"
"google/dopamine" -> "google-deepmind/lab"
"google/dopamine" -> "google-deepmind/sonnet"
"openai/multiagent-particle-envs" -> "openai/maddpg"
"openai/multiagent-particle-envs" -> "oxwhirl/pymarl"
"openai/multiagent-particle-envs" -> "LantaoYu/MARL-Papers"
"openai/multiagent-particle-envs" -> "Farama-Foundation/PettingZoo"
"openai/multiagent-particle-envs" -> "marlbenchmark/on-policy"
"openai/multiagent-particle-envs" -> "oxwhirl/smac"
"openai/multiagent-particle-envs" -> "starry-sky6688/MARL-Algorithms"
"openai/multiagent-particle-envs" -> "geek-ai/MAgent"
"openai/multiagent-particle-envs" -> "xuehy/pytorch-maddpg"
"openai/multiagent-particle-envs" -> "shariqiqbal2810/maddpg-pytorch"
"openai/multiagent-particle-envs" -> "shariqiqbal2810/MAAC"
"openai/multiagent-particle-envs" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"openai/multiagent-particle-envs" -> "sisl/MADRL"
"openai/multiagent-particle-envs" -> "starry-sky6688/MADDPG"
"openai/multiagent-particle-envs" -> "openai/multi-agent-emergence-environments"
"google-deepmind/ai-safety-gridworlds" -> "google-deepmind/pycolab"
"google-deepmind/ai-safety-gridworlds" -> "openai/safety-gym" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "openai/safety-starter-agents" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "Farama-Foundation/Minigrid"
"google-deepmind/ai-safety-gridworlds" -> "befelix/safe_learning" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "google-deepmind/bsuite"
"google-deepmind/ai-safety-gridworlds" -> "PKU-Alignment/safety-gymnasium" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "google-deepmind/scalable_agent"
"google-deepmind/ai-safety-gridworlds" -> "kenjyoung/MinAtar" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "chauncygu/Safe-Reinforcement-Learning-Baselines" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "mila-iqia/atari-representation-learning" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "openai/coinrun"
"google-deepmind/ai-safety-gridworlds" -> "astooke/rlpyt"
"google-deepmind/ai-safety-gridworlds" -> "eleurent/rl-agents" ["e"=1]
"google-deepmind/ai-safety-gridworlds" -> "quanvuong/handful-of-trials-pytorch" ["e"=1]
"google-deepmind/pycolab" -> "google-deepmind/ai-safety-gridworlds"
"google-deepmind/pycolab" -> "Farama-Foundation/Minigrid"
"google-deepmind/pycolab" -> "google-deepmind/scalable_agent"
"google-deepmind/pycolab" -> "zuoxingdong/mazelab"
"google-deepmind/pycolab" -> "google-deepmind/bsuite"
"google-deepmind/pycolab" -> "google-research/batch-ppo"
"google-deepmind/pycolab" -> "junhyukoh/value-prediction-network"
"google-deepmind/pycolab" -> "openai/multiagent-competition"
"google-deepmind/pycolab" -> "mila-iqia/atari-representation-learning" ["e"=1]
"google-deepmind/pycolab" -> "mpSchrader/gym-sokoban"
"google-deepmind/pycolab" -> "google-deepmind/dnc" ["e"=1]
"google-deepmind/pycolab" -> "miyosuda/unreal"
"google-deepmind/pycolab" -> "benelot/pybullet-gym"
"google-deepmind/pycolab" -> "google-deepmind/lab2d"
"google-deepmind/pycolab" -> "lcswillems/rl-starter-files"
"thu-ml/tianshou" -> "DLR-RM/stable-baselines3"
"thu-ml/tianshou" -> "AI4Finance-Foundation/ElegantRL"
"thu-ml/tianshou" -> "vwxyzjn/cleanrl"
"thu-ml/tianshou" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"thu-ml/tianshou" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"thu-ml/tianshou" -> "openai/spinningup"
"thu-ml/tianshou" -> "openai/baselines"
"thu-ml/tianshou" -> "datawhalechina/easy-rl"
"thu-ml/tianshou" -> "NeuronDance/DeepRL"
"thu-ml/tianshou" -> "LantaoYu/MARL-Papers"
"thu-ml/tianshou" -> "opendilab/DI-engine" ["e"=1]
"thu-ml/tianshou" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"thu-ml/tianshou" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"thu-ml/tianshou" -> "hill-a/stable-baselines"
"thu-ml/tianshou" -> "PaddlePaddle/PARL"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "brynhayder/reinforcement_learning_an_introduction"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "vojtamolda/reinforcement-learning-an-introduction"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "iamhectorotero/rlai-exercises"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "dennybritz/reinforcement-learning"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "DLR-RM/stable-baselines3"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "NeuronDance/DeepRL"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "vwxyzjn/cleanrl"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "thu-ml/tianshou"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "LantaoYu/MARL-Papers"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "openai/spinningup"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "ShangtongZhang/DeepRL"
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" -> "AI4Finance-Foundation/ElegantRL"
"openai/sonic-on-ray" -> "openai/post--example"
"openai/sonic-on-ray" -> "openai/retro-contest"
"openai/sonic-on-ray" -> "openai/ceph-chef"
"openai/sonic-on-ray" -> "openai/retro-baselines"
"openai/spinningup" -> "openai/baselines"
"openai/spinningup" -> "DLR-RM/stable-baselines3"
"openai/spinningup" -> "thu-ml/tianshou"
"openai/spinningup" -> "openai/gym"
"openai/spinningup" -> "hill-a/stable-baselines"
"openai/spinningup" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"openai/spinningup" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"openai/spinningup" -> "dennybritz/reinforcement-learning"
"openai/spinningup" -> "vwxyzjn/cleanrl"
"openai/spinningup" -> "google/dopamine"
"openai/spinningup" -> "openai/mujoco-py"
"openai/spinningup" -> "Farama-Foundation/Gymnasium"
"openai/spinningup" -> "rlworkgroup/garage"
"openai/spinningup" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"openai/spinningup" -> "LantaoYu/MARL-Papers"
"openai/coinrun" -> "openai/train-procgen"
"openai/coinrun" -> "pokaxpoka/netrand"
"openai/coinrun" -> "openai/random-network-distillation"
"openai/coinrun" -> "google-deepmind/scalable_agent"
"openai/coinrun" -> "vitchyr/multiworld"
"openai/coinrun" -> "openai/EPG"
"openai/coinrun" -> "openai/large-scale-curiosity"
"openai/coinrun" -> "katerakelly/oyster" ["e"=1]
"openai/coinrun" -> "iclavera/learning_to_adapt" ["e"=1]
"openai/coinrun" -> "openai/retro-baselines"
"openai/coinrun" -> "openai/procgen"
"openai/coinrun" -> "WilsonWangTHU/mbbl" ["e"=1]
"openai/post--example" -> "openai/ceph-chef"
"openai/post--example" -> "openai/aws-fluent-plugin-kinesis"
"openai/post--example" -> "openai/retask"
"google-deepmind/dm_control" -> "google-deepmind/mujoco_menagerie" ["e"=1]
"google-deepmind/dm_control" -> "google-deepmind/mujoco" ["e"=1]
"google-deepmind/dm_control" -> "openai/mujoco-py"
"google-deepmind/dm_control" -> "ARISE-Initiative/robosuite" ["e"=1]
"google-deepmind/dm_control" -> "rll/rllab"
"google-deepmind/dm_control" -> "rail-berkeley/rlkit"
"google-deepmind/dm_control" -> "google/brax" ["e"=1]
"google-deepmind/dm_control" -> "Farama-Foundation/D4RL" ["e"=1]
"google-deepmind/dm_control" -> "google-deepmind/acme"
"google-deepmind/dm_control" -> "isaac-sim/IsaacGymEnvs" ["e"=1]
"google-deepmind/dm_control" -> "google-deepmind/lab"
"google-deepmind/dm_control" -> "Farama-Foundation/Metaworld" ["e"=1]
"google-deepmind/dm_control" -> "openai/baselines"
"google-deepmind/dm_control" -> "rlworkgroup/garage"
"google-deepmind/dm_control" -> "google-deepmind/mujoco_mpc" ["e"=1]
"Khrylx/PyTorch-RL" -> "reinforcement-learning-kr/lets-do-irl"
"Khrylx/PyTorch-RL" -> "ikostrikov/pytorch-trpo"
"Khrylx/PyTorch-RL" -> "ShangtongZhang/DeepRL"
"Khrylx/PyTorch-RL" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"Khrylx/PyTorch-RL" -> "rail-berkeley/rlkit"
"Khrylx/PyTorch-RL" -> "openai/imitation"
"Khrylx/PyTorch-RL" -> "MatthewJA/Inverse-Reinforcement-Learning"
"Khrylx/PyTorch-RL" -> "pranz24/pytorch-soft-actor-critic"
"Khrylx/PyTorch-RL" -> "astooke/rlpyt"
"Khrylx/PyTorch-RL" -> "Kaixhin/imitation-learning"
"Khrylx/PyTorch-RL" -> "quantumiracle/Popular-RL-Algorithms"
"Khrylx/PyTorch-RL" -> "yrlu/irl-imitation"
"Khrylx/PyTorch-RL" -> "shariqiqbal2810/MAAC"
"Khrylx/PyTorch-RL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"Khrylx/PyTorch-RL" -> "jingweiz/pytorch-rl"
"rlworkgroup/garage" -> "rll/rllab"
"rlworkgroup/garage" -> "Farama-Foundation/Metaworld" ["e"=1]
"rlworkgroup/garage" -> "rail-berkeley/rlkit"
"rlworkgroup/garage" -> "astooke/rlpyt"
"rlworkgroup/garage" -> "rail-berkeley/softlearning"
"rlworkgroup/garage" -> "hill-a/stable-baselines"
"rlworkgroup/garage" -> "Farama-Foundation/D4RL" ["e"=1]
"rlworkgroup/garage" -> "google-deepmind/dm_control"
"rlworkgroup/garage" -> "Farama-Foundation/Minigrid"
"rlworkgroup/garage" -> "google-deepmind/acme"
"rlworkgroup/garage" -> "araffin/rl-baselines-zoo"
"rlworkgroup/garage" -> "openai/baselines"
"rlworkgroup/garage" -> "IntelLabs/coach"
"rlworkgroup/garage" -> "stepjam/RLBench" ["e"=1]
"rlworkgroup/garage" -> "google-deepmind/bsuite"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "Shmuma/ptan"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "udacity/deep-reinforcement-learning"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "NeuronDance/DeepRL"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "ShangtongZhang/DeepRL"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "simoninithomas/Deep_reinforcement_learning_Course"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "andri27-ts/Reinforcement-Learning"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "LantaoYu/MARL-Papers"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "PacktPublishing/Deep-Learning-with-Keras" ["e"=1]
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "openai/spinningup"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "dennybritz/reinforcement-learning"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" -> "hill-a/stable-baselines"
"uber-research/poet" -> "ucl-dark/paired"
"uber-research/poet" -> "yaricom/goNEAT_NS"
"uber-research/poet" -> "icaros-usc/pyribs" ["e"=1]
"uber-research/poet" -> "facebookresearch/dcd"
"google-deepmind/open_spiel" -> "Farama-Foundation/PettingZoo"
"google-deepmind/open_spiel" -> "google-deepmind/bsuite"
"google-deepmind/open_spiel" -> "google-deepmind/acme"
"google-deepmind/open_spiel" -> "datamllab/rlcard" ["e"=1]
"google-deepmind/open_spiel" -> "werner-duvaud/muzero-general" ["e"=1]
"google-deepmind/open_spiel" -> "LantaoYu/MARL-Papers"
"google-deepmind/open_spiel" -> "astooke/rlpyt"
"google-deepmind/open_spiel" -> "oxwhirl/pymarl"
"google-deepmind/open_spiel" -> "hill-a/stable-baselines"
"google-deepmind/open_spiel" -> "Farama-Foundation/Minigrid"
"google-deepmind/open_spiel" -> "google/dopamine"
"google-deepmind/open_spiel" -> "google-deepmind/dm_control"
"google-deepmind/open_spiel" -> "suragnair/alpha-zero-general" ["e"=1]
"google-deepmind/open_spiel" -> "openai/baselines"
"google-deepmind/open_spiel" -> "google-deepmind/trfl"
"google-research/football" -> "oxwhirl/smac"
"google-research/football" -> "oxwhirl/pymarl"
"google-research/football" -> "marlbenchmark/on-policy"
"google-research/football" -> "google-deepmind/open_spiel"
"google-research/football" -> "Farama-Foundation/PettingZoo"
"google-research/football" -> "starry-sky6688/MARL-Algorithms"
"google-research/football" -> "LantaoYu/MARL-Papers"
"google-research/football" -> "openai/multiagent-particle-envs"
"google-research/football" -> "hijkzzz/pymarl2"
"google-research/football" -> "BazkieBumpercar/GameplayFootball"
"google-research/football" -> "hill-a/stable-baselines"
"google-research/football" -> "astooke/rlpyt"
"google-research/football" -> "google-deepmind/acme"
"google-research/football" -> "geek-ai/MAgent"
"google-research/football" -> "openai/maddpg"
"vy007vikas/PyTorch-ActorCriticRL" -> "ghliu/pytorch-ddpg"
"vy007vikas/PyTorch-ActorCriticRL" -> "ikostrikov/pytorch-ddpg-naf"
"vy007vikas/PyTorch-ActorCriticRL" -> "ikostrikov/pytorch-a3c"
"vy007vikas/PyTorch-ActorCriticRL" -> "MorvanZhou/pytorch-A3C"
"vy007vikas/PyTorch-ActorCriticRL" -> "Khrylx/PyTorch-RL"
"vy007vikas/PyTorch-ActorCriticRL" -> "xuehy/pytorch-maddpg"
"vy007vikas/PyTorch-ActorCriticRL" -> "ChenglongChen/pytorch-DRL"
"vy007vikas/PyTorch-ActorCriticRL" -> "dgriff777/a3c_continuous"
"vy007vikas/PyTorch-ActorCriticRL" -> "pranz24/pytorch-soft-actor-critic"
"vy007vikas/PyTorch-ActorCriticRL" -> "nikhilbarhate99/Actor-Critic-PyTorch"
"vy007vikas/PyTorch-ActorCriticRL" -> "sfujim/TD3"
"vy007vikas/PyTorch-ActorCriticRL" -> "floodsung/DDPG"
"vy007vikas/PyTorch-ActorCriticRL" -> "nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2"
"vy007vikas/PyTorch-ActorCriticRL" -> "starry-sky6688/MADDPG"
"vy007vikas/PyTorch-ActorCriticRL" -> "dgriff777/rl_a3c_pytorch"
"HumanCompatibleAI/overcooked_ai" -> "HumanCompatibleAI/human_aware_rl"
"HumanCompatibleAI/overcooked_ai" -> "Stanford-ILIAD/PantheonRL"
"HumanCompatibleAI/overcooked_ai" -> "rosewang2008/gym-cooking"
"HumanCompatibleAI/overcooked_ai" -> "oxwhirl/smac"
"HumanCompatibleAI/overcooked_ai" -> "uoe-agents/epymarl"
"HumanCompatibleAI/overcooked_ai" -> "FLAIROx/JaxMARL" ["e"=1]
"HumanCompatibleAI/overcooked_ai" -> "marlbenchmark/on-policy"
"HumanCompatibleAI/overcooked_ai" -> "HumanCompatibleAI/overcooked-demo"
"HumanCompatibleAI/overcooked_ai" -> "google-deepmind/meltingpot"
"HumanCompatibleAI/overcooked_ai" -> "oxwhirl/pymarl"
"HumanCompatibleAI/overcooked_ai" -> "schroederdewitt/multiagent_mujoco"
"HumanCompatibleAI/overcooked_ai" -> "oxwhirl/smacv2"
"HumanCompatibleAI/overcooked_ai" -> "sail-sg/envpool" ["e"=1]
"HumanCompatibleAI/overcooked_ai" -> "Farama-Foundation/D4RL" ["e"=1]
"HumanCompatibleAI/overcooked_ai" -> "HumanCompatibleAI/imitation"
"shivaverma/OpenAIGym" -> "shivaverma/Orbit"
"shivaverma/OpenAIGym" -> "nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2"
"sfujim/TD3" -> "rail-berkeley/rlkit"
"sfujim/TD3" -> "pranz24/pytorch-soft-actor-critic"
"sfujim/TD3" -> "haarnoja/sac"
"sfujim/TD3" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"sfujim/TD3" -> "rail-berkeley/softlearning"
"sfujim/TD3" -> "sfujim/BCQ" ["e"=1]
"sfujim/TD3" -> "ShangtongZhang/DeepRL"
"sfujim/TD3" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"sfujim/TD3" -> "AI4Finance-Foundation/ElegantRL"
"sfujim/TD3" -> "Farama-Foundation/D4RL" ["e"=1]
"sfujim/TD3" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"sfujim/TD3" -> "astooke/rlpyt"
"sfujim/TD3" -> "hill-a/stable-baselines"
"sfujim/TD3" -> "sfujim/TD3_BC" ["e"=1]
"sfujim/TD3" -> "rll/rllab"
"wangshusen/DeepLearning" -> "wangshusen/DRL"
"wangshusen/DeepLearning" -> "wangshusen/RecommenderSystem" ["e"=1]
"wangshusen/DeepLearning" -> "datawhalechina/easy-rl"
"wangshusen/DeepLearning" -> "boyu-ai/Hands-on-RL"
"wangshusen/DeepLearning" -> "zhoubolei/introRL"
"wangshusen/DeepLearning" -> "mli/paper-reading" ["e"=1]
"wangshusen/DeepLearning" -> "DA-southampton/NLP_ability" ["e"=1]
"wangshusen/DeepLearning" -> "wangshusen/AdvancedAlgorithms"
"wangshusen/DeepLearning" -> "youngfish42/Awesome-FL" ["e"=1]
"wangshusen/DeepLearning" -> "innovation-cat/Awesome-Federated-Machine-Learning" ["e"=1]
"wangshusen/DeepLearning" -> "dair-ai/ml-visuals" ["e"=1]
"wangshusen/DeepLearning" -> "NeuronDance/DeepRL"
"wangshusen/DeepLearning" -> "datawhalechina/fun-rec" ["e"=1]
"wangshusen/DeepLearning" -> "thu-ml/tianshou"
"wangshusen/DeepLearning" -> "chaoyanghe/Awesome-Federated-Learning" ["e"=1]
"takuseno/ppo" -> "uidilr/ppo_tf"
"takuseno/ppo" -> "shareeff/PPO"
"simoninithomas/Deep_reinforcement_learning_Course" -> "udacity/deep-reinforcement-learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "dennybritz/reinforcement-learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "andri27-ts/Reinforcement-Learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"simoninithomas/Deep_reinforcement_learning_Course" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"simoninithomas/Deep_reinforcement_learning_Course" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"simoninithomas/Deep_reinforcement_learning_Course" -> "rlcode/reinforcement-learning"
"simoninithomas/Deep_reinforcement_learning_Course" -> "aikorea/awesome-rl"
"simoninithomas/Deep_reinforcement_learning_Course" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"simoninithomas/Deep_reinforcement_learning_Course" -> "openai/baselines"
"simoninithomas/Deep_reinforcement_learning_Course" -> "yandexdataschool/Practical_RL"
"simoninithomas/Deep_reinforcement_learning_Course" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"simoninithomas/Deep_reinforcement_learning_Course" -> "openai/spinningup"
"simoninithomas/Deep_reinforcement_learning_Course" -> "keras-rl/keras-rl"
"simoninithomas/Deep_reinforcement_learning_Course" -> "ShangtongZhang/DeepRL"
"openai/maddpg" -> "openai/multiagent-particle-envs"
"openai/maddpg" -> "xuehy/pytorch-maddpg"
"openai/maddpg" -> "shariqiqbal2810/maddpg-pytorch"
"openai/maddpg" -> "oxwhirl/pymarl"
"openai/maddpg" -> "marlbenchmark/on-policy"
"openai/maddpg" -> "starry-sky6688/MADDPG"
"openai/maddpg" -> "LantaoYu/MARL-Papers"
"openai/maddpg" -> "shariqiqbal2810/MAAC"
"openai/maddpg" -> "starry-sky6688/MARL-Algorithms"
"openai/maddpg" -> "geek-ai/MAgent"
"openai/maddpg" -> "oxwhirl/smac"
"openai/maddpg" -> "sisl/MADRL"
"openai/maddpg" -> "Farama-Foundation/PettingZoo"
"openai/maddpg" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"openai/maddpg" -> "Lizhi-sjtu/MARL-code-pytorch"
"reinforcement-learning-kr/lets-do-irl" -> "MatthewJA/Inverse-Reinforcement-Learning"
"reinforcement-learning-kr/lets-do-irl" -> "yrlu/irl-imitation"
"reinforcement-learning-kr/lets-do-irl" -> "Kaixhin/imitation-learning"
"reinforcement-learning-kr/lets-do-irl" -> "jangirrishabh/toyCarIRL"
"reinforcement-learning-kr/lets-do-irl" -> "Khrylx/PyTorch-RL"
"reinforcement-learning-kr/lets-do-irl" -> "qzed/irl-maxent"
"reinforcement-learning-kr/lets-do-irl" -> "justinjfu/inverse_rl"
"reinforcement-learning-kr/lets-do-irl" -> "kristery/Awesome-Imitation-Learning"
"reinforcement-learning-kr/lets-do-irl" -> "HumanCompatibleAI/imitation"
"reinforcement-learning-kr/lets-do-irl" -> "toshikwa/gail-airl-ppo.pytorch"
"reinforcement-learning-kr/lets-do-irl" -> "ermongroup/MA-AIRL"
"reinforcement-learning-kr/lets-do-irl" -> "MCZhi/Driving-IRL-NGSIM" ["e"=1]
"reinforcement-learning-kr/lets-do-irl" -> "ahq1993/inverse_rl"
"reinforcement-learning-kr/lets-do-irl" -> "andrewliao11/gail-tf"
"reinforcement-learning-kr/lets-do-irl" -> "seolhokim/InverseRL-Pytorch"
"higgsfield-ai/higgsfield" -> "higgsfield/RL-Adventure"
"higgsfield-ai/higgsfield" -> "ShangtongZhang/DeepRL"
"higgsfield-ai/higgsfield" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"higgsfield-ai/higgsfield" -> "rail-berkeley/rlkit"
"higgsfield-ai/higgsfield" -> "hill-a/stable-baselines"
"higgsfield-ai/higgsfield" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"higgsfield-ai/higgsfield" -> "rll/rllab"
"higgsfield-ai/higgsfield" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"higgsfield-ai/higgsfield" -> "yandexdataschool/Practical_RL"
"higgsfield-ai/higgsfield" -> "seungeunrho/minimalRL"
"higgsfield-ai/higgsfield" -> "sfujim/TD3"
"higgsfield-ai/higgsfield" -> "openai/baselines"
"higgsfield-ai/higgsfield" -> "astooke/rlpyt"
"higgsfield-ai/higgsfield" -> "rlworkgroup/garage"
"higgsfield-ai/higgsfield" -> "udacity/deep-reinforcement-learning"
"Kautenja/nes-py" -> "Kautenja/gym-super-mario-bros"
"Kautenja/nes-py" -> "PyAndy/Py3NES"
"nikhilbarhate99/PPO-PyTorch" -> "ericyangyu/PPO-for-Beginners"
"nikhilbarhate99/PPO-PyTorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"nikhilbarhate99/PPO-PyTorch" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"nikhilbarhate99/PPO-PyTorch" -> "pranz24/pytorch-soft-actor-critic"
"nikhilbarhate99/PPO-PyTorch" -> "sfujim/TD3"
"nikhilbarhate99/PPO-PyTorch" -> "marlbenchmark/on-policy"
"nikhilbarhate99/PPO-PyTorch" -> "Lizhi-sjtu/DRL-code-pytorch"
"nikhilbarhate99/PPO-PyTorch" -> "vwxyzjn/ppo-implementation-details"
"nikhilbarhate99/PPO-PyTorch" -> "seungeunrho/minimalRL"
"nikhilbarhate99/PPO-PyTorch" -> "DLR-RM/stable-baselines3"
"nikhilbarhate99/PPO-PyTorch" -> "starry-sky6688/MARL-Algorithms"
"nikhilbarhate99/PPO-PyTorch" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"nikhilbarhate99/PPO-PyTorch" -> "AI4Finance-Foundation/ElegantRL"
"nikhilbarhate99/PPO-PyTorch" -> "opendilab/PPOxFamily" ["e"=1]
"nikhilbarhate99/PPO-PyTorch" -> "thu-ml/tianshou"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "thu-ml/tianshou"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "sfujim/TD3"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "nikhilbarhate99/PPO-PyTorch"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "AI4Finance-Foundation/ElegantRL"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "higgsfield/RL-Adventure"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "ShangtongZhang/DeepRL"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "Lizhi-sjtu/DRL-code-pytorch"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "NeuronDance/DeepRL"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "starry-sky6688/MARL-Algorithms"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "openai/baselines"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "LantaoYu/MARL-Papers"
"sweetice/Deep-reinforcement-learning-with-pytorch" -> "DLR-RM/stable-baselines3"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "thu-ml/tianshou"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "astooke/rlpyt"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "openai/spinningup"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "ShangtongZhang/DeepRL"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "DLR-RM/stable-baselines3"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "hill-a/stable-baselines"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "openai/baselines"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "NeuronDance/DeepRL"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "AI4Finance-Foundation/ElegantRL"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "dennybritz/reinforcement-learning"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "rail-berkeley/rlkit"
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" -> "vwxyzjn/cleanrl"
"oxwhirl/pymarl" -> "oxwhirl/smac"
"oxwhirl/pymarl" -> "starry-sky6688/MARL-Algorithms"
"oxwhirl/pymarl" -> "marlbenchmark/on-policy"
"oxwhirl/pymarl" -> "LantaoYu/MARL-Papers"
"oxwhirl/pymarl" -> "hijkzzz/pymarl2"
"oxwhirl/pymarl" -> "uoe-agents/epymarl"
"oxwhirl/pymarl" -> "Farama-Foundation/PettingZoo"
"oxwhirl/pymarl" -> "openai/maddpg"
"oxwhirl/pymarl" -> "openai/multiagent-particle-envs"
"oxwhirl/pymarl" -> "shariqiqbal2810/MAAC"
"oxwhirl/pymarl" -> "Replicable-MARL/MARLlib"
"oxwhirl/pymarl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"oxwhirl/pymarl" -> "geek-ai/MAgent"
"oxwhirl/pymarl" -> "marlbenchmark/off-policy"
"oxwhirl/pymarl" -> "shariqiqbal2810/maddpg-pytorch"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "mimoralea/gdrl"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "kengz/SLM-Lab"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "qfettes/DeepRL-Tutorials"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "kengz/awesome-deep-rl"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "PacktPublishing/Hands-On-Reinforcement-Learning-with-Python"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "Khrylx/PyTorch-RL"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "quantumiracle/Popular-RL-Algorithms"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "ZhiqingXiao/rl-book"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "xuehy/pytorch-maddpg"
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "maxpumperla/deep_learning_and_the_game_of_go" ["e"=1]
"DeepReinforcementLearning/DeepReinforcementLearningInAction" -> "udacity/deep-reinforcement-learning"
"seungeunrho/minimalRL" -> "astooke/rlpyt"
"seungeunrho/minimalRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"seungeunrho/minimalRL" -> "ShangtongZhang/DeepRL"
"seungeunrho/minimalRL" -> "vwxyzjn/cleanrl"
"seungeunrho/minimalRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"seungeunrho/minimalRL" -> "higgsfield-ai/higgsfield"
"seungeunrho/minimalRL" -> "higgsfield/RL-Adventure"
"seungeunrho/minimalRL" -> "Curt-Park/rainbow-is-all-you-need"
"seungeunrho/minimalRL" -> "hill-a/stable-baselines"
"seungeunrho/minimalRL" -> "rail-berkeley/rlkit"
"seungeunrho/minimalRL" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"seungeunrho/minimalRL" -> "nikhilbarhate99/PPO-PyTorch"
"seungeunrho/minimalRL" -> "LantaoYu/MARL-Papers"
"seungeunrho/minimalRL" -> "quantumiracle/Popular-RL-Algorithms"
"seungeunrho/minimalRL" -> "Farama-Foundation/Minigrid"
"kathgironpe/awesome-omscs" -> "pyjarrett/OMSCS_Survival_Guide"
"udacity/deep-reinforcement-learning" -> "ShangtongZhang/DeepRL"
"udacity/deep-reinforcement-learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"udacity/deep-reinforcement-learning" -> "dennybritz/reinforcement-learning"
"udacity/deep-reinforcement-learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"udacity/deep-reinforcement-learning" -> "udacity/deep-learning-v2-pytorch" ["e"=1]
"udacity/deep-reinforcement-learning" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"udacity/deep-reinforcement-learning" -> "rlcode/reinforcement-learning"
"udacity/deep-reinforcement-learning" -> "aikorea/awesome-rl"
"udacity/deep-reinforcement-learning" -> "udacity/deep-learning" ["e"=1]
"udacity/deep-reinforcement-learning" -> "andri27-ts/Reinforcement-Learning"
"udacity/deep-reinforcement-learning" -> "openai/baselines"
"udacity/deep-reinforcement-learning" -> "yandexdataschool/Practical_RL"
"udacity/deep-reinforcement-learning" -> "keras-rl/keras-rl"
"udacity/deep-reinforcement-learning" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"udacity/deep-reinforcement-learning" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"greerviau/SnakeAI" -> "ArztSamuel/Applying_EANNs"
"greerviau/SnakeAI" -> "Chrispresso/SnakeAI"
"greerviau/SnakeAI" -> "Code-Bullet/SnakeFusion"
"greerviau/SnakeAI" -> "greerviau/TetrisAI"
"greerviau/SnakeAI" -> "chuyangliu/snake" ["e"=1]
"greerviau/SnakeAI" -> "techwithtim/NEAT-Flappy-Bird" ["e"=1]
"greerviau/SnakeAI" -> "maurock/snake-ga" ["e"=1]
"greerviau/SnakeAI" -> "patrickloeber/snake-ai-pytorch" ["e"=1]
"greerviau/SnakeAI" -> "linyiLYi/snake-ai" ["e"=1]
"greerviau/SnakeAI" -> "greerviau/DoodleJumpAI"
"greerviau/SnakeAI" -> "ssusnic/Machine-Learning-Flappy-Bird"
"greerviau/SnakeAI" -> "streamlit/demo-self-driving" ["e"=1]
"greerviau/SnakeAI" -> "openai/multi-agent-emergence-environments"
"greerviau/SnakeAI" -> "Chrispresso/SuperMarioBros-AI"
"greerviau/SnakeAI" -> "Code-Bullet/Google-Chrome-Dino-Game-AI"
"Farama-Foundation/Minigrid" -> "lcswillems/rl-starter-files"
"Farama-Foundation/Minigrid" -> "Farama-Foundation/Miniworld"
"Farama-Foundation/Minigrid" -> "Farama-Foundation/Metaworld" ["e"=1]
"Farama-Foundation/Minigrid" -> "Farama-Foundation/D4RL" ["e"=1]
"Farama-Foundation/Minigrid" -> "astooke/rlpyt"
"Farama-Foundation/Minigrid" -> "openai/procgen"
"Farama-Foundation/Minigrid" -> "rail-berkeley/rlkit"
"Farama-Foundation/Minigrid" -> "Farama-Foundation/PettingZoo"
"Farama-Foundation/Minigrid" -> "hill-a/stable-baselines"
"Farama-Foundation/Minigrid" -> "mila-iqia/babyai"
"Farama-Foundation/Minigrid" -> "clvrai/awesome-rl-envs"
"Farama-Foundation/Minigrid" -> "google-deepmind/bsuite"
"Farama-Foundation/Minigrid" -> "rlworkgroup/garage"
"Farama-Foundation/Minigrid" -> "google-deepmind/dm_control"
"Farama-Foundation/Minigrid" -> "google-deepmind/acme"
"MorvanZhou/pytorch-A3C" -> "ikostrikov/pytorch-a3c"
"MorvanZhou/pytorch-A3C" -> "dgriff777/a3c_continuous"
"MorvanZhou/pytorch-A3C" -> "dgriff777/rl_a3c_pytorch"
"MorvanZhou/pytorch-A3C" -> "ghliu/pytorch-ddpg"
"MorvanZhou/pytorch-A3C" -> "quantumiracle/Popular-RL-Algorithms"
"MorvanZhou/pytorch-A3C" -> "jingweiz/pytorch-rl"
"MorvanZhou/pytorch-A3C" -> "vy007vikas/PyTorch-ActorCriticRL"
"MorvanZhou/pytorch-A3C" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"MorvanZhou/pytorch-A3C" -> "ikostrikov/pytorch-trpo"
"MorvanZhou/pytorch-A3C" -> "sfujim/TD3"
"MorvanZhou/pytorch-A3C" -> "ShangtongZhang/DeepRL"
"MorvanZhou/pytorch-A3C" -> "Khrylx/PyTorch-RL"
"MorvanZhou/pytorch-A3C" -> "ChenglongChen/pytorch-DRL"
"MorvanZhou/pytorch-A3C" -> "shariqiqbal2810/MAAC"
"MorvanZhou/pytorch-A3C" -> "NVlabs/GA3C"
"tensorflow/agents" -> "tensorforce/tensorforce"
"tensorflow/agents" -> "hill-a/stable-baselines"
"tensorflow/agents" -> "google-deepmind/trfl"
"tensorflow/agents" -> "google-deepmind/acme"
"tensorflow/agents" -> "google/dopamine"
"tensorflow/agents" -> "keras-rl/keras-rl"
"tensorflow/agents" -> "facebookresearch/ReAgent"
"tensorflow/agents" -> "IntelLabs/coach"
"tensorflow/agents" -> "astooke/rlpyt"
"tensorflow/agents" -> "openai/baselines"
"tensorflow/agents" -> "rlworkgroup/garage"
"tensorflow/agents" -> "google-deepmind/bsuite"
"tensorflow/agents" -> "openai/spinningup"
"tensorflow/agents" -> "araffin/rl-baselines-zoo"
"tensorflow/agents" -> "google-deepmind/open_spiel"
"beyretb/AnimalAI-Olympics" -> "mdcrosby/animal-ai"
"beyretb/AnimalAI-Olympics" -> "openai/coinrun"
"beyretb/AnimalAI-Olympics" -> "Unity-Technologies/obstacle-tower-env"
"beyretb/AnimalAI-Olympics" -> "seungjaeryanlee/awesome-rl-competitions"
"beyretb/AnimalAI-Olympics" -> "google-deepmind/bsuite"
"beyretb/AnimalAI-Olympics" -> "Farama-Foundation/Miniworld"
"beyretb/AnimalAI-Olympics" -> "astooke/rlpyt"
"beyretb/AnimalAI-Olympics" -> "benelot/pybullet-gym"
"beyretb/AnimalAI-Olympics" -> "junhyukoh/self-imitation-learning"
"beyretb/AnimalAI-Olympics" -> "kandouss/marlgrid"
"beyretb/AnimalAI-Olympics" -> "MultiAgentLearning/playground"
"beyretb/AnimalAI-Olympics" -> "uber-research/go-explore"
"beyretb/AnimalAI-Olympics" -> "unixpickle/anyrl-py"
"beyretb/AnimalAI-Olympics" -> "MushroomRL/mushroom-rl"
"lexfridman/deeptraffic" -> "lexfridman/mit-deep-learning" ["e"=1]
"lexfridman/deeptraffic" -> "lexfridman/deeptesla" ["e"=1]
"lexfridman/deeptraffic" -> "yenchenlin/DeepLearningFlappyBird"
"lexfridman/deeptraffic" -> "facebookresearch/ReAgent"
"lexfridman/deeptraffic" -> "udacity/self-driving-car-sim" ["e"=1]
"lexfridman/deeptraffic" -> "lengstrom/fast-style-transfer" ["e"=1]
"lexfridman/deeptraffic" -> "google-deepmind/trfl"
"lexfridman/deeptraffic" -> "udacity/deep-reinforcement-learning"
"lexfridman/deeptraffic" -> "udacity/deep-learning-v2-pytorch" ["e"=1]
"lexfridman/deeptraffic" -> "udacity/self-driving-car" ["e"=1]
"lexfridman/deeptraffic" -> "ndrplz/self-driving-car" ["e"=1]
"lexfridman/deeptraffic" -> "Farama-Foundation/HighwayEnv" ["e"=1]
"lexfridman/deeptraffic" -> "simoninithomas/Deep_reinforcement_learning_Course"
"lexfridman/deeptraffic" -> "andri27-ts/Reinforcement-Learning"
"lexfridman/deeptraffic" -> "lgsvl/simulator" ["e"=1]
"philtabor/Youtube-Code-Repository" -> "philtabor/Deep-Q-Learning-Paper-To-Code"
"philtabor/Youtube-Code-Repository" -> "philtabor/Actor-Critic-Methods-Paper-To-Code"
"philtabor/Youtube-Code-Repository" -> "pythonlessons/Reinforcement_Learning"
"philtabor/Youtube-Code-Repository" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"philtabor/Youtube-Code-Repository" -> "rail-berkeley/softlearning"
"philtabor/Youtube-Code-Repository" -> "sfujim/TD3"
"philtabor/Youtube-Code-Repository" -> "nikhilbarhate99/PPO-PyTorch"
"philtabor/Youtube-Code-Repository" -> "araffin/rl-tutorial-jnrr19"
"philtabor/Youtube-Code-Repository" -> "higgsfield-ai/higgsfield"
"philtabor/Youtube-Code-Repository" -> "tensorflow/agents"
"philtabor/Youtube-Code-Repository" -> "marlbenchmark/on-policy"
"philtabor/Youtube-Code-Repository" -> "Khrylx/PyTorch-RL"
"philtabor/Youtube-Code-Repository" -> "vwxyzjn/ppo-implementation-details"
"philtabor/Youtube-Code-Repository" -> "archsyscall/DeepRL-TensorFlow2"
"philtabor/Youtube-Code-Repository" -> "hill-a/stable-baselines"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "zhoubolei/introRL"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "NeuronDance/DeepRL"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "applenob/rl_learn"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "princewen/tensorflow_practice" ["e"=1]
"wwxFromTju/awesome-reinforcement-learning-zh" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "datawhalechina/easy-rl"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "LantaoYu/MARL-Papers"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "qqiang00/Reinforce"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "ucla-rlcourse/RLexample"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "thu-ml/tianshou"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "AI4Finance-Foundation/ElegantRL"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "tigerneil/awesome-deep-rl"
"wwxFromTju/awesome-reinforcement-learning-zh" -> "openai/spinningup"
"NeuronDance/DeepRL" -> "AI4Finance-Foundation/ElegantRL"
"NeuronDance/DeepRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"NeuronDance/DeepRL" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"NeuronDance/DeepRL" -> "zhoubolei/introRL"
"NeuronDance/DeepRL" -> "thu-ml/tianshou"
"NeuronDance/DeepRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"NeuronDance/DeepRL" -> "starry-sky6688/MARL-Algorithms"
"NeuronDance/DeepRL" -> "oxwhirl/pymarl"
"NeuronDance/DeepRL" -> "LantaoYu/MARL-Papers"
"NeuronDance/DeepRL" -> "PaddlePaddle/PARL"
"NeuronDance/DeepRL" -> "wangshusen/DRL"
"NeuronDance/DeepRL" -> "ShangtongZhang/DeepRL"
"NeuronDance/DeepRL" -> "datawhalechina/easy-rl"
"NeuronDance/DeepRL" -> "zhangchuheng123/Reinforcement-Implementation"
"NeuronDance/DeepRL" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"astooke/rlpyt" -> "rail-berkeley/rlkit"
"astooke/rlpyt" -> "ShangtongZhang/DeepRL"
"astooke/rlpyt" -> "rlworkgroup/garage"
"astooke/rlpyt" -> "hill-a/stable-baselines"
"astooke/rlpyt" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"astooke/rlpyt" -> "Farama-Foundation/Minigrid"
"astooke/rlpyt" -> "google-deepmind/bsuite"
"astooke/rlpyt" -> "google-deepmind/acme"
"astooke/rlpyt" -> "rll/rllab"
"astooke/rlpyt" -> "facebookresearch/torchbeast"
"astooke/rlpyt" -> "rail-berkeley/softlearning"
"astooke/rlpyt" -> "facebookresearch/ReAgent"
"astooke/rlpyt" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"astooke/rlpyt" -> "seungeunrho/minimalRL"
"astooke/rlpyt" -> "Farama-Foundation/D4RL" ["e"=1]
"rlgraph/rlgraph" -> "PKU-RL/DGN"
"rlgraph/rlgraph" -> "ray-project/rl-experiments"
"rlgraph/rlgraph" -> "SurrealAI/surreal" ["e"=1]
"rlgraph/rlgraph" -> "google-deepmind/scalable_agent"
"rlgraph/rlgraph" -> "Officium/RL-Experiments"
"rlgraph/rlgraph" -> "medipixel/rl_algorithms"
"hill-a/stable-baselines" -> "araffin/rl-baselines-zoo"
"hill-a/stable-baselines" -> "openai/baselines"
"hill-a/stable-baselines" -> "DLR-RM/stable-baselines3"
"hill-a/stable-baselines" -> "astooke/rlpyt"
"hill-a/stable-baselines" -> "rail-berkeley/rlkit"
"hill-a/stable-baselines" -> "openai/spinningup"
"hill-a/stable-baselines" -> "tensorforce/tensorforce"
"hill-a/stable-baselines" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"hill-a/stable-baselines" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"hill-a/stable-baselines" -> "rlworkgroup/garage"
"hill-a/stable-baselines" -> "rll/rllab"
"hill-a/stable-baselines" -> "ShangtongZhang/DeepRL"
"hill-a/stable-baselines" -> "tensorflow/agents"
"hill-a/stable-baselines" -> "IntelLabs/coach"
"hill-a/stable-baselines" -> "google/dopamine"
"Unity-Technologies/obstacle-tower-env" -> "Unity-Technologies/obstacle-tower-challenge"
"Unity-Technologies/obstacle-tower-env" -> "Unity-Technologies/obstacle-tower-source"
"Unity-Technologies/obstacle-tower-env" -> "unixpickle/obs-tower2"
"Unity-Technologies/obstacle-tower-env" -> "openai/random-network-distillation"
"Unity-Technologies/obstacle-tower-env" -> "Farama-Foundation/Miniworld"
"Unity-Technologies/obstacle-tower-env" -> "junhyukoh/self-imitation-learning"
"Unity-Technologies/obstacle-tower-env" -> "uber-research/ape-x"
"Unity-Technologies/obstacle-tower-env" -> "beyretb/AnimalAI-Olympics"
"Unity-Technologies/obstacle-tower-env" -> "openai/coinrun"
"Unity-Technologies/obstacle-tower-env" -> "Unity-Technologies/marathon-envs"
"Unity-Technologies/obstacle-tower-env" -> "openai/procgen"
"Unity-Technologies/obstacle-tower-env" -> "google-research/planet" ["e"=1]
"Unity-Technologies/obstacle-tower-env" -> "pathak22/noreward-rl"
"Unity-Technologies/obstacle-tower-env" -> "Kaixhin/PlaNet" ["e"=1]
"Unity-Technologies/obstacle-tower-env" -> "uber-research/go-explore"
"navneet-nmk/pytorch-rl" -> "MillionIntegrals/vel"
"navneet-nmk/pytorch-rl" -> "jingweiz/pytorch-rl"
"navneet-nmk/pytorch-rl" -> "zuoxingdong/lagom"
"navneet-nmk/pytorch-rl" -> "denisyarats/pytorch_sac_ae" ["e"=1]
"navneet-nmk/pytorch-rl" -> "Khrylx/PyTorch-RL"
"navneet-nmk/pytorch-rl" -> "vitchyr/multiworld"
"navneet-nmk/pytorch-rl" -> "TianhongDai/hindsight-experience-replay"
"navneet-nmk/pytorch-rl" -> "higgsfield/Imagination-Augmented-Agents" ["e"=1]
"navneet-nmk/pytorch-rl" -> "openai/coinrun"
"navneet-nmk/pytorch-rl" -> "araffin/robotics-rl-srl" ["e"=1]
"navneet-nmk/pytorch-rl" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"navneet-nmk/pytorch-rl" -> "ShangtongZhang/DeepRL"
"navneet-nmk/pytorch-rl" -> "TianhongDai/reinforcement-learning-algorithms"
"navneet-nmk/pytorch-rl" -> "qfettes/DeepRL-Tutorials"
"navneet-nmk/pytorch-rl" -> "kashif/firedup"
"HumanCompatibleAI/adversarial-policies" -> "chenhongge/StateAdvDRL"
"HumanCompatibleAI/adversarial-policies" -> "huanzhang12/ATLA_robust_RL"
"HumanCompatibleAI/adversarial-policies" -> "openai/train-procgen"
"HumanCompatibleAI/adversarial-policies" -> "nuwuxian/rl_adv_valuediff"
"HumanCompatibleAI/adversarial-policies" -> "openai/multiagent-competition"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "ugurkanates/awesome-real-world-rl"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "brianspiering/awesome-deep-rl"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "dongminlee94/deep_rl"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "TianhongDai/reinforcement-learning-algorithms"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "rlcode/reinforcement-learning"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "quantumiracle/Popular-RL-Algorithms"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "pranz24/pytorch-soft-actor-critic"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "StepNeverStop/RLs"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "udacity/deep-reinforcement-learning"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "TimeBreaker/MARL-papers-with-code"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "Lizhi-sjtu/DRL-code-pytorch"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "NeuronDance/DeepRL"
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" -> "PacktPublishing/Python-Reinforcement-Learning-Projects"
"Curt-Park/rainbow-is-all-you-need" -> "MrSyee/pg-is-all-you-need"
"Curt-Park/rainbow-is-all-you-need" -> "Kaixhin/Rainbow"
"Curt-Park/rainbow-is-all-you-need" -> "higgsfield/RL-Adventure"
"Curt-Park/rainbow-is-all-you-need" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"Curt-Park/rainbow-is-all-you-need" -> "seungeunrho/minimalRL"
"Curt-Park/rainbow-is-all-you-need" -> "NeuronDance/DeepRL"
"Curt-Park/rainbow-is-all-you-need" -> "medipixel/rl_algorithms"
"Curt-Park/rainbow-is-all-you-need" -> "thu-ml/tianshou"
"Curt-Park/rainbow-is-all-you-need" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"Curt-Park/rainbow-is-all-you-need" -> "ShangtongZhang/DeepRL"
"Curt-Park/rainbow-is-all-you-need" -> "qfettes/DeepRL-Tutorials"
"Curt-Park/rainbow-is-all-you-need" -> "vwxyzjn/cleanrl"
"Curt-Park/rainbow-is-all-you-need" -> "astooke/rlpyt"
"Curt-Park/rainbow-is-all-you-need" -> "rail-berkeley/rlkit"
"Curt-Park/rainbow-is-all-you-need" -> "sfujim/TD3"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/Flappy-Bird-AI"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/RubiksCubeAI"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/2048-AI"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/SnakeFusion"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/Pool_AI"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/PacNeat"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/Enigma-Simulator"
"Code-Bullet/Google-Chrome-Dino-Game-AI" -> "Code-Bullet/Asteroids-with-NEAT"
"Kaixhin/Rainbow" -> "Curt-Park/rainbow-is-all-you-need"
"Kaixhin/Rainbow" -> "higgsfield/RL-Adventure"
"Kaixhin/Rainbow" -> "rail-berkeley/rlkit"
"Kaixhin/Rainbow" -> "astooke/rlpyt"
"Kaixhin/Rainbow" -> "ShangtongZhang/DeepRL"
"Kaixhin/Rainbow" -> "ikostrikov/pytorch-a3c"
"Kaixhin/Rainbow" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"Kaixhin/Rainbow" -> "qfettes/DeepRL-Tutorials"
"Kaixhin/Rainbow" -> "rll/rllab"
"Kaixhin/Rainbow" -> "pathak22/noreward-rl"
"Kaixhin/Rainbow" -> "openai/random-network-distillation"
"Kaixhin/Rainbow" -> "Farama-Foundation/Minigrid"
"Kaixhin/Rainbow" -> "google-deepmind/scalable_agent"
"Kaixhin/Rainbow" -> "sfujim/TD3"
"Kaixhin/Rainbow" -> "higgsfield-ai/higgsfield"
"Hanabi-Live/hanabi-live" -> "hanabi/hanabi.github.io"
"Hanabi-Live/hanabi-live" -> "WuTheFWasThat/hanabi.rs"
"Hanabi-Live/hanabi-live" -> "will-hanabi-bot/hanabi-bot"
"Hanabi-Live/hanabi-live" -> "Quuxplusone/Hanabi"
"philtabor/Reinforcement-Learning-In-Motion" -> "philtabor/Simple-Neural-Network"
"IntelLabs/coach" -> "tensorforce/tensorforce"
"IntelLabs/coach" -> "rll/rllab"
"IntelLabs/coach" -> "hill-a/stable-baselines"
"IntelLabs/coach" -> "facebookresearch/ReAgent"
"IntelLabs/coach" -> "rlworkgroup/garage"
"IntelLabs/coach" -> "google-deepmind/trfl"
"IntelLabs/coach" -> "astooke/rlpyt"
"IntelLabs/coach" -> "keras-rl/keras-rl"
"IntelLabs/coach" -> "ShangtongZhang/DeepRL"
"IntelLabs/coach" -> "chainer/chainerrl"
"IntelLabs/coach" -> "google/dopamine"
"IntelLabs/coach" -> "tensorflow/agents"
"IntelLabs/coach" -> "openai/baselines"
"IntelLabs/coach" -> "google-deepmind/dm_control"
"IntelLabs/coach" -> "rail-berkeley/rlkit"
"MultiAgentLearning/playground" -> "geek-ai/MAgent"
"MultiAgentLearning/playground" -> "sisl/MADRL"
"MultiAgentLearning/playground" -> "oxwhirl/smac"
"MultiAgentLearning/playground" -> "google-deepmind/hanabi-learning-environment"
"MultiAgentLearning/playground" -> "tambetm/pommerman-baselines"
"MultiAgentLearning/playground" -> "mlii/mfrl"
"MultiAgentLearning/playground" -> "oxwhirl/pymarl"
"MultiAgentLearning/playground" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"MultiAgentLearning/playground" -> "crowdAI/marLo"
"MultiAgentLearning/playground" -> "Farama-Foundation/Minigrid"
"MultiAgentLearning/playground" -> "xuehy/pytorch-maddpg"
"MultiAgentLearning/playground" -> "PKU-RL/DGN"
"MultiAgentLearning/playground" -> "minqi/learning-to-communicate-pytorch"
"MultiAgentLearning/playground" -> "BorealisAI/pommerman-baseline"
"MultiAgentLearning/playground" -> "openai/multi-agent-emergence-environments"
"PaddlePaddle/PARL" -> "thu-ml/tianshou"
"PaddlePaddle/PARL" -> "NeuronDance/DeepRL"
"PaddlePaddle/PARL" -> "AI4Finance-Foundation/ElegantRL"
"PaddlePaddle/PARL" -> "datawhalechina/easy-rl"
"PaddlePaddle/PARL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"PaddlePaddle/PARL" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"PaddlePaddle/PARL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"PaddlePaddle/PARL" -> "PaddlePaddle/MetaGym"
"PaddlePaddle/PARL" -> "oxwhirl/pymarl"
"PaddlePaddle/PARL" -> "zhoubolei/introRL"
"PaddlePaddle/PARL" -> "openai/baselines"
"PaddlePaddle/PARL" -> "starry-sky6688/MARL-Algorithms"
"PaddlePaddle/PARL" -> "wangshusen/DRL"
"PaddlePaddle/PARL" -> "sfujim/TD3"
"PaddlePaddle/PARL" -> "PaddlePaddle/PGL" ["e"=1]
"andri27-ts/Reinforcement-Learning" -> "udacity/deep-reinforcement-learning"
"andri27-ts/Reinforcement-Learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"andri27-ts/Reinforcement-Learning" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"andri27-ts/Reinforcement-Learning" -> "aikorea/awesome-rl"
"andri27-ts/Reinforcement-Learning" -> "dennybritz/reinforcement-learning"
"andri27-ts/Reinforcement-Learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"andri27-ts/Reinforcement-Learning" -> "rlcode/reinforcement-learning"
"andri27-ts/Reinforcement-Learning" -> "yandexdataschool/Practical_RL"
"andri27-ts/Reinforcement-Learning" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"andri27-ts/Reinforcement-Learning" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"andri27-ts/Reinforcement-Learning" -> "astorfi/Deep-Learning-Roadmap" ["e"=1]
"andri27-ts/Reinforcement-Learning" -> "ShangtongZhang/DeepRL"
"andri27-ts/Reinforcement-Learning" -> "google/dopamine"
"andri27-ts/Reinforcement-Learning" -> "higgsfield/RL-Adventure"
"andri27-ts/Reinforcement-Learning" -> "higgsfield-ai/higgsfield"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "starry-sky6688/MARL-Algorithms"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "oxwhirl/pymarl"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "koulanurag/ma-gym"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "sisl/MADRL"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "shariqiqbal2810/MAAC"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "oxwhirl/smac"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "xuehy/pytorch-maddpg"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "marlbenchmark/on-policy"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "shariqiqbal2810/maddpg-pytorch"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "hijkzzz/pymarl2"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "mlii/mfrl"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "marlbenchmark/off-policy"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "LantaoYu/MARL-Papers"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "geek-ai/MAgent"
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" -> "uoe-agents/epymarl"
"CETC-TFAI/MaCA" -> "SJTUwbl/MaCA" ["e"=1]
"CETC-TFAI/MaCA" -> "liuqh16/LAG" ["e"=1]
"CETC-TFAI/MaCA" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"CETC-TFAI/MaCA" -> "sjtu-marl/malib"
"CETC-TFAI/MaCA" -> "wjh720/QPLEX"
"AI4Finance-Foundation/ElegantRL" -> "thu-ml/tianshou"
"AI4Finance-Foundation/ElegantRL" -> "AI4Finance-Foundation/FinRL" ["e"=1]
"AI4Finance-Foundation/ElegantRL" -> "DLR-RM/stable-baselines3"
"AI4Finance-Foundation/ElegantRL" -> "NeuronDance/DeepRL"
"AI4Finance-Foundation/ElegantRL" -> "marlbenchmark/on-policy"
"AI4Finance-Foundation/ElegantRL" -> "AI4Finance-Foundation/FinRL-Meta" ["e"=1]
"AI4Finance-Foundation/ElegantRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"AI4Finance-Foundation/ElegantRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"AI4Finance-Foundation/ElegantRL" -> "vwxyzjn/cleanrl"
"AI4Finance-Foundation/ElegantRL" -> "starry-sky6688/MARL-Algorithms"
"AI4Finance-Foundation/ElegantRL" -> "kaixindelele/DRLib"
"AI4Finance-Foundation/ElegantRL" -> "opendilab/DI-engine" ["e"=1]
"AI4Finance-Foundation/ElegantRL" -> "sfujim/TD3"
"AI4Finance-Foundation/ElegantRL" -> "Lizhi-sjtu/DRL-code-pytorch"
"AI4Finance-Foundation/ElegantRL" -> "PaddlePaddle/PARL"
"starry-sky6688/MARL-Algorithms" -> "oxwhirl/pymarl"
"starry-sky6688/MARL-Algorithms" -> "oxwhirl/smac"
"starry-sky6688/MARL-Algorithms" -> "marlbenchmark/on-policy"
"starry-sky6688/MARL-Algorithms" -> "hijkzzz/pymarl2"
"starry-sky6688/MARL-Algorithms" -> "Lizhi-sjtu/MARL-code-pytorch"
"starry-sky6688/MARL-Algorithms" -> "marlbenchmark/off-policy"
"starry-sky6688/MARL-Algorithms" -> "Replicable-MARL/MARLlib"
"starry-sky6688/MARL-Algorithms" -> "shariqiqbal2810/MAAC"
"starry-sky6688/MARL-Algorithms" -> "LantaoYu/MARL-Papers"
"starry-sky6688/MARL-Algorithms" -> "starry-sky6688/MADDPG"
"starry-sky6688/MARL-Algorithms" -> "uoe-agents/epymarl"
"starry-sky6688/MARL-Algorithms" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"starry-sky6688/MARL-Algorithms" -> "shariqiqbal2810/maddpg-pytorch"
"starry-sky6688/MARL-Algorithms" -> "openai/maddpg"
"starry-sky6688/MARL-Algorithms" -> "tinyzqh/light_mappo"
"openai/large-scale-curiosity" -> "pathak22/noreward-rl"
"openai/large-scale-curiosity" -> "openai/random-network-distillation"
"openai/large-scale-curiosity" -> "uber-research/go-explore"
"openai/large-scale-curiosity" -> "openai/vime"
"openai/large-scale-curiosity" -> "pathak22/exploration-by-disagreement"
"openai/large-scale-curiosity" -> "google-deepmind/scalable_agent"
"openai/large-scale-curiosity" -> "ml-jku/baselines-rudder"
"openai/large-scale-curiosity" -> "openai/coinrun"
"openai/large-scale-curiosity" -> "google-research/planet" ["e"=1]
"openai/large-scale-curiosity" -> "openai/mlsh"
"openai/large-scale-curiosity" -> "rail-berkeley/softlearning"
"openai/large-scale-curiosity" -> "google-research/episodic-curiosity" ["e"=1]
"openai/large-scale-curiosity" -> "openai/multiagent-competition"
"openai/large-scale-curiosity" -> "google-deepmind/trfl"
"openai/large-scale-curiosity" -> "openai/imitation"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "NeuronDance/DeepRL"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "dbsxdbsx/rl-intro-book-chinese"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "qqiang00/Reinforce"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "rl-cn/rl-cn"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "GAOYANGAU/DRLPytorch"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "zhoubolei/introRL"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "finint/RL-Solutions" ["e"=1]
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "ZhiqingXiao/rl-book"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "Zhenye-Na/reinforcement-learning-stanford"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "Skylark0924/Machine-Learning-is-ALL-You-Need"
"qiwihui/reinforcement-learning-an-introduction-chinese" -> "boyu-ai/Hands-on-RL"
"Farama-Foundation/Miniworld" -> "Farama-Foundation/Minigrid"
"Farama-Foundation/Miniworld" -> "lcswillems/rl-starter-files"
"Farama-Foundation/Miniworld" -> "openai/procgen"
"Farama-Foundation/Miniworld" -> "kenjyoung/MinAtar" ["e"=1]
"Farama-Foundation/Miniworld" -> "clvrai/awesome-rl-envs"
"Farama-Foundation/Miniworld" -> "google-deepmind/bsuite"
"Farama-Foundation/Miniworld" -> "danijar/dreamer" ["e"=1]
"Farama-Foundation/Miniworld" -> "Farama-Foundation/Metaworld" ["e"=1]
"Farama-Foundation/Miniworld" -> "danijar/dreamerv2" ["e"=1]
"Farama-Foundation/Miniworld" -> "astooke/rlpyt"
"Farama-Foundation/Miniworld" -> "rll-research/url_benchmark" ["e"=1]
"Farama-Foundation/Miniworld" -> "vitchyr/multiworld"
"Farama-Foundation/Miniworld" -> "danijar/crafter" ["e"=1]
"Farama-Foundation/Miniworld" -> "mpSchrader/gym-sokoban"
"Farama-Foundation/Miniworld" -> "jurgisp/memory-maze" ["e"=1]
"PaddlePaddle/MetaGym" -> "PaddlePaddle/PaddleRobotics" ["e"=1]
"PaddlePaddle/MetaGym" -> "PaddlePaddle/PARL"
"openai/mlsh" -> "openai/robosumo"
"openai/mlsh" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"openai/mlsh" -> "hoangminhle/hierarchical_IL_RL"
"openai/mlsh" -> "florensacc/snn4hrl"
"openai/mlsh" -> "openai/multiagent-competition"
"openai/mlsh" -> "openai/EPG"
"openai/mlsh" -> "jeanharb/option_critic"
"openai/mlsh" -> "openai/imitation"
"openai/mlsh" -> "pathak22/noreward-rl"
"openai/mlsh" -> "openai/coinrun"
"openai/mlsh" -> "openai/large-scale-curiosity"
"openai/mlsh" -> "openai/evolution-strategies-starter"
"openai/mlsh" -> "google-deepmind/scalable_agent"
"openai/mlsh" -> "cbfinn/maml_rl" ["e"=1]
"openai/mlsh" -> "hi-abhi/tensorflow-value-iteration-networks"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/2048-AI"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/PacNeat"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/Asteroids-with-NEAT" -> "Code-Bullet/Piano-Tiles"
"rail-berkeley/rlkit" -> "rail-berkeley/softlearning"
"rail-berkeley/rlkit" -> "haarnoja/sac"
"rail-berkeley/rlkit" -> "Farama-Foundation/D4RL" ["e"=1]
"rail-berkeley/rlkit" -> "astooke/rlpyt"
"rail-berkeley/rlkit" -> "rlworkgroup/garage"
"rail-berkeley/rlkit" -> "rll/rllab"
"rail-berkeley/rlkit" -> "pranz24/pytorch-soft-actor-critic"
"rail-berkeley/rlkit" -> "sfujim/TD3"
"rail-berkeley/rlkit" -> "hill-a/stable-baselines"
"rail-berkeley/rlkit" -> "google-deepmind/dm_control"
"rail-berkeley/rlkit" -> "Farama-Foundation/Metaworld" ["e"=1]
"rail-berkeley/rlkit" -> "ShangtongZhang/DeepRL"
"rail-berkeley/rlkit" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"rail-berkeley/rlkit" -> "Farama-Foundation/Minigrid"
"rail-berkeley/rlkit" -> "ARISE-Initiative/robosuite" ["e"=1]
"koulanurag/ma-gym" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"koulanurag/ma-gym" -> "oxwhirl/pymarl"
"koulanurag/ma-gym" -> "uoe-agents/epymarl"
"koulanurag/ma-gym" -> "starry-sky6688/MARL-Algorithms"
"koulanurag/ma-gym" -> "sisl/MADRL"
"koulanurag/ma-gym" -> "oxwhirl/smac"
"koulanurag/ma-gym" -> "google-deepmind/meltingpot"
"koulanurag/ma-gym" -> "eugenevinitsky/sequential_social_dilemma_games"
"koulanurag/ma-gym" -> "PKU-RL/DGN"
"koulanurag/ma-gym" -> "schroederdewitt/multiagent_mujoco"
"koulanurag/ma-gym" -> "shariqiqbal2810/maddpg-pytorch"
"koulanurag/ma-gym" -> "Farama-Foundation/PettingZoo"
"koulanurag/ma-gym" -> "mlii/mfrl"
"koulanurag/ma-gym" -> "shariqiqbal2810/MAAC"
"koulanurag/ma-gym" -> "ArnaudFickinger/gym-multigrid"
"junhyukoh/value-prediction-network" -> "jeanharb/option_critic"
"junhyukoh/value-prediction-network" -> "oxwhirl/treeqn" ["e"=1]
"junhyukoh/value-prediction-network" -> "zhongwen/predictron"
"junhyukoh/value-prediction-network" -> "kentsommer/pytorch-value-iteration-networks"
"junhyukoh/value-prediction-network" -> "Breakend/DeepReinforcementLearningThatMatters"
"junhyukoh/value-prediction-network" -> "davidhershey/feudal_networks"
"junhyukoh/value-prediction-network" -> "zuoxingdong/VIN_PyTorch_Visdom"
"ZhiqingXiao/rl-book" -> "ucla-rlcourse/RLexample"
"ZhiqingXiao/rl-book" -> "NeuronDance/DeepRL"
"ZhiqingXiao/rl-book" -> "ZhiqingXiao/pytorch-book"
"ZhiqingXiao/rl-book" -> "StepNeverStop/RLs"
"ZhiqingXiao/rl-book" -> "zhoubolei/introRL"
"ZhiqingXiao/rl-book" -> "qqiang00/Reinforce"
"ZhiqingXiao/rl-book" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"ZhiqingXiao/rl-book" -> "AI4Finance-Foundation/ElegantRL"
"ZhiqingXiao/rl-book" -> "quqixun/RL-Python-Pytorch"
"ZhiqingXiao/rl-book" -> "zhangchuheng123/Reinforcement-Implementation"
"ZhiqingXiao/rl-book" -> "kaixindelele/DRLib"
"ZhiqingXiao/rl-book" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ZhiqingXiao/rl-book" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ZhiqingXiao/rl-book" -> "keiohta/tf2rl"
"ZhiqingXiao/rl-book" -> "wangshusen/DRL"
"RITCHIEHuang/DeepRL_Algorithms" -> "anita-hu/TF2-RL"
"RITCHIEHuang/DeepRL_Algorithms" -> "StepNeverStop/RLs"
"RITCHIEHuang/DeepRL_Algorithms" -> "dongminlee94/deep_rl"
"RITCHIEHuang/DeepRL_Algorithms" -> "archsyscall/DeepRL-TensorFlow2"
"RITCHIEHuang/DeepRL_Algorithms" -> "TianhongDai/reinforcement-learning-algorithms"
"RITCHIEHuang/DeepRL_Algorithms" -> "BY571/Soft-Actor-Critic-and-Extensions"
"RITCHIEHuang/DeepRL_Algorithms" -> "iffiX/machin"
"RITCHIEHuang/DeepRL_Algorithms" -> "abhisheksuran/Reinforcement_Learning"
"RITCHIEHuang/DeepRL_Algorithms" -> "BY571/DQN-Atari-Agents"
"RITCHIEHuang/DeepRL_Algorithms" -> "JohannesAck/tf2multiagentrl"
"RITCHIEHuang/DeepRL_Algorithms" -> "quantumiracle/Popular-RL-Algorithms"
"RITCHIEHuang/DeepRL_Algorithms" -> "kaixindelele/DRLib"
"RITCHIEHuang/DeepRL_Algorithms" -> "alirezakazemipour/DDPG-HER"
"microsoft/TextWorld" -> "microsoft/jericho"
"microsoft/TextWorld" -> "alfworld/alfworld" ["e"=1]
"microsoft/TextWorld" -> "mila-iqia/babyai"
"microsoft/TextWorld" -> "allenai/ScienceWorld"
"microsoft/TextWorld" -> "askforalfred/alfred" ["e"=1]
"microsoft/TextWorld" -> "rajammanabrolu/KG-DQN"
"microsoft/TextWorld" -> "IBM/commonsense-rl"
"microsoft/TextWorld" -> "Farama-Foundation/Minigrid"
"microsoft/TextWorld" -> "xingdi-eric-yuan/TextWorld-Coin-Collector"
"microsoft/TextWorld" -> "facebookresearch/nle"
"microsoft/TextWorld" -> "cognitiveailab/TextWorldExpress"
"microsoft/TextWorld" -> "google-research/rliable" ["e"=1]
"microsoft/TextWorld" -> "tambetm/gym-minecraft"
"microsoft/TextWorld" -> "Farama-Foundation/Miniworld"
"microsoft/TextWorld" -> "facebookresearch/ReAgent"
"reinforcement-learning-kr/pg_travel" -> "reinforcement-learning-kr/lets-do-irl"
"reinforcement-learning-kr/pg_travel" -> "ikostrikov/pytorch-trpo"
"reinforcement-learning-kr/pg_travel" -> "Khrylx/PyTorch-RL"
"reinforcement-learning-kr/pg_travel" -> "pat-coady/trpo"
"reinforcement-learning-kr/pg_travel" -> "chagmgang/distributed_reinforcement_learning" ["e"=1]
"reinforcement-learning-kr/pg_travel" -> "chagmgang/pytorch_ppo_rl"
"reinforcement-learning-kr/pg_travel" -> "takuseno/ppo"
"reinforcement-learning-kr/pg_travel" -> "Kaixhin/ACER"
"reinforcement-learning-kr/pg_travel" -> "qfettes/DeepRL-Tutorials"
"reinforcement-learning-kr/pg_travel" -> "ikostrikov/pytorch-a3c"
"reinforcement-learning-kr/pg_travel" -> "medipixel/rl_algorithms"
"yaricom/goNEAT" -> "yaricom/goNEAT_NS"
"dalmia/David-Silver-Reinforcement-learning" -> "enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning"
"dalmia/David-Silver-Reinforcement-learning" -> "udacity/deep-reinforcement-learning"
"dalmia/David-Silver-Reinforcement-learning" -> "astooke/rlpyt"
"dalmia/David-Silver-Reinforcement-learning" -> "rlworkgroup/garage"
"dalmia/David-Silver-Reinforcement-learning" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"dalmia/David-Silver-Reinforcement-learning" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"dalmia/David-Silver-Reinforcement-learning" -> "andri27-ts/Reinforcement-Learning"
"dalmia/David-Silver-Reinforcement-learning" -> "dennybritz/reinforcement-learning"
"dalmia/David-Silver-Reinforcement-learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"dalmia/David-Silver-Reinforcement-learning" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"dalmia/David-Silver-Reinforcement-learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"dalmia/David-Silver-Reinforcement-learning" -> "higgsfield/RL-Adventure"
"dalmia/David-Silver-Reinforcement-learning" -> "LantaoYu/MARL-Papers"
"dalmia/David-Silver-Reinforcement-learning" -> "qfettes/DeepRL-Tutorials"
"dalmia/David-Silver-Reinforcement-learning" -> "berkeleydeeprlcourse/homework"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "ShangtongZhang/DeepRL"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "openai/baselines"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "ikostrikov/pytorch-a3c"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "rail-berkeley/rlkit"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "hill-a/stable-baselines"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "rll/rllab"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "higgsfield-ai/higgsfield"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "astooke/rlpyt"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "DLR-RM/stable-baselines3"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "sfujim/TD3"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "nikhilbarhate99/PPO-PyTorch"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "higgsfield/RL-Adventure"
"ikostrikov/pytorch-a2c-ppo-acktr-gail" -> "LantaoYu/MARL-Papers"
"haarnoja/sac" -> "rail-berkeley/softlearning"
"haarnoja/sac" -> "rail-berkeley/rlkit"
"haarnoja/sac" -> "pranz24/pytorch-soft-actor-critic"
"haarnoja/sac" -> "sfujim/TD3"
"haarnoja/sac" -> "denisyarats/pytorch_sac" ["e"=1]
"haarnoja/sac" -> "haarnoja/softqlearning"
"haarnoja/sac" -> "rll/rllab"
"haarnoja/sac" -> "openai/random-network-distillation"
"haarnoja/sac" -> "cbfinn/maml_rl" ["e"=1]
"haarnoja/sac" -> "ikostrikov/pytorch-a3c"
"haarnoja/sac" -> "higgsfield-ai/higgsfield"
"haarnoja/sac" -> "rlworkgroup/garage"
"haarnoja/sac" -> "joschu/modular_rl"
"haarnoja/sac" -> "Farama-Foundation/D4RL" ["e"=1]
"haarnoja/sac" -> "ShangtongZhang/DeepRL"
"araffin/rl-baselines-zoo" -> "hill-a/stable-baselines"
"araffin/rl-baselines-zoo" -> "DLR-RM/rl-baselines3-zoo"
"araffin/rl-baselines-zoo" -> "araffin/rl-tutorial-jnrr19"
"araffin/rl-baselines-zoo" -> "rlworkgroup/garage"
"araffin/rl-baselines-zoo" -> "astooke/rlpyt"
"araffin/rl-baselines-zoo" -> "Farama-Foundation/Minigrid"
"araffin/rl-baselines-zoo" -> "Stable-Baselines-Team/stable-baselines"
"araffin/rl-baselines-zoo" -> "rail-berkeley/softlearning"
"araffin/rl-baselines-zoo" -> "benelot/pybullet-gym"
"araffin/rl-baselines-zoo" -> "rail-berkeley/rlkit"
"araffin/rl-baselines-zoo" -> "google-deepmind/bsuite"
"araffin/rl-baselines-zoo" -> "araffin/robotics-rl-srl" ["e"=1]
"araffin/rl-baselines-zoo" -> "openai/random-network-distillation"
"araffin/rl-baselines-zoo" -> "vitchyr/multiworld"
"araffin/rl-baselines-zoo" -> "haarnoja/sac"
"jcwleo/random-network-distillation-pytorch" -> "openai/random-network-distillation"
"jcwleo/random-network-distillation-pytorch" -> "jcwleo/curiosity-driven-exploration-pytorch"
"jcwleo/random-network-distillation-pytorch" -> "wizdom13/RND-Pytorch"
"jcwleo/random-network-distillation-pytorch" -> "orrivlin/MountainCar_DQN_RND"
"jcwleo/random-network-distillation-pytorch" -> "alirezakazemipour/PPO-RND"
"jcwleo/random-network-distillation-pytorch" -> "jcwleo/mario_rl"
"Stable-Baselines-Team/stable-baselines" -> "araffin/rl-baselines-zoo"
"Stable-Baselines-Team/stable-baselines" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"Stable-Baselines-Team/stable-baselines" -> "Stable-Baselines-Team/stable-baselines-tf2"
"Stable-Baselines-Team/stable-baselines" -> "Stable-Baselines-Team/rl-colab-notebooks"
"Stable-Baselines-Team/stable-baselines" -> "araffin/rl-tutorial-jnrr19"
"omerbsezer/Generative_Models_Tutorial_with_Demo" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"kengz/SLM-Lab" -> "ShangtongZhang/DeepRL"
"kengz/SLM-Lab" -> "IntelLabs/coach"
"kengz/SLM-Lab" -> "google-deepmind/bsuite"
"kengz/SLM-Lab" -> "astooke/rlpyt"
"kengz/SLM-Lab" -> "rlworkgroup/garage"
"kengz/SLM-Lab" -> "rail-berkeley/rlkit"
"kengz/SLM-Lab" -> "facebookresearch/ReAgent"
"kengz/SLM-Lab" -> "Farama-Foundation/Minigrid"
"kengz/SLM-Lab" -> "rail-berkeley/softlearning"
"kengz/SLM-Lab" -> "rll/rllab"
"kengz/SLM-Lab" -> "Khrylx/PyTorch-RL"
"kengz/SLM-Lab" -> "hill-a/stable-baselines"
"kengz/SLM-Lab" -> "higgsfield-ai/higgsfield"
"kengz/SLM-Lab" -> "zuoxingdong/lagom"
"kengz/SLM-Lab" -> "MushroomRL/mushroom-rl"
"lcswillems/rl-starter-files" -> "Farama-Foundation/Minigrid"
"lcswillems/rl-starter-files" -> "lcswillems/torch-ac"
"lcswillems/rl-starter-files" -> "Farama-Foundation/Miniworld"
"lcswillems/rl-starter-files" -> "mila-iqia/babyai"
"lcswillems/rl-starter-files" -> "facebookresearch/torchbeast"
"lcswillems/rl-starter-files" -> "openai/random-network-distillation"
"lcswillems/rl-starter-files" -> "vitchyr/multiworld"
"lcswillems/rl-starter-files" -> "ikostrikov/jaxrl" ["e"=1]
"lcswillems/rl-starter-files" -> "denisyarats/pytorch_sac" ["e"=1]
"lcswillems/rl-starter-files" -> "clvrai/awesome-rl-envs"
"lcswillems/rl-starter-files" -> "google-deepmind/pycolab"
"lcswillems/rl-starter-files" -> "astooke/rlpyt"
"lcswillems/rl-starter-files" -> "Farama-Foundation/D4RL" ["e"=1]
"lcswillems/rl-starter-files" -> "facebookresearch/impact-driven-exploration"
"lcswillems/rl-starter-files" -> "MattChanTK/gym-maze"
"red42/HTML5_Genetic_Cars" -> "pubnub/genetic-car-2"
"red42/HTML5_Genetic_Cars" -> "subprotocol/genetic-js"
"red42/HTML5_Genetic_Cars" -> "TomaszRewak/ML-games"
"red42/HTML5_Genetic_Cars" -> "parano/GeneticAlgorithm-TSP" ["e"=1]
"red42/HTML5_Genetic_Cars" -> "illogicz/NeuroEvolution"
"red42/HTML5_Genetic_Cars" -> "thopit/Creatures"
"red42/HTML5_Genetic_Cars" -> "ArztSamuel/Applying_EANNs"
"red42/HTML5_Genetic_Cars" -> "dolphin278/genetic"
"red42/HTML5_Genetic_Cars" -> "schteppe/p2.js" ["e"=1]
"geek-ai/MAgent" -> "oxwhirl/pymarl"
"geek-ai/MAgent" -> "LantaoYu/MARL-Papers"
"geek-ai/MAgent" -> "openai/maddpg"
"geek-ai/MAgent" -> "openai/multiagent-particle-envs"
"geek-ai/MAgent" -> "sisl/MADRL"
"geek-ai/MAgent" -> "mlii/mfrl"
"geek-ai/MAgent" -> "oxwhirl/smac"
"geek-ai/MAgent" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"geek-ai/MAgent" -> "shariqiqbal2810/MAAC"
"geek-ai/MAgent" -> "PKU-RL/DGN"
"geek-ai/MAgent" -> "Farama-Foundation/PettingZoo"
"geek-ai/MAgent" -> "Farama-Foundation/MAgent2"
"geek-ai/MAgent" -> "rll/rllab"
"geek-ai/MAgent" -> "openai/multiagent-competition"
"geek-ai/MAgent" -> "starry-sky6688/MARL-Algorithms"
"higgsfield/RL-Adventure" -> "higgsfield-ai/higgsfield"
"higgsfield/RL-Adventure" -> "ShangtongZhang/DeepRL"
"higgsfield/RL-Adventure" -> "yandexdataschool/Practical_RL"
"higgsfield/RL-Adventure" -> "qfettes/DeepRL-Tutorials"
"higgsfield/RL-Adventure" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"higgsfield/RL-Adventure" -> "Kaixhin/Rainbow"
"higgsfield/RL-Adventure" -> "Curt-Park/rainbow-is-all-you-need"
"higgsfield/RL-Adventure" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"higgsfield/RL-Adventure" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"higgsfield/RL-Adventure" -> "astooke/rlpyt"
"higgsfield/RL-Adventure" -> "hill-a/stable-baselines"
"higgsfield/RL-Adventure" -> "rail-berkeley/rlkit"
"higgsfield/RL-Adventure" -> "seungeunrho/minimalRL"
"higgsfield/RL-Adventure" -> "openai/baselines"
"higgsfield/RL-Adventure" -> "rll/rllab"
"iamhectorotero/rlai-exercises" -> "JKCooper2/rlai-exercises"
"iamhectorotero/rlai-exercises" -> "brynhayder/reinforcement_learning_an_introduction"
"iamhectorotero/rlai-exercises" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"iamhectorotero/rlai-exercises" -> "diegoalejogm/Reinforcement-Learning"
"iamhectorotero/rlai-exercises" -> "matteocasolari/reinforcement-learning-an-introduction-solutions"
"iamhectorotero/rlai-exercises" -> "mharbuz/rlbook-exercises"
"oxwhirl/smac" -> "oxwhirl/pymarl"
"oxwhirl/smac" -> "starry-sky6688/MARL-Algorithms"
"oxwhirl/smac" -> "marlbenchmark/on-policy"
"oxwhirl/smac" -> "oxwhirl/smacv2"
"oxwhirl/smac" -> "hijkzzz/pymarl2"
"oxwhirl/smac" -> "uoe-agents/epymarl"
"oxwhirl/smac" -> "Farama-Foundation/PettingZoo"
"oxwhirl/smac" -> "openai/multiagent-particle-envs"
"oxwhirl/smac" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"oxwhirl/smac" -> "openai/maddpg"
"oxwhirl/smac" -> "LantaoYu/MARL-Papers"
"oxwhirl/smac" -> "Replicable-MARL/MARLlib"
"oxwhirl/smac" -> "geek-ai/MAgent"
"oxwhirl/smac" -> "shariqiqbal2810/MAAC"
"oxwhirl/smac" -> "PKU-MARL/HARL"
"rlpy/rlpy" -> "NathanEpstein/reinforce"
"rlpy/rlpy" -> "jmacglashan/burlap"
"rlpy/rlpy" -> "amarack/python-rl"
"rlpy/rlpy" -> "Mononofu/reinforcement-learning"
"HumanCompatibleAI/imitation" -> "Kaixhin/imitation-learning"
"HumanCompatibleAI/imitation" -> "kristery/Awesome-Imitation-Learning"
"HumanCompatibleAI/imitation" -> "reinforcement-learning-kr/lets-do-irl"
"HumanCompatibleAI/imitation" -> "toshikwa/gail-airl-ppo.pytorch"
"HumanCompatibleAI/imitation" -> "MatthewJA/Inverse-Reinforcement-Learning"
"HumanCompatibleAI/imitation" -> "Farama-Foundation/D4RL" ["e"=1]
"HumanCompatibleAI/imitation" -> "ARISE-Initiative/robomimic" ["e"=1]
"HumanCompatibleAI/imitation" -> "DLR-RM/rl-baselines3-zoo"
"HumanCompatibleAI/imitation" -> "yrlu/irl-imitation"
"HumanCompatibleAI/imitation" -> "DLR-RM/stable-baselines3"
"HumanCompatibleAI/imitation" -> "Farama-Foundation/Metaworld" ["e"=1]
"HumanCompatibleAI/imitation" -> "takuseno/d3rlpy" ["e"=1]
"HumanCompatibleAI/imitation" -> "openai/imitation"
"HumanCompatibleAI/imitation" -> "qgallouedec/panda-gym" ["e"=1]
"HumanCompatibleAI/imitation" -> "google-deepmind/mujoco_menagerie" ["e"=1]
"shariqiqbal2810/MAAC" -> "shariqiqbal2810/maddpg-pytorch"
"shariqiqbal2810/MAAC" -> "starry-sky6688/MARL-Algorithms"
"shariqiqbal2810/MAAC" -> "oxwhirl/pymarl"
"shariqiqbal2810/MAAC" -> "xuehy/pytorch-maddpg"
"shariqiqbal2810/MAAC" -> "openai/maddpg"
"shariqiqbal2810/MAAC" -> "starry-sky6688/MADDPG"
"shariqiqbal2810/MAAC" -> "sisl/MADRL"
"shariqiqbal2810/MAAC" -> "marlbenchmark/on-policy"
"shariqiqbal2810/MAAC" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"shariqiqbal2810/MAAC" -> "marlbenchmark/off-policy"
"shariqiqbal2810/MAAC" -> "PKU-RL/DGN"
"shariqiqbal2810/MAAC" -> "mlii/mfrl"
"shariqiqbal2810/MAAC" -> "oxwhirl/smac"
"shariqiqbal2810/MAAC" -> "minqi/learning-to-communicate-pytorch"
"shariqiqbal2810/MAAC" -> "uoe-agents/epymarl"
"mimoralea/gdrl" -> "DeepReinforcementLearning/DeepReinforcementLearningInAction"
"mimoralea/gdrl" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"mimoralea/gdrl" -> "udacity/deep-reinforcement-learning"
"mimoralea/gdrl" -> "mimoralea/applied-reinforcement-learning"
"mimoralea/gdrl" -> "ShangtongZhang/DeepRL"
"mimoralea/gdrl" -> "marl-book/codebase"
"mimoralea/gdrl" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"mimoralea/gdrl" -> "higgsfield/RL-Adventure"
"mimoralea/gdrl" -> "TikhonJelvis/RL-book" ["e"=1]
"mimoralea/gdrl" -> "DLR-RM/rl-baselines3-zoo"
"mimoralea/gdrl" -> "quantumiracle/Popular-RL-Algorithms"
"mimoralea/gdrl" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"mimoralea/gdrl" -> "google-deepmind/acme"
"mimoralea/gdrl" -> "ucaiado/QLearning_Trading" ["e"=1]
"mimoralea/gdrl" -> "instadeepai/Mava"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "vietnh1009/Super-mario-bros-PPO-pytorch"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "Kautenja/gym-super-mario-bros"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "ikostrikov/pytorch-a3c"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "vietnh1009/Flappy-bird-deep-Q-learning-pytorch"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "MorvanZhou/pytorch-A3C"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "vietnh1009/QuickDraw" ["e"=1]
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "dgriff777/rl_a3c_pytorch"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "ShangtongZhang/DeepRL"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "vietnh1009/Tetris-deep-Q-learning-pytorch"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "pranz24/pytorch-soft-actor-critic"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "vietnh1009/Street-fighter-A3C-ICM-pytorch"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "tigerneil/awesome-deep-rl"
"vietnh1009/Super-mario-bros-A3C-pytorch" -> "iffiX/machin"
"google-research/batch-ppo" -> "williamFalcon/DeepRLHacks"
"google-research/batch-ppo" -> "tensorforce/tensorforce"
"google-research/batch-ppo" -> "rll/rllab"
"google-research/batch-ppo" -> "pathak22/noreward-rl"
"google-research/batch-ppo" -> "openai/universe-starter-agent"
"google-research/batch-ppo" -> "joschu/modular_rl"
"google-research/batch-ppo" -> "facebookresearch/ELF"
"google-research/batch-ppo" -> "miyosuda/async_deep_reinforce"
"google-research/batch-ppo" -> "google-deepmind/pycolab"
"google-research/batch-ppo" -> "google-deepmind/scalable_agent"
"google-research/batch-ppo" -> "NVlabs/GA3C"
"google-research/batch-ppo" -> "google-deepmind/trfl"
"google-research/batch-ppo" -> "IntelLabs/coach"
"google-research/batch-ppo" -> "miyosuda/unreal"
"google-research/batch-ppo" -> "pat-coady/trpo"
"Kautenja/gym-super-mario-bros" -> "Kautenja/nes-py"
"Kautenja/gym-super-mario-bros" -> "ppaquette/gym-super-mario"
"Kautenja/gym-super-mario-bros" -> "vietnh1009/Super-mario-bros-A3C-pytorch"
"Kautenja/gym-super-mario-bros" -> "vietnh1009/Super-mario-bros-PPO-pytorch"
"Kautenja/gym-super-mario-bros" -> "openai/random-network-distillation"
"Kautenja/gym-super-mario-bros" -> "Farama-Foundation/ViZDoom"
"Kautenja/gym-super-mario-bros" -> "openai/procgen"
"Kautenja/gym-super-mario-bros" -> "pathak22/noreward-rl"
"Kautenja/gym-super-mario-bros" -> "mpSchrader/gym-sokoban"
"Kautenja/gym-super-mario-bros" -> "openai/retro"
"Kautenja/gym-super-mario-bros" -> "Farama-Foundation/Minigrid"
"Kautenja/gym-super-mario-bros" -> "Farama-Foundation/Arcade-Learning-Environment"
"Kautenja/gym-super-mario-bros" -> "jcwleo/mario_rl"
"Kautenja/gym-super-mario-bros" -> "jcwleo/random-network-distillation-pytorch"
"Kautenja/gym-super-mario-bros" -> "hardmaru/slimevolleygym" ["e"=1]
"google-deepmind/trfl" -> "google/dopamine"
"google-deepmind/trfl" -> "facebookresearch/ReAgent"
"google-deepmind/trfl" -> "google-deepmind/graph_nets" ["e"=1]
"google-deepmind/trfl" -> "tensorforce/tensorforce"
"google-deepmind/trfl" -> "google-deepmind/bsuite"
"google-deepmind/trfl" -> "google-deepmind/scalable_agent"
"google-deepmind/trfl" -> "rll/rllab"
"google-deepmind/trfl" -> "tensorflow/agents"
"google-deepmind/trfl" -> "openai/baselines"
"google-deepmind/trfl" -> "keras-rl/keras-rl"
"google-deepmind/trfl" -> "IntelLabs/coach"
"google-deepmind/trfl" -> "astooke/rlpyt"
"google-deepmind/trfl" -> "google-deepmind/lab"
"google-deepmind/trfl" -> "tensorflow/adanet" ["e"=1]
"google-deepmind/trfl" -> "hill-a/stable-baselines"
"zhangchuheng123/Reinforcement-Implementation" -> "kaixindelele/DRLib"
"zhangchuheng123/Reinforcement-Implementation" -> "StepNeverStop/RLs"
"zhangchuheng123/Reinforcement-Implementation" -> "NeuronDance/DeepRL"
"zhangchuheng123/Reinforcement-Implementation" -> "AndyYue1893/Deep-reinforcement-learning-with-pytorch"
"zhangchuheng123/Reinforcement-Implementation" -> "tigerneil/awesome-deep-rl"
"zhangchuheng123/Reinforcement-Implementation" -> "dongminlee94/deep_rl"
"zhangchuheng123/Reinforcement-Implementation" -> "starry-sky6688/MARL-Algorithms"
"zhangchuheng123/Reinforcement-Implementation" -> "AI4Finance-Foundation/ElegantRL"
"zhangchuheng123/Reinforcement-Implementation" -> "Skylark0924/Machine-Learning-is-ALL-You-Need"
"zhangchuheng123/Reinforcement-Implementation" -> "sfujim/BCQ" ["e"=1]
"zhangchuheng123/Reinforcement-Implementation" -> "shariqiqbal2810/MAAC"
"zhangchuheng123/Reinforcement-Implementation" -> "eyounx/VirtualTaobao" ["e"=1]
"zhangchuheng123/Reinforcement-Implementation" -> "qqiang00/Reinforce"
"zhangchuheng123/Reinforcement-Implementation" -> "sfujim/TD3"
"zhangchuheng123/Reinforcement-Implementation" -> "hanjuku-kaso/awesome-offline-rl" ["e"=1]
"StepNeverStop/RLs" -> "zhangchuheng123/Reinforcement-Implementation"
"StepNeverStop/RLs" -> "RITCHIEHuang/DeepRL_Algorithms"
"StepNeverStop/RLs" -> "keiohta/tf2rl"
"StepNeverStop/RLs" -> "dongminlee94/deep_rl"
"StepNeverStop/RLs" -> "iffiX/machin"
"StepNeverStop/RLs" -> "quantumiracle/Popular-RL-Algorithms"
"StepNeverStop/RLs" -> "kaixindelele/DRLib"
"StepNeverStop/RLs" -> "NeuronDance/DeepRL"
"StepNeverStop/RLs" -> "anita-hu/TF2-RL"
"StepNeverStop/RLs" -> "gxywy/rl-plotter"
"StepNeverStop/RLs" -> "ZhiqingXiao/rl-book"
"StepNeverStop/RLs" -> "shariqiqbal2810/MAAC"
"StepNeverStop/RLs" -> "XinJingHao/TD3-BipedalWalkerHardcore-v2"
"StepNeverStop/RLs" -> "sisl/MADRL"
"StepNeverStop/RLs" -> "huawei-noah/xingtian"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "Shmuma/ptan"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "DeepReinforcementLearning/DeepReinforcementLearningInAction"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "mimoralea/gdrl"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "MrSyee/pg-is-all-you-need"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "kengz/SLM-Lab"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "Curt-Park/rainbow-is-all-you-need"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "Farama-Foundation/PettingZoo"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "boyu-ai/Hands-on-RL"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "udacity/deep-reinforcement-learning"
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" -> "pfnet/pfrl"
"SarvagyaVaish/FlappyBirdRL" -> "enhuiz/flappybird-ql"
"SarvagyaVaish/FlappyBirdRL" -> "chncyhn/flappybird-qlearning-bot"
"SarvagyaVaish/FlappyBirdRL" -> "floodsung/DRL-FlappyBird"
"SarvagyaVaish/FlappyBirdRL" -> "yenchenlin/DeepLearningFlappyBird"
"SarvagyaVaish/FlappyBirdRL" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"SarvagyaVaish/FlappyBirdRL" -> "siemanko/tensorflow-deepq"
"SarvagyaVaish/FlappyBirdRL" -> "nikitasrivatsan/DeepLearningVideoGames"
"SarvagyaVaish/FlappyBirdRL" -> "sourabhv/FlapPyBird"
"SarvagyaVaish/FlappyBirdRL" -> "muupan/deep-reinforcement-learning-papers"
"SarvagyaVaish/FlappyBirdRL" -> "YueDayu/AsmFlappyBird"
"sourabhv/FlapPyBird" -> "nikitasrivatsan/DeepLearningVideoGames"
"sourabhv/FlapPyBird" -> "yenchenlin/DeepLearningFlappyBird"
"sourabhv/FlapPyBird" -> "TimoWilken/flappy-bird-pygame"
"sourabhv/FlapPyBird" -> "justinmeister/Mario-Level-1" ["e"=1]
"sourabhv/FlapPyBird" -> "chncyhn/flappybird-qlearning-bot"
"sourabhv/FlapPyBird" -> "mx0c/super-mario-python" ["e"=1]
"sourabhv/FlapPyBird" -> "kidscancode/pygame_tutorials" ["e"=1]
"sourabhv/FlapPyBird" -> "ntasfi/PyGame-Learning-Environment"
"sourabhv/FlapPyBird" -> "floodsung/DRL-FlappyBird"
"sourabhv/FlapPyBird" -> "SarvagyaVaish/FlappyBirdRL"
"sourabhv/FlapPyBird" -> "justinmeister/The-Stolen-Crown-RPG" ["e"=1]
"sourabhv/FlapPyBird" -> "techwithtim/Pygame-Tutorials" ["e"=1]
"sourabhv/FlapPyBird" -> "estevaofon/angry-birds-python" ["e"=1]
"sourabhv/FlapPyBird" -> "ppizarror/pygame-menu" ["e"=1]
"sourabhv/FlapPyBird" -> "Mekire/pygame-samples" ["e"=1]
"OMSCentral/omscentral-legacy-angular" -> "cmaron/CS-7641-assignments"
"OMSCentral/omscentral-legacy-angular" -> "martzcodes/gt-course-surveys"
"OMSCentral/omscentral-legacy-angular" -> "pyjarrett/OMSCS_Survival_Guide"
"OMSCentral/omscentral-legacy-angular" -> "OMSCentral/omscentral-legacy-react"
"OMSCentral/omscentral-legacy-angular" -> "orsenthil/coursedocs"
"OMSCentral/omscentral-legacy-angular" -> "JonathanTay/CS-7641-assignment-1"
"OMSCentral/omscentral-legacy-angular" -> "iamjakewarner/jdf"
"OMSCentral/omscentral-legacy-angular" -> "CuriousLearner/jdf-latex"
"OMSCentral/omscentral-legacy-angular" -> "juanjose49/omscs-cs7641-machine-learning-assignment-4"
"M-J-Murray/MAMEToolkit" -> "vietnh1009/Street-fighter-A3C-ICM-pytorch"
"M-J-Murray/MAMEToolkit" -> "alito/mamele"
"M-J-Murray/MAMEToolkit" -> "google-deepmind/trfl"
"M-J-Murray/MAMEToolkit" -> "M-J-Murray/SFAgents"
"M-J-Murray/MAMEToolkit" -> "navneet-nmk/pytorch-rl"
"M-J-Murray/MAMEToolkit" -> "TorchCraft/TorchCraftAI" ["e"=1]
"M-J-Murray/MAMEToolkit" -> "openai/retro"
"M-J-Murray/MAMEToolkit" -> "openai/random-network-distillation"
"M-J-Murray/MAMEToolkit" -> "njustesen/botbowl"
"M-J-Murray/MAMEToolkit" -> "BYU-PCCL/holodeck" ["e"=1]
"M-J-Murray/MAMEToolkit" -> "MultiAgentLearning/playground"
"M-J-Murray/MAMEToolkit" -> "inoryy/reaver" ["e"=1]
"M-J-Murray/MAMEToolkit" -> "facebookresearch/ReAgent"
"M-J-Murray/MAMEToolkit" -> "openai/large-scale-curiosity"
"M-J-Murray/MAMEToolkit" -> "zuoxingdong/lagom"
"openai/neural-mmo" -> "NeuralMMO/environment"
"openai/neural-mmo" -> "NeuralMMO/client"
"openai/neural-mmo" -> "eugenevinitsky/sequential_social_dilemma_games"
"openai/neural-mmo" -> "openai/multi-agent-emergence-environments"
"openai/neural-mmo" -> "google-deepmind/hanabi-learning-environment"
"openai/neural-mmo" -> "oxwhirl/smac"
"openai/neural-mmo" -> "oxwhirl/pymarl"
"openai/neural-mmo" -> "MultiAgentLearning/playground"
"openai/neural-mmo" -> "openai/multiagent-competition"
"openai/neural-mmo" -> "geek-ai/MAgent"
"openai/neural-mmo" -> "openai/maddpg"
"openai/neural-mmo" -> "openai/multiagent-particle-envs"
"openai/neural-mmo" -> "openai/large-scale-curiosity"
"openai/neural-mmo" -> "LantaoYu/MARL-Papers"
"openai/neural-mmo" -> "google-research/planet" ["e"=1]
"shariqiqbal2810/maddpg-pytorch" -> "xuehy/pytorch-maddpg"
"shariqiqbal2810/maddpg-pytorch" -> "starry-sky6688/MADDPG"
"shariqiqbal2810/maddpg-pytorch" -> "shariqiqbal2810/MAAC"
"shariqiqbal2810/maddpg-pytorch" -> "openai/maddpg"
"shariqiqbal2810/maddpg-pytorch" -> "DKuan/MADDPG_torch"
"shariqiqbal2810/maddpg-pytorch" -> "starry-sky6688/MARL-Algorithms"
"shariqiqbal2810/maddpg-pytorch" -> "marlbenchmark/off-policy"
"shariqiqbal2810/maddpg-pytorch" -> "marlbenchmark/on-policy"
"shariqiqbal2810/maddpg-pytorch" -> "oxwhirl/pymarl"
"shariqiqbal2810/maddpg-pytorch" -> "Lizhi-sjtu/MARL-code-pytorch"
"shariqiqbal2810/maddpg-pytorch" -> "sisl/MADRL"
"shariqiqbal2810/maddpg-pytorch" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"shariqiqbal2810/maddpg-pytorch" -> "tinyzqh/light_mappo"
"shariqiqbal2810/maddpg-pytorch" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"shariqiqbal2810/maddpg-pytorch" -> "openai/multiagent-particle-envs"
"mam91/neat-genetic-mario" -> "pakoito/MarI-O"
"benelot/pybullet-gym" -> "erwincoumans/pybullet_robots" ["e"=1]
"benelot/pybullet-gym" -> "astooke/rlpyt"
"benelot/pybullet-gym" -> "hsp-iit/pybullet-robot-envs" ["e"=1]
"benelot/pybullet-gym" -> "google-deepmind/dm_control"
"benelot/pybullet-gym" -> "rail-berkeley/softlearning"
"benelot/pybullet-gym" -> "WilsonWangTHU/mbbl" ["e"=1]
"benelot/pybullet-gym" -> "stepjam/RLBench" ["e"=1]
"benelot/pybullet-gym" -> "araffin/robotics-rl-srl" ["e"=1]
"benelot/pybullet-gym" -> "openai/roboschool"
"benelot/pybullet-gym" -> "araffin/rl-baselines-zoo"
"benelot/pybullet-gym" -> "rail-berkeley/rlkit"
"benelot/pybullet-gym" -> "Farama-Foundation/Minigrid"
"benelot/pybullet-gym" -> "ARISE-Initiative/robosuite" ["e"=1]
"benelot/pybullet-gym" -> "rlworkgroup/garage"
"benelot/pybullet-gym" -> "google-research/ravens" ["e"=1]
"qfettes/DeepRL-Tutorials" -> "higgsfield/RL-Adventure"
"qfettes/DeepRL-Tutorials" -> "ShangtongZhang/DeepRL"
"qfettes/DeepRL-Tutorials" -> "astooke/rlpyt"
"qfettes/DeepRL-Tutorials" -> "Kaixhin/Rainbow"
"qfettes/DeepRL-Tutorials" -> "MrSyee/pg-is-all-you-need"
"qfettes/DeepRL-Tutorials" -> "higgsfield-ai/higgsfield"
"qfettes/DeepRL-Tutorials" -> "dongminlee94/deep_rl"
"qfettes/DeepRL-Tutorials" -> "Khrylx/PyTorch-RL"
"qfettes/DeepRL-Tutorials" -> "Curt-Park/rainbow-is-all-you-need"
"qfettes/DeepRL-Tutorials" -> "seungeunrho/minimalRL"
"qfettes/DeepRL-Tutorials" -> "TianhongDai/reinforcement-learning-algorithms"
"qfettes/DeepRL-Tutorials" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"qfettes/DeepRL-Tutorials" -> "sungyubkim/Deep_RL_with_pytorch"
"qfettes/DeepRL-Tutorials" -> "ikostrikov/pytorch-a3c"
"qfettes/DeepRL-Tutorials" -> "rail-berkeley/rlkit"
"mbaske/ml-agents-hyperparams" -> "mbaske/ml-explorer-drone"
"mbaske/ml-agents-hyperparams" -> "mbaske/grid-sensor"
"cts198859/deeprl_network" -> "cts198859/deeprl_signal_control" ["e"=1]
"cts198859/deeprl_network" -> "PKU-RL/DGN"
"cts198859/deeprl_network" -> "shariqiqbal2810/MAAC"
"cts198859/deeprl_network" -> "minqi/learning-to-communicate-pytorch"
"cts198859/deeprl_network" -> "sisl/MADRL"
"cts198859/deeprl_network" -> "starry-sky6688/MARL-Algorithms"
"cts198859/deeprl_network" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"cts198859/deeprl_network" -> "AndreaVidali/Deep-QLearning-Agent-for-Traffic-Signal-Control" ["e"=1]
"cts198859/deeprl_network" -> "xuehy/pytorch-maddpg"
"cts198859/deeprl_network" -> "docwza/sumolights" ["e"=1]
"cts198859/deeprl_network" -> "wingsweihua/colight" ["e"=1]
"cts198859/deeprl_network" -> "isp1tze/MAProj"
"cts198859/deeprl_network" -> "mohammadasghari/dqn-multi-agent-rl"
"cts198859/deeprl_network" -> "LucasAlegre/sumo-rl" ["e"=1]
"cts198859/deeprl_network" -> "mlii/mfrl"
"mohammadasghari/dqn-multi-agent-rl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"mohammadasghari/dqn-multi-agent-rl" -> "ChenglongChen/pytorch-DRL"
"mohammadasghari/dqn-multi-agent-rl" -> "sisl/MADRL"
"mohammadasghari/dqn-multi-agent-rl" -> "cts198859/deeprl_network"
"mohammadasghari/dqn-multi-agent-rl" -> "minqi/learning-to-communicate-pytorch"
"mohammadasghari/dqn-multi-agent-rl" -> "thesouther/MARL"
"mohammadasghari/dqn-multi-agent-rl" -> "xuehy/pytorch-maddpg"
"mohammadasghari/dqn-multi-agent-rl" -> "shkrwnd/Deep-Reinforcement-Learning-for-Dynamic-Spectrum-Access" ["e"=1]
"mohammadasghari/dqn-multi-agent-rl" -> "sumitrj/ConnectedQ-Multi-agent-Reinforcement-Learning-Algorithm" ["e"=1]
"mohammadasghari/dqn-multi-agent-rl" -> "PKU-RL/DGN"
"mohammadasghari/dqn-multi-agent-rl" -> "oxwhirl/pymarl"
"mohammadasghari/dqn-multi-agent-rl" -> "cyoon1729/Multi-agent-reinforcement-learning"
"mohammadasghari/dqn-multi-agent-rl" -> "koulanurag/ma-gym"
"mohammadasghari/dqn-multi-agent-rl" -> "shariqiqbal2810/MAAC"
"mohammadasghari/dqn-multi-agent-rl" -> "openai/maddpg"
"kristery/Awesome-Imitation-Learning" -> "apexrl/Imitation-Learning-Paper-Lists"
"kristery/Awesome-Imitation-Learning" -> "Kaixhin/imitation-learning"
"kristery/Awesome-Imitation-Learning" -> "HumanCompatibleAI/imitation"
"kristery/Awesome-Imitation-Learning" -> "reinforcement-learning-kr/lets-do-irl"
"kristery/Awesome-Imitation-Learning" -> "kristery/Imitation-Learning-from-Imperfect-Demonstration"
"kristery/Awesome-Imitation-Learning" -> "Ericonaldo/ILSwiss"
"kristery/Awesome-Imitation-Learning" -> "ARISE-Initiative/robomimic" ["e"=1]
"kristery/Awesome-Imitation-Learning" -> "junhyukoh/self-imitation-learning"
"kristery/Awesome-Imitation-Learning" -> "Div99/IQ-Learn"
"kristery/Awesome-Imitation-Learning" -> "openai/imitation"
"kristery/Awesome-Imitation-Learning" -> "stepjam/RLBench" ["e"=1]
"kristery/Awesome-Imitation-Learning" -> "siddhanthaldar/ROT"
"kristery/Awesome-Imitation-Learning" -> "Farama-Foundation/Metaworld" ["e"=1]
"kristery/Awesome-Imitation-Learning" -> "real-stanford/scalingup" ["e"=1]
"kristery/Awesome-Imitation-Learning" -> "yrlu/irl-imitation"
"Observerspy/CS234" -> "Observerspy/CS294"
"Observerspy/CS234" -> "zlpure/CS234"
"openai/retro" -> "openai/universe"
"openai/retro" -> "openai/baselines"
"openai/retro" -> "Farama-Foundation/Arcade-Learning-Environment"
"openai/retro" -> "openai/procgen"
"openai/retro" -> "google-deepmind/lab"
"openai/retro" -> "openai/roboschool"
"openai/retro" -> "rll/rllab"
"openai/retro" -> "openai/spinningup"
"openai/retro" -> "openai/mujoco-py"
"openai/retro" -> "google/dopamine"
"openai/retro" -> "google-deepmind/dm_control"
"openai/retro" -> "Farama-Foundation/ViZDoom"
"openai/retro" -> "pathak22/noreward-rl"
"openai/retro" -> "google-deepmind/open_spiel"
"openai/retro" -> "openai/gym"
"ucla-rlcourse/RLexample" -> "zhoubolei/introRL"
"ucla-rlcourse/RLexample" -> "ucla-rlcourse/DeepRL-Tutorials"
"ucla-rlcourse/RLexample" -> "cuhkrlcourse/ierg6130-assignment"
"ucla-rlcourse/RLexample" -> "sfujim/TD3"
"ucla-rlcourse/RLexample" -> "NeuronDance/DeepRL"
"ucla-rlcourse/RLexample" -> "ZhiqingXiao/rl-book"
"ucla-rlcourse/RLexample" -> "zhangchuheng123/Reinforcement-Implementation"
"ucla-rlcourse/RLexample" -> "datawhalechina/easy-rl"
"ucla-rlcourse/RLexample" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"ucla-rlcourse/RLexample" -> "qqiang00/Reinforce"
"ucla-rlcourse/RLexample" -> "PaddlePaddle/PARL"
"ucla-rlcourse/RLexample" -> "qfettes/DeepRL-Tutorials"
"ucla-rlcourse/RLexample" -> "AI4Finance-Foundation/ElegantRL"
"ucla-rlcourse/RLexample" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ucla-rlcourse/RLexample" -> "wangshusen/DRL"
"JaeDukSeo/reinforcement-learning-an-introduction" -> "MJeremy2017/reinforcement-learning-implementation"
"TianhongDai/reinforcement-learning-algorithms" -> "dongminlee94/deep_rl"
"TianhongDai/reinforcement-learning-algorithms" -> "Khrylx/PyTorch-RL"
"TianhongDai/reinforcement-learning-algorithms" -> "TianhongDai/hindsight-experience-replay"
"TianhongDai/reinforcement-learning-algorithms" -> "RITCHIEHuang/DeepRL_Algorithms"
"TianhongDai/reinforcement-learning-algorithms" -> "quantumiracle/Popular-RL-Algorithms"
"TianhongDai/reinforcement-learning-algorithms" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"TianhongDai/reinforcement-learning-algorithms" -> "ikostrikov/pytorch-trpo"
"TianhongDai/reinforcement-learning-algorithms" -> "qfettes/DeepRL-Tutorials"
"TianhongDai/reinforcement-learning-algorithms" -> "Kchu/DeepRL_PyTorch"
"TianhongDai/reinforcement-learning-algorithms" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"TianhongDai/reinforcement-learning-algorithms" -> "sisl/MADRL"
"TianhongDai/reinforcement-learning-algorithms" -> "ShangtongZhang/DeepRL"
"TianhongDai/reinforcement-learning-algorithms" -> "starry-sky6688/MADDPG"
"TianhongDai/reinforcement-learning-algorithms" -> "cts198859/deeprl_network"
"TianhongDai/reinforcement-learning-algorithms" -> "pranz24/pytorch-soft-actor-critic"
"qq456cvb/doudizhu-C" -> "charleschen003/doudizhu-rl"
"qq456cvb/doudizhu-C" -> "thuxugang/doudizhu"
"qq456cvb/doudizhu-C" -> "onestraw/doudizhu"
"qq456cvb/doudizhu-C" -> "zhozhou/DouDizhuAI"
"qq456cvb/doudizhu-C" -> "ZhouWeikuan/DouDiZhu"
"ermongroup/MA-AIRL" -> "ermongroup/multiagent-gail"
"ermongroup/MA-AIRL" -> "ermongroup/MetaIRL"
"ermongroup/MA-AIRL" -> "justinjfu/inverse_rl"
"ermongroup/MA-AIRL" -> "ahq1993/inverse_rl"
"ermongroup/MA-AIRL" -> "yrlu/irl-imitation"
"ermongroup/MA-AIRL" -> "qzed/irl-maxent"
"ermongroup/MA-AIRL" -> "twni2016/f-IRL"
"ermongroup/MA-AIRL" -> "toshikwa/gail-airl-ppo.pytorch"
"ermongroup/MA-AIRL" -> "jangirrishabh/toyCarIRL"
"ermongroup/MA-AIRL" -> "MCZhi/Driving-IRL-NGSIM" ["e"=1]
"ermongroup/MA-AIRL" -> "MatthewJA/Inverse-Reinforcement-Learning"
"ermongroup/MA-AIRL" -> "reinforcement-learning-kr/lets-do-irl"
"ermongroup/MA-AIRL" -> "SaminYeasar/Off_Policy_Adversarial_Inverse_Reinforcement_Learning"
"ermongroup/MA-AIRL" -> "HuangJiaLian/AIRL_MountainCar"
"cpnota/autonomous-learning-library" -> "MushroomRL/mushroom-rl"
"cpnota/autonomous-learning-library" -> "astooke/rlpyt"
"cpnota/autonomous-learning-library" -> "pfnet/pfrl"
"cpnota/autonomous-learning-library" -> "jannerm/mbpo" ["e"=1]
"cpnota/autonomous-learning-library" -> "kengz/awesome-deep-rl"
"cpnota/autonomous-learning-library" -> "learnables/cherry"
"cpnota/autonomous-learning-library" -> "zuoxingdong/lagom"
"cpnota/autonomous-learning-library" -> "rlworkgroup/garage"
"cpnota/autonomous-learning-library" -> "andyljones/reinforcement-learning-discord-wiki"
"cpnota/autonomous-learning-library" -> "fabiopardo/tonic" ["e"=1]
"cpnota/autonomous-learning-library" -> "quantumiracle/Popular-RL-Algorithms"
"cpnota/autonomous-learning-library" -> "Farama-Foundation/Minigrid"
"cpnota/autonomous-learning-library" -> "iffiX/machin"
"cpnota/autonomous-learning-library" -> "Spenhouet/tensorboard-aggregator"
"cpnota/autonomous-learning-library" -> "openai/procgen"
"quantumiracle/Popular-RL-Algorithms" -> "pranz24/pytorch-soft-actor-critic"
"quantumiracle/Popular-RL-Algorithms" -> "rail-berkeley/rlkit"
"quantumiracle/Popular-RL-Algorithms" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"quantumiracle/Popular-RL-Algorithms" -> "Khrylx/PyTorch-RL"
"quantumiracle/Popular-RL-Algorithms" -> "sfujim/TD3"
"quantumiracle/Popular-RL-Algorithms" -> "rail-berkeley/softlearning"
"quantumiracle/Popular-RL-Algorithms" -> "dongminlee94/deep_rl"
"quantumiracle/Popular-RL-Algorithms" -> "BY571/Soft-Actor-Critic-and-Extensions"
"quantumiracle/Popular-RL-Algorithms" -> "starry-sky6688/MADDPG"
"quantumiracle/Popular-RL-Algorithms" -> "denisyarats/pytorch_sac" ["e"=1]
"quantumiracle/Popular-RL-Algorithms" -> "shariqiqbal2810/MAAC"
"quantumiracle/Popular-RL-Algorithms" -> "seungeunrho/minimalRL"
"quantumiracle/Popular-RL-Algorithms" -> "marlbenchmark/on-policy"
"quantumiracle/Popular-RL-Algorithms" -> "clvrai/awesome-rl-envs"
"quantumiracle/Popular-RL-Algorithms" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "RylanSchaeffer/ucl-adv-dl-rl"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "dalmia/David-Silver-Reinforcement-learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "YidingYu/UCL-DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "mikezhang95/ML_Assignment"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "berkeleydeeprlcourse/homework"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "simoninithomas/Deep_reinforcement_learning_Course"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "udacity/deep-reinforcement-learning"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "dalmia/Deep-Learning-Book-Chapter-Summaries" ["e"=1]
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "NeuronDance/DeepRL"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "astooke/rlpyt"
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python"
"williamFalcon/DeepRLHacks" -> "rll/rllab"
"williamFalcon/DeepRLHacks" -> "google-research/batch-ppo"
"williamFalcon/DeepRLHacks" -> "pathak22/noreward-rl"
"williamFalcon/DeepRLHacks" -> "junhyukoh/deep-reinforcement-learning-papers"
"williamFalcon/DeepRLHacks" -> "facebookresearch/ELF"
"williamFalcon/DeepRLHacks" -> "astooke/rlpyt"
"williamFalcon/DeepRLHacks" -> "joschu/modular_rl"
"williamFalcon/DeepRLHacks" -> "tensorforce/tensorforce"
"williamFalcon/DeepRLHacks" -> "carpedm20/deep-rl-tensorflow"
"williamFalcon/DeepRLHacks" -> "berkeleydeeprlcourse/homework"
"williamFalcon/DeepRLHacks" -> "openai/evolution-strategies-starter"
"williamFalcon/DeepRLHacks" -> "ShangtongZhang/DeepRL"
"williamFalcon/DeepRLHacks" -> "miyosuda/async_deep_reinforce"
"williamFalcon/DeepRLHacks" -> "IntelLabs/coach"
"williamFalcon/DeepRLHacks" -> "openai/universe-starter-agent"
"Lyusungwon/apex_dqn_pytorch" -> "praveen-palanisamy/Ape-X-DQN"
"kristjankorjus/Replicating-DeepMind" -> "spragunr/deep_q_rl"
"kristjankorjus/Replicating-DeepMind" -> "muupan/dqn-in-the-caffe"
"kristjankorjus/Replicating-DeepMind" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"kristjankorjus/Replicating-DeepMind" -> "brian473/neural_rl"
"kristjankorjus/Replicating-DeepMind" -> "shawntan/neural-turing-machines" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "NathanEpstein/reinforce"
"kristjankorjus/Replicating-DeepMind" -> "gliese581gg/DQN_tensorflow"
"kristjankorjus/Replicating-DeepMind" -> "soumith/deepmind-atari" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "jbornschein/draw" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "TorontoDeepLearning/convnet" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "siemanko/tensorflow-deepq"
"kristjankorjus/Replicating-DeepMind" -> "tambetm/simple_dqn"
"kristjankorjus/Replicating-DeepMind" -> "222464/AILib"
"kristjankorjus/Replicating-DeepMind" -> "vitruvianscience/OpenDeep" ["e"=1]
"kristjankorjus/Replicating-DeepMind" -> "dmlc/cxxnet" ["e"=1]
"Shmuma/ptan" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"Shmuma/ptan" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"Shmuma/ptan" -> "yandexdataschool/AgentNet"
"Shmuma/ptan" -> "ShangtongZhang/DeepRL"
"Shmuma/ptan" -> "jingweiz/pytorch-rl"
"Shmuma/ptan" -> "Kaixhin/Rainbow"
"Shmuma/ptan" -> "kengz/SLM-Lab"
"Shmuma/ptan" -> "dgriff777/rl_a3c_pytorch"
"Shmuma/ptan" -> "hungtuchen/pytorch-dqn"
"Shmuma/ptan" -> "Khrylx/PyTorch-RL"
"Shmuma/ptan" -> "rll/rllab"
"Shmuma/ptan" -> "navneet-nmk/pytorch-rl"
"Shmuma/ptan" -> "higgsfield/RL-Adventure"
"Shmuma/ptan" -> "Kaixhin/ACER"
"Shmuma/ptan" -> "zuoxingdong/lagom"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/Pool_AI"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/2048-AI"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/SnakeFusion"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/RubiksCubeAI"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/PacNeat"
"Code-Bullet/Enigma-Simulator" -> "Code-Bullet/PacmanGame"
"Code-Bullet/Pool_AI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Pool_AI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Pool_AI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Pool_AI" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/Pool_AI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Pool_AI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Pool_AI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Pool_AI" -> "Code-Bullet/2048-AI"
"Code-Bullet/Pool_AI" -> "Code-Bullet/PacNeat"
"Code-Bullet/Pool_AI" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Pool_AI" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/Pool_AI" -> "Code-Bullet/PacmanGame"
"Code-Bullet/Pool_AI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/Pool_AI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Pool_AI" -> "Code-Bullet/Piano-Tiles"
"AboudyKreidieh/h-baselines" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"AboudyKreidieh/h-baselines" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"AboudyKreidieh/h-baselines" -> "011235813/hierarchical-marl"
"AboudyKreidieh/h-baselines" -> "jannerm/mbpo" ["e"=1]
"AboudyKreidieh/h-baselines" -> "watakandai/hiro_pytorch"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "PacktPublishing/Hands-On-Reinforcement-Learning-with-Python"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Meta-Learning-With-Python" ["e"=1]
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "omerbsezer/Reinforcement_learning_tutorial_with_demo"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "udacity/deep-reinforcement-learning"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "vmayoral/basic_reinforcement_learning"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "TianhongDai/reinforcement-learning-algorithms"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "andri27-ts/Reinforcement-Learning"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "qfettes/DeepRL-Tutorials"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "sudharsan13296/Awesome-Meta-Learning" ["e"=1]
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "simoninithomas/Deep_reinforcement_learning_Course"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "hill-a/stable-baselines"
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"inoryy/tensorflow2-deep-reinforcement-learning" -> "Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/2048-AI"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/PacNeat"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/WorldsHardestGameAI" -> "Code-Bullet/Pool_AI"
"MJeremy2017/reinforcement-learning-implementation" -> "JaeDukSeo/reinforcement-learning-an-introduction"
"MJeremy2017/reinforcement-learning-implementation" -> "MJeremy2017/machine-learning-algorithm-implemention"
"MJeremy2017/reinforcement-learning-implementation" -> "MJeremy2017/machine-learning-models"
"MJeremy2017/reinforcement-learning-implementation" -> "pythonlessons/Reinforcement_Learning"
"ChenglongChen/pytorch-DRL" -> "xuehy/pytorch-maddpg"
"ChenglongChen/pytorch-DRL" -> "sisl/MADRL"
"ChenglongChen/pytorch-DRL" -> "shariqiqbal2810/MAAC"
"ChenglongChen/pytorch-DRL" -> "mohammadasghari/dqn-multi-agent-rl"
"ChenglongChen/pytorch-DRL" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"ChenglongChen/pytorch-DRL" -> "Khrylx/PyTorch-RL"
"ChenglongChen/pytorch-DRL" -> "minqi/learning-to-communicate-pytorch"
"ChenglongChen/pytorch-DRL" -> "DongChen06/MARL_CAVs" ["e"=1]
"ChenglongChen/pytorch-DRL" -> "oxwhirl/pymarl"
"ChenglongChen/pytorch-DRL" -> "starry-sky6688/MARL-Algorithms"
"ChenglongChen/pytorch-DRL" -> "Lizhi-sjtu/DRL-code-pytorch"
"ChenglongChen/pytorch-DRL" -> "starry-sky6688/MADDPG"
"ChenglongChen/pytorch-DRL" -> "marlbenchmark/on-policy"
"ChenglongChen/pytorch-DRL" -> "mlii/mfrl"
"ChenglongChen/pytorch-DRL" -> "shariqiqbal2810/maddpg-pytorch"
"rail-berkeley/softlearning" -> "haarnoja/sac"
"rail-berkeley/softlearning" -> "rail-berkeley/rlkit"
"rail-berkeley/softlearning" -> "haarnoja/softqlearning"
"rail-berkeley/softlearning" -> "pranz24/pytorch-soft-actor-critic"
"rail-berkeley/softlearning" -> "Farama-Foundation/D4RL" ["e"=1]
"rail-berkeley/softlearning" -> "rlworkgroup/garage"
"rail-berkeley/softlearning" -> "sfujim/TD3"
"rail-berkeley/softlearning" -> "jannerm/mbpo" ["e"=1]
"rail-berkeley/softlearning" -> "astooke/rlpyt"
"rail-berkeley/softlearning" -> "sfujim/BCQ" ["e"=1]
"rail-berkeley/softlearning" -> "katerakelly/oyster" ["e"=1]
"rail-berkeley/softlearning" -> "rll/rllab"
"rail-berkeley/softlearning" -> "denisyarats/pytorch_sac" ["e"=1]
"rail-berkeley/softlearning" -> "Farama-Foundation/Metaworld" ["e"=1]
"rail-berkeley/softlearning" -> "google-research/planet" ["e"=1]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "omerbsezer/Generative_Models_Tutorial_with_Demo"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "qfettes/DeepRL-Tutorials"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "omerbsezer/LSTM_RNN_Tutorials_with_Demo" ["e"=1]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "vmayoral/basic_reinforcement_learning"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "lcswillems/rl-starter-files"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "dalmia/David-Silver-Reinforcement-learning"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "MrSyee/pg-is-all-you-need"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "andri27-ts/Reinforcement-Learning"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "dongminlee94/deep_rl"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "quantumiracle/Popular-RL-Algorithms"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "clvrai/awesome-rl-envs"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "TianhongDai/reinforcement-learning-algorithms"
"omerbsezer/Reinforcement_learning_tutorial_with_demo" -> "astooke/rlpyt"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Hands-On-Meta-Learning-With-Python" ["e"=1]
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Awesome-Meta-Learning" ["e"=1]
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Word2vec-from-scratch"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" -> "sudharsan13296/Getting-Started-with-Google-BERT" ["e"=1]
"greydanus/visualize_atari" -> "nikaashpuri/sarfa-saliency"
"greydanus/visualize_atari" -> "greydanus/baby-a3c"
"microsoft/jericho" -> "IBM/commonsense-rl"
"microsoft/jericho" -> "cognitiveailab/TextWorldExpress"
"microsoft/jericho" -> "microsoft/TextWorld"
"microsoft/jericho" -> "JerichoWorld/JerichoWorld"
"microsoft/jericho" -> "microsoft/nail_agent"
"microsoft/jericho" -> "microsoft/tdqn"
"microsoft/jericho" -> "princeton-nlp/XTX"
"microsoft/jericho" -> "allenai/ScienceWorld"
"microsoft/jericho" -> "princeton-nlp/calm-textgame"
"microsoft/jericho" -> "xingdi-eric-yuan/GATA-public"
"stevens-cs546-cs554/CS-546" -> "stevens-cs546-cs554/CS-554"
"stevens-cs546-cs554/CS-546" -> "ebonelli/PLaF"
"stevens-cs546-cs554/CS-546" -> "ParasGarg/Stevens-Computer-Science-Courses-Materials"
"qzed/irl-maxent" -> "yrlu/irl-imitation"
"qzed/irl-maxent" -> "reinforcement-learning-kr/lets-do-irl"
"qzed/irl-maxent" -> "ermongroup/MA-AIRL"
"qzed/irl-maxent" -> "krasheninnikov/max-causal-ent-irl"
"qzed/irl-maxent" -> "jangirrishabh/toyCarIRL"
"qzed/irl-maxent" -> "MCZhi/Driving-IRL-NGSIM" ["e"=1]
"qzed/irl-maxent" -> "MatthewJA/Inverse-Reinforcement-Learning"
"qzed/irl-maxent" -> "TroddenSpade/Maximum-Entropy-Deep-IRL"
"qzed/irl-maxent" -> "yfzhang/vehicle-motion-forecasting"
"qzed/irl-maxent" -> "twni2016/f-IRL"
"qzed/irl-maxent" -> "justinjfu/inverse_rl"
"qzed/irl-maxent" -> "Kaixhin/imitation-learning"
"qzed/irl-maxent" -> "ShivinDass/inverse_rl"
"qzed/irl-maxent" -> "HuangJiaLian/AIRL_MountainCar"
"qzed/irl-maxent" -> "nishantkr18/guided-cost-learning"
"hanabi/hanabi.github.io" -> "Hanabi-Live/hanabi-live"
"hanabi/hanabi.github.io" -> "Quuxplusone/Hanabi"
"hanabi/hanabi.github.io" -> "rjtobin/HanSim"
"hanabi/hanabi.github.io" -> "WuTheFWasThat/hanabi.rs"
"pranz24/pytorch-soft-actor-critic" -> "haarnoja/sac"
"pranz24/pytorch-soft-actor-critic" -> "denisyarats/pytorch_sac" ["e"=1]
"pranz24/pytorch-soft-actor-critic" -> "rail-berkeley/softlearning"
"pranz24/pytorch-soft-actor-critic" -> "rail-berkeley/rlkit"
"pranz24/pytorch-soft-actor-critic" -> "sfujim/TD3"
"pranz24/pytorch-soft-actor-critic" -> "quantumiracle/Popular-RL-Algorithms"
"pranz24/pytorch-soft-actor-critic" -> "BY571/Soft-Actor-Critic-and-Extensions"
"pranz24/pytorch-soft-actor-critic" -> "Farama-Foundation/D4RL" ["e"=1]
"pranz24/pytorch-soft-actor-critic" -> "aviralkumar2907/CQL" ["e"=1]
"pranz24/pytorch-soft-actor-critic" -> "Khrylx/PyTorch-RL"
"pranz24/pytorch-soft-actor-critic" -> "nikhilbarhate99/PPO-PyTorch"
"pranz24/pytorch-soft-actor-critic" -> "sfujim/BCQ" ["e"=1]
"pranz24/pytorch-soft-actor-critic" -> "katerakelly/oyster" ["e"=1]
"pranz24/pytorch-soft-actor-critic" -> "sfujim/TD3_BC" ["e"=1]
"pranz24/pytorch-soft-actor-critic" -> "ikostrikov/pytorch-trpo"
"cyoon1729/RLcycle" -> "cyoon1729/Policy-Gradient-Methods"
"cyoon1729/RLcycle" -> "cyoon1729/deep-Q-networks"
"cyoon1729/RLcycle" -> "medipixel/rl_algorithms"
"cyoon1729/RLcycle" -> "cyoon1729/distributedRL"
"cyoon1729/RLcycle" -> "google-research/realworldrl_suite" ["e"=1]
"cyoon1729/RLcycle" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"glample/Arnold" -> "akolishchak/doom-net-pytorch"
"glample/Arnold" -> "Farama-Foundation/ViZDoom"
"glample/Arnold" -> "mihahauke/VDAIC2017"
"glample/Arnold" -> "ruiminshen/yolo2-pytorch" ["e"=1]
"glample/Arnold" -> "Breakend/gym-extensions"
"glample/Arnold" -> "itaicaspi/keras-dqn-doom"
"glample/Arnold" -> "mehdiboubnan/Deep-Reinforcement-Learning-applied-to-DOOM"
"glample/Arnold" -> "isl-org/DirectFuturePrediction"
"glample/Arnold" -> "NVlabs/GA3C"
"glample/Arnold" -> "ikostrikov/pytorch-a3c"
"glample/Arnold" -> "devendrachaplot/DeepRL-Grounding" ["e"=1]
"glample/Arnold" -> "cheesecakeufo/topanga" ["e"=1]
"glample/Arnold" -> "jingweiz/pytorch-rl"
"glample/Arnold" -> "Kaixhin/PlaNet" ["e"=1]
"glample/Arnold" -> "miyosuda/unreal"
"GAOYANGAU/DRLPytorch" -> "catziyan/DRLPytorch-"
"GAOYANGAU/DRLPytorch" -> "Teacher-Guo/RL_code"
"GAOYANGAU/DRLPytorch" -> "gxnk/reinforcement-learning-code"
"jaybutera/tetrisRL" -> "lusob/gym-tetris"
"jaybutera/tetrisRL" -> "Uglemat/MaTris"
"jaybutera/tetrisRL" -> "ReinforcementLearning/Tetris"
"andrewliao11/gail-tf" -> "uidilr/gail_ppo_tf"
"andrewliao11/gail-tf" -> "openai/imitation"
"andrewliao11/gail-tf" -> "YunzhuLi/InfoGAIL"
"andrewliao11/gail-tf" -> "itaicaspi/mgail"
"andrewliao11/gail-tf" -> "yrlu/irl-imitation"
"andrewliao11/gail-tf" -> "navuboy/gail_gym"
"andrewliao11/gail-tf" -> "sisl/gail-driver"
"andrewliao11/gail-tf" -> "justinjfu/inverse_rl"
"andrewliao11/gail-tf" -> "ahq1993/inverse_rl"
"andrewliao11/gail-tf" -> "jangirrishabh/toyCarIRL"
"andrewliao11/gail-tf" -> "reinforcement-learning-kr/lets-do-irl"
"google-deepmind/hanabi-learning-environment" -> "facebookresearch/hanabi_SAD"
"google-deepmind/hanabi-learning-environment" -> "facebookresearch/Hanabi_SPARTA"
"google-deepmind/hanabi-learning-environment" -> "oxwhirl/smac"
"google-deepmind/hanabi-learning-environment" -> "eugenevinitsky/sequential_social_dilemma_games"
"google-deepmind/hanabi-learning-environment" -> "MultiAgentLearning/playground"
"google-deepmind/hanabi-learning-environment" -> "oxwhirl/pymarl"
"google-deepmind/hanabi-learning-environment" -> "google-deepmind/bsuite"
"google-deepmind/hanabi-learning-environment" -> "google-research/planet" ["e"=1]
"google-deepmind/hanabi-learning-environment" -> "openai/neural-mmo"
"google-deepmind/hanabi-learning-environment" -> "google-deepmind/scalable_agent"
"google-deepmind/hanabi-learning-environment" -> "Quuxplusone/Hanabi"
"google-deepmind/hanabi-learning-environment" -> "facebookresearch/torchbeast"
"google-deepmind/hanabi-learning-environment" -> "openai/large-scale-curiosity"
"google-deepmind/hanabi-learning-environment" -> "google-deepmind/spriteworld"
"google-deepmind/hanabi-learning-environment" -> "Farama-Foundation/Minigrid"
"openai/robosumo" -> "openai/EPG"
"openai/robosumo" -> "cbfinn/maml_rl" ["e"=1]
"openai/robosumo" -> "openai/mlsh"
"openai/robosumo" -> "openai/multiagent-competition"
"openai/robosumo" -> "junhyukoh/self-imitation-learning"
"openai/robosumo" -> "openai/vime"
"openai/robosumo" -> "iclavera/learning_to_adapt" ["e"=1]
"openai/robosumo" -> "jonasrothfuss/ProMP" ["e"=1]
"openai/robosumo" -> "junhyukoh/value-prediction-network"
"openai/robosumo" -> "openai/doom-py"
"openai/robosumo" -> "alshedivat/lola"
"dgriff777/a3c_continuous" -> "dgriff777/rl_a3c_pytorch"
"dgriff777/a3c_continuous" -> "MorvanZhou/pytorch-A3C"
"dgriff777/a3c_continuous" -> "rlbayes/rllabplusplus"
"junhyukoh/self-imitation-learning" -> "TianhongDai/self-imitation-learning-pytorch"
"junhyukoh/self-imitation-learning" -> "pathak22/zeroshot-imitation"
"junhyukoh/self-imitation-learning" -> "rlbayes/rllabplusplus"
"junhyukoh/self-imitation-learning" -> "tgangwani/SelfImitationDiverse"
"junhyukoh/self-imitation-learning" -> "Hwhitetooth/lirpg"
"junhyukoh/self-imitation-learning" -> "openai/atari-reset"
"junhyukoh/self-imitation-learning" -> "openai/imitation"
"junhyukoh/self-imitation-learning" -> "tianheyu927/mil"
"junhyukoh/self-imitation-learning" -> "openai/robosumo"
"dnddnjs/feudal-montezuma" -> "davidhershey/feudal_networks"
"dnddnjs/feudal-montezuma" -> "lweitkamp/feudalnets-pytorch"
"uber-research/go-explore" -> "openai/random-network-distillation"
"uber-research/go-explore" -> "openai/large-scale-curiosity"
"uber-research/go-explore" -> "junhyukoh/self-imitation-learning"
"uber-research/go-explore" -> "google-research/seed_rl"
"uber-research/go-explore" -> "google-research/episodic-curiosity" ["e"=1]
"uber-research/go-explore" -> "pathak22/noreward-rl"
"uber-research/go-explore" -> "junhyukoh/value-prediction-network"
"uber-research/go-explore" -> "google-deepmind/scalable_agent"
"uber-research/go-explore" -> "google-research/planet" ["e"=1]
"uber-research/go-explore" -> "facebookresearch/torchbeast"
"uber-research/go-explore" -> "mila-iqia/atari-representation-learning" ["e"=1]
"uber-research/go-explore" -> "dnddnjs/feudal-montezuma"
"uber-research/go-explore" -> "google-deepmind/bsuite"
"uber-research/go-explore" -> "openai/procgen"
"uber-research/go-explore" -> "uber-research/poet"
"hoangminhle/hierarchical_IL_RL" -> "davidhershey/feudal_networks"
"hoangminhle/hierarchical_IL_RL" -> "openai/mlsh"
"hoangminhle/hierarchical_IL_RL" -> "junhyukoh/self-imitation-learning"
"hoangminhle/hierarchical_IL_RL" -> "tianheyu927/mil"
"hoangminhle/hierarchical_IL_RL" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"hoangminhle/hierarchical_IL_RL" -> "mrkulk/hierarchical-deep-RL"
"hoangminhle/hierarchical_IL_RL" -> "EdenMacdonald/h-DQN"
"hoangminhle/hierarchical_IL_RL" -> "skumar9876/Hierarchical-DQN"
"hoangminhle/hierarchical_IL_RL" -> "yrlu/irl-imitation"
"hoangminhle/hierarchical_IL_RL" -> "andrewliao11/gail-tf"
"flyyufelix/VizDoom-Keras-RL" -> "itaicaspi/keras-dqn-doom"
"flyyufelix/VizDoom-Keras-RL" -> "akolishchak/doom-net-pytorch"
"flyyufelix/VizDoom-Keras-RL" -> "avdmitry/rl_3d"
"PKU-RL/DGN" -> "jiechuanjiang/pytorch_DGN"
"PKU-RL/DGN" -> "sumitsk/marl_transfer"
"PKU-RL/DGN" -> "IC3Net/IC3Net"
"PKU-RL/DGN" -> "shariqiqbal2810/MAAC"
"PKU-RL/DGN" -> "mlii/mfrl"
"PKU-RL/DGN" -> "tegg89/magnet"
"PKU-RL/DGN" -> "cts198859/deeprl_network"
"PKU-RL/DGN" -> "starry-sky6688/MARL-Algorithms"
"PKU-RL/DGN" -> "oxwhirl/pymarl"
"PKU-RL/DGN" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"PKU-RL/DGN" -> "CORE-Robotics-Lab/MAGIC"
"PKU-RL/DGN" -> "oxwhirl/smac"
"PKU-RL/DGN" -> "shariqiqbal2810/maddpg-pytorch"
"PKU-RL/DGN" -> "geek-ai/MAgent"
"PKU-RL/DGN" -> "koulanurag/ma-gym"
"mlii/mfrl" -> "PKU-RL/DGN"
"mlii/mfrl" -> "geek-ai/MAgent"
"mlii/mfrl" -> "shariqiqbal2810/MAAC"
"mlii/mfrl" -> "sjtu-marl/malib"
"mlii/mfrl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"mlii/mfrl" -> "minqi/learning-to-communicate-pytorch"
"mlii/mfrl" -> "sisl/MADRL"
"mlii/mfrl" -> "Farama-Foundation/MAgent"
"mlii/mfrl" -> "xuehy/pytorch-maddpg"
"mlii/mfrl" -> "oxwhirl/pymarl"
"mlii/mfrl" -> "IC3Net/IC3Net"
"mlii/mfrl" -> "oxwhirl/smac"
"mlii/mfrl" -> "koulanurag/ma-gym"
"mlii/mfrl" -> "BorealisAI/mtmfrl"
"mlii/mfrl" -> "shariqiqbal2810/maddpg-pytorch"
"unixpickle/anyrl-py" -> "openai/retro-baselines"
"Code-Bullet/Chess-AI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Chess-AI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Chess-AI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Chess-AI" -> "Code-Bullet/2048-AI"
"Code-Bullet/Chess-AI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Chess-AI" -> "Code-Bullet/PacNeat"
"Code-Bullet/Chess-AI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Chess-AI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/Chess-AI" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/Chess-AI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/Chess-AI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Chess-AI" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Chess-AI" -> "Code-Bullet/PacmanGame"
"Code-Bullet/Chess-AI" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Chess-AI" -> "Code-Bullet/WebsiteTest"
"openai/random-network-distillation" -> "jcwleo/random-network-distillation-pytorch"
"openai/random-network-distillation" -> "openai/large-scale-curiosity"
"openai/random-network-distillation" -> "pathak22/noreward-rl"
"openai/random-network-distillation" -> "google-deepmind/scalable_agent"
"openai/random-network-distillation" -> "uber-research/go-explore"
"openai/random-network-distillation" -> "openai/coinrun"
"openai/random-network-distillation" -> "haarnoja/sac"
"openai/random-network-distillation" -> "google-research/planet" ["e"=1]
"openai/random-network-distillation" -> "rail-berkeley/softlearning"
"openai/random-network-distillation" -> "uber-research/ape-x"
"openai/random-network-distillation" -> "junhyukoh/self-imitation-learning"
"openai/random-network-distillation" -> "Farama-Foundation/Minigrid"
"openai/random-network-distillation" -> "Kaixhin/Rainbow"
"openai/random-network-distillation" -> "MishaLaskin/curl" ["e"=1]
"openai/random-network-distillation" -> "miyosuda/unreal"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/2048-AI"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/Flappy-Bird-AI"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/WebsiteTest"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/PacNeat"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Hill-Climb-Racing-AI" -> "Code-Bullet/WorldsHardestGameAI"
"germain-hug/Deep-RL-Keras" -> "yanpanlau/DDPG-Keras-Torcs"
"germain-hug/Deep-RL-Keras" -> "keon/deep-q-learning"
"germain-hug/Deep-RL-Keras" -> "archsyscall/DeepRL-TensorFlow2"
"germain-hug/Deep-RL-Keras" -> "xiaochus/Deep-Reinforcement-Learning-Practice" ["e"=1]
"germain-hug/Deep-RL-Keras" -> "pythonlessons/Reinforcement_Learning"
"germain-hug/Deep-RL-Keras" -> "keiohta/tf2rl"
"germain-hug/Deep-RL-Keras" -> "keras-rl/keras-rl"
"germain-hug/Deep-RL-Keras" -> "anita-hu/TF2-RL"
"germain-hug/Deep-RL-Keras" -> "miroblog/deep_rl_trader" ["e"=1]
"germain-hug/Deep-RL-Keras" -> "cyoon1729/deep-Q-networks"
"germain-hug/Deep-RL-Keras" -> "inarikami/keras-rl2"
"germain-hug/Deep-RL-Keras" -> "fg91/Deep-Q-Learning"
"germain-hug/Deep-RL-Keras" -> "flyyufelix/C51-DDQN-Keras"
"germain-hug/Deep-RL-Keras" -> "jaromiru/AI-blog"
"germain-hug/Deep-RL-Keras" -> "cyoon1729/RLcycle"
"openai/atari-reset" -> "openai/atari-demo"
"openai/atari-reset" -> "reinforcement-learning-kr/rl-montezuma"
"openai/atari-reset" -> "openai/retro-baselines"
"openai/atari-reset" -> "junhyukoh/self-imitation-learning"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/2048-AI"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/PacNeat"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/PacmanGame"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/Pool_AI"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/AsteroidsAI" -> "Code-Bullet/WebsiteTest"
"vietnh1009/Flappy-bird-deep-Q-learning-pytorch" -> "vietnh1009/Super-mario-bros-A3C-pytorch"
"vietnh1009/Flappy-bird-deep-Q-learning-pytorch" -> "vietnh1009/Tetris-deep-Q-learning-pytorch"
"vietnh1009/Flappy-bird-deep-Q-learning-pytorch" -> "vietnh1009/Street-fighter-A3C-ICM-pytorch"
"vietnh1009/Flappy-bird-deep-Q-learning-pytorch" -> "vietnh1009/Super-mario-bros-PPO-pytorch"
"vietnh1009/Flappy-bird-deep-Q-learning-pytorch" -> "vietnh1009/AirGesture" ["e"=1]
"vivek3141/pacman-ai" -> "vivek3141/flappy-neat"
"vivek3141/pacman-ai" -> "vivek3141/learn-c"
"vivek3141/pacman-ai" -> "vivek3141/learn-js"
"vivek3141/pacman-ai" -> "vivek3141/tictactoe-minimax"
"vivek3141/pacman-ai" -> "vivek3141/snake-gym"
"minerllabs/minerl" -> "openai/Video-Pre-Training" ["e"=1]
"minerllabs/minerl" -> "minerllabs/baselines"
"minerllabs/minerl" -> "MineDojo/MineDojo" ["e"=1]
"minerllabs/minerl" -> "minerllabs/competition_submission_template"
"minerllabs/minerl" -> "crowdAI/marLo"
"minerllabs/minerl" -> "openai/procgen"
"minerllabs/minerl" -> "tambetm/gym-minecraft"
"minerllabs/minerl" -> "microsoft/malmo"
"minerllabs/minerl" -> "MineDojo/MineCLIP" ["e"=1]
"minerllabs/minerl" -> "danijar/crafter" ["e"=1]
"minerllabs/minerl" -> "danijar/dreamerv3" ["e"=1]
"minerllabs/minerl" -> "juliusfrost/dreamer-pytorch" ["e"=1]
"minerllabs/minerl" -> "Farama-Foundation/Miniworld"
"minerllabs/minerl" -> "Farama-Foundation/Minigrid"
"minerllabs/minerl" -> "Shalev-Lifshitz/STEVE-1" ["e"=1]
"NeuralMMO/environment" -> "openai/neural-mmo"
"NeuralMMO/environment" -> "NeuralMMO/client"
"NeuralMMO/environment" -> "NeuralMMO/baselines"
"NeuralMMO/environment" -> "eugenevinitsky/sequential_social_dilemma_games"
"NeuralMMO/environment" -> "danijar/crafter" ["e"=1]
"NeuralMMO/environment" -> "google-deepmind/meltingpot"
"NeuralMMO/environment" -> "oxwhirl/smac"
"NeuralMMO/environment" -> "Bam4d/Griddly"
"NeuralMMO/environment" -> "koulanurag/ma-gym"
"NeuralMMO/environment" -> "Farama-Foundation/PettingZoo"
"NeuralMMO/environment" -> "Farama-Foundation/SuperSuit" ["e"=1]
"NeuralMMO/environment" -> "clvrai/awesome-rl-envs"
"NeuralMMO/environment" -> "PufferAI/PufferLib" ["e"=1]
"NeuralMMO/environment" -> "openai/procgen"
"NeuralMMO/environment" -> "Farama-Foundation/Minigrid"
"Sohojoe/ActiveRagdollStyleTransfer" -> "Sohojoe/ActiveRagdollAssaultCourse"
"Sohojoe/ActiveRagdollStyleTransfer" -> "Sohojoe/ActiveRagdollControllers"
"Sohojoe/ActiveRagdollStyleTransfer" -> "Unity-Technologies/marathon-envs"
"Code-Bullet/Car-QLearning" -> "Code-Bullet/Tetris-AI-Javascript"
"Code-Bullet/Car-QLearning" -> "Code-Bullet/2048-AI"
"Code-Bullet/Car-QLearning" -> "Code-Bullet/NEAT-Template-JavaScript"
"vitchyr/multiworld" -> "vitchyr/viskit"
"vitchyr/multiworld" -> "TianhongDai/hindsight-experience-replay"
"vitchyr/multiworld" -> "mihdalal/sawyer_control"
"vitchyr/multiworld" -> "Kaixhin/PlaNet" ["e"=1]
"vitchyr/multiworld" -> "mengf1/DHER"
"vitchyr/multiworld" -> "rail-berkeley/rlkit"
"vitchyr/multiworld" -> "Farama-Foundation/Metaworld" ["e"=1]
"vitchyr/multiworld" -> "snasiriany/leap"
"vitchyr/multiworld" -> "zuoxingdong/mazelab"
"vitchyr/multiworld" -> "denisyarats/dmc2gym" ["e"=1]
"alexis-jacq/LOLA_DiCE" -> "alshedivat/lola"
"openai/multiagent-competition" -> "openai/robosumo"
"openai/multiagent-competition" -> "geek-ai/MAgent"
"openai/multiagent-competition" -> "openai/multi-agent-emergence-environments"
"openai/multiagent-competition" -> "openai/mlsh"
"openai/multiagent-competition" -> "openai/imitation"
"openai/multiagent-competition" -> "google-deepmind/scalable_agent"
"openai/multiagent-competition" -> "google-deepmind/pycolab"
"openai/multiagent-competition" -> "pathak22/noreward-rl"
"openai/multiagent-competition" -> "openai/multiagent-particle-envs"
"openai/multiagent-competition" -> "oxwhirl/smac"
"openai/multiagent-competition" -> "openai/maddpg"
"openai/multiagent-competition" -> "openai/evolution-strategies-starter"
"openai/multiagent-competition" -> "openai/large-scale-curiosity"
"openai/multiagent-competition" -> "openai/roboschool"
"openai/multiagent-competition" -> "oxwhirl/pymarl"
"Nasdin/ReinforcementLearning-AtariGame" -> "dgriff777/rl_a3c_pytorch"
"zuoxingdong/mazelab" -> "MattChanTK/gym-maze"
"zuoxingdong/mazelab" -> "vitchyr/multiworld"
"zuoxingdong/mazelab" -> "kenjyoung/MinAtar" ["e"=1]
"zuoxingdong/mazelab" -> "google-deepmind/pycolab"
"zuoxingdong/mazelab" -> "junhyukoh/value-prediction-network"
"zuoxingdong/mazelab" -> "zuoxingdong/lagom"
"zuoxingdong/mazelab" -> "Farama-Foundation/Minigrid"
"zuoxingdong/mazelab" -> "junhyukoh/self-imitation-learning"
"zuoxingdong/mazelab" -> "tambetm/gym-minecraft"
"zuoxingdong/mazelab" -> "facebookarchive/MazeBase"
"BorealisAI/pommerman-baseline" -> "rwightman/pytorch-pommerman-rl"
"BorealisAI/pommerman-baseline" -> "eugene/pommerman"
"BorealisAI/pommerman-baseline" -> "tambetm/pommerman-baselines"
"sumitsk/marl_transfer" -> "PKU-RL/DGN"
"sumitsk/marl_transfer" -> "qian18long/epciclr2020"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "AboudyKreidieh/h-baselines"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "YangRui2015/Sparse-Reward-Algorithms"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "openai/mlsh"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "YuhangSong/DEHRL"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "davidhershey/feudal_networks"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "011235813/hierarchical-marl"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "tdavchev/option-critic"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "hoangminhle/hierarchical_IL_RL"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "jesbu1/hidio"
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "florensacc/rllab-curriculum" ["e"=1]
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" -> "jeanharb/option_critic"
"NeuralMMO/client" -> "openai/neural-mmo"
"NeuralMMO/client" -> "NeuralMMO/environment"
"MrSyee/pg-is-all-you-need" -> "Curt-Park/rainbow-is-all-you-need"
"MrSyee/pg-is-all-you-need" -> "medipixel/rl_algorithms"
"MrSyee/pg-is-all-you-need" -> "clvrai/awesome-rl-envs"
"MrSyee/pg-is-all-you-need" -> "dongminlee94/deep_rl"
"MrSyee/pg-is-all-you-need" -> "qfettes/DeepRL-Tutorials"
"MrSyee/pg-is-all-you-need" -> "quantumiracle/Popular-RL-Algorithms"
"MrSyee/pg-is-all-you-need" -> "seungeunrho/minimalRL"
"MrSyee/pg-is-all-you-need" -> "Khrylx/PyTorch-RL"
"MrSyee/pg-is-all-you-need" -> "higgsfield/RL-Adventure"
"MrSyee/pg-is-all-you-need" -> "koulanurag/ma-gym"
"MrSyee/pg-is-all-you-need" -> "seungjaeryanlee/awesome-rl-competitions"
"MrSyee/pg-is-all-you-need" -> "RobertTLange/gymnax" ["e"=1]
"MrSyee/pg-is-all-you-need" -> "vwxyzjn/cleanrl"
"MrSyee/pg-is-all-you-need" -> "Kaixhin/Rainbow"
"MrSyee/pg-is-all-you-need" -> "reinforcement-learning-kr/lets-do-irl"
"vivek3141/super-mario-neat" -> "vivek3141/super-mario-rl"
"vivek3141/super-mario-neat" -> "vivek3141/ml"
"vivek3141/super-mario-neat" -> "vivek3141/vector-field-visualizer"
"vivek3141/super-mario-neat" -> "vivek3141/flappy-neat"
"vivek3141/super-mario-neat" -> "vivek3141/pacman-ai"
"vivek3141/super-mario-neat" -> "vivek3141/snake-gym"
"vivek3141/super-mario-neat" -> "vivek3141/learn-js"
"vivek3141/super-mario-neat" -> "vivek3141/forest-fire-predictor"
"vivek3141/super-mario-neat" -> "vivek3141/rubiks-cube-ai"
"vivek3141/super-mario-neat" -> "vivek3141/learn-c"
"vivek3141/super-mario-neat" -> "vivek3141/tictactoe-minimax"
"vivek3141/super-mario-neat" -> "vivek3141/CommunicationForDeaf"
"vivek3141/super-mario-neat" -> "vivek3141/NavigationForBlind"
"vivek3141/super-mario-neat" -> "vivek3141/streamer-ai"
"vivek3141/snake-gym" -> "vivek3141/learn-c"
"vivek3141/snake-gym" -> "vivek3141/learn-js"
"vivek3141/snake-gym" -> "vivek3141/flappy-neat"
"wwxFromTju/deepmind_MAS_enviroment" -> "Coac/CommNet-BiCnet"
"wwxFromTju/deepmind_MAS_enviroment" -> "IC3Net/IC3Net"
"wwxFromTju/deepmind_MAS_enviroment" -> "sisl/MADRL"
"wwxFromTju/deepmind_MAS_enviroment" -> "facebookarchive/CommNet"
"wwxFromTju/deepmind_MAS_enviroment" -> "TonghanWang/ROMA"
"ZhouWeikuan/DouDiZhu" -> "ZhouWeikuan/kuanli_server" ["e"=1]
"ZhouWeikuan/DouDiZhu" -> "songbaoming/DouDiZhu"
"ZhouWeikuan/DouDiZhu" -> "thuxugang/doudizhu"
"ZhouWeikuan/DouDiZhu" -> "onestraw/doudizhu"
"ZhouWeikuan/DouDiZhu" -> "qmarliu/ddzlib"
"ZhouWeikuan/DouDiZhu" -> "Netease-Games-AI-Lab-Guangzhou/PerfectDou"
"ZhouWeikuan/DouDiZhu" -> "yinjimmy/chessAndCard-2dx" ["e"=1]
"ZhouWeikuan/DouDiZhu" -> "qq456cvb/doudizhu-C"
"ZhouWeikuan/DouDiZhu" -> "fztcjjl/metoo" ["e"=1]
"ZhouWeikuan/DouDiZhu" -> "zhangshiqian1214/skynet-server" ["e"=1]
"ZhouWeikuan/DouDiZhu" -> "yuanfengyun/q_algorithm" ["e"=1]
"ZhouWeikuan/DouDiZhu" -> "yuanfengyun/mj_ai" ["e"=1]
"ZhouWeikuan/DouDiZhu" -> "qipaiprojects/mj_server" ["e"=1]
"IC3Net/IC3Net" -> "PKU-RL/I2C"
"IC3Net/IC3Net" -> "rhoowd/sched_net"
"IC3Net/IC3Net" -> "PKU-RL/DGN"
"IC3Net/IC3Net" -> "minqi/learning-to-communicate-pytorch"
"IC3Net/IC3Net" -> "facebookarchive/CommNet"
"IC3Net/IC3Net" -> "saizhang0218/VBC"
"IC3Net/IC3Net" -> "CORE-Robotics-Lab/MAGIC"
"IC3Net/IC3Net" -> "shariqiqbal2810/MAAC"
"IC3Net/IC3Net" -> "apsdehal/ic3net-envs"
"IC3Net/IC3Net" -> "sumitsk/marl_transfer"
"IC3Net/IC3Net" -> "tuladhay/ATOC_COMA_PyTorch"
"IC3Net/IC3Net" -> "hsvgbkhgbv/SQDDPG"
"IC3Net/IC3Net" -> "TonghanWang/ROMA"
"IC3Net/IC3Net" -> "saizhang0218/TMC"
"IC3Net/IC3Net" -> "Coac/CommNet-BiCnet"
"skumar9876/Hierarchical-DQN" -> "mrkulk/hierarchical-deep-RL"
"skumar9876/Hierarchical-DQN" -> "EdenMacdonald/h-DQN"
"skumar9876/Hierarchical-DQN" -> "gmargo11/hDQN"
"skumar9876/Hierarchical-DQN" -> "fedingo/Hierarchical-DQN"
"skumar9876/Hierarchical-DQN" -> "hungtuchen/pytorch-hdqn"
"skumar9876/Hierarchical-DQN" -> "wulfebw/hierarchical_rl"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/2048-AI"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/PacNeat"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Flappy-Bird-AI"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/RubiksCubeAI" -> "Code-Bullet/PacmanGame"
"uber-research/atari-model-zoo" -> "openai/retro-baselines"
"cmaron/CS-7641-assignments" -> "mohamedameen93/CS-7641-Machine-Learning-Notes"
"cmaron/CS-7641-assignments" -> "JonathanTay/CS-7641-assignment-1"
"cmaron/CS-7641-assignments" -> "hiive/mlrose"
"cmaron/CS-7641-assignments" -> "gkhayes/mlrose"
"cmaron/CS-7641-assignments" -> "pushkar/ABAGAIL"
"cmaron/CS-7641-assignments" -> "juanjose49/omscs-cs7641-machine-learning-assignment-4"
"cmaron/CS-7641-assignments" -> "kylewest520/CS-7641---Machine-Learning"
"mohamedameen93/CS-7641-Machine-Learning-Notes" -> "cmaron/CS-7641-assignments"
"mohamedameen93/CS-7641-Machine-Learning-Notes" -> "mohamedameen93/CS-7642-Reinforcement-Learning-Notes"
"mohamedameen93/CS-7641-Machine-Learning-Notes" -> "JonathanTay/CS-7641-assignment-1"
"mohamedameen93/CS-7641-Machine-Learning-Notes" -> "hiive/mlrose"
"google-deepmind/scalable_agent" -> "google-research/seed_rl"
"google-deepmind/scalable_agent" -> "facebookresearch/torchbeast"
"google-deepmind/scalable_agent" -> "google-deepmind/trfl"
"google-deepmind/scalable_agent" -> "openai/random-network-distillation"
"google-deepmind/scalable_agent" -> "uber-research/ape-x"
"google-deepmind/scalable_agent" -> "pathak22/noreward-rl"
"google-deepmind/scalable_agent" -> "NVlabs/GA3C"
"google-deepmind/scalable_agent" -> "rail-berkeley/softlearning"
"google-deepmind/scalable_agent" -> "google-deepmind/bsuite"
"google-deepmind/scalable_agent" -> "astooke/rlpyt"
"google-deepmind/scalable_agent" -> "junhyukoh/self-imitation-learning"
"google-deepmind/scalable_agent" -> "openai/large-scale-curiosity"
"google-deepmind/scalable_agent" -> "openai/coinrun"
"google-deepmind/scalable_agent" -> "google-deepmind/pycolab"
"google-deepmind/scalable_agent" -> "openai/multiagent-competition"
"Farzin-Negahbani/Namira-LogAnalyzer" -> "Cyrus2D/SoccerSimulationProxy"
"tianheyu927/mil" -> "pathak22/zeroshot-imitation"
"tianheyu927/mil" -> "pathak22/hierarchical-imitation"
"tianheyu927/mil" -> "junhyukoh/self-imitation-learning"
"tianheyu927/mil" -> "avisingh599/reward-learning-rl" ["e"=1]
"tianheyu927/mil" -> "wyndwarrior/imitation_from_observation"
"tianheyu927/mil" -> "andrewliao11/gail-tf"
"tianheyu927/mil" -> "stepjam/PyRep" ["e"=1]
"tianheyu927/mil" -> "cbfinn/maml_rl" ["e"=1]
"tianheyu927/mil" -> "hoangminhle/hierarchical_IL_RL"
"tianheyu927/mil" -> "cbfinn/gps"
"tianheyu927/mil" -> "jangirrishabh/Overcoming-exploration-from-demos"
"tianheyu927/mil" -> "jonasrothfuss/ProMP" ["e"=1]
"tianheyu927/mil" -> "stepjam/TecNets"
"minqi/learning-to-communicate-pytorch" -> "iassael/learning-to-communicate"
"minqi/learning-to-communicate-pytorch" -> "IC3Net/IC3Net"
"minqi/learning-to-communicate-pytorch" -> "shariqiqbal2810/MAAC"
"minqi/learning-to-communicate-pytorch" -> "xuehy/pytorch-maddpg"
"minqi/learning-to-communicate-pytorch" -> "eugenevinitsky/sequential_social_dilemma_games"
"minqi/learning-to-communicate-pytorch" -> "mlii/mfrl"
"minqi/learning-to-communicate-pytorch" -> "sisl/MADRL"
"minqi/learning-to-communicate-pytorch" -> "rhoowd/sched_net"
"minqi/learning-to-communicate-pytorch" -> "cts198859/deeprl_network"
"minqi/learning-to-communicate-pytorch" -> "facebookarchive/CommNet"
"minqi/learning-to-communicate-pytorch" -> "PKU-RL/DGN"
"minqi/learning-to-communicate-pytorch" -> "hsvgbkhgbv/SQDDPG"
"minqi/learning-to-communicate-pytorch" -> "oxwhirl/pymarl"
"minqi/learning-to-communicate-pytorch" -> "ermongroup/MA-AIRL"
"minqi/learning-to-communicate-pytorch" -> "wwxFromTju/deepmind_MAS_enviroment"
"eugenevinitsky/sequential_social_dilemma_games" -> "google-deepmind/meltingpot"
"eugenevinitsky/sequential_social_dilemma_games" -> "minqi/learning-to-communicate-pytorch"
"eugenevinitsky/sequential_social_dilemma_games" -> "social-dilemma/multiagent"
"eugenevinitsky/sequential_social_dilemma_games" -> "alshedivat/lola"
"eugenevinitsky/sequential_social_dilemma_games" -> "koulanurag/ma-gym"
"eugenevinitsky/sequential_social_dilemma_games" -> "kandouss/marlgrid"
"eugenevinitsky/sequential_social_dilemma_games" -> "PKU-RL/DGN"
"eugenevinitsky/sequential_social_dilemma_games" -> "oxwhirl/pymarl"
"eugenevinitsky/sequential_social_dilemma_games" -> "HumanCompatibleAI/human_aware_rl"
"eugenevinitsky/sequential_social_dilemma_games" -> "ermongroup/MA-AIRL"
"eugenevinitsky/sequential_social_dilemma_games" -> "IC3Net/IC3Net"
"eugenevinitsky/sequential_social_dilemma_games" -> "shariqiqbal2810/MAAC"
"eugenevinitsky/sequential_social_dilemma_games" -> "oxwhirl/smac"
"eugenevinitsky/sequential_social_dilemma_games" -> "TonghanWang/ROMA"
"eugenevinitsky/sequential_social_dilemma_games" -> "google-deepmind/hanabi-learning-environment"
"rhoowd/sched_net" -> "saizhang0218/VBC"
"rhoowd/sched_net" -> "IC3Net/IC3Net"
"rhoowd/sched_net" -> "tuladhay/ATOC_COMA_PyTorch"
"rhoowd/sched_net" -> "saizhang0218/TMC"
"louaaron/CS294_homework" -> "xuwd11/cs294-112_hws"
"ermongroup/multiagent-gail" -> "RITCHIEHuang/MAGAIL"
"ermongroup/multiagent-gail" -> "ermongroup/MA-AIRL"
"mpSchrader/gym-sokoban" -> "kenjyoung/MinAtar" ["e"=1]
"mpSchrader/gym-sokoban" -> "mila-iqia/atari-representation-learning" ["e"=1]
"mpSchrader/gym-sokoban" -> "Farama-Foundation/Miniworld"
"mpSchrader/gym-sokoban" -> "openai/procgen"
"mpSchrader/gym-sokoban" -> "Farama-Foundation/Minigrid"
"mpSchrader/gym-sokoban" -> "google-deepmind/pycolab"
"mpSchrader/gym-sokoban" -> "sail-sg/envpool" ["e"=1]
"mpSchrader/gym-sokoban" -> "Bam4d/Griddly"
"mpSchrader/gym-sokoban" -> "denisyarats/dmc2gym" ["e"=1]
"mpSchrader/gym-sokoban" -> "facebookresearch/mbrl-lib" ["e"=1]
"mpSchrader/gym-sokoban" -> "koulanurag/ma-gym"
"mpSchrader/gym-sokoban" -> "vitchyr/multiworld"
"mpSchrader/gym-sokoban" -> "rll-research/url_benchmark" ["e"=1]
"mpSchrader/gym-sokoban" -> "jannerm/mbpo" ["e"=1]
"mpSchrader/gym-sokoban" -> "google-research/rliable" ["e"=1]
"greydanus/baby-a3c" -> "greydanus/visualize_atari"
"greydanus/baby-a3c" -> "dgriff777/rl_a3c_pytorch"
"TianhongDai/hindsight-experience-replay" -> "mengf1/CHER" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "vitchyr/multiworld"
"TianhongDai/hindsight-experience-replay" -> "hemilpanchiwala/Hindsight-Experience-Replay"
"TianhongDai/hindsight-experience-replay" -> "alirezakazemipour/DDPG-HER"
"TianhongDai/hindsight-experience-replay" -> "mengf1/DHER"
"TianhongDai/hindsight-experience-replay" -> "qgallouedec/panda-gym" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "florensacc/rllab-curriculum" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "TianhongDai/reinforcement-learning-algorithms"
"TianhongDai/hindsight-experience-replay" -> "sfujim/BCQ" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "MishaLaskin/curl" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "facebookresearch/drqv2" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "Kaixhin/PlaNet" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "ikostrikov/jaxrl" ["e"=1]
"TianhongDai/hindsight-experience-replay" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"TianhongDai/hindsight-experience-replay" -> "denisyarats/pytorch_sac" ["e"=1]
"mila-iqia/babyai" -> "Farama-Foundation/Minigrid"
"mila-iqia/babyai" -> "lcswillems/rl-starter-files"
"mila-iqia/babyai" -> "microsoft/TextWorld"
"mila-iqia/babyai" -> "rll-research/url_benchmark" ["e"=1]
"mila-iqia/babyai" -> "askforalfred/alfred" ["e"=1]
"mila-iqia/babyai" -> "Farama-Foundation/Miniworld"
"mila-iqia/babyai" -> "LauraRuis/groundedSCAN" ["e"=1]
"mila-iqia/babyai" -> "google-deepmind/pycolab"
"mila-iqia/babyai" -> "flowersteam/Grounding_LLMs_with_online_RL" ["e"=1]
"mila-iqia/babyai" -> "facebookresearch/torchbeast"
"mila-iqia/babyai" -> "Farama-Foundation/Metaworld" ["e"=1]
"mila-iqia/babyai" -> "astooke/rlpyt"
"mila-iqia/babyai" -> "tkipf/c-swm" ["e"=1]
"mila-iqia/babyai" -> "google-research/clevr_robot_env" ["e"=1]
"mila-iqia/babyai" -> "jacobandreas/psketch" ["e"=1]
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "qqiang00/Reinforce"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "PacktPublishing/Reinforcement-Learning-Algorithms-with-Python"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "ZhiqingXiao/rl-book"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "tinyzqh/awesome-reinforcement-learning"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "gxnk/reinforcement-learning-code"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "simoninithomas/Deep_reinforcement_learning_Course"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "DeepReinforcementLearning/DeepReinforcementLearningInAction"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "andri27-ts/Reinforcement-Learning"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "pythonlessons/Reinforcement_Learning"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "TianhongDai/reinforcement-learning-algorithms"
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/PacNeat"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/2048-AI"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/Chess-AI"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/NEAT_Template" -> "Code-Bullet/PacmanGame"
"Unity-Technologies/marathon-envs" -> "Sohojoe/ActiveRagdollStyleTransfer"
"Unity-Technologies/marathon-envs" -> "mbaske/angry-ai"
"Unity-Technologies/marathon-envs" -> "Sohojoe/ActiveRagdollAssaultCourse"
"Unity-Technologies/marathon-envs" -> "kressdev/RagdollTrainer"
"Unity-Technologies/marathon-envs" -> "joanllobera/marathon-envs"
"Unity-Technologies/marathon-envs" -> "facebookresearch/ScaDiver" ["e"=1]
"Unity-Technologies/marathon-envs" -> "unixpickle/obs-tower2"
"senya-ashukha/quantile-regression-dqn-pytorch" -> "Silvicek/distributional-dqn"
"Code-Bullet/2048-AI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/2048-AI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/2048-AI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/2048-AI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/2048-AI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/2048-AI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/2048-AI" -> "Code-Bullet/NEAT-Template-JavaScript"
"sungyubkim/Deep_RL_with_pytorch" -> "Kchu/DeepRL_PyTorch"
"sungyubkim/Deep_RL_with_pytorch" -> "dannysdeng/dqn-pytorch"
"sungyubkim/Deep_RL_with_pytorch" -> "toshikwa/fqf-iqn-qrdqn.pytorch"
"Zhenye-Na/reinforcement-learning-stanford" -> "Huixxi/CS234-Reinforcement-Learning-Winter-2019"
"Zhenye-Na/reinforcement-learning-stanford" -> "zlpure/CS234"
"Zhenye-Na/reinforcement-learning-stanford" -> "arowdy98/Stanford-CS234"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "Zhenye-Na/reinforcement-learning-stanford"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "zlpure/CS234"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "arowdy98/Stanford-CS234"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "changebo/CS234-2020"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "tallamjr/stanford-cs234"
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" -> "ksang/cs234-assignments"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/2048-AI"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/PacNeat"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/Flappy-Bird-AI"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/Storm-The-House-Auto-Clicker" -> "Code-Bullet/AsteroidsAI"
"semitable/lb-foraging" -> "uoe-agents/epymarl"
"semitable/lb-foraging" -> "semitable/robotic-warehouse"
"semitable/lb-foraging" -> "schroederdewitt/multiagent_mujoco"
"semitable/lb-foraging" -> "ArnaudFickinger/gym-multigrid"
"semitable/lb-foraging" -> "lich14/CDS"
"semitable/lb-foraging" -> "FLAIROx/JaxMARL" ["e"=1]
"semitable/lb-foraging" -> "uoe-agents/lb-foraging"
"RchalYang/torchrl" -> "dongminlee94/deep_rl"
"Sonkyunghwan/QTRAN" -> "AnujMahajanOxf/MAVEN"
"Sonkyunghwan/QTRAN" -> "wjh720/QPLEX"
"Sonkyunghwan/QTRAN" -> "oxwhirl/wqmix"
"openai/EPG" -> "openai/robosumo"
"openai/EPG" -> "openai/vime"
"openai/EPG" -> "openai/retro-baselines"
"xbpeng/DeepLoco" -> "xbpeng/DeepTerrainRL"
"xbpeng/DeepLoco" -> "VincentYu68/SymmetryCurriculumLocomotion"
"Code-Bullet/PacmanGame" -> "Code-Bullet/PacNeat"
"Code-Bullet/PacmanGame" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/PacmanGame" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/PacmanGame" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Chess-AI"
"Code-Bullet/PacmanGame" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/PacmanGame" -> "Code-Bullet/2048-AI"
"Code-Bullet/PacmanGame" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Pool_AI"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/PacmanGame" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"flyyufelix/C51-DDQN-Keras" -> "go2sea/C51DQN"
"flyyufelix/C51-DDQN-Keras" -> "Silvicek/distributional-dqn"
"flyyufelix/C51-DDQN-Keras" -> "floringogianu/categorical-dqn"
"flyyufelix/C51-DDQN-Keras" -> "itaicaspi/keras-dqn-doom"
"rlworkgroup/dowel" -> "rlworkgroup/akro"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/2048-AI"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/PacNeat"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/Flappy-Bird-AI" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"chagmgang/pytorch_ppo_rl" -> "adik993/ppo-pytorch"
"rlcode/per" -> "pranz24/pytorch-soft-actor-critic"
"rlcode/per" -> "alirezamika/evostra" ["e"=1]
"rlcode/per" -> "cardwing/Codes-for-RL-PER"
"rlcode/per" -> "Jonathan-Pearce/DDPG_PER"
"rlcode/per" -> "TianhongDai/hindsight-experience-replay"
"rlcode/per" -> "Damcy/prioritized-experience-replay"
"rlcode/per" -> "ShangtongZhang/DeepRL"
"rlcode/per" -> "ghliu/pytorch-ddpg"
"rlcode/per" -> "shariqiqbal2810/MAAC"
"rlcode/per" -> "qfettes/DeepRL-Tutorials"
"rlcode/per" -> "BY571/Soft-Actor-Critic-and-Extensions"
"rlcode/per" -> "Howuhh/prioritized_experience_replay"
"xiaobaoonline/pytorch-in-action" -> "ZhiqingXiao/pytorch-book"
"crowdAI/marLo" -> "tambetm/gym-minecraft"
"crowdAI/marLo" -> "microsoft/malmo-challenge"
"crowdAI/marLo" -> "minerllabs/minerl"
"crowdAI/marLo" -> "MultiAgentLearning/playground"
"crowdAI/marLo" -> "crowdAI/marlo-multi-agent-starter-kit"
"joyiswu/UCL-Deep-learning-ans-Reinforcement-learning" -> "mikezhang95/ML_Assignment"
"joyiswu/UCL-Deep-learning-ans-Reinforcement-learning" -> "RylanSchaeffer/ucl-adv-dl-rl"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/2048-AI"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/PacNeat"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/Chess-AI"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/minesweeper-AI" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/PacNeat"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/2048-AI"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/Pool_AI"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/2048-AI"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/Flappy-Bird-AI"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/PacNeat"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/Chess-AI"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/WebsiteTest"
"Code-Bullet/NEAT-Template-JavaScript" -> "Code-Bullet/AsteroidsAI"
"jmacglashan/burlap" -> "jmacglashan/burlap_examples"
"jmacglashan/burlap" -> "pushkar/ABAGAIL"
"jmacglashan/burlap" -> "rlpy/rlpy"
"jmacglashan/burlap" -> "chen0040/java-reinforcement-learning"
"jmacglashan/burlap" -> "h2r/burlapcraft"
"jmacglashan/burlap" -> "rlpark/rlpark"
"jmacglashan/burlap" -> "samindaa/RLLib" ["e"=1]
"Silvicek/distributional-dqn" -> "senya-ashukha/quantile-regression-dqn-pytorch"
"Silvicek/distributional-dqn" -> "flyyufelix/C51-DDQN-Keras"
"Silvicek/distributional-dqn" -> "go2sea/C51DQN"
"Silvicek/distributional-dqn" -> "floringogianu/categorical-dqn"
"openai/retro-baselines" -> "openai/retro-contest"
"openai/retro-baselines" -> "openai/sonic-on-ray"
"openai/retro-baselines" -> "unixpickle/anyrl-py"
"pathak22/zeroshot-imitation" -> "junhyukoh/self-imitation-learning"
"pathak22/zeroshot-imitation" -> "tianheyu927/mil"
"pathak22/zeroshot-imitation" -> "pathak22/exploration-by-disagreement"
"adik993/ppo-pytorch" -> "chagmgang/pytorch_ppo_rl"
"adik993/ppo-pytorch" -> "jcwleo/curiosity-driven-exploration-pytorch"
"pathak22/modular-assemblies" -> "pathak22/exploration-by-disagreement"
"pathak22/modular-assemblies" -> "facebookresearch/adversarially-motivated-intrinsic-goals"
"pathak22/modular-assemblies" -> "jhejna/hierarchical_morphology_transfer"
"mrahtz/learning-from-human-preferences" -> "nottombrown/rl-teacher"
"mrahtz/learning-from-human-preferences" -> "csmile-1006/PreferenceTransformer" ["e"=1]
"mrahtz/learning-from-human-preferences" -> "HumanCompatibleAI/learning-from-human-preferences"
"mrahtz/learning-from-human-preferences" -> "ZachisGit/LearningFromHumanPreferences"
"mrahtz/learning-from-human-preferences" -> "machine-intelligence/rl-teacher-atari"
"mrahtz/learning-from-human-preferences" -> "rddy/ReQueST"
"mrahtz/learning-from-human-preferences" -> "rll-research/BPref" ["e"=1]
"mrahtz/learning-from-human-preferences" -> "mrahtz/easy-tf-log"
"mrahtz/learning-from-human-preferences" -> "malayandi/DemPrefCode"
"mrahtz/learning-from-human-preferences" -> "mrahtz/gym-moving-dot"
"medipixel/rl_algorithms" -> "MrSyee/pg-is-all-you-need"
"medipixel/rl_algorithms" -> "cyoon1729/RLcycle"
"medipixel/rl_algorithms" -> "opherlieber/rltime"
"medipixel/rl_algorithms" -> "Curt-Park/rainbow-is-all-you-need"
"medipixel/rl_algorithms" -> "cyoon1729/distributedRL"
"medipixel/rl_algorithms" -> "kairproject/kair_algorithms_draft"
"medipixel/rl_algorithms" -> "openai/random-network-distillation"
"medipixel/rl_algorithms" -> "astooke/rlpyt"
"medipixel/rl_algorithms" -> "j-marple-dev/model_compression" ["e"=1]
"medipixel/rl_algorithms" -> "qfettes/DeepRL-Tutorials"
"medipixel/rl_algorithms" -> "reinforcement-learning-kr/rl_bootcamp" ["e"=1]
"medipixel/rl_algorithms" -> "rlgraph/rlgraph"
"medipixel/rl_algorithms" -> "kakaoenterprise/JORLDY" ["e"=1]
"medipixel/rl_algorithms" -> "dongminlee94/deep_rl"
"medipixel/rl_algorithms" -> "unixpickle/anyrl-py"
"pathak22/exploration-by-disagreement" -> "pathak22/modular-assemblies"
"pathak22/exploration-by-disagreement" -> "aravindsrinivas/upn"
"donadigo/TMTrackNN" -> "donadigo/pygbx"
"hzm2016/Peg_in_hole_assembly" -> "DengYuelin/vrep_peg_in_hole"
"hzm2016/Peg_in_hole_assembly" -> "robot0102/rl-peg-in-hole-assembly-webots"
"hzm2016/Peg_in_hole_assembly" -> "jeongeun980906/peg-in-hole"
"SongleChen2015/Peg_in_hole_assembly" -> "robot0102/rl-peg-in-hole-assembly-webots"
"Junzhuodu/ur5_vrep_python" -> "zanebin/UR5-manipulator-control"
"Ianlande/Vrep_yolov3_ddpg_pytorch" -> "zanebin/UR5-manipulator-control"
"Ianlande/Vrep_yolov3_ddpg_pytorch" -> "alishbaimran/Robotics-DDPG-HER"
"DengYuelin/vrep_peg_in_hole" -> "jeongeun980906/peg-in-hole"
"DengYuelin/vrep_peg_in_hole" -> "robot0102/rl-peg-in-hole-assembly-webots"
"DengYuelin/vrep_peg_in_hole" -> "hzm2016/Peg_in_hole_assembly"
"DengYuelin/vrep_peg_in_hole" -> "pengzou1/RL_PEG_IN_HOLE"
"zanebin/UR5-manipulator-control" -> "Junzhuodu/ur5_vrep_python"
"mtrazzi/two-step-task" -> "mtrazzi/harlow"
"ml3705454/mapr2" -> "rommeoijcai2019/rommeo"
"unixpickle/obs-tower2" -> "Unity-Technologies/obstacle-tower-source"
"unixpickle/obs-tower2" -> "Unity-Technologies/obstacle-tower-challenge"
"unixpickle/obs-tower2" -> "Sohojoe/ppo-dash"
"unixpickle/obs-tower2" -> "Unity-Technologies/obstacle-tower-env"
"tegg89/magnet" -> "PKU-RL/DGN"
"kashif/firedup" -> "Kaixhin/spinning-up-basic"
"kashif/firedup" -> "MishaLaskin/torchingup"
"Kaixhin/spinning-up-basic" -> "kashif/firedup"
"Kaixhin/spinning-up-basic" -> "MishaLaskin/torchingup"
"MishaLaskin/torchingup" -> "kashif/firedup"
"MishaLaskin/torchingup" -> "Kaixhin/spinning-up-basic"
"zhuang0/BoYaDDZ" -> "foreverimps/doudizhu"
"diplomacy/research" -> "diplomacy/diplomacy"
"diplomacy/research" -> "rowatc/Diplomacy-AI"
"diplomacy/research" -> "google-deepmind/diplomacy"
"diplomacy/research" -> "facebookresearch/diplomacy_searchbot"
"llSourcell/deep_q_learning" -> "ppaquette/gym-super-mario"
"llSourcell/deep_q_learning" -> "llSourcell/q_learning_demo"
"facebookresearch/craftassist" -> "facebookresearch/fairo" ["e"=1]
"facebookresearch/craftassist" -> "minerllabs/baselines"
"facebookresearch/craftassist" -> "minerllabs/minerl"
"facebookresearch/craftassist" -> "microsoft/task_oriented_dialogue_as_dataflow_synthesis" ["e"=1]
"facebookresearch/craftassist" -> "crowdAI/marLo"
"facebookresearch/craftassist" -> "Tigermouthbear/Theia" ["e"=1]
"facebookresearch/craftassist" -> "microsoft/malmo"
"facebookresearch/craftassist" -> "kami-blue/client" ["e"=1]
"nuno-faria/tetris-ai" -> "vietnh1009/Tetris-deep-Q-learning-pytorch"
"nuno-faria/tetris-ai" -> "Kautenja/gym-tetris"
"nuno-faria/tetris-ai" -> "LeeYiyuan/tetrisai"
"nuno-faria/tetris-ai" -> "michiel-cox/Tetris-DQN"
"nuno-faria/tetris-ai" -> "greerviau/TetrisAI"
"nuno-faria/tetris-ai" -> "jaybutera/tetrisRL"
"nuno-faria/tetris-ai" -> "fthomasmorel/Tetris-AI"
"nuno-faria/tetris-ai" -> "hrpan/tetris_mcts" ["e"=1]
"diegoalejogm/Reinforcement-Learning" -> "mharbuz/rlbook-exercises"
"diegoalejogm/Reinforcement-Learning" -> "iamhectorotero/rlai-exercises"
"diegoalejogm/Reinforcement-Learning" -> "JKCooper2/rlai-exercises"
"diegoalejogm/Reinforcement-Learning" -> "brynhayder/reinforcement_learning_an_introduction"
"diegoalejogm/Reinforcement-Learning" -> "diegoalejogm/deep-q-learning"
"zhuliquan/reinforcement_learning_basic_book" -> "gxnk/reinforcement-learning-code"
"zhuliquan/reinforcement_learning_basic_book" -> "applenob/rl_learn"
"zhuliquan/reinforcement_learning_basic_book" -> "apachecn/stanford-cs234-notes-zh"
"brynhayder/reinforcement_learning_an_introduction" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"brynhayder/reinforcement_learning_an_introduction" -> "vojtamolda/reinforcement-learning-an-introduction"
"brynhayder/reinforcement_learning_an_introduction" -> "iamhectorotero/rlai-exercises"
"brynhayder/reinforcement_learning_an_introduction" -> "diegoalejogm/Reinforcement-Learning"
"brynhayder/reinforcement_learning_an_introduction" -> "mharbuz/rlbook-exercises"
"brynhayder/reinforcement_learning_an_introduction" -> "mtrazzi/rl-book-challenge" ["e"=1]
"brynhayder/reinforcement_learning_an_introduction" -> "habanoz/reinforcement-learning-an-introduction"
"brynhayder/reinforcement_learning_an_introduction" -> "JKCooper2/rlai-exercises"
"rajammanabrolu/KG-DQN" -> "rajammanabrolu/KG-A2C"
"rajammanabrolu/KG-DQN" -> "xingdi-eric-yuan/GATA-public"
"rajammanabrolu/KG-DQN" -> "microsoft/tdqn"
"cyoon1729/deep-Q-networks" -> "cyoon1729/Policy-Gradient-Methods"
"cyoon1729/deep-Q-networks" -> "cyoon1729/RLcycle"
"cyoon1729/deep-Q-networks" -> "dxyang/DQN_pytorch"
"cyoon1729/deep-Q-networks" -> "BY571/DQN-Atari-Agents"
"cyoon1729/deep-Q-networks" -> "sherjilozair/dqn"
"cyoon1729/deep-Q-networks" -> "deligentfool/dqn_zoo"
"neka-nat/inv_rl" -> "aravindsiv/irl-lab"
"Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning" -> "chagmgang/tf2.0_reinforcement_learning"
"Kautenja/gym-tetris" -> "lusob/gym-tetris"
"diplomacy/diplomacy" -> "diplomacy/research"
"diplomacy/diplomacy" -> "google-deepmind/diplomacy"
"diplomacy/diplomacy" -> "facebookresearch/diplomacy_searchbot"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/2048-AI"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/PacNeat"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/WebsiteTest"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Piano-Tiles" -> "Code-Bullet/Flappy-Bird-AI"
"brianspiering/awesome-deep-rl" -> "PacktPublishing/Python-Reinforcement-Learning-Projects"
"brianspiering/awesome-deep-rl" -> "ugurkanates/awesome-real-world-rl"
"brianspiering/awesome-deep-rl" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"brianspiering/awesome-deep-rl" -> "brianspiering/rl-course" ["e"=1]
"nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2" -> "createamind/DRL"
"geyang/ml-logger" -> "justinjfu/doodad"
"RylanSchaeffer/ucl-adv-dl-rl" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"RylanSchaeffer/ucl-adv-dl-rl" -> "mikezhang95/ML_Assignment"
"RylanSchaeffer/ucl-adv-dl-rl" -> "YidingYu/UCL-DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning"
"ml-jku/baselines-rudder" -> "openai/large-scale-curiosity"
"lcswillems/torch-ac" -> "lcswillems/rl-starter-files"
"Spenhouet/tensorboard-aggregator" -> "janosh/tensorboard-reducer"
"JonathanTay/CS-7641-assignment-1" -> "JonathanTay/CS-7641-assignment-3"
"JonathanTay/CS-7641-assignment-1" -> "cmaron/CS-7641-assignments"
"JonathanTay/CS-7641-assignment-1" -> "eternalmothra/ml_cheat_sheet"
"JonathanTay/CS-7641-assignment-1" -> "JonathanTay/CS-7641-assignment-4"
"JonathanTay/CS-7641-assignment-1" -> "juanjose49/omscs-cs7641-machine-learning-assignment-4"
"JonathanTay/CS-7641-assignment-1" -> "JonathanTay/CS-7641-assignment-2"
"JonathanTay/CS-7641-assignment-1" -> "mohamedameen93/CS-7641-Machine-Learning-Notes"
"JonathanTay/CS-7641-assignment-3" -> "JonathanTay/CS-7641-assignment-4"
"JonathanTay/CS-7641-assignment-4" -> "JonathanTay/CS-7641-assignment-3"
"alshedivat/lola" -> "alexis-jacq/LOLA_DiCE"
"alshedivat/lola" -> "hhexiy/opponent"
"stacyste/TheoryOfMindInferenceModels" -> "clbaker/BToM"
"stacyste/TheoryOfMindInferenceModels" -> "julianje/Bishop"
"stacyste/TheoryOfMindInferenceModels" -> "CILAB-MA/Machine_ToM"
"mbaske/ml-drone-collection" -> "mbaske/ml-explorer-drone"
"mbaske/ml-drone-collection" -> "pochl/dqn-uav" ["e"=1]
"mbaske/ml-drone-collection" -> "dracolytch/ML-Simplest-Scenario"
"vivek3141/streamer-ai" -> "vivek3141/learn-c"
"vivek3141/streamer-ai" -> "vivek3141/learn-js"
"vivek3141/streamer-ai" -> "vivek3141/tictactoe-minimax"
"vivek3141/streamer-ai" -> "vivek3141/CommunicationForDeaf"
"vivek3141/streamer-ai" -> "vivek3141/flappy-neat"
"LeeYiyuan/tetrisai" -> "IdreesInc/TetNet"
"LeeYiyuan/tetrisai" -> "nuno-faria/tetris-ai"
"Unity-Technologies/obstacle-tower-challenge" -> "Unity-Technologies/obstacle-tower-env"
"Unity-Technologies/obstacle-tower-challenge" -> "unixpickle/obs-tower2"
"Unity-Technologies/obstacle-tower-challenge" -> "Unity-Technologies/obstacle-tower-source"
"minerllabs/baselines" -> "minerllabs/competition_submission_template"
"minerllabs/baselines" -> "minerllabs/minerl"
"minerllabs/baselines" -> "manuelsh/minerl-docker"
"minerllabs/baselines" -> "amiranas/minerl_imitation_learning"
"minerllabs/baselines" -> "kimbring2/minecraft_ai"
"minerllabs/baselines" -> "MichalOp/MineRL2020"
"fg91/Deep-Q-Learning" -> "AdrianHsu/breakout-Deep-Q-Network"
"fg91/Deep-Q-Learning" -> "sebtheiler/tutorials"
"TianhongDai/distributed-ppo" -> "alexis-jacq/Pytorch-DPPO"
"openai/retro-contest" -> "openai/retro-baselines"
"openai/retro-contest" -> "openai/sonic-on-ray"
"thanard/causal-infogan" -> "thanard/hallucinative-topological-memory"
"thanard/causal-infogan" -> "aravindsrinivas/upn"
"yfzhang/vehicle-motion-forecasting" -> "ganlumomo/minicheetah-traversability-irl"
"xuwd11/cs294-112_hws" -> "louaaron/CS294_homework"
"xuwd11/cs294-112_hws" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"apachecn/stanford-cs234-notes-zh" -> "apachecn/ucb-cs294-112-notes-zh"
"apachecn/stanford-cs234-notes-zh" -> "applenob/rl_learn"
"apachecn/stanford-cs234-notes-zh" -> "apachecn/stanford-cs224n-notes-zh" ["e"=1]
"apachecn/stanford-cs234-notes-zh" -> "zlpure/CS234"
"JonathanTay/CS-7641-assignment-2" -> "JonathanTay/CS-7641-assignment-3"
"JonathanTay/CS-7641-assignment-2" -> "JonathanTay/CS-7641-assignment-4"
"zuoxingdong/lagom" -> "MillionIntegrals/vel"
"zuoxingdong/lagom" -> "vitchyr/multiworld"
"zuoxingdong/lagom" -> "zuoxingdong/mazelab"
"zuoxingdong/lagom" -> "ml-jku/baselines-rudder"
"EvoEsports/EvoSC" -> "PyPlanet/PyPlanet"
"EvoEsports/EvoSC" -> "reaby/TMModeTemplate"
"EvoEsports/EvoSC" -> "ManiaControl/ManiaControl"
"EvoEsports/EvoSC" -> "snixtho/clifga"
"EvoEsports/EvoSC" -> "EvoEsports/gbxclient-node"
"EvoEsports/EvoSC" -> "EvoEsports/EvoSC-sharp"
"keiohta/tf2rl" -> "archsyscall/DeepRL-TensorFlow2"
"keiohta/tf2rl" -> "anita-hu/TF2-RL"
"keiohta/tf2rl" -> "StepNeverStop/RLs"
"keiohta/tf2rl" -> "ymd-h/cpprb" ["e"=1]
"keiohta/tf2rl" -> "reinforcement-learning-kr/lets-do-irl"
"keiohta/tf2rl" -> "RITCHIEHuang/DeepRL_Algorithms"
"keiohta/tf2rl" -> "inoryy/tensorflow2-deep-reinforcement-learning"
"keiohta/tf2rl" -> "tensorflow/agents"
"keiohta/tf2rl" -> "astooke/rlpyt"
"keiohta/tf2rl" -> "WilsonWangTHU/mbbl" ["e"=1]
"keiohta/tf2rl" -> "araffin/rl-baselines-zoo"
"keiohta/tf2rl" -> "Kaixhin/imitation-learning"
"keiohta/tf2rl" -> "DeepX-inc/machina" ["e"=1]
"keiohta/tf2rl" -> "Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning"
"keiohta/tf2rl" -> "tensorlayer/RLzoo" ["e"=1]
"hiwonjoon/ICML2019-TREX" -> "dsbrown1331/CoRL2019-DREX"
"hiwonjoon/ICML2019-TREX" -> "kristery/Imitation-Learning-from-Imperfect-Demonstration"
"dxyang/DQN_pytorch" -> "hungtuchen/pytorch-dqn"
"dxyang/DQN_pytorch" -> "cyoon1729/deep-Q-networks"
"dxyang/DQN_pytorch" -> "google-deepmind/dqn"
"dxyang/DQN_pytorch" -> "devsisters/DQN-tensorflow"
"dxyang/DQN_pytorch" -> "higgsfield/RL-Adventure"
"dxyang/DQN_pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"dxyang/DQN_pytorch" -> "ghliu/pytorch-ddpg"
"dxyang/DQN_pytorch" -> "Parsa33033/Deep-Reinforcement-Learning-DQN"
"dxyang/DQN_pytorch" -> "ShangtongZhang/DeepRL"
"dxyang/DQN_pytorch" -> "gouxiangchen/dueling-DQN-pytorch"
"dxyang/DQN_pytorch" -> "starry-sky6688/MADDPG"
"dxyang/DQN_pytorch" -> "ChenglongChen/pytorch-DRL"
"dxyang/DQN_pytorch" -> "sherjilozair/dqn"
"dxyang/DQN_pytorch" -> "ikostrikov/pytorch-a3c"
"dxyang/DQN_pytorch" -> "rlcode/per"
"gkhayes/mlrose" -> "hiive/mlrose"
"gkhayes/mlrose" -> "cmaron/CS-7641-assignments"
"gkhayes/mlrose" -> "hiive/hiivemdptoolbox"
"gkhayes/mlrose" -> "pushkar/ABAGAIL"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "andrew-j-levy/Hierarchical-Actor-Critc-HAC-"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "AboudyKreidieh/h-baselines"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "watakandai/hiro_pytorch"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "lweitkamp/option-critic-pytorch"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "skumar9876/Hierarchical-DQN"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "martius-lab/HiTS"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "jannerm/mbpo" ["e"=1]
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "011235813/hierarchical-marl"
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" -> "lweitkamp/feudalnets-pytorch"
"manaski/MARL" -> "thesouther/MARL"
"onestraw/doudizhu" -> "thuxugang/doudizhu"
"onestraw/doudizhu" -> "mailgyc/doudizhu"
"onestraw/doudizhu" -> "ZhouWeikuan/DouDiZhu"
"onestraw/doudizhu" -> "qq456cvb/doudizhu-C"
"onestraw/doudizhu" -> "zhozhou/DouDizhuAI"
"onestraw/doudizhu" -> "songbaoming/DouDiZhu"
"rcsoccersim/manual" -> "Cyrus2D/Pyrus-SS2D-Base"
"rcsoccersim/manual" -> "rcsoccersim/rcssmonitor"
"Code-Bullet/WebsiteTest" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/WebsiteTest" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/WebsiteTest" -> "Code-Bullet/2048-AI"
"PacktPublishing/Python-Reinforcement-Learning-Projects" -> "brianspiering/awesome-deep-rl"
"PacktPublishing/Python-Reinforcement-Learning-Projects" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"PacktPublishing/Python-Reinforcement-Learning-Projects" -> "PacktPublishing/Keras-Reinforcement-Learning-Projects"
"PacktPublishing/Python-Reinforcement-Learning-Projects" -> "ugurkanates/awesome-real-world-rl"
"Uglemat/MaTris" -> "lusob/gym-tetris"
"BazkieBumpercar/GameplayFootball" -> "BazkieBumpercar/Blunted2"
"BazkieBumpercar/GameplayFootball" -> "vi3itor/GameplayFootball"
"BazkieBumpercar/GameplayFootball" -> "google-research/football"
"BazkieBumpercar/GameplayFootball" -> "OussemaMaatouk/Unity_FootBall"
"MillionIntegrals/vel" -> "zuoxingdong/lagom"
"Code-Bullet/PacNeat" -> "Code-Bullet/PacmanGame"
"Code-Bullet/PacNeat" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/PacNeat" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/PacNeat" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/PacNeat" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/PacNeat" -> "Code-Bullet/2048-AI"
"Code-Bullet/PacNeat" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/PacNeat" -> "Code-Bullet/Chess-AI"
"Code-Bullet/PacNeat" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/PacNeat" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/PacNeat" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/PacNeat" -> "Code-Bullet/Piano-Tiles"
"Code-Bullet/PacNeat" -> "Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial"
"Code-Bullet/PacNeat" -> "Code-Bullet/AsteroidsAI"
"Code-Bullet/PacNeat" -> "Code-Bullet/NEAT-Template-JavaScript"
"uidilr/ppo_tf" -> "takuseno/ppo"
"idthanm/env_build" -> "idthanm/mpg"
"idthanm/env_build" -> "mahaitongdae/safe_exp_env"
"Kchu/DeepRL_PyTorch" -> "toshikwa/fqf-iqn-qrdqn.pytorch"
"Kchu/DeepRL_PyTorch" -> "sungyubkim/Deep_RL_with_pytorch"
"Kchu/DeepRL_PyTorch" -> "Kchu/LifelongRL"
"Kchu/DeepRL_PyTorch" -> "BY571/IQN-and-Extensions"
"ZhiqingXiao/pytorch-book" -> "ZhiqingXiao/rl-book"
"vitchyr/viskit" -> "justinjfu/doodad"
"ucla-rlcourse/DeepRL-Tutorials" -> "ucla-rlcourse/RLexample"
"ucla-rlcourse/DeepRL-Tutorials" -> "cuhkrlcourse/ierg6130-assignment"
"ucla-rlcourse/DeepRL-Tutorials" -> "xmfbit/DQN-FlappyBird"
"ucla-rlcourse/DeepRL-Tutorials" -> "zhoubolei/introRL"
"ucla-rlcourse/DeepRL-Tutorials" -> "sfujim/TD3"
"ucla-rlcourse/DeepRL-Tutorials" -> "zhangchuheng123/Reinforcement-Implementation"
"ucla-rlcourse/DeepRL-Tutorials" -> "qfettes/DeepRL-Tutorials"
"ermongroup/InfoGAIL" -> "sen-pai/InfoGAIL"
"gmargo11/hDQN" -> "hungtuchen/pytorch-hdqn"
"alversafa/option-critic-arch" -> "mklissa/PPOC"
"alversafa/option-critic-arch" -> "hzm2016/option-critic-pytorch"
"uber-research/ape-x" -> "younggyoseo/Ape-X"
"uber-research/ape-x" -> "jingweiz/pytorch-distributed"
"uber-research/ape-x" -> "google-deepmind/scalable_agent"
"uber-research/ape-x" -> "unixpickle/anyrl-py"
"GallagherAiden/footballSimulationEngine" -> "atas76/openengine"
"GallagherAiden/footballSimulationEngine" -> "ZOXEXIVO/open-football"
"GallagherAiden/footballSimulationEngine" -> "ElliotJBall/FootballManagerSimulator"
"GallagherAiden/footballSimulationEngine" -> "ihofmann/open-websoccer"
"applenob/rl_learn" -> "zhuliquan/reinforcement_learning_basic_book"
"applenob/rl_learn" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"applenob/rl_learn" -> "apachecn/stanford-cs234-notes-zh"
"applenob/rl_learn" -> "zhoubolei/introRL"
"applenob/rl_learn" -> "bat67/awesome-deep-learning-and-machine-learning-questions" ["e"=1]
"applenob/rl_learn" -> "mengwanglalala/RL-algorithms" ["e"=1]
"applenob/rl_learn" -> "manaski/MARL"
"Breakend/DeepReinforcementLearningThatMatters" -> "junhyukoh/value-prediction-network"
"Breakend/DeepReinforcementLearningThatMatters" -> "rlbayes/rllabplusplus"
"MG2033/A2C" -> "pemami4911/deep-rl"
"MG2033/A2C" -> "miyosuda/async_deep_reinforce"
"rl-cn/rl-cn" -> "dbsxdbsx/rl-intro-book-chinese"
"mbaske/robot-ants" -> "mbaske/ml-explorer-drone"
"mbaske/robot-ants" -> "mbaske/angry-ai"
"mbaske/robot-ants" -> "mbaske/ml-table-football"
"mbaske/ml-explorer-drone" -> "mbaske/ml-table-football"
"mbaske/ml-explorer-drone" -> "mbaske/ml-eve"
"mbaske/ml-explorer-drone" -> "mbaske/ml-audio-sensor"
"mihdalal/sawyer_control" -> "rlworkgroup/gym-sawyer"
"uidilr/gail_ppo_tf" -> "andrewliao11/gail-tf"
"uidilr/gail_ppo_tf" -> "navuboy/gail_gym"
"uidilr/gail_ppo_tf" -> "YunzhuLi/InfoGAIL"
"uidilr/gail_ppo_tf" -> "ahq1993/inverse_rl"
"uidilr/gail_ppo_tf" -> "sisl/hgail"
"vivek3141/super-mario-rl" -> "vivek3141/flappy-neat"
"vivek3141/super-mario-rl" -> "vivek3141/learn-c"
"vivek3141/super-mario-rl" -> "vivek3141/learn-js"
"vivek3141/super-mario-rl" -> "vivek3141/snake-gym"
"vivek3141/super-mario-rl" -> "vivek3141/vector-field-visualizer"
"vivek3141/super-mario-rl" -> "vivek3141/forest-fire-predictor"
"Unity-Technologies/obstacle-tower-source" -> "unixpickle/obs-tower2"
"cyoon1729/Policy-Gradient-Methods" -> "cyoon1729/RLcycle"
"zlpure/CS234" -> "lisidi/SLPlayer" ["e"=1]
"Observerspy/CS294" -> "Observerspy/CS234"
"Observerspy/CS294" -> "xuwd11/cs294-112_hws"
"Observerspy/CS294" -> "louaaron/CS294_homework"
"kanakkabara/Autonomous-Drifting" -> "caipeide/drift_drl"
"kanakkabara/Autonomous-Drifting" -> "harvitronix/reinforcement-learning-car"
"kanakkabara/Autonomous-Drifting" -> "angloth/auto-drift"
"jcwleo/curiosity-driven-exploration-pytorch" -> "jcwleo/random-network-distillation-pytorch"
"jcwleo/curiosity-driven-exploration-pytorch" -> "chagmgang/pytorch_ppo_rl"
"jcwleo/curiosity-driven-exploration-pytorch" -> "adik993/ppo-pytorch"
"Code-Bullet/MarbleCalculator" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/MarbleCalculator" -> "Code-Bullet/Asteroids-with-NEAT"
"Code-Bullet/MarbleCalculator" -> "Code-Bullet/2048-AI"
"Code-Bullet/MarbleCalculator" -> "Code-Bullet/PacNeat"
"Code-Bullet/MarbleCalculator" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/MarbleCalculator" -> "Code-Bullet/Chess-AI"
"Code-Bullet/MarbleCalculator" -> "Code-Bullet/NEAT_Template"
"neka-nat/distributed_rl" -> "jinbeizame007/pytorch-r2d2-DPG"
"neka-nat/distributed_rl" -> "cyoon1729/distributedRL"
"naderzare/CYRUS2014" -> "Cyrus2D/Pyrus-SS2D-Base"
"openai/ceph-chef" -> "openai/post--example"
"seungjaeryanlee/awesome-rl-competitions" -> "ugurkanates/awesome-real-world-rl"
"seungjaeryanlee/awesome-rl-competitions" -> "minerllabs/baselines"
"seungjaeryanlee/awesome-rl-competitions" -> "minerllabs/competition_submission_template"
"seungjaeryanlee/awesome-rl-competitions" -> "google-research/batch_rl" ["e"=1]
"seungjaeryanlee/awesome-rl-competitions" -> "MrSyee/pg-is-all-you-need"
"seungjaeryanlee/awesome-rl-competitions" -> "clvrai/awesome-rl-envs"
"seungjaeryanlee/awesome-rl-competitions" -> "Farama-Foundation/Miniworld"
"microsoft/MazeExplorer" -> "agiantwhale/NavDoom"
"younggyoseo/Ape-X" -> "jingweiz/pytorch-distributed"
"younggyoseo/Ape-X" -> "uber-research/ape-x"
"younggyoseo/Ape-X" -> "cyoon1729/distributedRL"
"younggyoseo/Ape-X" -> "praveen-palanisamy/Ape-X-DQN"
"younggyoseo/Ape-X" -> "haje01/distper"
"ying-wen/malib_deprecated" -> "ml3705454/mapr2"
"jangirrishabh/Overcoming-exploration-from-demos" -> "jangirrishabh/HER-learn-InverseKinematics"
"jangirrishabh/Overcoming-exploration-from-demos" -> "cychai1995/DDPGfD"
"vivek3141/sql-injection-demo" -> "vivek3141/learn-js"
"AdrianHsu/breakout-Deep-Q-Network" -> "ShanHaoYu/Deep-Q-Network-Breakout"
"AdrianHsu/breakout-Deep-Q-Network" -> "wetliu/dqn_pytorch"
"rowatc/Diplomacy-AI" -> "diplomacy/research"
"createamind/DRL" -> "createamind/Distributed-DRL"
"mcmachado/count_based_exploration_sr" -> "bonniesjli/DQN_SR"
"jingweiz/pytorch-distributed" -> "younggyoseo/Ape-X"
"jingweiz/pytorch-distributed" -> "cyoon1729/distributedRL"
"vivek3141/vector-field-visualizer" -> "vivek3141/flappy-neat"
"vivek3141/vector-field-visualizer" -> "vivek3141/learn-c"
"vivek3141/vector-field-visualizer" -> "vivek3141/learn-js"
"vivek3141/vector-field-visualizer" -> "vivek3141/tictactoe-minimax"
"vivek3141/vector-field-visualizer" -> "vivek3141/ml"
"fry404006308/-Learning-materials-" -> "fry404006308/IT_book"
"mklissa/PPOC" -> "kkhetarpal/ioc"
"mikezhang95/ML_Assignment" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"mikezhang95/ML_Assignment" -> "RylanSchaeffer/ucl-adv-dl-rl"
"wisnunugroho21/reinforcement_learning_ppo_rnd" -> "alirezakazemipour/PPO-RND"
"ElliotJBall/FootballManagerSimulator" -> "ZOXEXIVO/open-football"
"ElliotJBall/FootballManagerSimulator" -> "AllenThomasDev/Football-Simulator"
"mehdiboubnan/Deep-Reinforcement-Learning-applied-to-DOOM" -> "akolishchak/doom-net-pytorch"
"mehdiboubnan/Deep-Reinforcement-Learning-applied-to-DOOM" -> "sadeqa/Super-Mario-Bros-RL"
"mehdiboubnan/Deep-Reinforcement-Learning-applied-to-DOOM" -> "shakenes/vizdoomgym"
"Coac/CommNet-BiCnet" -> "KornbergFresnel/CommNet"
"johannah/bootstrap_dqn" -> "JoungheeKim/bootsrapped-dqn"
"vivek3141/rubiks-cube-ai" -> "vivek3141/flappy-neat"
"rlworkgroup/gym-sawyer" -> "mihdalal/sawyer_control"
"rlworkgroup/gym-sawyer" -> "rlworkgroup/akro"
"jeanharb/a2oc_delib" -> "kkhetarpal/ioc"
"jeanharb/a2oc_delib" -> "ronsailer/A2OC_A2C"
"jeanharb/a2oc_delib" -> "mklissa/PPOC"
"HumanCompatibleAI/overcooked-demo" -> "HumanCompatibleAI/human_aware_rl"
"HumanCompatibleAI/overcooked-demo" -> "HumanCompatibleAI/overcooked-hAI-exp"
"hsvgbkhgbv/SQDDPG" -> "hsvgbkhgbv/shapley-q-learning"
"hsvgbkhgbv/SQDDPG" -> "AnujMahajanOxf/MAVEN"
"ahq1993/inverse_rl" -> "justinjfu/inverse_rl"
"ahq1993/inverse_rl" -> "ermongroup/MetaIRL"
"ahq1993/inverse_rl" -> "jangirrishabh/toyCarIRL"
"ahq1993/inverse_rl" -> "seolhokim/InverseRL-Pytorch"
"ahq1993/inverse_rl" -> "HumanCompatibleAI/population-irl"
"ahq1993/inverse_rl" -> "ermongroup/MA-AIRL"
"tesslerc/ActionRobustRL" -> "huanzhang12/ATLA_robust_RL"
"tesslerc/ActionRobustRL" -> "tuomaso/radial_rl_v2"
"navuboy/gail_gym" -> "uidilr/gail_ppo_tf"
"navuboy/gail_gym" -> "andrewliao11/gail-tf"
"navuboy/gail_gym" -> "nikhilbarhate99/Deterministic-GAIL-PyTorch"
"navuboy/gail_gym" -> "jatinarora2702/gail-pytorch"
"zoli333/Weight-Normalization" -> "victorcampos7/weightnorm-init"
"Kautenja/playing-mario-with-deep-reinforcement-learning" -> "robmsylvester/Super-Mario-Bros-DQN"
"atas76/openengine" -> "GallagherAiden/footballSimulationEngine"
"atas76/openengine" -> "ZOXEXIVO/open-football"
"Sohojoe/ActiveRagdollControllers" -> "Sohojoe/ActiveRagdollAssaultCourse"
"Sohojoe/ActiveRagdollAssaultCourse" -> "Sohojoe/ActiveRagdollControllers"
"kristery/Imitation-Learning-from-Imperfect-Demonstration" -> "dsbrown1331/CoRL2019-DREX"
"kristery/Imitation-Learning-from-Imperfect-Demonstration" -> "CORE-Robotics-Lab/SSRR"
"mtrazzi/harlow" -> "mtrazzi/two-step-task"
"pawan47/Fetchreach_gym" -> "alishbaimran/Robotics-DDPG-HER"
"alishbaimran/Robotics-DDPG-HER" -> "pawan47/Fetchreach_gym"
"tambetm/pommerman-baselines" -> "rwightman/pytorch-pommerman-rl"
"tambetm/pommerman-baselines" -> "BorealisAI/pommerman-baseline"
"tambetm/pommerman-baselines" -> "eugene/pommerman"
"tambetm/pommerman-baselines" -> "papkov/pommerman-x"
"tambetm/pommerman-baselines" -> "dist1ll/pomcpp"
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" -> "joyiswu/UCL-Deep-learning-ans-Reinforcement-learning"
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" -> "YidingYu/UCL-DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning"
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" -> "mikezhang95/ML_Assignment"
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" -> "RylanSchaeffer/ucl-adv-dl-rl"
"TheAILearner/Snake-Game-with-Deep-learning" -> "TheAILearner/Training-Snake-Game-With-Genetic-Algorithm"
"mbaske/angry-ai" -> "mbaske/ml-explorer-drone"
"mbaske/angry-ai" -> "mbaske/robot-ants"
"mbaske/angry-ai" -> "mbaske/ml-table-football"
"mbaske/angry-ai" -> "mbaske/grid-sensor"
"vivek3141/forest-fire-predictor" -> "vivek3141/learn-js"
"vivek3141/forest-fire-predictor" -> "vivek3141/learn-c"
"mwydmuch/PyOblige" -> "crowdAI/vizdoom2018-singleplayer-starter-kit"
"ShanHaoYu/Deep-Q-Network-Breakout" -> "yunjhongwu/Double-DQN-Breakout"
"mohitsharma0690/DirectedInfo-GAIL" -> "sharma-arjun/DirectedInfo-GAIL"
"rwightman/pytorch-pommerman-rl" -> "BorealisAI/pommerman-baseline"
"rwightman/pytorch-pommerman-rl" -> "tambetm/pommerman-baselines"
"rwightman/pytorch-pommerman-rl" -> "eugene/pommerman"
"agiantwhale/NavDoom" -> "microsoft/MazeExplorer"
"agiantwhale/NavDoom" -> "crowdAI/vizdoom2018-singleplayer-starter-kit"
"Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing" -> "Code-Bullet/MarbleCalculator"
"crowdAI/vizdoom2018-singleplayer-starter-kit" -> "mwydmuch/PyOblige"
"crowdAI/vizdoom2018-singleplayer-starter-kit" -> "crowdAI/vizdoom2018-multiplayer-starter-kit"
"crowdAI/vizdoom2018-singleplayer-starter-kit" -> "agiantwhale/NavDoom"
"crowdAI/vizdoom2018-singleplayer-starter-kit" -> "mihahauke/dqn_vizdoom_theano"
"arowdy98/Stanford-CS234" -> "changebo/CS234-2020"
"arowdy98/Stanford-CS234" -> "zlpure/CS234"
"HumanCompatibleAI/atari-irl" -> "HumanCompatibleAI/population-irl"
"eugene/pommerman" -> "BorealisAI/pommerman-baseline"
"eugene/pommerman" -> "rwightman/pytorch-pommerman-rl"
"eugene/pommerman" -> "tambetm/pommerman-baselines"
"erwanbou/sf-deep-rl" -> "bonniesjli/DQN_SR"
"erwanbou/sf-deep-rl" -> "AndreaTirinzoni/iw-transfer-rl"
"erwanbou/sf-deep-rl" -> "mike-gimelfarb/deep-successor-features-for-transfer"
"tesslerc/Sparse-IL" -> "microsoft/tdqn"
"vivek3141/ml" -> "vivek3141/learn-js"
"vivek3141/ml" -> "vivek3141/learn-c"
"vivek3141/ml" -> "vivek3141/flappy-neat"
"sharma-arjun/DirectedInfo-GAIL" -> "mohitsharma0690/DirectedInfo-GAIL"
"DLR-RM/stable-baselines3" -> "DLR-RM/rl-baselines3-zoo"
"DLR-RM/stable-baselines3" -> "vwxyzjn/cleanrl"
"DLR-RM/stable-baselines3" -> "Farama-Foundation/Gymnasium"
"DLR-RM/stable-baselines3" -> "thu-ml/tianshou"
"DLR-RM/stable-baselines3" -> "openai/baselines"
"DLR-RM/stable-baselines3" -> "hill-a/stable-baselines"
"DLR-RM/stable-baselines3" -> "openai/spinningup"
"DLR-RM/stable-baselines3" -> "AI4Finance-Foundation/ElegantRL"
"DLR-RM/stable-baselines3" -> "Farama-Foundation/PettingZoo"
"DLR-RM/stable-baselines3" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"DLR-RM/stable-baselines3" -> "openai/gym"
"DLR-RM/stable-baselines3" -> "pytorch/rl"
"DLR-RM/stable-baselines3" -> "ray-project/ray" ["e"=1]
"DLR-RM/stable-baselines3" -> "google-deepmind/mujoco" ["e"=1]
"DLR-RM/stable-baselines3" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"google-deepmind/acme" -> "google-deepmind/rlax" ["e"=1]
"google-deepmind/acme" -> "google-deepmind/reverb" ["e"=1]
"google-deepmind/acme" -> "astooke/rlpyt"
"google-deepmind/acme" -> "google-deepmind/dm-haiku" ["e"=1]
"google-deepmind/acme" -> "google-deepmind/bsuite"
"google-deepmind/acme" -> "google-deepmind/dm_control"
"google-deepmind/acme" -> "google-deepmind/open_spiel"
"google-deepmind/acme" -> "hill-a/stable-baselines"
"google-deepmind/acme" -> "rlworkgroup/garage"
"google-deepmind/acme" -> "rail-berkeley/rlkit"
"google-deepmind/acme" -> "Farama-Foundation/Minigrid"
"google-deepmind/acme" -> "tensorflow/agents"
"google-deepmind/acme" -> "google/brax" ["e"=1]
"google-deepmind/acme" -> "vwxyzjn/cleanrl"
"google-deepmind/acme" -> "DLR-RM/stable-baselines3"
"facebookresearch/minihack" -> "facebookresearch/nle"
"facebookresearch/minihack" -> "danijar/crafter" ["e"=1]
"facebookresearch/minihack" -> "google-research/rliable" ["e"=1]
"facebookresearch/minihack" -> "facebookresearch/moolib"
"facebookresearch/minihack" -> "facebookresearch/dcd"
"facebookresearch/minihack" -> "jurgisp/memory-maze" ["e"=1]
"facebookresearch/minihack" -> "MichaelTMatthews/Craftax" ["e"=1]
"facebookresearch/minihack" -> "Bam4d/Griddly"
"facebookresearch/minihack" -> "kenjyoung/MinAtar" ["e"=1]
"facebookresearch/minihack" -> "Farama-Foundation/Minigrid"
"facebookresearch/minihack" -> "facebookresearch/impact-driven-exploration"
"facebookresearch/minihack" -> "facebookresearch/torchbeast"
"facebookresearch/minihack" -> "heiner/nle"
"facebookresearch/minihack" -> "openai/procgen"
"facebookresearch/minihack" -> "eloialonso/iris" ["e"=1]
"clvrai/awesome-rl-envs" -> "kengz/awesome-deep-rl"
"clvrai/awesome-rl-envs" -> "Farama-Foundation/Metaworld" ["e"=1]
"clvrai/awesome-rl-envs" -> "RobertTLange/gymnax" ["e"=1]
"clvrai/awesome-rl-envs" -> "Farama-Foundation/D4RL" ["e"=1]
"clvrai/awesome-rl-envs" -> "Farama-Foundation/Minigrid"
"clvrai/awesome-rl-envs" -> "facebookresearch/mbrl-lib" ["e"=1]
"clvrai/awesome-rl-envs" -> "takuseno/d3rlpy" ["e"=1]
"clvrai/awesome-rl-envs" -> "hanjuku-kaso/awesome-offline-rl" ["e"=1]
"clvrai/awesome-rl-envs" -> "Farama-Foundation/Gymnasium-Robotics" ["e"=1]
"clvrai/awesome-rl-envs" -> "sail-sg/envpool" ["e"=1]
"clvrai/awesome-rl-envs" -> "google-research/rliable" ["e"=1]
"clvrai/awesome-rl-envs" -> "stepjam/RLBench" ["e"=1]
"clvrai/awesome-rl-envs" -> "Farama-Foundation/Miniworld"
"clvrai/awesome-rl-envs" -> "rail-berkeley/rlkit"
"clvrai/awesome-rl-envs" -> "MrSyee/pg-is-all-you-need"
"Farama-Foundation/PettingZoo" -> "Farama-Foundation/SuperSuit" ["e"=1]
"Farama-Foundation/PettingZoo" -> "oxwhirl/pymarl"
"Farama-Foundation/PettingZoo" -> "marlbenchmark/on-policy"
"Farama-Foundation/PettingZoo" -> "Replicable-MARL/MARLlib"
"Farama-Foundation/PettingZoo" -> "oxwhirl/smac"
"Farama-Foundation/PettingZoo" -> "LantaoYu/MARL-Papers"
"Farama-Foundation/PettingZoo" -> "openai/multiagent-particle-envs"
"Farama-Foundation/PettingZoo" -> "starry-sky6688/MARL-Algorithms"
"Farama-Foundation/PettingZoo" -> "vwxyzjn/cleanrl"
"Farama-Foundation/PettingZoo" -> "DLR-RM/stable-baselines3"
"Farama-Foundation/PettingZoo" -> "uoe-agents/epymarl"
"Farama-Foundation/PettingZoo" -> "Farama-Foundation/Gymnasium"
"Farama-Foundation/PettingZoo" -> "Farama-Foundation/Minigrid"
"Farama-Foundation/PettingZoo" -> "google-deepmind/open_spiel"
"Farama-Foundation/PettingZoo" -> "instadeepai/Mava"
"zhoubolei/introRL" -> "ucla-rlcourse/RLexample"
"zhoubolei/introRL" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"zhoubolei/introRL" -> "datawhalechina/easy-rl"
"zhoubolei/introRL" -> "NeuronDance/DeepRL"
"zhoubolei/introRL" -> "ucla-rlcourse/DeepRL-Tutorials"
"zhoubolei/introRL" -> "thu-ml/tianshou"
"zhoubolei/introRL" -> "wangshusen/DRL"
"zhoubolei/introRL" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"zhoubolei/introRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"zhoubolei/introRL" -> "sfujim/TD3"
"zhoubolei/introRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"zhoubolei/introRL" -> "PaddlePaddle/PARL"
"zhoubolei/introRL" -> "AI4Finance-Foundation/ElegantRL"
"zhoubolei/introRL" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"zhoubolei/introRL" -> "qqiang00/Reinforce"
"datawhalechina/easy-rl" -> "boyu-ai/Hands-on-RL"
"datawhalechina/easy-rl" -> "wangshusen/DRL"
"datawhalechina/easy-rl" -> "thu-ml/tianshou"
"datawhalechina/easy-rl" -> "zhoubolei/introRL"
"datawhalechina/easy-rl" -> "datawhalechina/leedl-tutorial" ["e"=1]
"datawhalechina/easy-rl" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"datawhalechina/easy-rl" -> "DLR-RM/stable-baselines3"
"datawhalechina/easy-rl" -> "AI4Finance-Foundation/ElegantRL"
"datawhalechina/easy-rl" -> "MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning"
"datawhalechina/easy-rl" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"datawhalechina/easy-rl" -> "NeuronDance/DeepRL"
"datawhalechina/easy-rl" -> "mli/paper-reading" ["e"=1]
"datawhalechina/easy-rl" -> "datawhalechina/pumpkin-book" ["e"=1]
"datawhalechina/easy-rl" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"datawhalechina/easy-rl" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"flint-xf-fan/Byzantine-Federated-RL" -> "siomvas/awesome-federated-reinforcement-learning"
"flint-xf-fan/Byzantine-Federated-RL" -> "liamhebert/FedFormer"
"flint-xf-fan/Byzantine-Federated-RL" -> "pengyang7881187/FedRL"
"iffiX/machin" -> "StepNeverStop/RLs"
"iffiX/machin" -> "RITCHIEHuang/DeepRL_Algorithms"
"iffiX/machin" -> "AboudyKreidieh/h-baselines"
"iffiX/machin" -> "DKuan/MADDPG_torch"
"iffiX/machin" -> "sjtu-marl/malib"
"iffiX/machin" -> "dongminlee94/deep_rl"
"iffiX/machin" -> "Kaixhin/imitation-learning"
"iffiX/machin" -> "henry-prior/jax-rl"
"iffiX/machin" -> "SforAiDl/genrl" ["e"=1]
"iffiX/machin" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"iffiX/machin" -> "toshikwa/rljax"
"PWhiddy/PokemonRedExperiments" -> "Baekalfen/PyBoy"
"PWhiddy/PokemonRedExperiments" -> "spdustin/ChatGPT-AutoExpert" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "Farama-Foundation/Gymnasium"
"PWhiddy/PokemonRedExperiments" -> "DLR-RM/stable-baselines3"
"PWhiddy/PokemonRedExperiments" -> "joonspk-research/generative_agents" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "PufferAI/PufferLib" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "vwxyzjn/cleanrl"
"PWhiddy/PokemonRedExperiments" -> "a16z-infra/ai-town" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "MineDojo/Voyager" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "LazyVim/LazyVim" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "pagefaultgames/pokerogue" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "roboflow/supervision" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "pret/pokered" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "OpenBMB/ChatDev" ["e"=1]
"PWhiddy/PokemonRedExperiments" -> "danijar/dreamerv3" ["e"=1]
"ihofmann/open-websoccer" -> "delight-im/OpenSoccer"
"ihofmann/open-websoccer" -> "ZOXEXIVO/open-football"
"anita-hu/TF2-RL" -> "archsyscall/DeepRL-TensorFlow2"
"anita-hu/TF2-RL" -> "RITCHIEHuang/DeepRL_Algorithms"
"anita-hu/TF2-RL" -> "keiohta/tf2rl"
"anita-hu/TF2-RL" -> "louisnino/RLcode"
"anita-hu/TF2-RL" -> "StepNeverStop/RLs"
"anita-hu/TF2-RL" -> "PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook"
"pfnet/pfrl" -> "astooke/rlpyt"
"pfnet/pfrl" -> "chainer/chainerrl"
"pfnet/pfrl" -> "takuseno/d3rlpy" ["e"=1]
"pfnet/pfrl" -> "google-deepmind/acme"
"pfnet/pfrl" -> "Farama-Foundation/D4RL" ["e"=1]
"pfnet/pfrl" -> "facebookresearch/torchbeast"
"pfnet/pfrl" -> "sail-sg/envpool" ["e"=1]
"pfnet/pfrl" -> "hanjuku-kaso/awesome-offline-rl" ["e"=1]
"pfnet/pfrl" -> "cpnota/autonomous-learning-library"
"pfnet/pfrl" -> "Farama-Foundation/Miniworld"
"pfnet/pfrl" -> "facebookresearch/mbrl-lib" ["e"=1]
"pfnet/pfrl" -> "Farama-Foundation/Minigrid"
"pfnet/pfrl" -> "google-research/rliable" ["e"=1]
"pfnet/pfrl" -> "WilsonWangTHU/mbbl" ["e"=1]
"pfnet/pfrl" -> "Khrylx/PyTorch-RL"
"facebookresearch/nle" -> "facebookresearch/minihack"
"facebookresearch/nle" -> "facebookresearch/torchbeast"
"facebookresearch/nle" -> "Farama-Foundation/Minigrid"
"facebookresearch/nle" -> "danijar/crafter" ["e"=1]
"facebookresearch/nle" -> "heiner/nle"
"facebookresearch/nle" -> "openai/procgen"
"facebookresearch/nle" -> "Farama-Foundation/Miniworld"
"facebookresearch/nle" -> "MichaelTMatthews/Craftax" ["e"=1]
"facebookresearch/nle" -> "facebookresearch/impact-driven-exploration"
"facebookresearch/nle" -> "alex-petrenko/sample-factory"
"facebookresearch/nle" -> "facebookresearch/level-replay"
"facebookresearch/nle" -> "Bam4d/Griddly"
"facebookresearch/nle" -> "ngoodger/nle-language-wrapper"
"facebookresearch/nle" -> "facebookresearch/dcd"
"facebookresearch/nle" -> "google-deepmind/bsuite"
"JBLanier/pipeline-psro" -> "indylab/nxdo"
"JBLanier/pipeline-psro" -> "diversepsro/diverse_psro"
"JBLanier/pipeline-psro" -> "JBLanier/stratego_env"
"JBLanier/pipeline-psro" -> "aicenter/openspiel_reproductions"
"JBLanier/pipeline-psro" -> "indylab/tabular_xdo"
"JBLanier/pipeline-psro" -> "npvoid/OnlineDoubleOracle"
"datamllab/awesome-game-ai" -> "datamllab/rlcard" ["e"=1]
"datamllab/awesome-game-ai" -> "datamllab/rlcard-showdown"
"datamllab/awesome-game-ai" -> "kwai/DouZero"
"datamllab/awesome-game-ai" -> "datamllab/rlcard-tutorial"
"datamllab/awesome-game-ai" -> "Netease-Games-AI-Lab-Guangzhou/PerfectDou"
"datamllab/awesome-game-ai" -> "tigerneil/awesome-deep-rl"
"datamllab/awesome-game-ai" -> "google-deepmind/open_spiel"
"datamllab/awesome-game-ai" -> "EricSteinberger/PokerRL" ["e"=1]
"datamllab/awesome-game-ai" -> "Farama-Foundation/PettingZoo"
"datamllab/awesome-game-ai" -> "happypepper/DeepHoldem" ["e"=1]
"datamllab/awesome-game-ai" -> "qq456cvb/doudizhu-C"
"datamllab/awesome-game-ai" -> "Agony5757/mahjong" ["e"=1]
"datamllab/awesome-game-ai" -> "datamllab/autovideo"
"datamllab/awesome-game-ai" -> "daochenzha/rapid"
"datamllab/awesome-game-ai" -> "ZhouWeikuan/DouDiZhu"
"Turing-Project/WriteGPT" -> "Morizeyao/GPT2-Chinese" ["e"=1]
"Turing-Project/WriteGPT" -> "imcaspar/gpt2-ml" ["e"=1]
"Turing-Project/WriteGPT" -> "babysor/MockingBird" ["e"=1]
"Turing-Project/WriteGPT" -> "yangjianxin1/GPT2-chitchat" ["e"=1]
"Turing-Project/WriteGPT" -> "kwai/DouZero"
"Turing-Project/WriteGPT" -> "Turing-Project/EssayTopicPredictV2"
"Turing-Project/WriteGPT" -> "kangvcar/InfoSpider" ["e"=1]
"Turing-Project/WriteGPT" -> "brightmart/nlp_chinese_corpus" ["e"=1]
"Turing-Project/WriteGPT" -> "menzi11/BullshitGenerator" ["e"=1]
"Turing-Project/WriteGPT" -> "thunlp/WantWords" ["e"=1]
"Turing-Project/WriteGPT" -> "ssssssss-team/spider-flow" ["e"=1]
"Turing-Project/WriteGPT" -> "FengQuanLi/ResnetGPT"
"Turing-Project/WriteGPT" -> "Turing-Project/AntiFraudChatBot" ["e"=1]
"Turing-Project/WriteGPT" -> "Baiyuetribe/paper2gui" ["e"=1]
"Turing-Project/WriteGPT" -> "Wechat-ggGitHub/Awesome-GitHub-Repo" ["e"=1]
"opherlieber/rltime" -> "cyoon1729/distributedRL"
"opherlieber/rltime" -> "facebookresearch/rela"
"chauby/V-REP-YouBot-Demo" -> "Junzhuodu/ur5_vrep_python"
"chauby/V-REP-YouBot-Demo" -> "Ianlande/Vrep_yolov3_ddpg_pytorch"
"chauby/V-REP-YouBot-Demo" -> "chauby/PyDMPs_Chauby" ["e"=1]
"chauby/V-REP-YouBot-Demo" -> "xuhuairuogu/V-REP-Simulation-Projects" ["e"=1]
"chauby/V-REP-YouBot-Demo" -> "zanebin/UR5-manipulator-control"
"chauby/V-REP-YouBot-Demo" -> "chauby/CoppeliaSimRL"
"chauby/V-REP-YouBot-Demo" -> "chauby/BipedalWalkingRobots" ["e"=1]
"wangshusen/AdvancedAlgorithms" -> "wangshusen/DeepLearning"
"wangshusen/AdvancedAlgorithms" -> "wangshusen/RecommenderSystem" ["e"=1]
"wangshusen/AdvancedAlgorithms" -> "wangshusen/DRL"
"wangshusen/AdvancedAlgorithms" -> "stevens-cs546-cs554/CS-554"
"wangshusen/AdvancedAlgorithms" -> "wangshusen/SearchEngine" ["e"=1]
"wangshusen/DRL" -> "DeepRLChinese/DeepRL-Chinese"
"wangshusen/DRL" -> "wangshusen/DeepLearning"
"wangshusen/DRL" -> "datawhalechina/easy-rl"
"wangshusen/DRL" -> "boyu-ai/Hands-on-RL"
"wangshusen/DRL" -> "XinJingHao/DRL-Pytorch"
"wangshusen/DRL" -> "NeuronDance/DeepRL"
"wangshusen/DRL" -> "AI4Finance-Foundation/ElegantRL"
"wangshusen/DRL" -> "thu-ml/tianshou"
"wangshusen/DRL" -> "MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning"
"wangshusen/DRL" -> "zhoubolei/introRL"
"wangshusen/DRL" -> "wangshusen/RecommenderSystem" ["e"=1]
"wangshusen/DRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"wangshusen/DRL" -> "DLR-RM/stable-baselines3"
"wangshusen/DRL" -> "vincen-github/mlimpl"
"wangshusen/DRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"subprotocol/genetic-js" -> "dolphin278/genetic"
"subprotocol/genetic-js" -> "subprotocol/verlet-js" ["e"=1]
"subprotocol/genetic-js" -> "wagenaartje/neataptic" ["e"=1]
"subprotocol/genetic-js" -> "mkmarek/forex.analytics"
"subprotocol/genetic-js" -> "Tom-Alexander/regression-js" ["e"=1]
"subprotocol/genetic-js" -> "gekkowarez/gekkoga" ["e"=1]
"subprotocol/genetic-js" -> "panchishin/geneticalgorithm"
"subprotocol/genetic-js" -> "TomaszRewak/ML-games"
"subprotocol/genetic-js" -> "lagodiuk/genetic-algorithm" ["e"=1]
"kandouss/marlgrid" -> "ArnaudFickinger/gym-multigrid"
"ArnaudFickinger/gym-multigrid" -> "kandouss/marlgrid"
"ArnaudFickinger/gym-multigrid" -> "semitable/lb-foraging"
"ArnaudFickinger/gym-multigrid" -> "koulanurag/ma-gym"
"semitable/robotic-warehouse" -> "semitable/lb-foraging"
"semitable/robotic-warehouse" -> "uoe-agents/epymarl"
"semitable/robotic-warehouse" -> "uoe-agents/robotic-warehouse"
"semitable/robotic-warehouse" -> "uoe-agents/task-assignment-robotic-warehouse"
"semitable/robotic-warehouse" -> "Cognitive-AI-Systems/pogema" ["e"=1]
"semitable/robotic-warehouse" -> "Jiaoyang-Li/RHCR" ["e"=1]
"semitable/robotic-warehouse" -> "FLAIROx/JaxMARL" ["e"=1]
"semitable/robotic-warehouse" -> "proroklab/VectorizedMultiAgentSimulator"
"semitable/robotic-warehouse" -> "LyapunovJingci/Warehouse_Robot_Path_Planning" ["e"=1]
"semitable/robotic-warehouse" -> "marmotlab/PRIMAL2" ["e"=1]
"semitable/robotic-warehouse" -> "instadeepai/Mava"
"semitable/robotic-warehouse" -> "gsartoretti/PRIMAL" ["e"=1]
"semitable/robotic-warehouse" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"semitable/robotic-warehouse" -> "instadeepai/og-marl"
"semitable/robotic-warehouse" -> "atb033/multi_agent_path_planning" ["e"=1]
"DLR-RM/rl-baselines3-zoo" -> "DLR-RM/stable-baselines3"
"DLR-RM/rl-baselines3-zoo" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"DLR-RM/rl-baselines3-zoo" -> "araffin/rl-baselines-zoo"
"DLR-RM/rl-baselines3-zoo" -> "araffin/rl-tutorial-jnrr19"
"DLR-RM/rl-baselines3-zoo" -> "vwxyzjn/cleanrl"
"DLR-RM/rl-baselines3-zoo" -> "qgallouedec/panda-gym" ["e"=1]
"DLR-RM/rl-baselines3-zoo" -> "Farama-Foundation/PettingZoo"
"DLR-RM/rl-baselines3-zoo" -> "Farama-Foundation/D4RL" ["e"=1]
"DLR-RM/rl-baselines3-zoo" -> "hill-a/stable-baselines"
"DLR-RM/rl-baselines3-zoo" -> "Farama-Foundation/Gymnasium"
"DLR-RM/rl-baselines3-zoo" -> "HumanCompatibleAI/imitation"
"DLR-RM/rl-baselines3-zoo" -> "AI4Finance-Foundation/ElegantRL"
"DLR-RM/rl-baselines3-zoo" -> "pytorch/rl"
"DLR-RM/rl-baselines3-zoo" -> "google-deepmind/dm_control"
"DLR-RM/rl-baselines3-zoo" -> "rlworkgroup/garage"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "DLR-RM/rl-baselines3-zoo"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "araffin/sbx" ["e"=1]
"Stable-Baselines-Team/stable-baselines3-contrib" -> "DLR-RM/stable-baselines3"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "qgallouedec/panda-gym" ["e"=1]
"Stable-Baselines-Team/stable-baselines3-contrib" -> "Stable-Baselines-Team/rl-colab-notebooks"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "Stable-Baselines-Team/stable-baselines"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "Farama-Foundation/SuperSuit" ["e"=1]
"Stable-Baselines-Team/stable-baselines3-contrib" -> "araffin/rl-tutorial-jnrr19"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "sail-sg/envpool" ["e"=1]
"Stable-Baselines-Team/stable-baselines3-contrib" -> "HumanCompatibleAI/imitation"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "vwxyzjn/ppo-implementation-details"
"Stable-Baselines-Team/stable-baselines3-contrib" -> "Farama-Foundation/Gymnasium-Robotics" ["e"=1]
"Stable-Baselines-Team/stable-baselines3-contrib" -> "MarcoMeter/recurrent-ppo-truncated-bptt" ["e"=1]
"Stable-Baselines-Team/stable-baselines3-contrib" -> "google-research/rliable" ["e"=1]
"Stable-Baselines-Team/stable-baselines3-contrib" -> "takuseno/d3rlpy" ["e"=1]
"kengz/awesome-deep-rl" -> "clvrai/awesome-rl-envs"
"kengz/awesome-deep-rl" -> "quantumiracle/Popular-RL-Algorithms"
"kengz/awesome-deep-rl" -> "hanjuku-kaso/awesome-offline-rl" ["e"=1]
"kengz/awesome-deep-rl" -> "tigerneil/awesome-deep-rl"
"kengz/awesome-deep-rl" -> "google-deepmind/rlax" ["e"=1]
"kengz/awesome-deep-rl" -> "cpnota/autonomous-learning-library"
"kengz/awesome-deep-rl" -> "tshrjn/env-zoo"
"kengz/awesome-deep-rl" -> "kengz/SLM-Lab"
"kengz/awesome-deep-rl" -> "jannerm/mbpo" ["e"=1]
"kengz/awesome-deep-rl" -> "RobertTLange/gymnax" ["e"=1]
"kengz/awesome-deep-rl" -> "chauncygu/Safe-Reinforcement-Learning-Baselines" ["e"=1]
"kengz/awesome-deep-rl" -> "andyljones/reinforcement-learning-discord-wiki"
"kengz/awesome-deep-rl" -> "MrSyee/pg-is-all-you-need"
"kengz/awesome-deep-rl" -> "Allenpandas/Reinforcement-Learning-Papers" ["e"=1]
"kengz/awesome-deep-rl" -> "StepNeverStop/RLs"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Actor-Critic-Methods-Paper-To-Code"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Reinforcement-Learning-In-Motion"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Youtube-Code-Repository"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Advanced-Replay-Strategies"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "RITCHIEHuang/DeepRL_Algorithms"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "philtabor/ProtoRL"
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "Zuox99/Deep-Reinforcement-Learning-Based-Resource-Provisioning-and-Task-Scheduling-for-Cloud-Service-Provid" ["e"=1]
"philtabor/Deep-Q-Learning-Paper-To-Code" -> "dongminlee94/deep_rl"
"openai/multi-agent-emergence-environments" -> "openai/mujoco-worldgen"
"openai/multi-agent-emergence-environments" -> "oxwhirl/pymarl"
"openai/multi-agent-emergence-environments" -> "oxwhirl/smac"
"openai/multi-agent-emergence-environments" -> "openai/multiagent-particle-envs"
"openai/multi-agent-emergence-environments" -> "openai/multiagent-competition"
"openai/multi-agent-emergence-environments" -> "LantaoYu/MARL-Papers"
"openai/multi-agent-emergence-environments" -> "Farama-Foundation/PettingZoo"
"openai/multi-agent-emergence-environments" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"openai/multi-agent-emergence-environments" -> "geek-ai/MAgent"
"openai/multi-agent-emergence-environments" -> "openai/maddpg"
"openai/multi-agent-emergence-environments" -> "starry-sky6688/MARL-Algorithms"
"openai/multi-agent-emergence-environments" -> "marlbenchmark/on-policy"
"openai/multi-agent-emergence-environments" -> "Farama-Foundation/Minigrid"
"openai/multi-agent-emergence-environments" -> "openai/mujoco-py"
"openai/multi-agent-emergence-environments" -> "openai/neural-mmo"
"openfootmanager/openfootmanager" -> "ZOXEXIVO/open-football"
"openfootmanager/openfootmanager" -> "esports-manager/esports-manager"
"openfootmanager/openfootmanager" -> "atas76/openengine"
"openfootmanager/openfootmanager" -> "AllenThomasDev/Football-Simulator"
"openfootmanager/openfootmanager" -> "ihofmann/open-websoccer"
"openfootmanager/openfootmanager" -> "vi3itor/GameplayFootball"
"openfootmanager/openfootmanager" -> "kieran-murphy/footballgame"
"Chrispresso/SuperMarioBros-AI" -> "Chrispresso/SnakeAI"
"Chrispresso/SuperMarioBros-AI" -> "octavio-santiago/Super-Mario-Land-AI"
"Chrispresso/SuperMarioBros-AI" -> "aleju/mario-ai"
"Chrispresso/SuperMarioBros-AI" -> "Farama-Foundation/stable-retro"
"mdeib/berkeley-deep-RL-pytorch-solutions" -> "mdeib/berkeley-deep-RL-pytorch-starter"
"mdeib/berkeley-deep-RL-pytorch-solutions" -> "erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch"
"mdeib/berkeley-deep-RL-pytorch-solutions" -> "vamsianumula/cs285-deeprl-ucberkeley-2021"
"mdeib/berkeley-deep-RL-pytorch-solutions" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"google-research/seed_rl" -> "google-deepmind/scalable_agent"
"google-research/seed_rl" -> "facebookresearch/torchbeast"
"google-research/seed_rl" -> "google-deepmind/reverb" ["e"=1]
"google-research/seed_rl" -> "google-deepmind/acme"
"google-research/seed_rl" -> "MishaLaskin/curl" ["e"=1]
"google-research/seed_rl" -> "sjtu-marl/malib"
"google-research/seed_rl" -> "alex-petrenko/sample-factory"
"google-research/seed_rl" -> "sail-sg/envpool" ["e"=1]
"google-research/seed_rl" -> "astooke/rlpyt"
"google-research/seed_rl" -> "uber-research/go-explore"
"google-research/seed_rl" -> "openai/random-network-distillation"
"google-research/seed_rl" -> "google-research/rliable" ["e"=1]
"google-research/seed_rl" -> "NVlabs/cule" ["e"=1]
"google-research/seed_rl" -> "google-deepmind/rlax" ["e"=1]
"google-research/seed_rl" -> "google-deepmind/bsuite"
"google-deepmind/bsuite" -> "google-deepmind/spriteworld"
"google-deepmind/bsuite" -> "google-deepmind/rlax" ["e"=1]
"google-deepmind/bsuite" -> "astooke/rlpyt"
"google-deepmind/bsuite" -> "google-deepmind/acme"
"google-deepmind/bsuite" -> "google-deepmind/open_spiel"
"google-deepmind/bsuite" -> "google-deepmind/trfl"
"google-deepmind/bsuite" -> "Farama-Foundation/Minigrid"
"google-deepmind/bsuite" -> "google-research/rliable" ["e"=1]
"google-deepmind/bsuite" -> "rlworkgroup/garage"
"google-deepmind/bsuite" -> "google-deepmind/dm_control"
"google-deepmind/bsuite" -> "RobertTLange/gymnax" ["e"=1]
"google-deepmind/bsuite" -> "ikostrikov/jaxrl" ["e"=1]
"google-deepmind/bsuite" -> "Farama-Foundation/Miniworld"
"google-deepmind/bsuite" -> "google-deepmind/scalable_agent"
"google-deepmind/bsuite" -> "rll/rllab"
"spragunr/deep_q_rl" -> "kristjankorjus/Replicating-DeepMind"
"spragunr/deep_q_rl" -> "tambetm/simple_dqn"
"spragunr/deep_q_rl" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"spragunr/deep_q_rl" -> "siemanko/tensorflow-deepq"
"spragunr/deep_q_rl" -> "nikitasrivatsan/DeepLearningVideoGames"
"spragunr/deep_q_rl" -> "muupan/deep-reinforcement-learning-papers"
"spragunr/deep_q_rl" -> "Farama-Foundation/Arcade-Learning-Environment"
"spragunr/deep_q_rl" -> "muupan/dqn-in-the-caffe"
"spragunr/deep_q_rl" -> "coreylynch/async-rl"
"spragunr/deep_q_rl" -> "muupan/async-rl"
"spragunr/deep_q_rl" -> "shawntan/neural-turing-machines" ["e"=1]
"spragunr/deep_q_rl" -> "Lasagne/Lasagne" ["e"=1]
"spragunr/deep_q_rl" -> "miyosuda/async_deep_reinforce"
"spragunr/deep_q_rl" -> "rll/rllab"
"spragunr/deep_q_rl" -> "joschu/modular_rl"
"openai/mujoco-worldgen" -> "openai/multi-agent-emergence-environments"
"openai/mujoco-worldgen" -> "openai/mujoco-py"
"openai/mujoco-worldgen" -> "vikashplus/robohive" ["e"=1]
"openai/mujoco-worldgen" -> "aravindr93/mjrl" ["e"=1]
"openai/mujoco-worldgen" -> "google-deepmind/mujoco_mpc" ["e"=1]
"openai/mujoco-worldgen" -> "openai/coinrun"
"openai/mujoco-worldgen" -> "koulanurag/ma-gym"
"uoe-agents/lb-foraging" -> "uoe-agents/robotic-warehouse"
"uoe-agents/lb-foraging" -> "uoe-agents/seac"
"uoe-agents/robotic-warehouse" -> "uoe-agents/lb-foraging"
"uoe-agents/robotic-warehouse" -> "uoe-agents/seac"
"BY571/IQN-and-Extensions" -> "BY571/FQF-and-Extensions"
"BY571/IQN-and-Extensions" -> "xtma/dsac"
"BY571/IQN-and-Extensions" -> "BY571/QR-DQN"
"BY571/IQN-and-Extensions" -> "toshikwa/fqf-iqn-qrdqn.pytorch"
"openai/procgen" -> "openai/train-procgen"
"openai/procgen" -> "Farama-Foundation/Minigrid"
"openai/procgen" -> "sail-sg/envpool" ["e"=1]
"openai/procgen" -> "Farama-Foundation/Miniworld"
"openai/procgen" -> "google-research/rliable" ["e"=1]
"openai/procgen" -> "Farama-Foundation/Metaworld" ["e"=1]
"openai/procgen" -> "openai/safety-gym" ["e"=1]
"openai/procgen" -> "astooke/rlpyt"
"openai/procgen" -> "danijar/crafter" ["e"=1]
"openai/procgen" -> "google-deepmind/bsuite"
"openai/procgen" -> "openai/phasic-policy-gradient"
"openai/procgen" -> "openai/gym3"
"openai/procgen" -> "eloialonso/iris" ["e"=1]
"openai/procgen" -> "facebookresearch/drqv2" ["e"=1]
"openai/procgen" -> "openai/retro"
"dongminlee94/deep_rl" -> "TianhongDai/reinforcement-learning-algorithms"
"dongminlee94/deep_rl" -> "MrSyee/pg-is-all-you-need"
"dongminlee94/deep_rl" -> "quantumiracle/Popular-RL-Algorithms"
"dongminlee94/deep_rl" -> "RchalYang/torchrl"
"dongminlee94/deep_rl" -> "RITCHIEHuang/DeepRL_Algorithms"
"dongminlee94/deep_rl" -> "zhangchuheng123/Reinforcement-Implementation"
"dongminlee94/deep_rl" -> "dongminlee94/Samsung-DRL-Code" ["e"=1]
"dongminlee94/deep_rl" -> "Khrylx/PyTorch-RL"
"dongminlee94/deep_rl" -> "StepNeverStop/RLs"
"dongminlee94/deep_rl" -> "reinforcement-learning-kr/lets-do-irl"
"dongminlee94/deep_rl" -> "qfettes/DeepRL-Tutorials"
"dongminlee94/deep_rl" -> "BY571/Soft-Actor-Critic-and-Extensions"
"dongminlee94/deep_rl" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"dongminlee94/deep_rl" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"dongminlee94/deep_rl" -> "MishaLaskin/curl" ["e"=1]
"snasiriany/leap" -> "bhairavmehta95/data-efficient-hrl"
"Kaixhin/imitation-learning" -> "HumanCompatibleAI/imitation"
"Kaixhin/imitation-learning" -> "reinforcement-learning-kr/lets-do-irl"
"Kaixhin/imitation-learning" -> "kristery/Awesome-Imitation-Learning"
"Kaixhin/imitation-learning" -> "toshikwa/gail-airl-ppo.pytorch"
"Kaixhin/imitation-learning" -> "yrlu/irl-imitation"
"Kaixhin/imitation-learning" -> "CherryPieSexy/imitation_learning"
"Kaixhin/imitation-learning" -> "Ericonaldo/ILSwiss"
"Kaixhin/imitation-learning" -> "Khrylx/PyTorch-RL"
"Kaixhin/imitation-learning" -> "qzed/irl-maxent"
"Kaixhin/imitation-learning" -> "Div99/IQ-Learn"
"Kaixhin/imitation-learning" -> "juliusfrost/dreamer-pytorch" ["e"=1]
"Kaixhin/imitation-learning" -> "ermongroup/MA-AIRL"
"Kaixhin/imitation-learning" -> "MatthewJA/Inverse-Reinforcement-Learning"
"Kaixhin/imitation-learning" -> "hcnoh/gail-pytorch"
"Kaixhin/imitation-learning" -> "sfujim/TD3_BC" ["e"=1]
"toshikwa/gail-airl-ppo.pytorch" -> "HuangJiaLian/AIRL_MountainCar"
"toshikwa/gail-airl-ppo.pytorch" -> "hcnoh/gail-pytorch"
"toshikwa/gail-airl-ppo.pytorch" -> "seolhokim/InverseRL-Pytorch"
"toshikwa/gail-airl-ppo.pytorch" -> "justinjfu/inverse_rl"
"toshikwa/gail-airl-ppo.pytorch" -> "Kaixhin/imitation-learning"
"toshikwa/gail-airl-ppo.pytorch" -> "twni2016/f-IRL"
"toshikwa/gail-airl-ppo.pytorch" -> "dsbrown1331/CoRL2019-DREX"
"toshikwa/gail-airl-ppo.pytorch" -> "ermongroup/MA-AIRL"
"toshikwa/gail-airl-ppo.pytorch" -> "kristery/Imitation-Learning-from-Imperfect-Demonstration"
"toshikwa/gail-airl-ppo.pytorch" -> "reinforcement-learning-kr/lets-do-irl"
"toshikwa/gail-airl-ppo.pytorch" -> "Ericonaldo/ILSwiss"
"toshikwa/gail-airl-ppo.pytorch" -> "HumanCompatibleAI/imitation"
"toshikwa/gail-airl-ppo.pytorch" -> "navuboy/gail_gym"
"toshikwa/gail-airl-ppo.pytorch" -> "Div99/IQ-Learn"
"toshikwa/gail-airl-ppo.pytorch" -> "yunke-wang/WGAIL"
"salesforce/ai-economist" -> "google-deepmind/meltingpot"
"salesforce/ai-economist" -> "salesforce/warp-drive"
"salesforce/ai-economist" -> "Farama-Foundation/PettingZoo"
"salesforce/ai-economist" -> "eugenevinitsky/sequential_social_dilemma_games"
"salesforce/ai-economist" -> "antontarasenko/awesome-economics" ["e"=1]
"salesforce/ai-economist" -> "instadeepai/Mava"
"salesforce/ai-economist" -> "AB-CE/abce" ["e"=1]
"salesforce/ai-economist" -> "jofmi/agentpy" ["e"=1]
"salesforce/ai-economist" -> "py-why/EconML" ["e"=1]
"salesforce/ai-economist" -> "davidrpugh/pyeconomics" ["e"=1]
"salesforce/ai-economist" -> "EconForge/dolo.py" ["e"=1]
"salesforce/ai-economist" -> "openai/multi-agent-emergence-environments"
"salesforce/ai-economist" -> "KennethJudd/CompEcon2020" ["e"=1]
"salesforce/ai-economist" -> "projectmesa/mesa" ["e"=1]
"salesforce/ai-economist" -> "jidiai/TaxAI"
"JohannesAck/MATD3implementation" -> "JohannesAck/tf2multiagentrl"
"JohannesAck/MATD3implementation" -> "MJ10/matd3-pytorch"
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "Skylark0924/Reinforcement-Learning-in-Robotics" ["e"=1]
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "zhangchuheng123/Reinforcement-Implementation"
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "kaixindelele/DRLib"
"Skylark0924/Machine-Learning-is-ALL-You-Need" -> "StepNeverStop/RLs"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "vietnh1009/Super-mario-bros-A3C-pytorch"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "Kautenja/gym-super-mario-bros"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "nikhilbarhate99/PPO-PyTorch"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "ericyangyu/PPO-for-Beginners"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "vietnh1009/Tetris-deep-Q-learning-pytorch"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "ikostrikov/pytorch-a3c"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "vietnh1009/Contra-PPO-pytorch"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "vwxyzjn/ppo-implementation-details"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "StepNeverStop/RLs"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "MrSyee/pg-is-all-you-need"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "sfujim/TD3"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "starry-sky6688/MARL-Algorithms"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "higgsfield/RL-Adventure"
"vietnh1009/Super-mario-bros-PPO-pytorch" -> "sail-sg/envpool" ["e"=1]
"thesouther/MARL" -> "manaski/MARL"
"thesouther/MARL" -> "Skylarking/MARL"
"thesouther/MARL" -> "verystrongjoe/qmix"
"thesouther/MARL" -> "starry-sky6688/MARL-Algorithms"
"thesouther/MARL" -> "marlbenchmark/off-policy"
"thesouther/MARL" -> "yangchen1997/Multi-Agent-Reinforcement-Learning"
"thesouther/MARL" -> "oxwhirl/wqmix"
"thesouther/MARL" -> "jianzhnie/deep-marl-toolkit"
"facebookresearch/torchbeast" -> "google-deepmind/scalable_agent"
"facebookresearch/torchbeast" -> "google-research/seed_rl"
"facebookresearch/torchbeast" -> "astooke/rlpyt"
"facebookresearch/torchbeast" -> "facebookresearch/nle"
"facebookresearch/torchbeast" -> "facebookresearch/moolib"
"facebookresearch/torchbeast" -> "facebookresearch/drqv2" ["e"=1]
"facebookresearch/torchbeast" -> "Farama-Foundation/Minigrid"
"facebookresearch/torchbeast" -> "google-research/rliable" ["e"=1]
"facebookresearch/torchbeast" -> "kenjyoung/MinAtar" ["e"=1]
"facebookresearch/torchbeast" -> "alex-petrenko/sample-factory"
"facebookresearch/torchbeast" -> "ikostrikov/jaxrl" ["e"=1]
"facebookresearch/torchbeast" -> "sail-sg/envpool" ["e"=1]
"facebookresearch/torchbeast" -> "google-deepmind/bsuite"
"facebookresearch/torchbeast" -> "facebookresearch/minihack"
"facebookresearch/torchbeast" -> "rll-research/url_benchmark" ["e"=1]
"fry404006308/fry_course_materials" -> "fry404006308/IT_book"
"fry404006308/fry_course_materials" -> "Kautenja/nes-py"
"fry404006308/fry_course_materials" -> "lansinuote/Simple_Reinforcement_Learning"
"fry404006308/fry_course_materials" -> "bubbliiiing/Siamese-pytorch" ["e"=1]
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" -> "berkeleydeeprlcourse/homework_fall2019"
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" -> "erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch"
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" -> "xuwd11/cs294-112_hws"
"microsoft/maro" -> "microsoft/FOST"
"microsoft/maro" -> "hubbs5/or-gym" ["e"=1]
"microsoft/maro" -> "sjtu-marl/malib"
"microsoft/maro" -> "ANL-CEEESA/MIPLearn" ["e"=1]
"microsoft/maro" -> "Farama-Foundation/PettingZoo"
"microsoft/maro" -> "instadeepai/Mava"
"microsoft/maro" -> "ai4co/rl4co" ["e"=1]
"microsoft/maro" -> "hongzimao/decima-sim" ["e"=1]
"microsoft/maro" -> "Replicable-MARL/MARLlib"
"microsoft/maro" -> "Hanjun-Dai/graph_comb_opt" ["e"=1]
"microsoft/maro" -> "ds4dm/ecole" ["e"=1]
"microsoft/maro" -> "oxwhirl/pymarl"
"microsoft/maro" -> "starry-sky6688/MARL-Algorithms"
"microsoft/maro" -> "khalil-research/PyEPO" ["e"=1]
"microsoft/maro" -> "pemami4911/neural-combinatorial-rl-pytorch" ["e"=1]
"huangwl18/modular-rl" -> "pathak22/modular-assemblies"
"huangwl18/modular-rl" -> "yobibyte/amorpheus"
"pythonlessons/Reinforcement_Learning" -> "pythonlessons/RL-Bitcoin-trading-bot" ["e"=1]
"pythonlessons/Reinforcement_Learning" -> "archsyscall/DeepRL-TensorFlow2"
"pythonlessons/Reinforcement_Learning" -> "abhisheksuran/Reinforcement_Learning"
"pythonlessons/Reinforcement_Learning" -> "philtabor/Youtube-Code-Repository"
"pythonlessons/Reinforcement_Learning" -> "germain-hug/Deep-RL-Keras"
"pythonlessons/Reinforcement_Learning" -> "RITCHIEHuang/DeepRL_Algorithms"
"pythonlessons/Reinforcement_Learning" -> "pythonlessons/FinRock" ["e"=1]
"pythonlessons/Reinforcement_Learning" -> "philtabor/Actor-Critic-Methods-Paper-To-Code"
"pythonlessons/Reinforcement_Learning" -> "anita-hu/TF2-RL"
"pythonlessons/Reinforcement_Learning" -> "shivaverma/OpenAIGym"
"sergioabreu-g/active-ragdolls" -> "ashleve/ActiveRagdoll"
"sergioabreu-g/active-ragdolls" -> "dci05049/Active-Ragdoll-Tutorial-Unity"
"sergioabreu-g/active-ragdolls" -> "davidkimighty/TARS"
"sergioabreu-g/active-ragdolls" -> "tiredamage42/DynamicRagdoll"
"sergioabreu-g/active-ragdolls" -> "hairibar/Hairibar.Ragdoll"
"sergioabreu-g/active-ragdolls" -> "Tvtig/UnityLightsaber"
"sergioabreu-g/active-ragdolls" -> "kressdev/RagdollTrainer"
"sergioabreu-g/active-ragdolls" -> "dgreenheck/OpenFracture" ["e"=1]
"vietnh1009/Tetris-deep-Q-learning-pytorch" -> "nuno-faria/tetris-ai"
"vietnh1009/Tetris-deep-Q-learning-pytorch" -> "Kautenja/gym-tetris"
"vietnh1009/Tetris-deep-Q-learning-pytorch" -> "jaybutera/tetrisRL"
"vietnh1009/Tetris-deep-Q-learning-pytorch" -> "vietnh1009/Super-mario-bros-PPO-pytorch"
"vietnh1009/Tetris-deep-Q-learning-pytorch" -> "michiel-cox/Tetris-DQN"
"vietnh1009/Tetris-deep-Q-learning-pytorch" -> "vietnh1009/Flappy-bird-deep-Q-learning-pytorch"
"vietnh1009/Tetris-deep-Q-learning-pytorch" -> "lusob/gym-tetris"
"louisnino/RLcode" -> "kaixindelele/DRLib"
"louisnino/RLcode" -> "anita-hu/TF2-RL"
"louisnino/RLcode" -> "zhangchuheng123/Reinforcement-Implementation"
"louisnino/RLcode" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"louisnino/RLcode" -> "Lizhi-sjtu/DRL-code-pytorch"
"louisnino/RLcode" -> "AI4Finance-Foundation/ElegantRL"
"louisnino/RLcode" -> "NeuronDance/DeepRL"
"louisnino/RLcode" -> "StepNeverStop/RLs"
"louisnino/RLcode" -> "boyu-ai/Hands-on-RL"
"louisnino/RLcode" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"louisnino/RLcode" -> "sfujim/TD3"
"louisnino/RLcode" -> "marlbenchmark/on-policy"
"louisnino/RLcode" -> "qqiang00/Reinforce"
"louisnino/RLcode" -> "starry-sky6688/MARL-Algorithms"
"louisnino/RLcode" -> "wangshusen/DRL"
"m4ttsch/omscs-notes" -> "m4ttsch/omscs-notes-notes"
"ericyangyu/PPO-for-Beginners" -> "nikhilbarhate99/PPO-PyTorch"
"ericyangyu/PPO-for-Beginners" -> "vwxyzjn/ppo-implementation-details"
"ericyangyu/PPO-for-Beginners" -> "marlbenchmark/on-policy"
"ericyangyu/PPO-for-Beginners" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ericyangyu/PPO-for-Beginners" -> "DLR-RM/rl-baselines3-zoo"
"ericyangyu/PPO-for-Beginners" -> "MrSyee/pg-is-all-you-need"
"ericyangyu/PPO-for-Beginners" -> "quantumiracle/Popular-RL-Algorithms"
"ericyangyu/PPO-for-Beginners" -> "Lizhi-sjtu/DRL-code-pytorch"
"ericyangyu/PPO-for-Beginners" -> "vietnh1009/Super-mario-bros-PPO-pytorch"
"ericyangyu/PPO-for-Beginners" -> "vwxyzjn/cleanrl"
"ericyangyu/PPO-for-Beginners" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"ericyangyu/PPO-for-Beginners" -> "Khrylx/PyTorch-RL"
"ericyangyu/PPO-for-Beginners" -> "seungeunrho/minimalRL"
"ericyangyu/PPO-for-Beginners" -> "pranz24/pytorch-soft-actor-critic"
"ericyangyu/PPO-for-Beginners" -> "opendilab/PPOxFamily" ["e"=1]
"trackmania-rl/tmrl" -> "AndrejGobeX/TrackMania_AI"
"trackmania-rl/tmrl" -> "yannbouteiller/rtgym"
"trackmania-rl/tmrl" -> "Linesight-RL/linesight"
"trackmania-rl/tmrl" -> "yannbouteiller/vgamepad"
"trackmania-rl/tmrl" -> "AgileRL/AgileRL" ["e"=1]
"trackmania-rl/tmrl" -> "coax-dev/coax"
"trackmania-rl/tmrl" -> "learn-to-race/l2r" ["e"=1]
"222464/AILib" -> "222464/ERL"
"ashleve/ActiveRagdoll" -> "sergioabreu-g/active-ragdolls"
"ashleve/ActiveRagdoll" -> "davidkimighty/TARS"
"ashleve/ActiveRagdoll" -> "Oladipupo/Active-Ragdolls-Unity"
"ashleve/ActiveRagdoll" -> "hairibar/Hairibar.Ragdoll"
"ashleve/ActiveRagdoll" -> "TildeAsterisk/Physicanim"
"ashleve/ActiveRagdoll" -> "AitorSimona/Traverser"
"ashleve/ActiveRagdoll" -> "Sopiro/Unity-Procedural-Animation"
"xtma/dsac" -> "Jingliang-Duan/DSAC-v1"
"vojtamolda/reinforcement-learning-an-introduction" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"vojtamolda/reinforcement-learning-an-introduction" -> "brynhayder/reinforcement_learning_an_introduction"
"vojtamolda/reinforcement-learning-an-introduction" -> "habanoz/reinforcement-learning-an-introduction"
"vojtamolda/reinforcement-learning-an-introduction" -> "enjeeneer/sutton_and_barto"
"Stable-Baselines-Team/rl-colab-notebooks" -> "araffin/rl-tutorial-jnrr19"
"Stable-Baselines-Team/rl-colab-notebooks" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"archsyscall/DeepRL-TensorFlow2" -> "anita-hu/TF2-RL"
"archsyscall/DeepRL-TensorFlow2" -> "keiohta/tf2rl"
"archsyscall/DeepRL-TensorFlow2" -> "RITCHIEHuang/DeepRL_Algorithms"
"archsyscall/DeepRL-TensorFlow2" -> "pythonlessons/Reinforcement_Learning"
"archsyscall/DeepRL-TensorFlow2" -> "PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook"
"archsyscall/DeepRL-TensorFlow2" -> "germain-hug/Deep-RL-Keras"
"archsyscall/DeepRL-TensorFlow2" -> "StepNeverStop/RLs"
"archsyscall/DeepRL-TensorFlow2" -> "quantumiracle/Popular-RL-Algorithms"
"archsyscall/DeepRL-TensorFlow2" -> "abhisheksuran/Reinforcement_Learning"
"archsyscall/DeepRL-TensorFlow2" -> "archsyscall/DistRL-TensorFlow2"
"archsyscall/DeepRL-TensorFlow2" -> "TianhongDai/reinforcement-learning-algorithms"
"archsyscall/DeepRL-TensorFlow2" -> "Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning"
"archsyscall/DeepRL-TensorFlow2" -> "dongminlee94/deep_rl"
"archsyscall/DeepRL-TensorFlow2" -> "inoryy/tensorflow2-deep-reinforcement-learning"
"archsyscall/DeepRL-TensorFlow2" -> "NeuronDance/DeepRL"
"Bam4d/Griddly" -> "Farama-Foundation/MicroRTS-Py" ["e"=1]
"Bam4d/Griddly" -> "danijar/crafter" ["e"=1]
"Bam4d/Griddly" -> "facebookresearch/minihack"
"Bam4d/Griddly" -> "Farama-Foundation/MicroRTS" ["e"=1]
"Bam4d/Griddly" -> "ucl-dark/paired"
"Bam4d/Griddly" -> "facebookresearch/dcd"
"datamllab/rlcard-showdown" -> "datamllab/rlcard-tutorial"
"datamllab/rlcard-showdown" -> "datamllab/awesome-game-ai"
"datamllab/rlcard-showdown" -> "datamllab/rlcard" ["e"=1]
"datamllab/rlcard-showdown" -> "kwai/DouZero"
"datamllab/rlcard-showdown" -> "Netease-Games-AI-Lab-Guangzhou/PerfectDou"
"datamllab/rlcard-showdown" -> "datamllab/autovideo"
"datamllab/rlcard-showdown" -> "Vincentzyx/Douzero_Resnet"
"datamllab/rlcard-showdown" -> "qq456cvb/doudizhu-C"
"datamllab/rlcard-showdown" -> "datamllab/BED_main"
"datamllab/rlcard-showdown" -> "daochenzha/rapid"
"datamllab/rlcard-showdown" -> "bupticybee/AlphaNLHoldem"
"datamllab/rlcard-showdown" -> "Vincentzyx/DouZero_For_HLDDZ_FullAuto"
"datamllab/rlcard-showdown" -> "munanfan/Mahjong_Game_RLCard_RL" ["e"=1]
"datamllab/rlcard-showdown" -> "submit-paper/Danzero_plus"
"datamllab/rlcard-showdown" -> "ZDZX-T/cardRecorder"
"openai/phasic-policy-gradient" -> "rraileanu/idaac" ["e"=1]
"openai/phasic-policy-gradient" -> "lucidrains/ppo"
"openai/phasic-policy-gradient" -> "openai/train-procgen"
"openai/phasic-policy-gradient" -> "sfujim/TD3_BC" ["e"=1]
"openai/phasic-policy-gradient" -> "openai/EPG"
"apexrl/Imitation-Learning-Paper-Lists" -> "kristery/Awesome-Imitation-Learning"
"apexrl/Imitation-Learning-Paper-Lists" -> "Ericonaldo/ILSwiss"
"apexrl/Imitation-Learning-Paper-Lists" -> "KamyarGh/rl_swiss"
"ugurkanates/awesome-real-world-rl" -> "brianspiering/awesome-deep-rl"
"ugurkanates/awesome-real-world-rl" -> "PacktPublishing/Python-Reinforcement-Learning-Projects"
"ugurkanates/awesome-real-world-rl" -> "sudharsan13296/Deep-Reinforcement-Learning-With-Python"
"ugurkanates/awesome-real-world-rl" -> "seungjaeryanlee/awesome-rl-competitions"
"ugurkanates/awesome-real-world-rl" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"ugurkanates/awesome-real-world-rl" -> "araffin/robotics-rl-srl" ["e"=1]
"ugurkanates/awesome-real-world-rl" -> "jr-robotics/robo-gym" ["e"=1]
"ugurkanates/awesome-real-world-rl" -> "opherlieber/rltime"
"AitorSimona/Traverser" -> "ashleve/ActiveRagdoll"
"facebookresearch/jps" -> "facebookresearch/off-belief-learning"
"daochenzha/Meta-AAD" -> "daochenzha/rapid"
"daochenzha/Meta-AAD" -> "ynchuang/DiscoverPath"
"hairibar/Hairibar.Ragdoll" -> "ashleve/ActiveRagdoll"
"hairibar/Hairibar.Ragdoll" -> "davidkimighty/TARS"
"hairibar/Hairibar.Ragdoll" -> "AitorSimona/Traverser"
"xingdi-eric-yuan/GATA-public" -> "microsoft/tdqn"
"Chrispresso/SnakeAI" -> "Chrispresso/SuperMarioBros-AI"
"Chrispresso/SnakeAI" -> "TheAILearner/Training-Snake-Game-With-Genetic-Algorithm"
"Chrispresso/SnakeAI" -> "Ackeraa/snake"
"ZOXEXIVO/open-football" -> "ElliotJBall/FootballManagerSimulator"
"ZOXEXIVO/open-football" -> "openfootmanager/openfootmanager"
"ZOXEXIVO/open-football" -> "AllenThomasDev/Football-Simulator"
"ZOXEXIVO/open-football" -> "GallagherAiden/footballSimulationEngine"
"ZOXEXIVO/open-football" -> "atas76/openengine"
"ZOXEXIVO/open-football" -> "ihofmann/open-websoccer"
"facebookresearch/hanabi_SAD" -> "facebookresearch/Hanabi_SPARTA"
"facebookresearch/hanabi_SAD" -> "facebookresearch/off-belief-learning"
"facebookresearch/hanabi_SAD" -> "facebookresearch/jps"
"facebookresearch/hanabi_SAD" -> "aronsar/hoad"
"borninfreedom/DeepLearning" -> "zhuliquan/reinforcement_learning_basic_book"
"borninfreedom/DeepLearning" -> "gxnk/reinforcement-learning-code"
"borninfreedom/DeepLearning" -> "applenob/rl_learn"
"borninfreedom/DeepLearning" -> "YJLAugus/Reinforcement-Learning-Notes"
"facebookresearch/Hanabi_SPARTA" -> "facebookresearch/hanabi_SAD"
"facebookresearch/Hanabi_SPARTA" -> "facebookresearch/off-belief-learning"
"facebookresearch/Hanabi_SPARTA" -> "Quuxplusone/Hanabi"
"facebookresearch/Hanabi_SPARTA" -> "facebookresearch/jps"
"DKuan/MADDPG_torch" -> "starry-sky6688/MADDPG"
"DKuan/MADDPG_torch" -> "shariqiqbal2810/maddpg-pytorch"
"DKuan/MADDPG_torch" -> "xuehy/pytorch-maddpg"
"DKuan/MADDPG_torch" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"DKuan/MADDPG_torch" -> "rosewang2008/rmaddpg"
"DKuan/MADDPG_torch" -> "shariqiqbal2810/MAAC"
"DKuan/MADDPG_torch" -> "marlbenchmark/off-policy"
"DKuan/MADDPG_torch" -> "sisl/MADRL"
"011235813/hierarchical-marl" -> "deligentfool/HAVEN"
"davide97l/rl-policies-attacks-defenses" -> "PavelCz/rl-adversarial-attack"
"davide97l/rl-policies-attacks-defenses" -> "chenhongge/StateAdvDRL"
"schroederdewitt/multiagent_mujoco" -> "cyanrain7/TRPO-in-MARL"
"schroederdewitt/multiagent_mujoco" -> "oxwhirl/facmac"
"schroederdewitt/multiagent_mujoco" -> "uoe-agents/epymarl"
"schroederdewitt/multiagent_mujoco" -> "TonghanWang/ROMA"
"schroederdewitt/multiagent_mujoco" -> "PKU-MARL/Multi-Agent-Transformer"
"schroederdewitt/multiagent_mujoco" -> "semitable/lb-foraging"
"schroederdewitt/multiagent_mujoco" -> "oxwhirl/smacv2"
"schroederdewitt/multiagent_mujoco" -> "marlbenchmark/on-policy"
"schroederdewitt/multiagent_mujoco" -> "PKU-MARL/DexterousHands" ["e"=1]
"schroederdewitt/multiagent_mujoco" -> "lich14/CDS"
"schroederdewitt/multiagent_mujoco" -> "marlbenchmark/off-policy"
"schroederdewitt/multiagent_mujoco" -> "hijkzzz/pymarl2"
"schroederdewitt/multiagent_mujoco" -> "wjh720/QPLEX"
"schroederdewitt/multiagent_mujoco" -> "TonghanWang/RODE"
"schroederdewitt/multiagent_mujoco" -> "PKU-MARL/HARL"
"chenhongge/StateAdvDRL" -> "huanzhang12/ATLA_robust_RL"
"chenhongge/StateAdvDRL" -> "tuomaso/radial_rl_v2"
"chenhongge/StateAdvDRL" -> "chenhongge/SA_DQN"
"chenhongge/StateAdvDRL" -> "tesslerc/ActionRobustRL"
"chenhongge/StateAdvDRL" -> "huanzhang12/SA_PPO"
"chenhongge/StateAdvDRL" -> "liuzuxin/safe-rl-robustness"
"chenhongge/StateAdvDRL" -> "umd-huang-lab/WocaR-RL"
"chenhongge/StateAdvDRL" -> "davide97l/rl-policies-attacks-defenses"
"vincen-github/mlimpl" -> "DeepRLChinese/DeepRL-Chinese"
"vincen-github/mlimpl" -> "wangshusen/DRL"
"vincen-github/mlimpl" -> "XinJingHao/DRL-Pytorch"
"vincen-github/mlimpl" -> "jwk1rose/RL_Learning"
"vincen-github/mlimpl" -> "QiangLong2017/Deep-Reiforcement-Learning"
"vincen-github/mlimpl" -> "fangvv/VN-MADDPG" ["e"=1]
"vincen-github/mlimpl" -> "lansinuote/Simple_Reinforcement_Learning"
"vincen-github/mlimpl" -> "ClownW/Reinforcement-learning-with-PyTorch"
"vincen-github/mlimpl" -> "boyu-ai/Hands-on-RL"
"vincen-github/mlimpl" -> "lansinuote/More_Simple_Reinforcement_Learning"
"The-Firexx/trackmania2020apidocumentation" -> "breeku/opentrackmania"
"The-Firexx/trackmania2020apidocumentation" -> "breeku/trackmania-api-node"
"The-Firexx/trackmania2020apidocumentation" -> "codecat/tm-dashboard"
"BigBang1112/gbx-net" -> "BigBang1112/nations-converter"
"BigBang1112/gbx-net" -> "skyslide22/blendermania-addon"
"BigBang1112/gbx-net" -> "codecat/tm-dashboard"
"BigBang1112/gbx-net" -> "donadigo/TMInterfaceClientPython"
"BigBang1112/gbx-net" -> "donadigo/pygbx"
"BigBang1112/gbx-net" -> "Electron-x/GbxDump"
"BigBang1112/gbx-net" -> "TwinkieTweaks/Twinkie"
"BigBang1112/gbx-net" -> "ArkadySK/GbxMapBrowser"
"BigBang1112/gbx-net" -> "resir014/TMViz"
"BigBang1112/gbx-net" -> "EvoEsports/EvoSC"
"alirezakazemipour/DDPG-HER" -> "alishbaimran/Robotics-DDPG-HER"
"mbaske/grid-sensor" -> "mbaske/ml-agents-hyperparams"
"mbaske/grid-sensor" -> "mbaske/angry-ai"
"mbaske/grid-sensor" -> "mbaske/ml-audio-sensor"
"mbaske/grid-sensor" -> "mbaske/robot-ants"
"mbaske/grid-sensor" -> "mbaske/ml-explorer-drone"
"HumanCompatibleAI/human_aware_rl" -> "HumanCompatibleAI/overcooked-demo"
"HumanCompatibleAI/human_aware_rl" -> "Stanford-ILIAD/PantheonRL"
"HumanCompatibleAI/human_aware_rl" -> "HosnLS/Hierarchical-Language-Agent"
"HumanCompatibleAI/human_aware_rl" -> "HumanCompatibleAI/overcooked-hAI-exp"
"HumanCompatibleAI/human_aware_rl" -> "Stanford-ILIAD/Diverse-Conventions"
"HumanCompatibleAI/human_aware_rl" -> "HumanCompatibleAI/overcooked_ai"
"Farama-Foundation/MAgent" -> "Farama-Foundation/Multi-Agent-ALE"
"Farama-Foundation/MAgent" -> "Farama-Foundation/hanabi-learning-environment"
"Farama-Foundation/MAgent" -> "Farama-Foundation/AutoROM"
"Farama-Foundation/MAgent" -> "mlii/mfrl"
"Farama-Foundation/MAgent" -> "uoe-agents/seps"
"Farama-Foundation/MAgent" -> "Farama-Foundation/SuperSuit" ["e"=1]
"Farama-Foundation/MAgent" -> "geek-ai/MAgent"
"Farama-Foundation/MAgent" -> "sjtu-marl/malib"
"m4ttsch/omscs-notes-notes" -> "stevenxchung/OMSCS-Notes"
"m4ttsch/omscs-notes-notes" -> "m4ttsch/omscs-notes"
"araffin/rl-tutorial-jnrr19" -> "Stable-Baselines-Team/rl-colab-notebooks"
"araffin/rl-tutorial-jnrr19" -> "DLR-RM/rl-baselines3-zoo"
"araffin/rl-tutorial-jnrr19" -> "araffin/rl-baselines-zoo"
"araffin/rl-tutorial-jnrr19" -> "hill-a/stable-baselines"
"araffin/rl-tutorial-jnrr19" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"araffin/rl-tutorial-jnrr19" -> "DLR-RM/stable-baselines3"
"araffin/rl-tutorial-jnrr19" -> "HumanCompatibleAI/imitation"
"araffin/rl-tutorial-jnrr19" -> "Stable-Baselines-Team/stable-baselines"
"araffin/rl-tutorial-jnrr19" -> "qgallouedec/panda-gym" ["e"=1]
"araffin/rl-tutorial-jnrr19" -> "rlworkgroup/garage"
"araffin/rl-tutorial-jnrr19" -> "Denys88/rl_games" ["e"=1]
"araffin/rl-tutorial-jnrr19" -> "Farama-Foundation/PettingZoo"
"araffin/rl-tutorial-jnrr19" -> "facebookresearch/mbrl-lib" ["e"=1]
"araffin/rl-tutorial-jnrr19" -> "openai/safety-gym" ["e"=1]
"araffin/rl-tutorial-jnrr19" -> "vwxyzjn/cleanrl"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "Kchu/DeepRL_PyTorch"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "BY571/IQN-and-Extensions"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "microsoft/FQF"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "xtma/dsac"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "johannah/bootstrap_dqn"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "BY571/FQF-and-Extensions"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "BY571/QR-DQN"
"toshikwa/fqf-iqn-qrdqn.pytorch" -> "deligentfool/dqn_zoo"
"qian18long/epciclr2020" -> "jiayu-ch15/Variational-Automatic-Curriculum-Learning"
"facebookresearch/CollaQ" -> "TonghanWang/ROMA"
"facebookresearch/CollaQ" -> "TonghanWang/DOP"
"facebookresearch/CollaQ" -> "oxwhirl/wqmix"
"facebookresearch/CollaQ" -> "MAS-anony/ASN"
"facebookresearch/CollaQ" -> "isp1tze/MAProj"
"starry-sky6688/MADDPG" -> "shariqiqbal2810/maddpg-pytorch"
"starry-sky6688/MADDPG" -> "Lizhi-sjtu/MARL-code-pytorch"
"starry-sky6688/MADDPG" -> "xuehy/pytorch-maddpg"
"starry-sky6688/MADDPG" -> "starry-sky6688/MARL-Algorithms"
"starry-sky6688/MADDPG" -> "DKuan/MADDPG_torch"
"starry-sky6688/MADDPG" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"starry-sky6688/MADDPG" -> "openai/maddpg"
"starry-sky6688/MADDPG" -> "shariqiqbal2810/MAAC"
"starry-sky6688/MADDPG" -> "tinyzqh/light_mappo"
"starry-sky6688/MADDPG" -> "marlbenchmark/off-policy"
"starry-sky6688/MADDPG" -> "marlbenchmark/on-policy"
"starry-sky6688/MADDPG" -> "Git-123-Hub/maddpg-pettingzoo-pytorch"
"starry-sky6688/MADDPG" -> "sisl/MADRL"
"starry-sky6688/MADDPG" -> "uoe-agents/epymarl"
"starry-sky6688/MADDPG" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"lweitkamp/option-critic-pytorch" -> "jeanharb/option_critic"
"lweitkamp/option-critic-pytorch" -> "alversafa/option-critic-arch"
"lweitkamp/option-critic-pytorch" -> "lweitkamp/feudalnets-pytorch"
"lweitkamp/option-critic-pytorch" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"lweitkamp/option-critic-pytorch" -> "mklissa/PPOC"
"lweitkamp/option-critic-pytorch" -> "jeanharb/a2oc_delib"
"lweitkamp/option-critic-pytorch" -> "kngwyu/Rainy"
"lweitkamp/option-critic-pytorch" -> "tdavchev/option-critic"
"lweitkamp/feudalnets-pytorch" -> "dnddnjs/feudal-montezuma"
"AmazingAng/WTF-DeepRL" -> "kaixindelele/DRLib"
"AmazingAng/WTF-DeepRL" -> "gxywy/rl-plotter"
"AmazingAng/WTF-DeepRL" -> "jmichaux/dqn-pytorch" ["e"=1]
"AmazingAng/WTF-DeepRL" -> "denisyarats/pytorch_sac_ae" ["e"=1]
"AmazingAng/WTF-DeepRL" -> "feidieufo/homework"
"AmazingAng/WTF-DeepRL" -> "BY571/DQN-Atari-Agents"
"AmazingAng/WTF-DeepRL" -> "pranz24/pytorch-soft-actor-critic"
"cyoon1729/distributedRL" -> "jingweiz/pytorch-distributed"
"cyoon1729/distributedRL" -> "neka-nat/distributed_rl"
"cyoon1729/distributedRL" -> "younggyoseo/Ape-X"
"cyoon1729/distributedRL" -> "opherlieber/rltime"
"tallamjr/stanford-cs234" -> "ksang/cs234-assignments"
"tallamjr/stanford-cs234" -> "changebo/CS234-2020"
"tallamjr/stanford-cs234" -> "Huixxi/CS234-Reinforcement-Learning-Winter-2019"
"tallamjr/stanford-cs234" -> "florist-notes/CS234_RL"
"Mauriyin/FLDRL-in-Wireless-Communication" -> "PeymanTehrani/FDRL-PC-Dyspan"
"Mauriyin/FLDRL-in-Wireless-Communication" -> "siomvas/awesome-federated-reinforcement-learning"
"Mauriyin/FLDRL-in-Wireless-Communication" -> "YidingYu/DLMA" ["e"=1]
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/2048-AI"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/Hill-Climb-Racing-AI"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/TheBigCB.com"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/NEAT-Template-JavaScript"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/WebsiteTest"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/WorldsHardestGameAI"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/Car-QLearning"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/NEAT_Template"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/Chess-AI"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/Flappy-Bird-AI"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/Tetris-AI-Javascript" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Jingliang-Duan/DSAC-v1" -> "Jingliang-Duan/DSAC-v2"
"Jingliang-Duan/DSAC-v1" -> "xtma/dsac"
"Jingliang-Duan/DSAC-v1" -> "idthanm/mpg"
"CherryPieSexy/imitation_learning" -> "Kaixhin/imitation-learning"
"openai/kubernetes" -> "openai/ceph-chef"
"PacktPublishing/Deep-Reinforcement-Learning-with-Python" -> "Apress/deep-reinforcement-learning-python"
"PacktPublishing/Deep-Reinforcement-Learning-with-Python" -> "PacktPublishing/Mastering-Reinforcement-Learning-with-Python"
"PhilS94/Unity-Procedural-IK-Wall-Walking-Spider" -> "Sopiro/Unity-Procedural-Animation"
"PhilS94/Unity-Procedural-IK-Wall-Walking-Spider" -> "timi-ty/procedural-animation"
"PhilS94/Unity-Procedural-IK-Wall-Walking-Spider" -> "lchaumartin/SpiderProceduralAnimation"
"AnujMahajanOxf/MAVEN" -> "IouJenLiu/CMAE"
"AnujMahajanOxf/MAVEN" -> "yalidu/liir"
"AnujMahajanOxf/MAVEN" -> "Sonkyunghwan/QTRAN"
"AnujMahajanOxf/MAVEN" -> "saizhang0218/TMC"
"TonghanWang/NDQ" -> "TonghanWang/ROMA"
"AutodeskRoboticsLab/RLRoboticAssembly" -> "hzm2016/Peg_in_hole_assembly"
"jeongeun980906/peg-in-hole" -> "DengYuelin/vrep_peg_in_hole"
"jeongeun980906/peg-in-hole" -> "guodashun/peg-in-hole-gym"
"arrival-ltd/catalyst-rl-tutorial" -> "guodashun/peg-in-hole-gym"
"arrival-ltd/catalyst-rl-tutorial" -> "hzm2016/Peg_in_hole_assembly"
"arrival-ltd/catalyst-rl-tutorial" -> "SongleChen2015/Peg_in_hole_assembly"
"arrival-ltd/catalyst-rl-tutorial" -> "robot0102/rl-peg-in-hole-assembly-webots"
"arrival-ltd/catalyst-rl-tutorial" -> "jeongeun980906/peg-in-hole"
"arrival-ltd/catalyst-rl-tutorial" -> "xieliang555/SFN"
"arrival-ltd/catalyst-rl-tutorial" -> "Scitator/catalyst-rl-framework"
"arrival-ltd/catalyst-rl-tutorial" -> "The132/franka-panda"
"arrival-ltd/catalyst-rl-tutorial" -> "DengYuelin/vrep_peg_in_hole"
"arrival-ltd/catalyst-rl-tutorial" -> "jhaardt/wrs" ["e"=1]
"arrival-ltd/catalyst-rl-tutorial" -> "Henry1iu/ierg5350_rl_course_project"
"arrival-ltd/catalyst-rl-tutorial" -> "zanebin/UR5-manipulator-control"
"manikandan-ravikiran/HCI_Notes" -> "jgajera/CS6750-Human-Computer-Interaction"
"manikandan-ravikiran/HCI_Notes" -> "stevenxchung/OMSCS-Notes"
"ksang/cs234-assignments" -> "changebo/CS234-2020"
"timi-ty/procedural-animation" -> "BattleDawnNZ/ProceduralAnimation"
"berkeleydeeprlcourse/homework_fall2020" -> "berkeleydeeprlcourse/homework_fall2019"
"berkeleydeeprlcourse/homework_fall2020" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"berkeleydeeprlcourse/homework_fall2020" -> "mdeib/berkeley-deep-RL-pytorch-solutions"
"berkeleydeeprlcourse/homework_fall2020" -> "berkeleydeeprlcourse/homework_fall2021"
"berkeleydeeprlcourse/homework_fall2020" -> "erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch"
"berkeleydeeprlcourse/homework_fall2020" -> "mdeib/berkeley-deep-RL-pytorch-starter"
"berkeleydeeprlcourse/homework_fall2020" -> "berkeleydeeprlcourse/homework"
"berkeleydeeprlcourse/homework_fall2019" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"berkeleydeeprlcourse/homework_fall2019" -> "berkeleydeeprlcourse/homework_fall2020"
"berkeleydeeprlcourse/homework_fall2019" -> "berkeleydeeprlcourse/homework"
"berkeleydeeprlcourse/homework_fall2019" -> "silencial/DeepRL"
"huawei-noah/xingtian" -> "huawei-noah/SMARTS" ["e"=1]
"huawei-noah/xingtian" -> "sjtu-marl/malib"
"huawei-noah/xingtian" -> "tencent-ailab/TLeague"
"huawei-noah/xingtian" -> "StepNeverStop/RLs"
"huawei-noah/xingtian" -> "ying-wen/malib_deprecated"
"huawei-noah/xingtian" -> "tencent-ailab/tleague_projpage"
"huawei-noah/xingtian" -> "sail-sg/envpool" ["e"=1]
"huawei-noah/xingtian" -> "google-research/seed_rl"
"huawei-noah/xingtian" -> "google-deepmind/scalable_agent"
"isp1tze/MAProj" -> "facebookresearch/CollaQ"
"isp1tze/MAProj" -> "0b01/CommNet"
"watakandai/hiro_pytorch" -> "ziangqin-stu/rl_hiro"
"watakandai/hiro_pytorch" -> "nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch"
"watakandai/hiro_pytorch" -> "lweitkamp/feudalnets-pytorch"
"openai/train-procgen" -> "openai/gym3"
"openai/train-procgen" -> "pokaxpoka/netrand"
"openai/train-procgen" -> "openai/procgen"
"openai/train-procgen" -> "openai/coinrun"
"openai/train-procgen" -> "AIcrowd/neurips2020-procgen-starter-kit"
"gxywy/rl-plotter" -> "mantle2048/rlplot"
"gxywy/rl-plotter" -> "kaixindelele/DRLib"
"gxywy/rl-plotter" -> "lich14/CDS"
"gxywy/rl-plotter" -> "cyanrain7/TRPO-in-MARL"
"gxywy/rl-plotter" -> "sjtu-marl/malib"
"LxzGordon/Deep-Reinforcement-Learning-with-pytorch" -> "gouxiangchen/dueling-DQN-pytorch"
"PacktPublishing/Mastering-Reinforcement-Learning-with-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-with-Python"
"kimbring2/minecraft_ai" -> "chscheller/minerl_agent"
"kimbring2/minecraft_ai" -> "cog-isa/forger"
"erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch" -> "xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning"
"AndyYue1893/Deep-reinforcement-learning-with-pytorch" -> "AndyYue1893/Reinforcement-learning-with-tensorflow"
"AndyYue1893/Deep-reinforcement-learning-with-pytorch" -> "AndyYue1893/Hands-On-Reinforcement-Learning-With-Python"
"BY571/Soft-Actor-Critic-and-Extensions" -> "denisyarats/pytorch_sac_ae" ["e"=1]
"BY571/Soft-Actor-Critic-and-Extensions" -> "denisyarats/pytorch_sac" ["e"=1]
"BY571/Soft-Actor-Critic-and-Extensions" -> "pranz24/pytorch-soft-actor-critic"
"BY571/Soft-Actor-Critic-and-Extensions" -> "Jonathan-Pearce/DDPG_PER"
"BY571/Soft-Actor-Critic-and-Extensions" -> "BY571/Munchausen-RL"
"BY571/Soft-Actor-Critic-and-Extensions" -> "BY571/DQN-Atari-Agents"
"BY571/Soft-Actor-Critic-and-Extensions" -> "quantumiracle/Popular-RL-Algorithms"
"BY571/Soft-Actor-Critic-and-Extensions" -> "toshikwa/sac-discrete.pytorch" ["e"=1]
"BY571/Soft-Actor-Critic-and-Extensions" -> "toshikwa/soft-actor-critic.pytorch" ["e"=1]
"BY571/Soft-Actor-Critic-and-Extensions" -> "shariqiqbal2810/MAAC"
"vi3itor/GameplayFootball" -> "BazkieBumpercar/GameplayFootball"
"vi3itor/GameplayFootball" -> "BazkieBumpercar/Blunted2"
"vi3itor/GameplayFootball" -> "atas76/openengine"
"muupan/dqn-in-the-caffe" -> "mhauskn/dqn"
"muupan/dqn-in-the-caffe" -> "kristjankorjus/Replicating-DeepMind"
"muupan/dqn-in-the-caffe" -> "spragunr/deep_q_rl"
"muupan/dqn-in-the-caffe" -> "brian473/neural_rl"
"muupan/dqn-in-the-caffe" -> "muupan/async-rl"
"muupan/dqn-in-the-caffe" -> "sailorsb/caffe-parallel" ["e"=1]
"muupan/dqn-in-the-caffe" -> "5vision/DARQN"
"muupan/dqn-in-the-caffe" -> "aiworld/dqn"
"wendelinboehmer/dcg" -> "sisl/DICG"
"wendelinboehmer/dcg" -> "TonghanWang/ROMA"
"aronsar/hoad" -> "rocanaan/hanabi-ad-hoc-learning"
"iamjakewarner/jdf" -> "hiive/mlrose"
"iamjakewarner/jdf" -> "CuriousLearner/jdf-latex"
"iamjakewarner/jdf" -> "jlm429/bettermdptools"
"iamjakewarner/jdf" -> "jgajera/CS6750-Human-Computer-Interaction"
"stevens-cs546-cs554/CS-554" -> "stevens-cs546-cs554/CS-546"
"minerllabs/competition_submission_template" -> "minerllabs/baselines"
"minerllabs/competition_submission_template" -> "manuelsh/minerl-docker"
"MichalOp/MineRL2020" -> "amiranas/minerl_imitation_learning"
"robot0102/rl-peg-in-hole-assembly-webots" -> "SongleChen2015/Peg_in_hole_assembly"
"robot0102/rl-peg-in-hole-assembly-webots" -> "DengYuelin/vrep_peg_in_hole"
"robot0102/rl-peg-in-hole-assembly-webots" -> "The132/franka-panda"
"alex-petrenko/megaverse" -> "shacklettbp/bps-nav"
"alex-petrenko/megaverse" -> "shacklettbp/bps3D"
"pytorch/workshops" -> "hiive/mlrose"
"KamyarGh/rl_swiss" -> "twni2016/f-IRL"
"twni2016/f-IRL" -> "LucasCJYSDL/HierAIRL"
"twni2016/f-IRL" -> "SaminYeasar/Off_Policy_Adversarial_Inverse_Reinforcement_Learning"
"twni2016/f-IRL" -> "HuangJiaLian/AIRL_MountainCar"
"twni2016/f-IRL" -> "KamyarGh/rl_swiss"
"philtabor/Actor-Critic-Methods-Paper-To-Code" -> "philtabor/Deep-Q-Learning-Paper-To-Code"
"philtabor/Actor-Critic-Methods-Paper-To-Code" -> "philtabor/Reinforcement-Learning-In-Motion"
"philtabor/Actor-Critic-Methods-Paper-To-Code" -> "philtabor/Advanced-Replay-Strategies"
"philtabor/Actor-Critic-Methods-Paper-To-Code" -> "philtabor/Advanced-Actor-Critic-Methods"
"philtabor/Actor-Critic-Methods-Paper-To-Code" -> "philtabor/Youtube-Code-Repository"
"kestasjk/webDiplomacy" -> "Sleepcap/vDiplomacy"
"kestasjk/webDiplomacy" -> "diplomacy/diplomacy"
"kestasjk/webDiplomacy" -> "diplomacy/research"
"kestasjk/webDiplomacy" -> "spamguy/dipl.io"
"siemanko/guided-policy-search" -> "marcino239/gps"
"siemanko/guided-policy-search" -> "robotsorcerer/gps"
"google-deepmind/spriteworld" -> "google-deepmind/bsuite"
"google-deepmind/spriteworld" -> "google-deepmind/multi_object_datasets" ["e"=1]
"google-deepmind/spriteworld" -> "google-deepmind/hanabi-learning-environment"
"google-deepmind/spriteworld" -> "google-deepmind/open_spiel"
"analoganddigital/sekiro_tensorflow" -> "analoganddigital/DQN_play_sekiro"
"analoganddigital/sekiro_tensorflow" -> "ricagj/pysekiro_with_RL"
"stevenxchung/OMSCS-Notes" -> "jgajera/CS6750-Human-Computer-Interaction"
"stevenxchung/OMSCS-Notes" -> "m4ttsch/omscs-notes-notes"
"stevenxchung/OMSCS-Notes" -> "manikandan-ravikiran/HCI_Notes"
"stevenxchung/OMSCS-Notes" -> "valsamovich/omscs"
"hahayonghuming/VDACs" -> "simsimiSION/pymarl-algorithm-extension-via-starcraft"
"deligentfool/dqn_zoo" -> "deligentfool/policy_based_RL"
"deligentfool/dqn_zoo" -> "xtma/dsac"
"Code-Bullet/TheBigCB.com" -> "Code-Bullet/2048-AI"
"Code-Bullet/TheBigCB.com" -> "Code-Bullet/Tetris-AI-Javascript"
"Code-Bullet/TheBigCB.com" -> "Code-Bullet/minesweeper-AI"
"Code-Bullet/TheBigCB.com" -> "Code-Bullet/MarbleCalculator"
"Code-Bullet/TheBigCB.com" -> "Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing"
"Code-Bullet/TheBigCB.com" -> "Code-Bullet/Storm-The-House-Auto-Clicker"
"Code-Bullet/TheBigCB.com" -> "Code-Bullet/Chess-AI"
"XiaoxiaoGuo/rcdqn" -> "tesslerc/Sparse-IL"
"XiaoxiaoGuo/rcdqn" -> "microsoft/tdqn"
"XiaoxiaoGuo/rcdqn" -> "princeton-nlp/XTX"
"PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook" -> "abhisheksuran/Reinforcement_Learning"
"facebookresearch/impact-driven-exploration" -> "yfletberliac/adversarially-guided-actor-critic"
"facebookresearch/impact-driven-exploration" -> "tianjunz/NovelD"
"facebookresearch/impact-driven-exploration" -> "facebookresearch/e3b"
"facebookresearch/impact-driven-exploration" -> "sparisi/cbet"
"facebookresearch/impact-driven-exploration" -> "daochenzha/rapid"
"facebookresearch/impact-driven-exploration" -> "facebookresearch/adversarially-motivated-intrinsic-goals"
"facebookresearch/impact-driven-exploration" -> "tedmoskovitz/TOP"
"saizhang0218/VBC" -> "saizhang0218/TMC"
"saizhang0218/VBC" -> "rhoowd/sched_net"
"Tvtig/UnityLightsaber" -> "KristinLague/Mesh-Cutting"
"Tvtig/UnityLightsaber" -> "sergioabreu-g/active-ragdolls"
"IBM/commonsense-rl" -> "IBM/LOA" ["e"=1]
"IBM/commonsense-rl" -> "microsoft/tdqn"
"IBM/commonsense-rl" -> "xingdi-eric-yuan/GATA-public"
"IouJenLiu/PIC" -> "IouJenLiu/HTS-RL"
"IouJenLiu/PIC" -> "IouJenLiu/CMAE"
"TonghanWang/ROMA" -> "TonghanWang/RODE"
"TonghanWang/ROMA" -> "shariqiqbal2810/REFIL"
"TonghanWang/ROMA" -> "wendelinboehmer/dcg"
"TonghanWang/ROMA" -> "TonghanWang/NDQ"
"TonghanWang/ROMA" -> "wjh720/QPLEX"
"TonghanWang/ROMA" -> "TonghanWang/DOP"
"TonghanWang/ROMA" -> "facebookresearch/CollaQ"
"TonghanWang/ROMA" -> "oxwhirl/wqmix"
"TonghanWang/ROMA" -> "schroederdewitt/multiagent_mujoco"
"TonghanWang/ROMA" -> "QDPP-GitHub/QDPP"
"TonghanWang/ROMA" -> "AnujMahajanOxf/MAVEN"
"TonghanWang/ROMA" -> "Sonkyunghwan/QTRAN"
"TonghanWang/ROMA" -> "oxwhirl/smacv2"
"toshikwa/rljax" -> "henry-prior/jax-rl"
"leox1v/FirstTextWorldProblems" -> "pvl/CogniTextWorldAgent"
"caipeide/drift_drl" -> "kanakkabara/Autonomous-Drifting"
"caipeide/drift_drl" -> "karanamrahul/Autonomous-Drifting-using-deep-Reinforcement-Learning"
"caipeide/drift_drl" -> "angloth/auto-drift"
"caipeide/drift_drl" -> "mcemilg/min-carla-env"
"caipeide/drift_drl" -> "YoungYoung619/reinforcement-learning-based-driving-decision-in-Carla" ["e"=1]
"caipeide/drift_drl" -> "caipeide/VTGNet"
"wjh720/QPLEX" -> "TonghanWang/RODE"
"wjh720/QPLEX" -> "oxwhirl/wqmix"
"wjh720/QPLEX" -> "Sonkyunghwan/QTRAN"
"wjh720/QPLEX" -> "lich14/CDS"
"wjh720/QPLEX" -> "TonghanWang/ROMA"
"wjh720/QPLEX" -> "hahayonghuming/VDACs"
"openai/gym3" -> "openai/train-procgen"
"hiive/mlrose" -> "hiive/hiivemdptoolbox"
"hiive/mlrose" -> "gkhayes/mlrose"
"hiive/mlrose" -> "jlm429/bettermdptools"
"hiive/mlrose" -> "jlm429/pyperch"
"hiive/mlrose" -> "knakamura13/mlrose-ky"
"idthanm/mpg" -> "idthanm/env_build"
"gouxiangchen/soft-Q-learning" -> "seolhokim/InverseRL-Pytorch"
"Farama-Foundation/hanabi-learning-environment" -> "Farama-Foundation/Multi-Agent-ALE"
"greerviau/TetrisAI" -> "knagaitsev/tetris-ai"
"guodashun/peg-in-hole-gym" -> "jeongeun980906/peg-in-hole"
"guodashun/peg-in-hole-gym" -> "xieliang555/SFN"
"guodashun/peg-in-hole-gym" -> "robot0102/rl-peg-in-hole-assembly-webots"
"breeku/opentrackmania" -> "The-Firexx/trackmania2020apidocumentation"
"ermongroup/MetaIRL" -> "ahq1993/inverse_rl"
"ermongroup/MetaIRL" -> "ermongroup/MA-AIRL"
"Farama-Foundation/AutoROM" -> "Farama-Foundation/Multi-Agent-ALE"
"Farama-Foundation/AutoROM" -> "Farama-Foundation/hanabi-learning-environment"
"Farama-Foundation/Multi-Agent-ALE" -> "Farama-Foundation/hanabi-learning-environment"
"Farama-Foundation/Multi-Agent-ALE" -> "Farama-Foundation/AutoROM"
"mohamedameen93/CS-7642-Reinforcement-Learning-Notes" -> "mohamedameen93/CS-7641-Machine-Learning-Notes"
"BattleDawnNZ/ProceduralAnimation" -> "timi-ty/procedural-animation"
"BattleDawnNZ/ProceduralAnimation" -> "Sopiro/Unity-Procedural-Animation"
"BattleDawnNZ/ProceduralAnimation" -> "lchaumartin/SpiderProceduralAnimation"
"shariqiqbal2810/REFIL" -> "Cranial-XIX/marl-copa"
"shariqiqbal2810/REFIL" -> "TonghanWang/ROMA"
"shariqiqbal2810/REFIL" -> "zyfsjycc/GoMARL"
"rajammanabrolu/KG-A2C" -> "XiaoxiaoGuo/rcdqn"
"rajammanabrolu/KG-A2C" -> "princeton-nlp/XTX"
"pokaxpoka/sunrise" -> "johannah/bootstrap_dqn"
"chscheller/minerl_agent" -> "Miffyli/minecraft-bc"
"changebo/CS234-2020" -> "arowdy98/Stanford-CS234"
"changebo/CS234-2020" -> "pranav-s/Stanford_CS234_RL_2021"
"openai/prometheus" -> "openai/robot_controllers"
"mbaske/ml-dogfight" -> "mbaske/ml-eve"
"amiranas/minerl_imitation_learning" -> "MichalOp/MineRL2020"
"amiranas/minerl_imitation_learning" -> "chscheller/minerl_agent"
"Henry1iu/ierg5350_rl_course_project" -> "stanford-iprl-lab/multimodal_representation"
"princeton-nlp/calm-textgame" -> "princeton-nlp/blindfold-textgame"
"princeton-nlp/calm-textgame" -> "XiaoxiaoGuo/rcdqn"
"princeton-nlp/calm-textgame" -> "rajammanabrolu/KG-A2C"
"RITCHIEHuang/MAGAIL" -> "wsjeon/multiagent-gail"
"RITCHIEHuang/MAGAIL" -> "ermongroup/multiagent-gail"
"RITCHIEHuang/MAGAIL" -> "yangmuzhi/airl"
"MadryLab/implementation-matters" -> "implementation-matters/code-for-paper"
"yalidu/liir" -> "AnujMahajanOxf/MAVEN"
"henry-prior/jax-rl" -> "toshikwa/rljax"
"henry-prior/jax-rl" -> "bmazoure/ppo_jax"
"henry-prior/jax-rl" -> "ethanluoyc/magi"
"cuhkrlcourse/ierg6130-assignment" -> "ucla-rlcourse/DeepRL-Tutorials"
"cuhkrlcourse/ierg6130-assignment" -> "dayekuaipao/ierg6130-assignment"
"cuhkrlcourse/ierg6130-assignment" -> "Aguin/cuhkrlcourse-ierg6130"
"cuhkrlcourse/ierg6130-assignment" -> "ucla-rlcourse/RLexample"
"createamind/Distributed-DRL" -> "LiuShuai26/Distributed-RL"
"mhauskn/dqn" -> "iassael/torch-bootstrapped-dqn"
"mhauskn/dqn" -> "muupan/dqn-in-the-caffe"
"rjtobin/HanSim" -> "Quuxplusone/Hanabi"
"stanford-iprl-lab/multimodal_representation" -> "Henry1iu/ierg5350_rl_course_project"
"datamllab/rlcard-tutorial" -> "daochenzha/Meta-AAD"
"datamllab/rlcard-tutorial" -> "datamllab/rlcard-showdown"
"datamllab/rlcard-tutorial" -> "daochenzha/rapid"
"rohitrango/BC-regularized-GAIL" -> "sen-pai/InfoGAIL"
"xingdi-eric-yuan/qait_public" -> "tesslerc/Sparse-IL"
"implementation-matters/code-for-paper" -> "MadryLab/implementation-matters"
"YangRui2015/Sparse-Reward-Algorithms" -> "YangRui2015/Modular_HER"
"chenhongge/SA_DQN" -> "tuomaso/radial_rl_v2"
"chenhongge/SA_DQN" -> "chenhongge/StateAdvDRL"
"Miffyli/minecraft-bc" -> "chscheller/minerl_agent"
"mbaske/ml-hover-bike-race" -> "mbaske/ml-eve"
"mbaske/ml-table-football" -> "mbaske/ml-eve"
"HuangJiaLian/AIRL_MountainCar" -> "LucasCJYSDL/HierAIRL"
"HuangJiaLian/AIRL_MountainCar" -> "twni2016/f-IRL"
"dsbrown1331/CoRL2019-DREX" -> "CORE-Robotics-Lab/SSRR"
"dsbrown1331/CoRL2019-DREX" -> "kristery/Imitation-Learning-from-Imperfect-Demonstration"
"microsoft/tdqn" -> "tesslerc/Sparse-IL"
"microsoft/tdqn" -> "XiaoxiaoGuo/rcdqn"
"microsoft/tdqn" -> "cognitiveailab/neurosymbolic"
"wetliu/dqn_pytorch" -> "lukeluocn/dqn-breakout"
"pvl/CogniTextWorldAgent" -> "tesslerc/Sparse-IL"
"mbaske/ml-simple-driver" -> "mbaske/ml-eve"
"TroddenSpade/Samurai-theme-vscode" -> "TroddenSpade/LoAX"
"TroddenSpade/Samurai-theme-vscode" -> "TroddenSpade/Judge-Cpp"
"TroddenSpade/Samurai-theme-vscode" -> "TroddenSpade/GAN-NST"
"snixtho/clifga" -> "EvoEsports/gbxclient-node"
"snixtho/clifga" -> "reaby/TMModeTemplate"
"google-deepmind/meltingpot" -> "eugenevinitsky/sequential_social_dilemma_games"
"google-deepmind/meltingpot" -> "FLAIROx/JaxMARL" ["e"=1]
"google-deepmind/meltingpot" -> "facebookresearch/BenchMARL"
"google-deepmind/meltingpot" -> "google-deepmind/lab2d"
"google-deepmind/meltingpot" -> "google-deepmind/concordia"
"google-deepmind/meltingpot" -> "Farama-Foundation/PettingZoo"
"google-deepmind/meltingpot" -> "instadeepai/Mava"
"google-deepmind/meltingpot" -> "koulanurag/ma-gym"
"google-deepmind/meltingpot" -> "rstrivedi/Melting-Pot-Contest-2023"
"google-deepmind/meltingpot" -> "uoe-agents/epymarl"
"google-deepmind/meltingpot" -> "instadeepai/jumanji" ["e"=1]
"google-deepmind/meltingpot" -> "proroklab/VectorizedMultiAgentSimulator"
"google-deepmind/meltingpot" -> "Replicable-MARL/MARLlib"
"google-deepmind/meltingpot" -> "HumanCompatibleAI/overcooked_ai"
"google-deepmind/meltingpot" -> "Farama-Foundation/MAgent2"
"marlbenchmark/on-policy" -> "tinyzqh/light_mappo"
"marlbenchmark/on-policy" -> "marlbenchmark/off-policy"
"marlbenchmark/on-policy" -> "starry-sky6688/MARL-Algorithms"
"marlbenchmark/on-policy" -> "oxwhirl/pymarl"
"marlbenchmark/on-policy" -> "oxwhirl/smac"
"marlbenchmark/on-policy" -> "Replicable-MARL/MARLlib"
"marlbenchmark/on-policy" -> "PKU-MARL/HARL"
"marlbenchmark/on-policy" -> "uoe-agents/epymarl"
"marlbenchmark/on-policy" -> "Lizhi-sjtu/MARL-code-pytorch"
"marlbenchmark/on-policy" -> "hijkzzz/pymarl2"
"marlbenchmark/on-policy" -> "PKU-MARL/Multi-Agent-Transformer"
"marlbenchmark/on-policy" -> "openai/maddpg"
"marlbenchmark/on-policy" -> "Farama-Foundation/PettingZoo"
"marlbenchmark/on-policy" -> "shariqiqbal2810/maddpg-pytorch"
"marlbenchmark/on-policy" -> "openai/multiagent-particle-envs"
"marlbenchmark/off-policy" -> "marlbenchmark/on-policy"
"marlbenchmark/off-policy" -> "Lizhi-sjtu/MARL-code-pytorch"
"marlbenchmark/off-policy" -> "tinyzqh/light_mappo"
"marlbenchmark/off-policy" -> "starry-sky6688/MARL-Algorithms"
"marlbenchmark/off-policy" -> "hijkzzz/pymarl2"
"marlbenchmark/off-policy" -> "Replicable-MARL/MARLlib"
"marlbenchmark/off-policy" -> "uoe-agents/epymarl"
"marlbenchmark/off-policy" -> "PKU-MARL/Multi-Agent-Transformer"
"marlbenchmark/off-policy" -> "PKU-MARL/HARL"
"marlbenchmark/off-policy" -> "shariqiqbal2810/maddpg-pytorch"
"marlbenchmark/off-policy" -> "oxwhirl/pymarl"
"marlbenchmark/off-policy" -> "starry-sky6688/MADDPG"
"marlbenchmark/off-policy" -> "cyanrain7/TRPO-in-MARL"
"marlbenchmark/off-policy" -> "shariqiqbal2810/MAAC"
"marlbenchmark/off-policy" -> "yangchen1997/Multi-Agent-Reinforcement-Learning"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "kwai/DouZero"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "Vincentzyx/DouZero_For_HLDDZ_FullAuto"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "datamllab/rlcard" ["e"=1]
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "greycodee/wechat-backup" ["e"=1]
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "Tencent/GameAISDK" ["e"=1]
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "FengQuanLi/WZCQ"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "datamllab/rlcard-showdown"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "Turing-Project/WriteGPT"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "Netease-Games-AI-Lab-Guangzhou/PerfectDou"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "yqchilde/JDMemberCloseAccount" ["e"=1]
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "official-pikafish/Pikafish" ["e"=1]
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "FengQuanLi/ResnetGPT"
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "ainilili/ratel" ["e"=1]
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "xfangfang/Macast" ["e"=1]
"tianqiraf/DouZero_For_HappyDouDiZhu" -> "OpenEthan/SMSBoom" ["e"=1]
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "spragunr/deep_q_rl"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "tambetm/simple_dqn"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "devsisters/DQN-tensorflow"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "siemanko/tensorflow-deepq"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "Farama-Foundation/Arcade-Learning-Environment"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "rll/rllab"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "kristjankorjus/Replicating-DeepMind"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "miyosuda/async_deep_reinforce"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "nikitasrivatsan/DeepLearningVideoGames"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "carpedm20/deep-rl-tensorflow"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "junhyukoh/deep-reinforcement-learning-papers"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "muupan/deep-reinforcement-learning-papers"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "Element-Research/rnn" ["e"=1]
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "openai/universe-starter-agent"
"kuz/DeepMind-Atari-Deep-Q-Learner" -> "Kaixhin/Atari"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "tianqiraf/DouZero_For_HappyDouDiZhu"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "kwai/DouZero"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "Netease-Games-AI-Lab-Guangzhou/PerfectDou"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "Vincentzyx/Douzero_Resnet"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "EdwardPooh/douzero-resnet-2.0"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "datamllab/rlcard-showdown"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "Vincentzyx/VinXiangQi" ["e"=1]
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "datamllab/rlcard" ["e"=1]
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "datamllab/awesome-game-ai"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "official-pikafish/Pikafish" ["e"=1]
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "Equim-chan/Mortal" ["e"=1]
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "submit-paper/Doudizhu_plus"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "ZDZX-T/cardRecorder"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "RuBP17/AlphaDou"
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" -> "ZhouWeikuan/DouDiZhu"
"Baekalfen/PyBoy" -> "PWhiddy/PokemonRedExperiments"
"Baekalfen/PyBoy" -> "gbdev/awesome-gbdev" ["e"=1]
"Baekalfen/PyBoy" -> "mgba-emu/mgba" ["e"=1]
"Baekalfen/PyBoy" -> "HFO4/gameboy.live" ["e"=1]
"Baekalfen/PyBoy" -> "LIJI32/SameBoy" ["e"=1]
"Baekalfen/PyBoy" -> "openai/retro"
"Baekalfen/PyBoy" -> "kitao/pyxel" ["e"=1]
"Baekalfen/PyBoy" -> "marblexu/PythonPlantsVsZombies" ["e"=1]
"Baekalfen/PyBoy" -> "chrismaltby/gb-studio" ["e"=1]
"Baekalfen/PyBoy" -> "google-research/football"
"Baekalfen/PyBoy" -> "grantjenks/free-python-games" ["e"=1]
"Baekalfen/PyBoy" -> "Humpheh/goboy" ["e"=1]
"Baekalfen/PyBoy" -> "Farama-Foundation/Minigrid"
"Baekalfen/PyBoy" -> "DLR-RM/stable-baselines3"
"Baekalfen/PyBoy" -> "pret/pokered" ["e"=1]
"JohannesAck/tf2multiagentrl" -> "JohannesAck/MATD3implementation"
"JohannesAck/tf2multiagentrl" -> "ffelten/MASAC"
"JohannesAck/tf2multiagentrl" -> "marlbenchmark/off-policy"
"JohannesAck/tf2multiagentrl" -> "jaimeluengo/masac"
"kwai/DouZero" -> "tianqiraf/DouZero_For_HappyDouDiZhu"
"kwai/DouZero" -> "datamllab/rlcard" ["e"=1]
"kwai/DouZero" -> "Vincentzyx/DouZero_For_HLDDZ_FullAuto"
"kwai/DouZero" -> "datamllab/awesome-game-ai"
"kwai/DouZero" -> "datamllab/rlcard-showdown"
"kwai/DouZero" -> "facebookresearch/AugLy" ["e"=1]
"kwai/DouZero" -> "Netease-Games-AI-Lab-Guangzhou/PerfectDou"
"kwai/DouZero" -> "FengQuanLi/ResnetGPT"
"kwai/DouZero" -> "junxiaosong/AlphaZero_Gomoku" ["e"=1]
"kwai/DouZero" -> "google-deepmind/open_spiel"
"kwai/DouZero" -> "Turing-Project/WriteGPT"
"kwai/DouZero" -> "AI4Finance-Foundation/ElegantRL"
"kwai/DouZero" -> "thu-ml/tianshou"
"kwai/DouZero" -> "FengQuanLi/WZCQ"
"kwai/DouZero" -> "Farama-Foundation/PettingZoo"
"FengQuanLi/ResnetGPT" -> "FengQuanLi/WZCQ"
"FengQuanLi/ResnetGPT" -> "Tencent/GameAISDK" ["e"=1]
"FengQuanLi/ResnetGPT" -> "kwai/DouZero"
"FengQuanLi/ResnetGPT" -> "SamLynnEvans/Transformer" ["e"=1]
"FengQuanLi/ResnetGPT" -> "Turing-Project/WriteGPT"
"FengQuanLi/ResnetGPT" -> "tencent-ailab/hok_env"
"FengQuanLi/ResnetGPT" -> "Morizeyao/GPT2-Chinese" ["e"=1]
"FengQuanLi/ResnetGPT" -> "tianqiraf/DouZero_For_HappyDouDiZhu"
"FengQuanLi/ResnetGPT" -> "IrisRainbowNeko/genshin_auto_fish" ["e"=1]
"FengQuanLi/ResnetGPT" -> "alex-damian/pulse" ["e"=1]
"FengQuanLi/ResnetGPT" -> "tobyqin/kog-money" ["e"=1]
"FengQuanLi/ResnetGPT" -> "wangshub/RL-Stock" ["e"=1]
"FengQuanLi/ResnetGPT" -> "myBoris/wzry_ai"
"FengQuanLi/ResnetGPT" -> "openstf/minitouch" ["e"=1]
"FengQuanLi/ResnetGPT" -> "Wechat-ggGitHub/Awesome-GitHub-Repo" ["e"=1]
"indylab/nxdo" -> "JBLanier/pipeline-psro"
"indylab/nxdo" -> "npvoid/OnlineDoubleOracle"
"indylab/nxdo" -> "indylab/tabular_xdo"
"indylab/nxdo" -> "diversepsro/diverse_psro"
"analoganddigital/DQN_play_sekiro" -> "ricagj/train_your_own_game_AI"
"analoganddigital/DQN_play_sekiro" -> "analoganddigital/sekiro_tensorflow"
"analoganddigital/DQN_play_sekiro" -> "Turing-Project/Black-Myth-Wukong-AI"
"analoganddigital/DQN_play_sekiro" -> "ricagj/pysekiro_with_RL"
"analoganddigital/DQN_play_sekiro" -> "Skaiyin/DQN_play_blood"
"analoganddigital/DQN_play_sekiro" -> "ailec0623/DQN_HollowKnight"
"analoganddigital/DQN_play_sekiro" -> "ChenWendi2001/alpha-sekiro"
"analoganddigital/DQN_play_sekiro" -> "Helen-Cheung/RL-Sekiro"
"analoganddigital/DQN_play_sekiro" -> "BAAI-Agents/Cradle" ["e"=1]
"analoganddigital/DQN_play_sekiro" -> "AARG-FAN/Yolo_for_Wukong"
"analoganddigital/DQN_play_sekiro" -> "XR-stb/DQN_WUKONG"
"FengQuanLi/WZCQ" -> "FengQuanLi/ResnetGPT"
"FengQuanLi/WZCQ" -> "tencent-ailab/hok_env"
"FengQuanLi/WZCQ" -> "myBoris/wzry_ai"
"FengQuanLi/WZCQ" -> "Tencent/GameAISDK" ["e"=1]
"FengQuanLi/WZCQ" -> "kwai/DouZero"
"FengQuanLi/WZCQ" -> "Changanyue/AI-moba-game"
"FengQuanLi/WZCQ" -> "tianqiraf/DouZero_For_HappyDouDiZhu"
"FengQuanLi/WZCQ" -> "analoganddigital/DQN_play_sekiro"
"FengQuanLi/WZCQ" -> "tobyqin/kog-money" ["e"=1]
"FengQuanLi/WZCQ" -> "wangshub/RL-Stock" ["e"=1]
"FengQuanLi/WZCQ" -> "ricagj/train_your_own_game_AI"
"FengQuanLi/WZCQ" -> "linyiLYi/street-fighter-ai" ["e"=1]
"FengQuanLi/WZCQ" -> "Vincentzyx/DouZero_For_HLDDZ_FullAuto"
"FengQuanLi/WZCQ" -> "BAAI-Agents/Cradle" ["e"=1]
"FengQuanLi/WZCQ" -> "boyu-ai/Hands-on-RL"
"boyu-ai/Hands-on-RL" -> "datawhalechina/easy-rl"
"boyu-ai/Hands-on-RL" -> "wangshusen/DRL"
"boyu-ai/Hands-on-RL" -> "XinJingHao/DRL-Pytorch"
"boyu-ai/Hands-on-RL" -> "MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning"
"boyu-ai/Hands-on-RL" -> "DeepRLChinese/DeepRL-Chinese"
"boyu-ai/Hands-on-RL" -> "AI4Finance-Foundation/ElegantRL"
"boyu-ai/Hands-on-RL" -> "DLR-RM/stable-baselines3"
"boyu-ai/Hands-on-RL" -> "Lizhi-sjtu/DRL-code-pytorch"
"boyu-ai/Hands-on-RL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"boyu-ai/Hands-on-RL" -> "thu-ml/tianshou"
"boyu-ai/Hands-on-RL" -> "marlbenchmark/on-policy"
"boyu-ai/Hands-on-RL" -> "NeuronDance/DeepRL"
"boyu-ai/Hands-on-RL" -> "vwxyzjn/cleanrl"
"boyu-ai/Hands-on-RL" -> "opendilab/DI-engine" ["e"=1]
"boyu-ai/Hands-on-RL" -> "opendilab/PPOxFamily" ["e"=1]
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "starry-sky6688/MADDPG"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "Lizhi-sjtu/MARL-code-pytorch"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "shariqiqbal2810/maddpg-pytorch"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "DKuan/MADDPG_torch"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "xuehy/pytorch-maddpg"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "philtabor/Multi-Agent-Reinforcement-Learning"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "openai/maddpg"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "marlbenchmark/off-policy"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "reinshift/MADDPG_Multi_UAV_Roundup"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "shariqiqbal2810/MAAC"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "tinyzqh/light_mappo"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "fangvv/VN-MADDPG" ["e"=1]
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "JohannesAck/tf2multiagentrl"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "Git-123-Hub/maddpg-pettingzoo-pytorch"
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" -> "starry-sky6688/MARL-Algorithms"
"sjtu-marl/malib" -> "Replicable-MARL/MARLlib"
"sjtu-marl/malib" -> "marlbenchmark/on-policy"
"sjtu-marl/malib" -> "marlbenchmark/off-policy"
"sjtu-marl/malib" -> "oxwhirl/pymarl"
"sjtu-marl/malib" -> "uoe-agents/epymarl"
"sjtu-marl/malib" -> "hijkzzz/pymarl2"
"sjtu-marl/malib" -> "shariqiqbal2810/MAAC"
"sjtu-marl/malib" -> "PKU-MARL/HARL"
"sjtu-marl/malib" -> "mlii/mfrl"
"sjtu-marl/malib" -> "oxwhirl/smac"
"sjtu-marl/malib" -> "PKU-MARL/Multi-Agent-Transformer"
"sjtu-marl/malib" -> "instadeepai/Mava"
"sjtu-marl/malib" -> "facebookresearch/BenchMARL"
"sjtu-marl/malib" -> "tjuHaoXiaotian/pymarl3"
"sjtu-marl/malib" -> "cyanrain7/TRPO-in-MARL"
"ailec0623/DQN_HollowKnight" -> "seermer/HollowKnight_RL"
"ailec0623/DQN_HollowKnight" -> "analoganddigital/DQN_play_sekiro"
"ailec0623/DQN_HollowKnight" -> "ricagj/train_your_own_game_AI"
"jiechuanjiang/pytorch_DGN" -> "PKU-RL/DGN"
"jiechuanjiang/pytorch_DGN" -> "CORE-Robotics-Lab/MAGIC"
"jiechuanjiang/pytorch_DGN" -> "Jacklinkk/Graph_CAVs" ["e"=1]
"jiechuanjiang/pytorch_DGN" -> "Amanda2024/GCS_aamas337"
"jiechuanjiang/pytorch_DGN" -> "PKU-RL/I2C"
"hijkzzz/pymarl2" -> "starry-sky6688/MARL-Algorithms"
"hijkzzz/pymarl2" -> "uoe-agents/epymarl"
"hijkzzz/pymarl2" -> "oxwhirl/pymarl"
"hijkzzz/pymarl2" -> "WorldDbs/specs-actors" ["e"=1]
"hijkzzz/pymarl2" -> "oxwhirl/smac"
"hijkzzz/pymarl2" -> "yl-yue/yue-library" ["e"=1]
"hijkzzz/pymarl2" -> "marlbenchmark/off-policy"
"hijkzzz/pymarl2" -> "PercyJon/PercyJon.github.io" ["e"=1]
"hijkzzz/pymarl2" -> "marlbenchmark/on-policy"
"hijkzzz/pymarl2" -> "springmonster/RestfulTool-Retrofit" ["e"=1]
"hijkzzz/pymarl2" -> "tjuHaoXiaotian/pymarl3"
"hijkzzz/pymarl2" -> "v2ray-links/v2ray-free" ["e"=1]
"hijkzzz/pymarl2" -> "HeisenbergEmpire/studynote" ["e"=1]
"hijkzzz/pymarl2" -> "admin360bug/bypass" ["e"=1]
"hijkzzz/pymarl2" -> "DataCanvasIO/HyperGBM" ["e"=1]
"marl-book/codebase" -> "marl-book/slides"
"marl-book/codebase" -> "uoe-agents/epymarl"
"marl-book/codebase" -> "facebookresearch/BenchMARL"
"marl-book/codebase" -> "FLAIROx/JaxMARL" ["e"=1]
"marl-book/codebase" -> "Replicable-MARL/MARLlib"
"marl-book/codebase" -> "google-deepmind/meltingpot"
"marl-book/codebase" -> "TimeBreaker/MARL-papers-with-code"
"marl-book/codebase" -> "proroklab/VectorizedMultiAgentSimulator"
"marl-book/codebase" -> "uoe-agents/smaclite"
"marl-book/codebase" -> "marlbenchmark/on-policy"
"marl-book/codebase" -> "chrisyrniu/Recent-Advances-in-Multi-Agent-Reinforcement-Learning"
"marl-book/codebase" -> "PKU-MARL/HARL"
"marl-book/codebase" -> "uoe-agents/lb-foraging"
"marl-book/codebase" -> "instadeepai/Mava"
"marl-book/codebase" -> "instadeepai/og-marl"
"uoe-agents/epymarl" -> "hijkzzz/pymarl2"
"uoe-agents/epymarl" -> "Replicable-MARL/MARLlib"
"uoe-agents/epymarl" -> "oxwhirl/pymarl"
"uoe-agents/epymarl" -> "marlbenchmark/on-policy"
"uoe-agents/epymarl" -> "FLAIROx/JaxMARL" ["e"=1]
"uoe-agents/epymarl" -> "facebookresearch/BenchMARL"
"uoe-agents/epymarl" -> "oxwhirl/smacv2"
"uoe-agents/epymarl" -> "starry-sky6688/MARL-Algorithms"
"uoe-agents/epymarl" -> "oxwhirl/smac"
"uoe-agents/epymarl" -> "semitable/lb-foraging"
"uoe-agents/epymarl" -> "marlbenchmark/off-policy"
"uoe-agents/epymarl" -> "PKU-MARL/HARL"
"uoe-agents/epymarl" -> "marl-book/codebase"
"uoe-agents/epymarl" -> "instadeepai/Mava"
"uoe-agents/epymarl" -> "cyanrain7/TRPO-in-MARL"
"hcnoh/gail-pytorch" -> "toshikwa/gail-airl-ppo.pytorch"
"hcnoh/gail-pytorch" -> "hcnoh/rl-collection-pytorch"
"hcnoh/gail-pytorch" -> "navuboy/gail_gym"
"hcnoh/gail-pytorch" -> "Ericonaldo/ILSwiss"
"hcnoh/gail-pytorch" -> "seolhokim/InverseRL-Pytorch"
"kaixindelele/DRLib" -> "kaixindelele/RHER"
"kaixindelele/DRLib" -> "AI4Finance-Foundation/ElegantRL"
"kaixindelele/DRLib" -> "zhangchuheng123/Reinforcement-Implementation"
"kaixindelele/DRLib" -> "Lizhi-sjtu/DRL-code-pytorch"
"kaixindelele/DRLib" -> "borninfreedom/kuka-reach-drl" ["e"=1]
"kaixindelele/DRLib" -> "gxywy/rl-plotter"
"kaixindelele/DRLib" -> "BIT-aerial-robotics/AquaML"
"kaixindelele/DRLib" -> "marlbenchmark/off-policy"
"kaixindelele/DRLib" -> "Lizhi-sjtu/MARL-code-pytorch"
"kaixindelele/DRLib" -> "YangRui2015/Sparse-Reward-Algorithms"
"kaixindelele/DRLib" -> "sjtu-marl/malib"
"kaixindelele/DRLib" -> "louisnino/RLcode"
"kaixindelele/DRLib" -> "shariqiqbal2810/MAAC"
"kaixindelele/DRLib" -> "tinyzqh/light_mappo"
"kaixindelele/DRLib" -> "cyanrain7/TRPO-in-MARL"
"instadeepai/Mava" -> "instadeepai/jumanji" ["e"=1]
"instadeepai/Mava" -> "FLAIROx/JaxMARL" ["e"=1]
"instadeepai/Mava" -> "uoe-agents/epymarl"
"instadeepai/Mava" -> "EdanToledo/Stoix" ["e"=1]
"instadeepai/Mava" -> "instadeepai/flashbax" ["e"=1]
"instadeepai/Mava" -> "RobertTLange/gymnax" ["e"=1]
"instadeepai/Mava" -> "luchris429/purejaxrl" ["e"=1]
"instadeepai/Mava" -> "instadeepai/og-marl"
"instadeepai/Mava" -> "facebookresearch/BenchMARL"
"instadeepai/Mava" -> "Farama-Foundation/PettingZoo"
"instadeepai/Mava" -> "oxwhirl/pymarl"
"instadeepai/Mava" -> "instadeepai/catx" ["e"=1]
"instadeepai/Mava" -> "adaptive-intelligent-robotics/QDax" ["e"=1]
"instadeepai/Mava" -> "Replicable-MARL/MARLlib"
"instadeepai/Mava" -> "sjtu-marl/malib"
"tinyzqh/light_mappo" -> "marlbenchmark/on-policy"
"tinyzqh/light_mappo" -> "Lizhi-sjtu/MARL-code-pytorch"
"tinyzqh/light_mappo" -> "marlbenchmark/off-policy"
"tinyzqh/light_mappo" -> "PKU-MARL/HARL"
"tinyzqh/light_mappo" -> "starry-sky6688/MADDPG"
"tinyzqh/light_mappo" -> "starry-sky6688/MARL-Algorithms"
"tinyzqh/light_mappo" -> "Replicable-MARL/MARLlib"
"tinyzqh/light_mappo" -> "shariqiqbal2810/maddpg-pytorch"
"tinyzqh/light_mappo" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation" ["e"=1]
"tinyzqh/light_mappo" -> "uoe-agents/epymarl"
"tinyzqh/light_mappo" -> "PKU-MARL/Multi-Agent-Transformer"
"tinyzqh/light_mappo" -> "TimeBreaker/MARL-papers-with-code"
"tinyzqh/light_mappo" -> "Lizhi-sjtu/DRL-code-pytorch"
"tinyzqh/light_mappo" -> "hijkzzz/pymarl2"
"tinyzqh/light_mappo" -> "oxwhirl/pymarl"
"TimeBreaker/MARL-resources-collection" -> "TimeBreaker/Multi-Agent-Reinforcement-Learning-papers"
"TimeBreaker/MARL-resources-collection" -> "TimeBreaker/MARL-papers-with-code"
"TimeBreaker/MARL-resources-collection" -> "TimeBreaker/Adversarial-Reinforcement-Learning-Papers"
"TimeBreaker/MARL-resources-collection" -> "uoe-agents/epymarl"
"TimeBreaker/MARL-resources-collection" -> "lich14/CDS"
"TimeBreaker/MARL-resources-collection" -> "Replicable-MARL/MARLlib"
"berkeleydeeprlcourse/homework_fall2021" -> "vamsianumula/cs285-deeprl-ucberkeley-2021"
"ebonelli/PLaF" -> "notalim/intermediate-statistics-ca-notes"
"openai/GPT-3-Encoder" -> "openai/pytorch"
"openai/GPT-3-Encoder" -> "openai/websockify"
"salesforce/warp-drive" -> "instadeepai/Mava"
"salesforce/warp-drive" -> "Bam4d/Griddly"
"salesforce/warp-drive" -> "sail-sg/envpool" ["e"=1]
"salesforce/warp-drive" -> "sjtu-marl/malib"
"salesforce/warp-drive" -> "uoe-agents/epymarl"
"salesforce/warp-drive" -> "google-deepmind/meltingpot"
"salesforce/warp-drive" -> "coax-dev/coax"
"salesforce/warp-drive" -> "proroklab/VectorizedMultiAgentSimulator"
"salesforce/warp-drive" -> "salesforce/ai-economist"
"salesforce/warp-drive" -> "facebookresearch/minimax" ["e"=1]
"salesforce/warp-drive" -> "MadryLab/implementation-matters"
"CORE-Robotics-Lab/SSRR" -> "dsbrown1331/CoRL2019-DREX"
"Sopiro/Unity-Procedural-Animation" -> "PhilS94/Unity-Procedural-IK-Wall-Walking-Spider"
"Sopiro/Unity-Procedural-Animation" -> "lchaumartin/SpiderProceduralAnimation"
"Sopiro/Unity-Procedural-Animation" -> "BattleDawnNZ/ProceduralAnimation"
"Sopiro/Unity-Procedural-Animation" -> "timi-ty/procedural-animation"
"Sopiro/Unity-Procedural-Animation" -> "ashleve/ActiveRagdoll"
"oxwhirl/wqmix" -> "wjh720/QPLEX"
"oxwhirl/wqmix" -> "Sonkyunghwan/QTRAN"
"oxwhirl/wqmix" -> "wendelinboehmer/dcg"
"oxwhirl/wqmix" -> "TonghanWang/DOP"
"oxwhirl/wqmix" -> "TonghanWang/ROMA"
"oxwhirl/wqmix" -> "facebookresearch/CollaQ"
"oxwhirl/wqmix" -> "AnujMahajanOxf/MAVEN"
"rlchina/RLCN" -> "jidiai/ai_lib"
"rlchina/RLCN" -> "sjtu-marl/malib"
"LARG/HFO" -> "openai/gym-soccer"
"LARG/HFO" -> "mhauskn/dqn-hfo"
"LARG/HFO" -> "rcsoccersim/rcssserver"
"LARG/HFO" -> "herodrigues/robocup2d-tutorial"
"LARG/HFO" -> "wrighteagle2d/wrighteaglebase"
"LARG/HFO" -> "ltzheng/pddpg-hfo"
"LARG/HFO" -> "kengz/robocup-soccer"
"LARG/HFO" -> "Cyrus2D/Cyrus2DBase"
"mhauskn/dqn-hfo" -> "LARG/HFO"
"ethanluoyc/magi" -> "henry-prior/jax-rl"
"seolhokim/InverseRL-Pytorch" -> "ahq1993/inverse_rl"
"seolhokim/InverseRL-Pytorch" -> "toshikwa/gail-airl-ppo.pytorch"
"seolhokim/InverseRL-Pytorch" -> "HuangJiaLian/AIRL_MountainCar"
"seolhokim/InverseRL-Pytorch" -> "gouxiangchen/soft-Q-learning"
"TimeBreaker/MARL-papers-with-code" -> "TimeBreaker/Multi-Agent-Reinforcement-Learning-papers"
"TimeBreaker/MARL-papers-with-code" -> "TimeBreaker/MARL-resources-collection"
"TimeBreaker/MARL-papers-with-code" -> "TimeBreaker/Adversarial-Reinforcement-Learning-Papers"
"TimeBreaker/MARL-papers-with-code" -> "tinyzqh/light_mappo"
"TimeBreaker/MARL-papers-with-code" -> "Lizhi-sjtu/MARL-code-pytorch"
"TimeBreaker/MARL-papers-with-code" -> "uoe-agents/epymarl"
"TimeBreaker/MARL-papers-with-code" -> "oxwhirl/pymarl"
"TimeBreaker/MARL-papers-with-code" -> "marlbenchmark/off-policy"
"TimeBreaker/MARL-papers-with-code" -> "starry-sky6688/MARL-Algorithms"
"TimeBreaker/MARL-papers-with-code" -> "marlbenchmark/on-policy"
"TimeBreaker/MARL-papers-with-code" -> "Replicable-MARL/MARLlib"
"TimeBreaker/MARL-papers-with-code" -> "facebookresearch/BenchMARL"
"TimeBreaker/MARL-papers-with-code" -> "DongChen06/MARL_CAVs" ["e"=1]
"TimeBreaker/MARL-papers-with-code" -> "hijkzzz/pymarl2"
"bbitmaster/ale_python_interface" -> "openai/atari-py"
"ricagj/train_your_own_game_AI" -> "ricagj/pysekiro_with_RL"
"ricagj/train_your_own_game_AI" -> "analoganddigital/DQN_play_sekiro"
"jgajera/CS6750-Human-Computer-Interaction" -> "manikandan-ravikiran/HCI_Notes"
"rosewang2008/gym-cooking" -> "HumanCompatibleAI/human_aware_rl"
"rosewang2008/gym-cooking" -> "HumanCompatibleAI/overcooked_ai"
"rosewang2008/gym-cooking" -> "Stanford-ILIAD/PantheonRL"
"rosewang2008/gym-cooking" -> "facebookresearch/hanabi_SAD"
"rosewang2008/gym-cooking" -> "HosnLS/Hierarchical-Language-Agent"
"rosewang2008/gym-cooking" -> "clbaker/BToM"
"rosewang2008/gym-cooking" -> "HumanCompatibleAI/overcooked-demo"
"codecat/tm-dashboard" -> "codecat/tm-better-chat"
"codecat/tm-dashboard" -> "domino54/title-packs"
"uoe-agents/seac" -> "uoe-agents/lb-foraging"
"tencent-ailab/tleague_projpage" -> "tencent-ailab/TLeague"
"andyljones/reinforcement-learning-discord-wiki" -> "google-research/rliable" ["e"=1]
"andyljones/reinforcement-learning-discord-wiki" -> "laszukdawid/ai-traineree"
"herodrigues/robocup2d-tutorial" -> "wrighteagle2d/wrighteaglebase"
"herodrigues/robocup2d-tutorial" -> "bestpredicts/Robocup2dInstall"
"herodrigues/robocup2d-tutorial" -> "wrighteagle2d/autotest2d"
"herodrigues/robocup2d-tutorial" -> "rcsoccersim/rcssmonitor"
"herodrigues/robocup2d-tutorial" -> "rcsoccersim/rcssserver"
"herodrigues/robocup2d-tutorial" -> "helios-base/helios-base"
"herodrigues/robocup2d-tutorial" -> "bestpredicts/Robocup2DMining"
"herodrigues/robocup2d-tutorial" -> "dark-0ne/RcssAnalyzer"
"herodrigues/robocup2d-tutorial" -> "rcsoccersim/rcsoccersim.github.io"
"huanzhang12/ATLA_robust_RL" -> "chenhongge/StateAdvDRL"
"huanzhang12/ATLA_robust_RL" -> "tuomaso/radial_rl_v2"
"huanzhang12/ATLA_robust_RL" -> "tesslerc/ActionRobustRL"
"huanzhang12/ATLA_robust_RL" -> "chenhongge/SA_DQN"
"Ericonaldo/ILSwiss" -> "apexrl/Imitation-Learning-Paper-Lists"
"Ericonaldo/ILSwiss" -> "toshikwa/gail-airl-ppo.pytorch"
"Ericonaldo/ILSwiss" -> "KamyarGh/rl_swiss"
"Ericonaldo/ILSwiss" -> "hcnoh/gail-pytorch"
"Ericonaldo/ILSwiss" -> "kristery/Awesome-Imitation-Learning"
"kressdev/RagdollTrainer" -> "tavik000/MazeGameAI"
"Theohhhu/UPDeT" -> "mttga/pymarl_transformers"
"Theohhhu/UPDeT" -> "tjuHaoXiaotian/pymarl3"
"songbaoming/DouDiZhu" -> "ZhouWeikuan/DouDiZhu"
"songbaoming/DouDiZhu" -> "thuxugang/doudizhu"
"songbaoming/DouDiZhu" -> "mailgyc/doudizhu"
"songbaoming/DouDiZhu" -> "onestraw/doudizhu"
"songbaoming/DouDiZhu" -> "Coselding/PlaneWar-MFC" ["e"=1]
"songbaoming/DouDiZhu" -> "yuanfengyun/q_algorithm" ["e"=1]
"songbaoming/DouDiZhu" -> "dwg255/landlord" ["e"=1]
"songbaoming/DouDiZhu" -> "zhozhou/DouDizhuAI"
"songbaoming/DouDiZhu" -> "donnki/ddz_skynet" ["e"=1]
"songbaoming/DouDiZhu" -> "ksky521/DouZero"
"google-deepmind/lab2d" -> "google-deepmind/meltingpot"
"google-deepmind/lab2d" -> "Bam4d/Griddly"
"google-deepmind/lab2d" -> "kandouss/marlgrid"
"google-deepmind/lab2d" -> "google-deepmind/dm_env" ["e"=1]
"google-deepmind/lab2d" -> "alex-petrenko/sample-factory"
"google-deepmind/lab2d" -> "rosewang2008/gym-cooking"
"google-deepmind/lab2d" -> "Stanford-ILIAD/PantheonRL"
"google-deepmind/lab2d" -> "google-deepmind/pycolab"
"google-deepmind/lab2d" -> "eugenevinitsky/sequential_social_dilemma_games"
"google-deepmind/lab2d" -> "google-deepmind/bsuite"
"google-deepmind/lab2d" -> "facebookresearch/torchbeast"
"google-deepmind/lab2d" -> "danijar/crafter" ["e"=1]
"google-deepmind/lab2d" -> "facebookresearch/minihack"
"google-deepmind/lab2d" -> "google-deepmind/dm_alchemy" ["e"=1]
"tedmoskovitz/TOP" -> "philipjball/TD3_PyTorch"
"mdcrosby/animal-ai" -> "Kinds-of-Intelligence-CFI/animal-ai"
"donadigo/pygbx" -> "donadigo/gbxtools"
"TonghanWang/RODE" -> "TonghanWang/ROMA"
"TonghanWang/RODE" -> "wjh720/QPLEX"
"TonghanWang/RODE" -> "TonghanWang/DOP"
"diversepsro/diverse_psro" -> "sjtu-marl/bd_rd_psro"
"eternalmothra/ml_cheat_sheet" -> "JonathanTay/CS-7641-assignment-4"
"indylab/tabular_xdo" -> "npvoid/OnlineDoubleOracle"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "brianspiering/awesome-deep-rl"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "PacktPublishing/Python-Reinforcement-Learning-Projects"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "Rafael1s/Deep-Reinforcement-Learning-Algorithms"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "ugurkanates/awesome-real-world-rl"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "yrlu/irl-imitation"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "qzed/irl-maxent"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "ermongroup/MA-AIRL"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "MatthewJA/Inverse-Reinforcement-Learning"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "jangirrishabh/toyCarIRL"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "PacktPublishing/PyTorch-1.x-Reinforcement-Learning-Cookbook"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "PacktPublishing/Deep-Reinforcement-Learning-with-Python"
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" -> "sudharsan13296/Hands-On-Meta-Learning-With-Python" ["e"=1]
"saizhang0218/TMC" -> "saizhang0218/VBC"
"Farama-Foundation/ViZDoom" -> "glample/Arnold"
"Farama-Foundation/ViZDoom" -> "pathak22/noreward-rl"
"Farama-Foundation/ViZDoom" -> "google-deepmind/lab"
"Farama-Foundation/ViZDoom" -> "Farama-Foundation/Arcade-Learning-Environment"
"Farama-Foundation/ViZDoom" -> "rll/rllab"
"Farama-Foundation/ViZDoom" -> "Farama-Foundation/Minigrid"
"Farama-Foundation/ViZDoom" -> "openai/retro"
"Farama-Foundation/ViZDoom" -> "alex-petrenko/sample-factory"
"Farama-Foundation/ViZDoom" -> "openai/universe-starter-agent"
"Farama-Foundation/ViZDoom" -> "openai/roboschool"
"Farama-Foundation/ViZDoom" -> "TorchCraft/TorchCraft" ["e"=1]
"Farama-Foundation/ViZDoom" -> "ikostrikov/pytorch-a3c"
"Farama-Foundation/ViZDoom" -> "coreylynch/async-rl"
"Farama-Foundation/ViZDoom" -> "microsoft/malmo"
"Farama-Foundation/ViZDoom" -> "IntelLabs/coach"
"ArkadySK/GbxMapBrowser" -> "Ze-Rax/TrackmaniaFlagRush"
"tencent-ailab/TLeague" -> "tencent-ailab/tleague_projpage"
"tencent-ailab/TLeague" -> "JBLanier/pipeline-psro"
"tencent-ailab/TLeague" -> "aicenter/openspiel_reproductions"
"NathanEpstein/reinforce" -> "rlpy/rlpy"
"mike-gimelfarb/deep-successor-features-for-transfer" -> "erwanbou/sf-deep-rl"
"mike-gimelfarb/deep-successor-features-for-transfer" -> "manantomar/DSR"
"AndrejGobeX/TrackMania_AI" -> "trackmania-rl/tmrl"
"mailgyc/doudizhu" -> "dwg255/landlord" ["e"=1]
"mailgyc/doudizhu" -> "onestraw/doudizhu"
"mailgyc/doudizhu" -> "songbaoming/DouDiZhu"
"mailgyc/doudizhu" -> "tinyshu/ddz_game" ["e"=1]
"mailgyc/doudizhu" -> "thuxugang/doudizhu"
"mailgyc/doudizhu" -> "Zhang19910325/nodejs-server-wechat-landLordGame" ["e"=1]
"mailgyc/doudizhu" -> "matchvs/Poke" ["e"=1]
"mailgyc/doudizhu" -> "WhoIsYourBaby/chess" ["e"=1]
"mailgyc/doudizhu" -> "gochenzl/chess" ["e"=1]
"mailgyc/doudizhu" -> "qq456cvb/doudizhu-C"
"mailgyc/doudizhu" -> "ksky521/DouZero"
"mailgyc/doudizhu" -> "dwg255/fish" ["e"=1]
"mailgyc/doudizhu" -> "yuanfengyun/q_algorithm" ["e"=1]
"mailgyc/doudizhu" -> "zhuang0/BoYaDDZ"
"nishantkr18/federated-model-averaging-for-DQN" -> "TroddenSpade/Federated-DRL"
"aicenter/openspiel_reproductions" -> "npvoid/OnlineDoubleOracle"
"lich14/CDS" -> "mansicer/MAIC"
"lich14/CDS" -> "Jiwonjeon9603/MASER"
"lich14/CDS" -> "wjh720/QPLEX"
"facebookresearch/level-replay" -> "facebookresearch/dcd"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "TimeBreaker/MARL-resources-collection"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "TimeBreaker/MARL-papers-with-code"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "TimeBreaker/Adversarial-Reinforcement-Learning-Papers"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "uoe-agents/epymarl"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "Lizhi-sjtu/MARL-code-pytorch"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "lich14/CDS"
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" -> "PKU-MARL/Multi-Agent-Transformer"
"Stanford-ILIAD/PantheonRL" -> "HumanCompatibleAI/human_aware_rl"
"Stanford-ILIAD/PantheonRL" -> "HumanCompatibleAI/overcooked_ai"
"Stanford-ILIAD/PantheonRL" -> "HumanCompatibleAI/overcooked-demo"
"Stanford-ILIAD/PantheonRL" -> "HumanCompatibleAI/overcooked-hAI-exp"
"Stanford-ILIAD/PantheonRL" -> "samjia2000/HSP"
"Stanford-ILIAD/PantheonRL" -> "rosewang2008/gym-cooking"
"Stanford-ILIAD/PantheonRL" -> "HumanCompatibleAI/human_ai_robustness"
"Stanford-ILIAD/PantheonRL" -> "rstrivedi/Melting-Pot-Contest-2023"
"yannbouteiller/vgamepad" -> "jangxx/node-ViGEmClient"
"YiqinYang/ICQ" -> "ling-pan/OMAR"
"YiqinYang/ICQ" -> "thu-rllab/CFCQL"
"YiqinYang/ICQ" -> "ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer"
"hsvgbkhgbv/shapley-q-learning" -> "hsvgbkhgbv/SQDDPG"
"hsvgbkhgbv/shapley-q-learning" -> "hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning"
"mbaske/ml-audio-sensor" -> "mbaske/ml-chord-detection"
"sisl/DICG" -> "wendelinboehmer/dcg"
"donadigo/gbxtools" -> "donadigo/pygbx"
"coax-dev/coax" -> "toshikwa/rljax"
"coax-dev/coax" -> "henry-prior/jax-rl"
"openai/tabulate" -> "openai/post--example"
"janosh/tensorboard-reducer" -> "Spenhouet/tensorboard-aggregator"
"lchaumartin/HumanoidProceduralAnimation" -> "pickles976/UnityProceduralBiped"
"davidkimighty/TARS" -> "ashleve/ActiveRagdoll"
"datamllab/autovideo" -> "datamllab/BED_main"
"datamllab/autovideo" -> "daochenzha/rapid"
"datamllab/autovideo" -> "daochenzha/Meta-AAD"
"datamllab/autovideo" -> "datamllab/ltsm"
"datamllab/autovideo" -> "datamllab/rlcard-showdown"
"datamllab/autovideo" -> "ynchuang/DiscoverPath"
"datamllab/autovideo" -> "JiabenChen/iQuery"
"datamllab/autovideo" -> "IBM/action-recognition-pytorch" ["e"=1]
"datamllab/autovideo" -> "VITA-Group/Deep_GCN_Benchmarking"
"datamllab/autovideo" -> "datamllab/tods" ["e"=1]
"datamllab/autovideo" -> "datamllab/xdeep" ["e"=1]
"datamllab/autovideo" -> "datamllab/AutoRec" ["e"=1]
"daochenzha/rapid" -> "daochenzha/Meta-AAD"
"daochenzha/rapid" -> "datamllab/BED_main"
"skyslide22/blendermania-addon" -> "ataulien/blender-trackmania-tools"
"helios-base/librcsc" -> "helios-base/fedit2"
"helios-base/librcsc" -> "helios-base/helios-base"
"sen-pai/InfoGAIL" -> "XinZhang525/cGAIL"
"sen-pai/InfoGAIL" -> "mohitsharma0690/DirectedInfo-GAIL"
"CORE-Robotics-Lab/MAGIC" -> "sisl/DICG"
"hijkzzz/noisy-mappo" -> "sanmuyang/multi-agent-PPO-on-SMAC"
"trzhang0116/HRAC" -> "martius-lab/HiTS"
"trzhang0116/HRAC" -> "junsu-kim97/HIGL"
"trzhang0116/HRAC" -> "HeegerGao/CRIL"
"fangzai/DeepQLearning" -> "brian473/neural_rl"
"Cyrus2D/Pyrus2D" -> "Cyrus2D/Cyrus2DBase"
"Cyrus2D/Pyrus2D" -> "Cyrus2D/SoccerSimulationProxy"
"Cyrus2D/Pyrus2D" -> "Cyrus2D/Pyrus-SS2D-Base"
"lchaumartin/SpiderProceduralAnimation" -> "Sopiro/Unity-Procedural-Animation"
"lchaumartin/SpiderProceduralAnimation" -> "lchaumartin/HumanoidProceduralAnimation"
"lchaumartin/SpiderProceduralAnimation" -> "BattleDawnNZ/ProceduralAnimation"
"PKU-RL/I2C" -> "UnrealTracking/ToM2C"
"nadeo/trackmania-doc" -> "openplanet-nl/example-scripts"
"dci05049/Active-Ragdoll-Tutorial-Unity" -> "sergioabreu-g/active-ragdolls"
"helios-base/helios-base" -> "helios-base/fedit2"
"helios-base/helios-base" -> "Cyrus2D/Cyrus2DBase"
"helios-base/helios-base" -> "helios-base/librcsc"
"helios-base/helios-base" -> "Cyrus2D/Pyrus-SS2D-Base"
"EvoEsports/docker-trackmania" -> "EvoEsports/gbxclient-node"
"ricagj/pysekiro_with_RL" -> "ricagj/train_your_own_game_AI"
"ricagj/pysekiro_with_RL" -> "ChenWendi2001/alpha-sekiro"
"ucl-dark/paired" -> "facebookresearch/dcd"
"facebookresearch/diplomacy_searchbot" -> "google-deepmind/diplomacy"
"facebookresearch/diplomacy_searchbot" -> "diplomacy/research"
"jidiai/ai_lib" -> "rlchina/RLCN"
"jidiai/ai_lib" -> "jidiai/Competition_3v3snakes"
"jidiai/ai_lib" -> "CarlossShi/Competition_3v3snakes"
"jidiai/ai_lib" -> "JBLanier/pipeline-psro"
"jidiai/ai_lib" -> "jidiai/SummerCourse2021"
"jidiai/ai_lib" -> "indylab/nxdo"
"MatPoliquin/stable-retro" -> "Farama-Foundation/stable-retro"
"mahaitongdae/safe_exp_env" -> "idthanm/env_build"
"shacklettbp/bps3D" -> "shacklettbp/rlpbr"
"shacklettbp/bps3D" -> "shacklettbp/bps-nav"
"shacklettbp/rlpbr" -> "shacklettbp/bps3D"
"shacklettbp/bps-nav" -> "shacklettbp/bps3D"
"shacklettbp/bps-nav" -> "shacklettbp/rlpbr"
"donadigo/TMInterfacePublic" -> "donadigo/TMInterfaceClientPython"
"XanderJC/scalable-birl" -> "HumanCompatibleAI/population-irl"
"ZiyuanMa/R2D2" -> "jinbeizame007/pytorch-r2d2-DPG"
"IouJenLiu/CMAE" -> "Jiwonjeon9603/MASER"
"IouJenLiu/CMAE" -> "AnujMahajanOxf/MAVEN"
"IouJenLiu/CMAE" -> "IouJenLiu/PIC"
"IouJenLiu/CMAE" -> "kikojay/EMC"
"yobibyte/amorpheus" -> "sunghoonhong/SWAT"
"helios-base/fedit2" -> "helios-base/librcsc"
"helios-base/fedit2" -> "helios-base/soccerwindow2"
"jidiai/Competition_3v3snakes" -> "CarlossShi/Competition_3v3snakes"
"helios-base/soccerwindow2" -> "helios-base/fedit2"
"TroddenSpade/Soccer-Players-Tracking" -> "TroddenSpade/GAN-NST"
"TroddenSpade/Soccer-Players-Tracking" -> "TroddenSpade/Curiosity-driven-Exploration-in-Navigation"
"TroddenSpade/Soccer-Players-Tracking" -> "TroddenSpade/Exhaustive-Reinforcement-Learning"
"TroddenSpade/Soccer-Players-Tracking" -> "TroddenSpade/Flexible-Conditional-Imitation-Learning"
"mbarnes1/vehicle-drift" -> "angloth/auto-drift"
"mbarnes1/vehicle-drift" -> "karanamrahul/Autonomous-Drifting-using-deep-Reinforcement-Learning"
"npvoid/OnlineDoubleOracle" -> "indylab/tabular_xdo"
"npvoid/OnlineDoubleOracle" -> "indylab/nxdo"
"alirezakazemipour/PPO-RND" -> "wisnunugroho21/reinforcement_learning_ppo_rnd"
"YunqiuXu/SHA-KG" -> "tesslerc/Sparse-IL"
"EvoEsports/gbxclient-node" -> "snixtho/clifga"
"aikorea/awesome-rl" -> "dennybritz/reinforcement-learning"
"aikorea/awesome-rl" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"aikorea/awesome-rl" -> "src-d/awesome-machine-learning-on-source-code" ["e"=1]
"aikorea/awesome-rl" -> "keon/awesome-nlp" ["e"=1]
"aikorea/awesome-rl" -> "openai/baselines"
"aikorea/awesome-rl" -> "openai/gym"
"aikorea/awesome-rl" -> "GoogleTrends/data" ["e"=1]
"aikorea/awesome-rl" -> "jbhuang0604/awesome-computer-vision" ["e"=1]
"aikorea/awesome-rl" -> "yandexdataschool/Practical_RL"
"aikorea/awesome-rl" -> "igrigorik/decisiontree" ["e"=1]
"aikorea/awesome-rl" -> "ChristosChristofidis/awesome-deep-learning" ["e"=1]
"aikorea/awesome-rl" -> "scikit-learn-contrib/lightning" ["e"=1]
"aikorea/awesome-rl" -> "jtoy/awesome-tensorflow" ["e"=1]
"aikorea/awesome-rl" -> "keras-rl/keras-rl"
"aikorea/awesome-rl" -> "nlintz/TensorFlow-Tutorials" ["e"=1]
"Replicable-MARL/MARLlib" -> "PKU-MARL/HARL"
"Replicable-MARL/MARLlib" -> "uoe-agents/epymarl"
"Replicable-MARL/MARLlib" -> "marlbenchmark/on-policy"
"Replicable-MARL/MARLlib" -> "starry-sky6688/MARL-Algorithms"
"Replicable-MARL/MARLlib" -> "sjtu-marl/malib"
"Replicable-MARL/MARLlib" -> "marlbenchmark/off-policy"
"Replicable-MARL/MARLlib" -> "oxwhirl/pymarl"
"Replicable-MARL/MARLlib" -> "Farama-Foundation/PettingZoo"
"Replicable-MARL/MARLlib" -> "hijkzzz/pymarl2"
"Replicable-MARL/MARLlib" -> "facebookresearch/BenchMARL"
"Replicable-MARL/MARLlib" -> "PKU-MARL/Multi-Agent-Transformer"
"Replicable-MARL/MARLlib" -> "FLAIROx/JaxMARL" ["e"=1]
"Replicable-MARL/MARLlib" -> "oxwhirl/smac"
"Replicable-MARL/MARLlib" -> "tinyzqh/light_mappo"
"Replicable-MARL/MARLlib" -> "cyanrain7/TRPO-in-MARL"
"facebookresearch/rlmeta" -> "facebookresearch/moolib"
"facebookresearch/rlmeta" -> "facebookresearch/torchbeast"
"facebookresearch/rlmeta" -> "facebookresearch/minihack"
"facebookresearch/rlmeta" -> "facebookresearch/AutoCAT"
"Turing-Project/EssayTopicPredictV2" -> "Turing-Project/WriteGPT"
"Turing-Project/EssayTopicPredictV2" -> "clansty/ClassTools" ["e"=1]
"Turing-Project/EssayTopicPredictV2" -> "visionsss/recommend_system_version2" ["e"=1]
"Turing-Project/EssayTopicPredictV2" -> "Turing-Project/AntiFraudChatBot" ["e"=1]
"pytorch/rl" -> "vwxyzjn/cleanrl"
"pytorch/rl" -> "DLR-RM/stable-baselines3"
"pytorch/rl" -> "facebookresearch/mbrl-lib" ["e"=1]
"pytorch/rl" -> "Farama-Foundation/D4RL" ["e"=1]
"pytorch/rl" -> "Farama-Foundation/Gymnasium"
"pytorch/rl" -> "google/brax" ["e"=1]
"pytorch/rl" -> "google-deepmind/rlax" ["e"=1]
"pytorch/rl" -> "Farama-Foundation/PettingZoo"
"pytorch/rl" -> "facebookresearch/BenchMARL"
"pytorch/rl" -> "google-deepmind/acme"
"pytorch/rl" -> "DLR-RM/rl-baselines3-zoo"
"pytorch/rl" -> "danijar/dreamerv3" ["e"=1]
"pytorch/rl" -> "isaac-sim/IsaacGymEnvs" ["e"=1]
"pytorch/rl" -> "kzl/decision-transformer" ["e"=1]
"pytorch/rl" -> "ikostrikov/jaxrl" ["e"=1]
"hhexiy/opponent" -> "alshedivat/lola"
"hhexiy/opponent" -> "alexis-jacq/LOLA_DiCE"
"NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player" -> "choo8/Tensorflow-DeepMind-Atari-Deep-Q-Learner-2Player"
"NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player" -> "DorianKodelja/DeepMind-Atari-Deep-Q-Learner-2Player"
"XinJingHao/DRL-Pytorch" -> "Lizhi-sjtu/DRL-code-pytorch"
"XinJingHao/DRL-Pytorch" -> "boyu-ai/Hands-on-RL"
"XinJingHao/DRL-Pytorch" -> "wangshusen/DRL"
"XinJingHao/DRL-Pytorch" -> "agi-brain/xuance"
"XinJingHao/DRL-Pytorch" -> "DeepRLChinese/DeepRL-Chinese"
"XinJingHao/DRL-Pytorch" -> "vwxyzjn/cleanrl"
"XinJingHao/DRL-Pytorch" -> "AI4Finance-Foundation/ElegantRL"
"XinJingHao/DRL-Pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"XinJingHao/DRL-Pytorch" -> "datawhalechina/easy-rl"
"XinJingHao/DRL-Pytorch" -> "MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning"
"XinJingHao/DRL-Pytorch" -> "thu-ml/tianshou"
"XinJingHao/DRL-Pytorch" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"XinJingHao/DRL-Pytorch" -> "Jingliang-Duan/DSAC-v2"
"XinJingHao/DRL-Pytorch" -> "DLR-RM/stable-baselines3"
"XinJingHao/DRL-Pytorch" -> "PKU-MARL/HARL"
"Lizhi-sjtu/DRL-code-pytorch" -> "Lizhi-sjtu/MARL-code-pytorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "XinJingHao/DRL-Pytorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "vwxyzjn/ppo-implementation-details"
"Lizhi-sjtu/DRL-code-pytorch" -> "marlbenchmark/on-policy"
"Lizhi-sjtu/DRL-code-pytorch" -> "tinyzqh/light_mappo"
"Lizhi-sjtu/DRL-code-pytorch" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "kaixindelele/DRLib"
"Lizhi-sjtu/DRL-code-pytorch" -> "AI4Finance-Foundation/ElegantRL"
"Lizhi-sjtu/DRL-code-pytorch" -> "nikhilbarhate99/PPO-PyTorch"
"Lizhi-sjtu/DRL-code-pytorch" -> "opendilab/PPOxFamily" ["e"=1]
"Lizhi-sjtu/DRL-code-pytorch" -> "starry-sky6688/MARL-Algorithms"
"Lizhi-sjtu/DRL-code-pytorch" -> "boyu-ai/Hands-on-RL"
"Lizhi-sjtu/DRL-code-pytorch" -> "Jingliang-Duan/DSAC-v2"
"Lizhi-sjtu/DRL-code-pytorch" -> "quantumiracle/Popular-RL-Algorithms"
"Lizhi-sjtu/DRL-code-pytorch" -> "sfujim/TD3"
"Lizhi-sjtu/MARL-code-pytorch" -> "tinyzqh/light_mappo"
"Lizhi-sjtu/MARL-code-pytorch" -> "marlbenchmark/off-policy"
"Lizhi-sjtu/MARL-code-pytorch" -> "starry-sky6688/MADDPG"
"Lizhi-sjtu/MARL-code-pytorch" -> "starry-sky6688/MARL-Algorithms"
"Lizhi-sjtu/MARL-code-pytorch" -> "marlbenchmark/on-policy"
"Lizhi-sjtu/MARL-code-pytorch" -> "Lizhi-sjtu/DRL-code-pytorch"
"Lizhi-sjtu/MARL-code-pytorch" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"Lizhi-sjtu/MARL-code-pytorch" -> "PKU-MARL/HARL"
"Lizhi-sjtu/MARL-code-pytorch" -> "shariqiqbal2810/maddpg-pytorch"
"Lizhi-sjtu/MARL-code-pytorch" -> "philtabor/Multi-Agent-Reinforcement-Learning"
"Lizhi-sjtu/MARL-code-pytorch" -> "Replicable-MARL/MARLlib"
"Lizhi-sjtu/MARL-code-pytorch" -> "Git-123-Hub/maddpg-pettingzoo-pytorch"
"Lizhi-sjtu/MARL-code-pytorch" -> "xuehy/pytorch-maddpg"
"Lizhi-sjtu/MARL-code-pytorch" -> "yangchen1997/Multi-Agent-Reinforcement-Learning"
"Lizhi-sjtu/MARL-code-pytorch" -> "uoe-agents/epymarl"
"Git-123-Hub/maddpg-pettingzoo-pytorch" -> "philtabor/Multi-Agent-Reinforcement-Learning"
"Git-123-Hub/maddpg-pettingzoo-pytorch" -> "ffelten/MASAC"
"Git-123-Hub/maddpg-pettingzoo-pytorch" -> "Lizhi-sjtu/MARL-code-pytorch"
"Git-123-Hub/maddpg-pettingzoo-pytorch" -> "starry-sky6688/MADDPG"
"Git-123-Hub/maddpg-pettingzoo-pytorch" -> "marlbenchmark/off-policy"
"proroklab/VectorizedMultiAgentSimulator" -> "facebookresearch/BenchMARL"
"proroklab/VectorizedMultiAgentSimulator" -> "uoe-agents/epymarl"
"proroklab/VectorizedMultiAgentSimulator" -> "oxwhirl/smacv2"
"proroklab/VectorizedMultiAgentSimulator" -> "FLAIROx/JaxMARL" ["e"=1]
"proroklab/VectorizedMultiAgentSimulator" -> "instadeepai/Mava"
"proroklab/VectorizedMultiAgentSimulator" -> "Farama-Foundation/MAgent2"
"proroklab/VectorizedMultiAgentSimulator" -> "Replicable-MARL/MARLlib"
"proroklab/VectorizedMultiAgentSimulator" -> "semitable/robotic-warehouse"
"proroklab/VectorizedMultiAgentSimulator" -> "instadeepai/flashbax" ["e"=1]
"proroklab/VectorizedMultiAgentSimulator" -> "google-deepmind/meltingpot"
"proroklab/VectorizedMultiAgentSimulator" -> "marlbenchmark/on-policy"
"proroklab/VectorizedMultiAgentSimulator" -> "schroederdewitt/multiagent_mujoco"
"proroklab/VectorizedMultiAgentSimulator" -> "instadeepai/og-marl"
"proroklab/VectorizedMultiAgentSimulator" -> "koulanurag/ma-gym"
"proroklab/VectorizedMultiAgentSimulator" -> "luchris429/purejaxrl" ["e"=1]
"DeepRLChinese/DeepRL-Chinese" -> "wangshusen/DRL"
"DeepRLChinese/DeepRL-Chinese" -> "vincen-github/mlimpl"
"DeepRLChinese/DeepRL-Chinese" -> "boyu-ai/Hands-on-RL"
"DeepRLChinese/DeepRL-Chinese" -> "XinJingHao/DRL-Pytorch"
"DeepRLChinese/DeepRL-Chinese" -> "Lizhi-sjtu/DRL-code-pytorch"
"DeepRLChinese/DeepRL-Chinese" -> "Jingliang-Duan/DSAC-v2"
"DeepRLChinese/DeepRL-Chinese" -> "jwk1rose/RL_Learning"
"DeepRLChinese/DeepRL-Chinese" -> "Lizhi-sjtu/MARL-code-pytorch"
"DeepRLChinese/DeepRL-Chinese" -> "lansinuote/Simple_Reinforcement_Learning"
"DeepRLChinese/DeepRL-Chinese" -> "wwxFromTju/awesome-reinforcement-learning-lib" ["e"=1]
"DeepRLChinese/DeepRL-Chinese" -> "datawhalechina/easy-rl"
"DeepRLChinese/DeepRL-Chinese" -> "wangshusen/DeepLearning"
"DeepRLChinese/DeepRL-Chinese" -> "lansinuote/More_Simple_Reinforcement_Learning"
"DeepRLChinese/DeepRL-Chinese" -> "kaixindelele/DRLib"
"DeepRLChinese/DeepRL-Chinese" -> "agi-brain/xuance"
"MatthewJA/Inverse-Reinforcement-Learning" -> "yrlu/irl-imitation"
"MatthewJA/Inverse-Reinforcement-Learning" -> "reinforcement-learning-kr/lets-do-irl"
"MatthewJA/Inverse-Reinforcement-Learning" -> "jangirrishabh/toyCarIRL"
"MatthewJA/Inverse-Reinforcement-Learning" -> "justinjfu/inverse_rl"
"MatthewJA/Inverse-Reinforcement-Learning" -> "qzed/irl-maxent"
"MatthewJA/Inverse-Reinforcement-Learning" -> "ermongroup/MA-AIRL"
"MatthewJA/Inverse-Reinforcement-Learning" -> "HumanCompatibleAI/imitation"
"MatthewJA/Inverse-Reinforcement-Learning" -> "Khrylx/PyTorch-RL"
"MatthewJA/Inverse-Reinforcement-Learning" -> "openai/imitation"
"MatthewJA/Inverse-Reinforcement-Learning" -> "MCZhi/Driving-IRL-NGSIM" ["e"=1]
"MatthewJA/Inverse-Reinforcement-Learning" -> "sjchoi86/irl_rocks"
"MatthewJA/Inverse-Reinforcement-Learning" -> "uidilr/gail_ppo_tf"
"MatthewJA/Inverse-Reinforcement-Learning" -> "rll/rllab"
"MatthewJA/Inverse-Reinforcement-Learning" -> "toshikwa/gail-airl-ppo.pytorch"
"MatthewJA/Inverse-Reinforcement-Learning" -> "Kaixhin/imitation-learning"
"siemanko/tensorflow-deepq" -> "coreylynch/async-rl"
"siemanko/tensorflow-deepq" -> "spragunr/deep_q_rl"
"siemanko/tensorflow-deepq" -> "tambetm/simple_dqn"
"siemanko/tensorflow-deepq" -> "carpedm20/deep-rl-tensorflow"
"siemanko/tensorflow-deepq" -> "nikitasrivatsan/DeepLearningVideoGames"
"siemanko/tensorflow-deepq" -> "devsisters/DQN-tensorflow"
"siemanko/tensorflow-deepq" -> "tensorflow/skflow" ["e"=1]
"siemanko/tensorflow-deepq" -> "muupan/deep-reinforcement-learning-papers"
"siemanko/tensorflow-deepq" -> "miyosuda/async_deep_reinforce"
"siemanko/tensorflow-deepq" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"siemanko/tensorflow-deepq" -> "google/prettytensor" ["e"=1]
"siemanko/tensorflow-deepq" -> "muupan/async-rl"
"siemanko/tensorflow-deepq" -> "ericjang/tdb" ["e"=1]
"siemanko/tensorflow-deepq" -> "rll/rllab"
"siemanko/tensorflow-deepq" -> "joschu/modular_rl"
"amacati/SoulsGym" -> "amacati/SoulsAI"
"amacati/SoulsGym" -> "ocram444/EldenRL"
"tambetm/simple_dqn" -> "devsisters/DQN-tensorflow"
"tambetm/simple_dqn" -> "spragunr/deep_q_rl"
"tambetm/simple_dqn" -> "siemanko/tensorflow-deepq"
"tambetm/simple_dqn" -> "carpedm20/deep-rl-tensorflow"
"tambetm/simple_dqn" -> "coreylynch/async-rl"
"tambetm/simple_dqn" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"tambetm/simple_dqn" -> "muupan/deep-reinforcement-learning-papers"
"tambetm/simple_dqn" -> "nikitasrivatsan/DeepLearningVideoGames"
"tambetm/simple_dqn" -> "miyosuda/async_deep_reinforce"
"tambetm/simple_dqn" -> "muupan/async-rl"
"tambetm/simple_dqn" -> "Kaixhin/Atari"
"tambetm/simple_dqn" -> "openai/universe-starter-agent"
"tambetm/simple_dqn" -> "muupan/dqn-in-the-caffe"
"tambetm/simple_dqn" -> "sherjilozair/dqn"
"tambetm/simple_dqn" -> "Farama-Foundation/Arcade-Learning-Environment"
"yunke-wang/SAIL" -> "yunke-wang/WGAIL"
"yunke-wang/SAIL" -> "yunke-wang/UID"
"agrimgupta92/derl" -> "agrimgupta92/metamorph"
"junhyukoh/deep-reinforcement-learning-papers" -> "muupan/deep-reinforcement-learning-papers"
"junhyukoh/deep-reinforcement-learning-papers" -> "rll/rllab"
"junhyukoh/deep-reinforcement-learning-papers" -> "carpedm20/deep-rl-tensorflow"
"junhyukoh/deep-reinforcement-learning-papers" -> "devsisters/DQN-tensorflow"
"junhyukoh/deep-reinforcement-learning-papers" -> "aikorea/awesome-rl"
"junhyukoh/deep-reinforcement-learning-papers" -> "keras-rl/keras-rl"
"junhyukoh/deep-reinforcement-learning-papers" -> "dennybritz/deeplearning-papernotes" ["e"=1]
"junhyukoh/deep-reinforcement-learning-papers" -> "rlcode/reinforcement-learning"
"junhyukoh/deep-reinforcement-learning-papers" -> "andrewliao11/Deep-Reinforcement-Learning-Survey"
"junhyukoh/deep-reinforcement-learning-papers" -> "awjuliani/DeepRL-Agents"
"junhyukoh/deep-reinforcement-learning-papers" -> "williamFalcon/DeepRLHacks"
"junhyukoh/deep-reinforcement-learning-papers" -> "miyosuda/async_deep_reinforce"
"junhyukoh/deep-reinforcement-learning-papers" -> "LantaoYu/MARL-Papers"
"junhyukoh/deep-reinforcement-learning-papers" -> "dennybritz/reinforcement-learning"
"junhyukoh/deep-reinforcement-learning-papers" -> "berkeleydeeprlcourse/homework"
"wrighteagle2d/wrighteaglebase" -> "herodrigues/robocup2d-tutorial"
"wrighteagle2d/wrighteaglebase" -> "helios-base/helios-base"
"wrighteagle2d/wrighteaglebase" -> "naderzare/CYRUS2014"
"wrighteagle2d/wrighteaglebase" -> "wrighteagle2d/autotest2d"
"wrighteagle2d/wrighteaglebase" -> "rcsoccersim/rcssserver"
"wrighteagle2d/wrighteaglebase" -> "kengz/robocup-soccer"
"wrighteagle2d/wrighteaglebase" -> "LARG/HFO"
"wrighteagle2d/wrighteaglebase" -> "Cyrus2D/Cyrus2DBase"
"vwxyzjn/ppo-implementation-details" -> "Lizhi-sjtu/DRL-code-pytorch"
"vwxyzjn/ppo-implementation-details" -> "nikhilbarhate99/PPO-PyTorch"
"vwxyzjn/ppo-implementation-details" -> "vwxyzjn/cleanrl"
"vwxyzjn/ppo-implementation-details" -> "marlbenchmark/on-policy"
"vwxyzjn/ppo-implementation-details" -> "sail-sg/envpool" ["e"=1]
"vwxyzjn/ppo-implementation-details" -> "ericyangyu/PPO-for-Beginners"
"vwxyzjn/ppo-implementation-details" -> "DLR-RM/rl-baselines3-zoo"
"vwxyzjn/ppo-implementation-details" -> "AI4Finance-Foundation/ElegantRL"
"vwxyzjn/ppo-implementation-details" -> "kaixindelele/DRLib"
"vwxyzjn/ppo-implementation-details" -> "vwxyzjn/invalid-action-masking"
"vwxyzjn/ppo-implementation-details" -> "Stable-Baselines-Team/stable-baselines3-contrib"
"vwxyzjn/ppo-implementation-details" -> "MarcoMeter/recurrent-ppo-truncated-bptt" ["e"=1]
"vwxyzjn/ppo-implementation-details" -> "opendilab/PPOxFamily" ["e"=1]
"vwxyzjn/ppo-implementation-details" -> "XinJingHao/PPO-Continuous-Pytorch"
"vwxyzjn/ppo-implementation-details" -> "kzl/decision-transformer" ["e"=1]
"nikitasrivatsan/DeepLearningVideoGames" -> "spragunr/deep_q_rl"
"nikitasrivatsan/DeepLearningVideoGames" -> "siemanko/tensorflow-deepq"
"nikitasrivatsan/DeepLearningVideoGames" -> "yenchenlin/DeepLearningFlappyBird"
"nikitasrivatsan/DeepLearningVideoGames" -> "tambetm/simple_dqn"
"nikitasrivatsan/DeepLearningVideoGames" -> "sourabhv/FlapPyBird"
"nikitasrivatsan/DeepLearningVideoGames" -> "coreylynch/async-rl"
"nikitasrivatsan/DeepLearningVideoGames" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"nikitasrivatsan/DeepLearningVideoGames" -> "harvitronix/reinforcement-learning-car"
"nikitasrivatsan/DeepLearningVideoGames" -> "devsisters/DQN-tensorflow"
"nikitasrivatsan/DeepLearningVideoGames" -> "carpedm20/deep-rl-tensorflow"
"nikitasrivatsan/DeepLearningVideoGames" -> "miyosuda/async_deep_reinforce"
"nikitasrivatsan/DeepLearningVideoGames" -> "muupan/async-rl"
"nikitasrivatsan/DeepLearningVideoGames" -> "gliese581gg/DQN_tensorflow"
"nikitasrivatsan/DeepLearningVideoGames" -> "openai/universe-starter-agent"
"nikitasrivatsan/DeepLearningVideoGames" -> "muupan/deep-reinforcement-learning-papers"
"EliFUT/android" -> "ihofmann/open-websoccer"
"sanmuyang/multi-agent-PPO-on-SMAC" -> "hijkzzz/noisy-mappo"
"oxwhirl/facmac" -> "schroederdewitt/multiagent_mujoco"
"oxwhirl/facmac" -> "tjuHaoXiaotian/pymarl3"
"pengyang7881187/FedRL" -> "glenn89/FederatedRL"
"pengyang7881187/FedRL" -> "hizircanbayram/FeDQN-A-Federated-Learning-Approach-for-Training-Reinforcement-Learning-Agent-of-Atari-Games"
"muupan/deep-reinforcement-learning-papers" -> "junhyukoh/deep-reinforcement-learning-papers"
"muupan/deep-reinforcement-learning-papers" -> "muupan/async-rl"
"muupan/deep-reinforcement-learning-papers" -> "spragunr/deep_q_rl"
"muupan/deep-reinforcement-learning-papers" -> "carpedm20/deep-rl-tensorflow"
"muupan/deep-reinforcement-learning-papers" -> "miyosuda/async_deep_reinforce"
"muupan/deep-reinforcement-learning-papers" -> "tambetm/simple_dqn"
"muupan/deep-reinforcement-learning-papers" -> "rll/rllab"
"muupan/deep-reinforcement-learning-papers" -> "siemanko/tensorflow-deepq"
"muupan/deep-reinforcement-learning-papers" -> "andrewliao11/Deep-Reinforcement-Learning-Survey"
"muupan/deep-reinforcement-learning-papers" -> "coreylynch/async-rl"
"muupan/deep-reinforcement-learning-papers" -> "joschu/modular_rl"
"muupan/deep-reinforcement-learning-papers" -> "devsisters/DQN-tensorflow"
"muupan/deep-reinforcement-learning-papers" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"muupan/deep-reinforcement-learning-papers" -> "hi-abhi/tensorflow-value-iteration-networks"
"muupan/deep-reinforcement-learning-papers" -> "chainer/chainerrl"
"yangchen1997/Multi-Agent-Reinforcement-Learning" -> "marlbenchmark/off-policy"
"yangchen1997/Multi-Agent-Reinforcement-Learning" -> "Lizhi-sjtu/MARL-code-pytorch"
"yangchen1997/Multi-Agent-Reinforcement-Learning" -> "oli233/corona-analysis" ["e"=1]
"yangchen1997/Multi-Agent-Reinforcement-Learning" -> "thesouther/MARL"
"yangchen1997/Multi-Agent-Reinforcement-Learning" -> "GaaraZhu/bamboo-on-teams" ["e"=1]
"XinJingHao/PPO-Continuous-Pytorch" -> "vwxyzjn/PPO-Implementation-Deep-Dive"
"AltmanD/guandan_mcc" -> "submit-paper/Danzero_plus"
"AltmanD/guandan_mcc" -> "samuelgzx/RL_GuanDan"
"Kaixhin/Atari" -> "Kaixhin/rlenvs" ["e"=1]
"Kaixhin/Atari" -> "twitter-archive/torch-twrl" ["e"=1]
"Kaixhin/Atari" -> "iassael/torch-bootstrapped-dqn"
"Kaixhin/Atari" -> "ludc/rltorch" ["e"=1]
"Kaixhin/Atari" -> "muupan/async-rl"
"Kaixhin/Atari" -> "miyosuda/async_deep_reinforce"
"Kaixhin/Atari" -> "google-deepmind/alewrap" ["e"=1]
"Kaixhin/Atari" -> "fmassa/optimize-net" ["e"=1]
"Kaixhin/Atari" -> "twitter-archive/torch-autograd" ["e"=1]
"Kaixhin/Atari" -> "vivanov879/draw" ["e"=1]
"Kaixhin/Atari" -> "yobibyte/atarigrandchallenge"
"Kaixhin/Atari" -> "Ardavans/DSR"
"5vision/DARQN" -> "iassael/torch-bootstrapped-dqn"
"5vision/DARQN" -> "SeanNaren/QlearningExample.torch" ["e"=1]
"5vision/DARQN" -> "yilunc2020/Attention-DQN" ["e"=1]
"5vision/DARQN" -> "5vision/deep-reinforcement-learning-networks"
"facebookarchive/MazeBase" -> "Kaixhin/rlenvs" ["e"=1]
"facebookarchive/MazeBase" -> "Ardavans/DSR"
"facebookarchive/MazeBase" -> "avivt/VIN"
"facebookarchive/MazeBase" -> "facebookarchive/learningSimpleAlgorithms"
"facebookarchive/MazeBase" -> "junhyukoh/value-prediction-network"
"cyanrain7/TRPO-in-MARL" -> "schroederdewitt/multiagent_mujoco"
"cyanrain7/TRPO-in-MARL" -> "PKU-MARL/Multi-Agent-Transformer"
"cyanrain7/TRPO-in-MARL" -> "PKU-MARL/HARL"
"cyanrain7/TRPO-in-MARL" -> "morning9393/HAPPO-HATRPO"
"cyanrain7/TRPO-in-MARL" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation" ["e"=1]
"cyanrain7/TRPO-in-MARL" -> "uoe-agents/epymarl"
"cyanrain7/TRPO-in-MARL" -> "morning9393/Optimal-Baseline-for-Multi-agent-Policy-Gradients"
"cyanrain7/TRPO-in-MARL" -> "oxwhirl/facmac"
"cyanrain7/TRPO-in-MARL" -> "marlbenchmark/off-policy"
"cyanrain7/TRPO-in-MARL" -> "Replicable-MARL/MARLlib"
"cyanrain7/TRPO-in-MARL" -> "marlbenchmark/on-policy"
"tjuHaoXiaotian/pymarl3" -> "oxwhirl/smacv2"
"tjuHaoXiaotian/pymarl3" -> "hijkzzz/pymarl2"
"tjuHaoXiaotian/pymarl3" -> "oxwhirl/facmac"
"tjuHaoXiaotian/pymarl3" -> "mttga/pymarl_transformers"
"tjuHaoXiaotian/pymarl3" -> "uoe-agents/epymarl"
"tjuHaoXiaotian/pymarl3" -> "PKU-MARL/Multi-Agent-Transformer"
"tjuHaoXiaotian/pymarl3" -> "PKU-MARL/HARL"
"tjuHaoXiaotian/pymarl3" -> "TonghanWang/ROMA"
"mrkulk/hierarchical-deep-RL" -> "EdenMacdonald/h-DQN"
"mrkulk/hierarchical-deep-RL" -> "skumar9876/Hierarchical-DQN"
"Code-Bullet/Jump-King" -> "Code-Bullet/Tetris-AI-Javascript"
"Code-Bullet/Jump-King" -> "Code-Bullet/RickAndMortai"
"tianjunz/NovelD" -> "tedmoskovitz/TOP"
"facebookresearch/off-belief-learning" -> "hengyuan-hu/instruct-rl"
"allenai/ScienceWorld" -> "cognitiveailab/TextWorldExpress"
"allenai/ScienceWorld" -> "alfworld/alfworld" ["e"=1]
"allenai/ScienceWorld" -> "SwiftSage/SwiftSage"
"allenai/ScienceWorld" -> "allenai/discoveryworld"
"allenai/ScienceWorld" -> "IBM/commonsense-rl"
"allenai/ScienceWorld" -> "microsoft/tdqn"
"allenai/ScienceWorld" -> "princeton-nlp/calm-textgame"
"allenai/ScienceWorld" -> "microsoft/SmartPlay" ["e"=1]
"robocode-dev/tank-royale" -> "robo-code/robocode"
"PeymanTehrani/FDRL-PC-Dyspan" -> "Mauriyin/FLDRL-in-Wireless-Communication"
"yunke-wang/UID" -> "yunke-wang/SAIL"
"yunke-wang/UID" -> "yunke-wang/WGAIL"
"tonylitianyu/Preference-Planning-Deep-IRL" -> "TroddenSpade/Maximum-Entropy-Deep-IRL"
"bic4907/Overcooked-AI" -> "samjia2000/HSP"
"bic4907/Overcooked-AI" -> "liyang619/COLE-Platform"
"instadeepai/awesome-marl" -> "instadeepai/matrax"
"instadeepai/awesome-marl" -> "instadeepai/marl-eval"
"Div99/IQ-Learn" -> "Farama-Foundation/D4RL-Evaluations" ["e"=1]
"Div99/IQ-Learn" -> "toshikwa/gail-airl-ppo.pytorch"
"Div99/IQ-Learn" -> "justinjfu/inverse_rl"
"Div99/IQ-Learn" -> "kristery/Awesome-Imitation-Learning"
"Div99/IQ-Learn" -> "syuntoku14/pytorch-rl-il" ["e"=1]
"Div99/IQ-Learn" -> "denisyarats/exorl" ["e"=1]
"Div99/IQ-Learn" -> "Ericonaldo/ILSwiss"
"ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer" -> "YiqinYang/ICQ"
"ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer" -> "instadeepai/og-marl"
"TroddenSpade/Federated-DRL" -> "TroddenSpade/Flexible-Conditional-Imitation-Learning"
"TroddenSpade/Federated-DRL" -> "TroddenSpade/Curiosity-driven-Exploration-in-Navigation"
"TroddenSpade/Federated-DRL" -> "TroddenSpade/Exhaustive-Reinforcement-Learning"
"TroddenSpade/Federated-DRL" -> "TroddenSpade/Soccer-Players-Tracking"
"TroddenSpade/Federated-DRL" -> "TroddenSpade/GAN-NST"
"google-deepmind/diplomacy" -> "facebookresearch/diplomacy_searchbot"
"google-deepmind/diplomacy" -> "diplomacy/research"
"agrimgupta92/metamorph" -> "agrimgupta92/derl"
"agrimgupta92/metamorph" -> "Khrylx/Transform2Act" ["e"=1]
"agrimgupta92/metamorph" -> "frt03/mxt_bench"
"ShprAlex/SproutLife" -> "thopit/Creatures"
"facebookresearch/moolib" -> "facebookresearch/rlmeta"
"facebookresearch/moolib" -> "facebookresearch/minihack"
"facebookresearch/moolib" -> "facebookresearch/torchbeast"
"facebookresearch/moolib" -> "ucl-dark/paired"
"facebookresearch/moolib" -> "mila-iqia/spr" ["e"=1]
"mansicer/MAIC" -> "UnrealTracking/ToM2C"
"mansicer/MAIC" -> "chenf-ai/Multi-Agent-Communication-Considering-Representation-Learning"
"UnrealTracking/ToM2C" -> "mansicer/MAIC"
"UnrealTracking/ToM2C" -> "PKU-RL/I2C"
"TARTRL/TiKick" -> "OpenRL-Lab/TiZero"
"TARTRL/TiKick" -> "WentseChen/Soft-QMIX"
"ling-pan/RES" -> "morning9393/Optimal-Baseline-for-Multi-agent-Policy-Gradients"
"ruizhaogit/maximum_entropy_population_based_training" -> "samjia2000/HSP"
"daochenzha/dreamshard" -> "daochenzha/autoshard"
"daochenzha/dreamshard" -> "daochenzha/neuroshard"
"microsoft/FOST" -> "HFrost0/TGC_torch" ["e"=1]
"microsoft/FOST" -> "microsoft/maro"
"microsoft/FOST" -> "thanhtrunghuynh93/estimate" ["e"=1]
"kevinzakka/ibc" -> "dsbrown1331/CoRL2019-DREX"
"waterhorse1/NAC" -> "diversepsro/diverse_psro"
"Cyrus2D/Cyrus2DBase" -> "helios-base/helios-base"
"Cyrus2D/Cyrus2DBase" -> "Cyrus2D/Pyrus2D"
"Cyrus2D/Cyrus2DBase" -> "Cyrus2D/Pyrus-SS2D-Base"
"Cyrus2D/Cyrus2DBase" -> "CLSFramework/py2d"
"Cyrus2D/Cyrus2DBase" -> "helios-base/librcsc"
"Cyrus2D/Cyrus2DBase" -> "Cyrus2D/SoccerSimulationProxy"
"datamllab/BED_main" -> "ynchuang/DiscoverPath"
"datamllab/BED_main" -> "daochenzha/Meta-AAD"
"datamllab/BED_main" -> "qxtian/Learning-Independent-SKills"
"datamllab/BED_main" -> "daochenzha/rapid"
"benellis3/pymarl2" -> "benellis3/mappo"
"angloth/auto-drift" -> "karanamrahul/Autonomous-Drifting-using-deep-Reinforcement-Learning"
"karanamrahul/Autonomous-Drifting-using-deep-Reinforcement-Learning" -> "angloth/auto-drift"
"martius-lab/HiTS" -> "trzhang0116/HRAC"
"siomvas/awesome-federated-reinforcement-learning" -> "bourbonut/reinforcedFL"
"siomvas/awesome-federated-reinforcement-learning" -> "hizircanbayram/FeDQN-A-Federated-Learning-Approach-for-Training-Reinforcement-Learning-Agent-of-Atari-Games"
"siomvas/awesome-federated-reinforcement-learning" -> "flint-xf-fan/Byzantine-Federated-RL"
"siomvas/awesome-federated-reinforcement-learning" -> "Mauriyin/FLDRL-in-Wireless-Communication"
"siomvas/awesome-federated-reinforcement-learning" -> "glenn89/FederatedRL"
"siomvas/awesome-federated-reinforcement-learning" -> "nishantkr18/federated-model-averaging-for-DQN"
"siomvas/awesome-federated-reinforcement-learning" -> "pengyang7881187/FedRL"
"bourbonut/reinforcedFL" -> "siomvas/awesome-federated-reinforcement-learning"
"liamhebert/FedFormer" -> "DesikRengarajan/FEDORA"
"sjtu-marl/bd_rd_psro" -> "diversepsro/diverse_psro"
"AlexKashi/AlphaHoldem" -> "bupticybee/AlphaNLHoldem"
"mrkulk/deepQN_tensorflow" -> "gliese581gg/DQN_tensorflow"
"mrkulk/deepQN_tensorflow" -> "Jabberwockyll/deep_rl_ale"
"Vincentzyx/Douzero_Resnet" -> "EdwardPooh/douzero-resnet-2.0"
"Vincentzyx/Douzero_Resnet" -> "submit-paper/Doudizhu_plus"
"Vincentzyx/Douzero_Resnet" -> "RuBP17/AlphaDou"
"maciej-sypetkowski/autoascend" -> "ngoodger/nle-language-wrapper"
"maciej-sypetkowski/autoascend" -> "upiterbarg/hihack"
"xieliang555/SFN" -> "guodashun/peg-in-hole-gym"
"xieliang555/SFN" -> "rasmushaugaard/peg-in-hole-visual-servoing"
"juanjose49/omscs-cs7641-machine-learning-assignment-4" -> "JonathanTay/CS-7641-assignment-4"
"juanjose49/omscs-cs7641-machine-learning-assignment-4" -> "JonathanTay/CS-7641-assignment-3"
"juanjose49/omscs-cs7641-machine-learning-assignment-4" -> "JonathanTay/CS-7641-assignment-1"
"juanjose49/omscs-cs7641-machine-learning-assignment-4" -> "mjs2600/ML-Final-Exam-Study-Notes"
"tuomaso/radial_rl_v2" -> "huanzhang12/ATLA_robust_RL"
"tuomaso/radial_rl_v2" -> "chenhongge/SA_DQN"
"tuomaso/radial_rl_v2" -> "umd-huang-lab/WocaR-RL"
"submit-paper/Doudizhu_plus" -> "EdwardPooh/douzero-resnet-2.0"
"submit-paper/Doudizhu_plus" -> "Vincentzyx/Douzero_Resnet"
"submit-paper/Doudizhu_plus" -> "1310183534/DouDiZhu"
"submit-paper/Doudizhu_plus" -> "RuBP17/AlphaDou"
"vzhong/silg" -> "microsoft/tdqn"
"amacati/SoulsAI" -> "amacati/SoulsGym"
"Ze-Rax/TrackmaniaFlagRush" -> "reaby/TMModeTemplate"
"sunghoonhong/SWAT" -> "frt03/mxt_bench"
"sunghoonhong/SWAT" -> "drdh/Synergy-RL"
"sunghoonhong/SWAT" -> "yobibyte/amorpheus"
"princeton-nlp/XTX" -> "XiaoxiaoGuo/rcdqn"
"daochenzha/autoshard" -> "daochenzha/dreamshard"
"daochenzha/autoshard" -> "daochenzha/neuroshard"
"yunke-wang/WGAIL" -> "yunke-wang/SAIL"
"yunke-wang/WGAIL" -> "yunke-wang/UID"
"Farama-Foundation/Gymnasium" -> "DLR-RM/stable-baselines3"
"Farama-Foundation/Gymnasium" -> "vwxyzjn/cleanrl"
"Farama-Foundation/Gymnasium" -> "openai/gym"
"Farama-Foundation/Gymnasium" -> "Farama-Foundation/PettingZoo"
"Farama-Foundation/Gymnasium" -> "google-deepmind/mujoco" ["e"=1]
"Farama-Foundation/Gymnasium" -> "openai/spinningup"
"Farama-Foundation/Gymnasium" -> "pytorch/rl"
"Farama-Foundation/Gymnasium" -> "DLR-RM/rl-baselines3-zoo"
"Farama-Foundation/Gymnasium" -> "thu-ml/tianshou"
"Farama-Foundation/Gymnasium" -> "openai/baselines"
"Farama-Foundation/Gymnasium" -> "isaac-sim/IsaacGymEnvs" ["e"=1]
"Farama-Foundation/Gymnasium" -> "isaac-sim/IsaacLab" ["e"=1]
"Farama-Foundation/Gymnasium" -> "google-deepmind/dm_control"
"Farama-Foundation/Gymnasium" -> "google-deepmind/mujoco_menagerie" ["e"=1]
"Farama-Foundation/Gymnasium" -> "ray-project/ray" ["e"=1]
"openai/gym" -> "openai/baselines"
"openai/gym" -> "dennybritz/reinforcement-learning"
"openai/gym" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"openai/gym" -> "Farama-Foundation/Gymnasium"
"openai/gym" -> "aikorea/awesome-rl"
"openai/gym" -> "openai/spinningup"
"openai/gym" -> "keras-team/keras" ["e"=1]
"openai/gym" -> "tensorflow/models" ["e"=1]
"openai/gym" -> "DLR-RM/stable-baselines3"
"openai/gym" -> "ray-project/ray" ["e"=1]
"openai/gym" -> "pytorch/pytorch" ["e"=1]
"openai/gym" -> "scikit-learn/scikit-learn" ["e"=1]
"openai/gym" -> "openai/universe"
"openai/gym" -> "tensorflow/tensorflow" ["e"=1]
"openai/gym" -> "google/dopamine"
"facebookresearch/AutoCAT" -> "facebookresearch/macta"
"openai/mujoco-py" -> "google-deepmind/dm_control"
"openai/mujoco-py" -> "google-deepmind/mujoco" ["e"=1]
"openai/mujoco-py" -> "Farama-Foundation/D4RL" ["e"=1]
"openai/mujoco-py" -> "openai/baselines"
"openai/mujoco-py" -> "rll/rllab"
"openai/mujoco-py" -> "openai/roboschool"
"openai/mujoco-py" -> "ARISE-Initiative/robosuite" ["e"=1]
"openai/mujoco-py" -> "openai/spinningup"
"openai/mujoco-py" -> "rail-berkeley/rlkit"
"openai/mujoco-py" -> "Farama-Foundation/Metaworld" ["e"=1]
"openai/mujoco-py" -> "rlworkgroup/garage"
"openai/mujoco-py" -> "google-deepmind/mujoco_menagerie" ["e"=1]
"openai/mujoco-py" -> "hill-a/stable-baselines"
"openai/mujoco-py" -> "isaac-sim/IsaacGymEnvs" ["e"=1]
"openai/mujoco-py" -> "DLR-RM/stable-baselines3"
"rll/rllab" -> "rlworkgroup/garage"
"rll/rllab" -> "joschu/modular_rl"
"rll/rllab" -> "tensorforce/tensorforce"
"rll/rllab" -> "openai/baselines"
"rll/rllab" -> "rail-berkeley/rlkit"
"rll/rllab" -> "keras-rl/keras-rl"
"rll/rllab" -> "junhyukoh/deep-reinforcement-learning-papers"
"rll/rllab" -> "carpedm20/deep-rl-tensorflow"
"rll/rllab" -> "openai/roboschool"
"rll/rllab" -> "google-deepmind/dm_control"
"rll/rllab" -> "berkeleydeeprlcourse/homework"
"rll/rllab" -> "ShangtongZhang/DeepRL"
"rll/rllab" -> "astooke/rlpyt"
"rll/rllab" -> "openai/universe-starter-agent"
"rll/rllab" -> "google-deepmind/lab"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "boyu-ai/Hands-on-RL"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "datawhalechina/easy-rl"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "jwk1rose/RL_Learning"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "wangshusen/DRL"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "vwxyzjn/cleanrl"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "XinJingHao/DRL-Pytorch"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "TianxingChen/Embodied-AI-Guide" ["e"=1]
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "DLR-RM/stable-baselines3"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "thu-ml/tianshou"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "openai/spinningup"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "OpenRLHF/OpenRLHF" ["e"=1]
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "Farama-Foundation/Gymnasium"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "AI4Finance-Foundation/ElegantRL"
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" -> "mli/paper-reading" ["e"=1]
"Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning" -> "iassael/torch-bootstrapped-dqn"
"miyosuda/async_deep_reinforce" -> "muupan/async-rl"
"miyosuda/async_deep_reinforce" -> "NVlabs/GA3C"
"miyosuda/async_deep_reinforce" -> "coreylynch/async-rl"
"miyosuda/async_deep_reinforce" -> "miyosuda/unreal"
"miyosuda/async_deep_reinforce" -> "openai/universe-starter-agent"
"miyosuda/async_deep_reinforce" -> "joschu/modular_rl"
"miyosuda/async_deep_reinforce" -> "yao62995/A3C"
"miyosuda/async_deep_reinforce" -> "carpedm20/deep-rl-tensorflow"
"miyosuda/async_deep_reinforce" -> "Kaixhin/Atari"
"miyosuda/async_deep_reinforce" -> "hi-abhi/tensorflow-value-iteration-networks"
"miyosuda/async_deep_reinforce" -> "steveKapturowski/tensorflow-rl"
"miyosuda/async_deep_reinforce" -> "Ardavans/DSR"
"miyosuda/async_deep_reinforce" -> "muupan/deep-reinforcement-learning-papers"
"miyosuda/async_deep_reinforce" -> "rll/rllab"
"miyosuda/async_deep_reinforce" -> "devsisters/DQN-tensorflow"
"coreylynch/async-rl" -> "miyosuda/async_deep_reinforce"
"coreylynch/async-rl" -> "muupan/async-rl"
"coreylynch/async-rl" -> "siemanko/tensorflow-deepq"
"coreylynch/async-rl" -> "carpedm20/deep-rl-tensorflow"
"coreylynch/async-rl" -> "openai/universe-starter-agent"
"coreylynch/async-rl" -> "osh/kerlym"
"coreylynch/async-rl" -> "joschu/modular_rl"
"coreylynch/async-rl" -> "tambetm/simple_dqn"
"coreylynch/async-rl" -> "NVlabs/GA3C"
"coreylynch/async-rl" -> "spragunr/deep_q_rl"
"coreylynch/async-rl" -> "rll/rllab"
"coreylynch/async-rl" -> "miyosuda/unreal"
"coreylynch/async-rl" -> "devsisters/DQN-tensorflow"
"coreylynch/async-rl" -> "keras-rl/keras-rl"
"coreylynch/async-rl" -> "muupan/deep-reinforcement-learning-papers"
"yukezhu/tensorflow-reinforce" -> "joschu/modular_rl"
"yukezhu/tensorflow-reinforce" -> "miyosuda/async_deep_reinforce"
"yukezhu/tensorflow-reinforce" -> "pemami4911/deep-rl"
"yukezhu/tensorflow-reinforce" -> "steveKapturowski/tensorflow-rl"
"yukezhu/tensorflow-reinforce" -> "carpedm20/deep-rl-tensorflow"
"yukezhu/tensorflow-reinforce" -> "yrlu/reinforcement_learning"
"yukezhu/tensorflow-reinforce" -> "awjuliani/DeepRL-Agents"
"yukezhu/tensorflow-reinforce" -> "muupan/async-rl"
"yukezhu/tensorflow-reinforce" -> "coreylynch/async-rl"
"yukezhu/tensorflow-reinforce" -> "pat-coady/trpo"
"yukezhu/tensorflow-reinforce" -> "facebookarchive/MIXER" ["e"=1]
"yukezhu/tensorflow-reinforce" -> "andrewliao11/Deep-Reinforcement-Learning-Survey"
"yukezhu/tensorflow-reinforce" -> "NVlabs/GA3C"
"yukezhu/tensorflow-reinforce" -> "rmst/ddpg"
"yukezhu/tensorflow-reinforce" -> "rll/rllab"
"Farama-Foundation/Arcade-Learning-Environment" -> "spragunr/deep_q_rl"
"Farama-Foundation/Arcade-Learning-Environment" -> "rll/rllab"
"Farama-Foundation/Arcade-Learning-Environment" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"Farama-Foundation/Arcade-Learning-Environment" -> "Farama-Foundation/Minigrid"
"Farama-Foundation/Arcade-Learning-Environment" -> "openai/retro"
"Farama-Foundation/Arcade-Learning-Environment" -> "google-deepmind/lab"
"Farama-Foundation/Arcade-Learning-Environment" -> "devsisters/DQN-tensorflow"
"Farama-Foundation/Arcade-Learning-Environment" -> "openai/atari-py"
"Farama-Foundation/Arcade-Learning-Environment" -> "google-deepmind/open_spiel"
"Farama-Foundation/Arcade-Learning-Environment" -> "pathak22/noreward-rl"
"Farama-Foundation/Arcade-Learning-Environment" -> "google-deepmind/dm_control"
"Farama-Foundation/Arcade-Learning-Environment" -> "tambetm/simple_dqn"
"Farama-Foundation/Arcade-Learning-Environment" -> "Farama-Foundation/ViZDoom"
"Farama-Foundation/Arcade-Learning-Environment" -> "ntasfi/PyGame-Learning-Environment"
"Farama-Foundation/Arcade-Learning-Environment" -> "openai/baselines"
"joschu/modular_rl" -> "ikostrikov/pytorch-trpo"
"joschu/modular_rl" -> "pat-coady/trpo"
"joschu/modular_rl" -> "rll/rllab"
"joschu/modular_rl" -> "miyosuda/async_deep_reinforce"
"joschu/modular_rl" -> "openai/imitation"
"joschu/modular_rl" -> "rlbayes/rllabplusplus"
"joschu/modular_rl" -> "muupan/async-rl"
"joschu/modular_rl" -> "hi-abhi/tensorflow-value-iteration-networks"
"joschu/modular_rl" -> "wojzaremba/trpo"
"joschu/modular_rl" -> "openai/vime"
"joschu/modular_rl" -> "coreylynch/async-rl"
"joschu/modular_rl" -> "rmst/ddpg"
"joschu/modular_rl" -> "cbfinn/gps"
"joschu/modular_rl" -> "openai/universe-starter-agent"
"joschu/modular_rl" -> "openai/roboschool"
"lansinuote/Simple_Reinforcement_Learning" -> "lansinuote/More_Simple_Reinforcement_Learning"
"lansinuote/Simple_Reinforcement_Learning" -> "rexrex9/reinforcement_torch_pfrl"
"lansinuote/Simple_Reinforcement_Learning" -> "DeepRLChinese/DeepRL-Chinese"
"lansinuote/Simple_Reinforcement_Learning" -> "ClownW/Reinforcement-learning-with-PyTorch"
"lansinuote/Simple_Reinforcement_Learning" -> "jwk1rose/RL_Learning"
"lansinuote/Simple_Reinforcement_Learning" -> "boyu-ai/Hands-on-RL"
"lansinuote/Simple_Reinforcement_Learning" -> "ziwenhahaha/Code-of-RL-Beginning"
"markriedl/transformer-walkthrough" -> "pytorch/workshops"
"markriedl/transformer-walkthrough" -> "iamjakewarner/jdf"
"microsoft/malmo" -> "google-deepmind/lab"
"microsoft/malmo" -> "openai/universe"
"microsoft/malmo" -> "minerllabs/minerl"
"microsoft/malmo" -> "rll/rllab"
"microsoft/malmo" -> "tambetm/gym-minecraft"
"microsoft/malmo" -> "Farama-Foundation/Arcade-Learning-Environment"
"microsoft/malmo" -> "Farama-Foundation/ViZDoom"
"microsoft/malmo" -> "crowdAI/marLo"
"microsoft/malmo" -> "google-deepmind/pysc2" ["e"=1]
"microsoft/malmo" -> "openai/baselines"
"microsoft/malmo" -> "Farama-Foundation/Minigrid"
"microsoft/malmo" -> "microsoft/CNTK" ["e"=1]
"microsoft/malmo" -> "openai/universe-starter-agent"
"microsoft/malmo" -> "TorchCraft/TorchCraft" ["e"=1]
"microsoft/malmo" -> "google/dopamine"
"yenchenlin/DeepLearningFlappyBird" -> "lengstrom/fast-style-transfer" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "nikitasrivatsan/DeepLearningVideoGames"
"yenchenlin/DeepLearningFlappyBird" -> "floodsung/DRL-FlappyBird"
"yenchenlin/DeepLearningFlappyBird" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"yenchenlin/DeepLearningFlappyBird" -> "Rochester-NRT/RocAlphaGo" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "dennybritz/reinforcement-learning"
"yenchenlin/DeepLearningFlappyBird" -> "devsisters/DQN-tensorflow"
"yenchenlin/DeepLearningFlappyBird" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"yenchenlin/DeepLearningFlappyBird" -> "junhyukoh/deep-reinforcement-learning-papers"
"yenchenlin/DeepLearningFlappyBird" -> "aikorea/awesome-rl"
"yenchenlin/DeepLearningFlappyBird" -> "udacity/deep-learning" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "kjw0612/awesome-deep-vision" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "carpedm20/DCGAN-tensorflow" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "tflearn/tflearn" ["e"=1]
"yenchenlin/DeepLearningFlappyBird" -> "sourabhv/FlapPyBird"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "muupan/deep-reinforcement-learning-papers"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "junhyukoh/deep-reinforcement-learning-papers"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "carpedm20/deep-rl-tensorflow"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "algorithmdog/Reinforcement_Learning_Blog"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "miyosuda/async_deep_reinforce"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "hi-abhi/tensorflow-value-iteration-networks"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "rlbayes/rllabplusplus"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "yukezhu/tensorflow-reinforce"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "rll/rllab"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "williamFalcon/DeepRLHacks"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "steveKapturowski/tensorflow-rl"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "Kaixhin/NoisyNet-A3C"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "0bserver07/Study-Reinforcement-Learning"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "5vision/DARQN"
"andrewliao11/Deep-Reinforcement-Learning-Survey" -> "alibaba/gym-starcraft" ["e"=1]
"instadeepai/marl-eval" -> "instadeepai/matrax"
"instadeepai/marl-eval" -> "instadeepai/awesome-marl"
"instadeepai/marl-eval" -> "uoe-agents/smaclite"
"instadeepai/marl-eval" -> "instadeepai/og-marl"
"devsisters/DQN-tensorflow" -> "carpedm20/deep-rl-tensorflow"
"devsisters/DQN-tensorflow" -> "tambetm/simple_dqn"
"devsisters/DQN-tensorflow" -> "rll/rllab"
"devsisters/DQN-tensorflow" -> "junhyukoh/deep-reinforcement-learning-papers"
"devsisters/DQN-tensorflow" -> "awjuliani/DeepRL-Agents"
"devsisters/DQN-tensorflow" -> "siemanko/tensorflow-deepq"
"devsisters/DQN-tensorflow" -> "miyosuda/async_deep_reinforce"
"devsisters/DQN-tensorflow" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"devsisters/DQN-tensorflow" -> "keras-rl/keras-rl"
"devsisters/DQN-tensorflow" -> "coreylynch/async-rl"
"devsisters/DQN-tensorflow" -> "keon/deep-q-learning"
"devsisters/DQN-tensorflow" -> "rlcode/reinforcement-learning"
"devsisters/DQN-tensorflow" -> "spragunr/deep_q_rl"
"devsisters/DQN-tensorflow" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"devsisters/DQN-tensorflow" -> "openai/universe-starter-agent"
"facebookresearch/diplomacy_cicero" -> "google-deepmind/diplomacy"
"facebookresearch/diplomacy_cicero" -> "facebookresearch/diplomacy_searchbot"
"facebookresearch/diplomacy_cicero" -> "Farama-Foundation/chatarena" ["e"=1]
"facebookresearch/diplomacy_cicero" -> "google-deepmind/mctx" ["e"=1]
"facebookresearch/diplomacy_cicero" -> "google-deepmind/meltingpot"
"facebookresearch/diplomacy_cicero" -> "allenai/RL4LMs" ["e"=1]
"facebookresearch/diplomacy_cicero" -> "eloialonso/iris" ["e"=1]
"facebookresearch/diplomacy_cicero" -> "diplomacy/diplomacy"
"facebookresearch/diplomacy_cicero" -> "diplomacy/research"
"facebookresearch/diplomacy_cicero" -> "HumanCompatibleAI/overcooked_ai"
"facebookresearch/diplomacy_cicero" -> "FLAIROx/JaxMARL" ["e"=1]
"facebookresearch/diplomacy_cicero" -> "facebookresearch/rebel" ["e"=1]
"facebookresearch/diplomacy_cicero" -> "CarperAI/trlx" ["e"=1]
"facebookresearch/diplomacy_cicero" -> "rosewang2008/gym-cooking"
"facebookresearch/diplomacy_cicero" -> "Farama-Foundation/PettingZoo"
"floodsung/DDPG" -> "stevenpjg/ddpg-aigym"
"floodsung/DDPG" -> "yanpanlau/DDPG-Keras-Torcs"
"floodsung/DDPG" -> "rmst/ddpg"
"floodsung/DDPG" -> "ghliu/pytorch-ddpg"
"floodsung/DDPG" -> "pemami4911/deep-rl"
"floodsung/DDPG" -> "rll/rllab"
"floodsung/DDPG" -> "MOCR/DDPG"
"floodsung/DDPG" -> "ikostrikov/pytorch-ddpg-naf"
"floodsung/DDPG" -> "joschu/modular_rl"
"floodsung/DDPG" -> "hi-abhi/tensorflow-value-iteration-networks"
"floodsung/DDPG" -> "devsisters/DQN-tensorflow"
"floodsung/DDPG" -> "coreylynch/async-rl"
"floodsung/DDPG" -> "pat-coady/trpo"
"floodsung/DDPG" -> "junhyukoh/deep-reinforcement-learning-papers"
"floodsung/DDPG" -> "yukezhu/tensorflow-reinforce"
"Shanghai-Digital-Brain-Laboratory/DB-Football" -> "jidiai/GRF_MARL"
"Shanghai-Digital-Brain-Laboratory/DB-Football" -> "OpenRL-Lab/TiZero"
"Shanghai-Digital-Brain-Laboratory/DB-Football" -> "JBLanier/pipeline-psro"
"Shanghai-Digital-Brain-Laboratory/DB-Football" -> "indylab/nxdo"
"rmst/ddpg" -> "stevenpjg/ddpg-aigym"
"rmst/ddpg" -> "MOCR/DDPG"
"rmst/ddpg" -> "floodsung/DDPG"
"rmst/ddpg" -> "cookbenjamin/DDPG"
"rmst/ddpg" -> "carpedm20/NAF-tensorflow"
"rmst/ddpg" -> "joschu/modular_rl"
"rmst/ddpg" -> "pemami4911/deep-rl"
"rmst/ddpg" -> "liampetti/DDPG"
"rmst/ddpg" -> "yanpanlau/DDPG-Keras-Torcs"
"rmst/ddpg" -> "miyosuda/async_deep_reinforce"
"harvitronix/reinforcement-learning-car" -> "kanakkabara/Autonomous-Drifting"
"harvitronix/reinforcement-learning-car" -> "harvitronix/rl-rc-car"
"harvitronix/reinforcement-learning-car" -> "nikitasrivatsan/DeepLearningVideoGames"
"harvitronix/reinforcement-learning-car" -> "MLJejuCamp2017/DRL_based_SelfDrivingCarControl" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "thibo73800/metacar" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "songyanho/Reinforcement-Learning-for-Self-Driving-Cars" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "miyosuda/async_deep_reinforce"
"harvitronix/reinforcement-learning-car" -> "ugo-nama-kun/gym_torcs" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "jangirrishabh/toyCarIRL"
"harvitronix/reinforcement-learning-car" -> "yanpanlau/DDPG-Keras-Torcs"
"harvitronix/reinforcement-learning-car" -> "spragunr/deep_q_rl"
"harvitronix/reinforcement-learning-car" -> "muupan/deep-reinforcement-learning-papers"
"harvitronix/reinforcement-learning-car" -> "kaihuchen/DRL-AutonomousVehicles"
"harvitronix/reinforcement-learning-car" -> "flyyufelix/donkey_rl" ["e"=1]
"harvitronix/reinforcement-learning-car" -> "sisl/gail-driver"
"openai/atari-py" -> "bbitmaster/ale_python_interface"
"openai/atari-py" -> "Farama-Foundation/Arcade-Learning-Environment"
"openai/atari-py" -> "openai/doom-py"
"floodsung/DQN-Atari-Tensorflow" -> "gliese581gg/DQN_tensorflow"
"floodsung/DQN-Atari-Tensorflow" -> "gtoubassi/dqn-atari"
"robo-code/robocode" -> "robocode-dev/tank-royale"
"robo-code/robocode" -> "turkishviking/Python-Robocode"
"robo-code/robocode" -> "Voidious/Diamond"
"robo-code/robocode" -> "stevenpjg/QlearningRobocodeNN"
"BigBang1112/randomizer-tmf" -> "ArkadySK/GbxMapBrowser"
"PKU-MARL/Multi-Agent-Transformer" -> "PKU-MARL/HARL"
"PKU-MARL/Multi-Agent-Transformer" -> "marlbenchmark/on-policy"
"PKU-MARL/Multi-Agent-Transformer" -> "cyanrain7/TRPO-in-MARL"
"PKU-MARL/Multi-Agent-Transformer" -> "marlbenchmark/off-policy"
"PKU-MARL/Multi-Agent-Transformer" -> "Replicable-MARL/MARLlib"
"PKU-MARL/Multi-Agent-Transformer" -> "schroederdewitt/multiagent_mujoco"
"PKU-MARL/Multi-Agent-Transformer" -> "oxwhirl/smacv2"
"PKU-MARL/Multi-Agent-Transformer" -> "tjuHaoXiaotian/pymarl3"
"PKU-MARL/Multi-Agent-Transformer" -> "hijkzzz/pymarl2"
"PKU-MARL/Multi-Agent-Transformer" -> "chauncygu/Multi-Agent-Constrained-Policy-Optimisation" ["e"=1]
"PKU-MARL/Multi-Agent-Transformer" -> "uoe-agents/epymarl"
"PKU-MARL/Multi-Agent-Transformer" -> "tinyzqh/light_mappo"
"PKU-MARL/Multi-Agent-Transformer" -> "starry-sky6688/MARL-Algorithms"
"PKU-MARL/Multi-Agent-Transformer" -> "Lizhi-sjtu/MARL-code-pytorch"
"PKU-MARL/Multi-Agent-Transformer" -> "oxwhirl/smac"
"floodsung/DRL-FlappyBird" -> "yenchenlin/DeepLearningFlappyBird"
"floodsung/DRL-FlappyBird" -> "SarvagyaVaish/FlappyBirdRL"
"floodsung/DRL-FlappyBird" -> "algorithmdog/Reinforcement_Learning_Blog"
"floodsung/DRL-FlappyBird" -> "li-haoran/DRL-FlappyBird" ["e"=1]
"floodsung/DRL-FlappyBird" -> "gliese581gg/DQN_tensorflow"
"floodsung/DRL-FlappyBird" -> "nikitasrivatsan/DeepLearningVideoGames"
"floodsung/DRL-FlappyBird" -> "devsisters/DQN-tensorflow"
"floodsung/DRL-FlappyBird" -> "yanpanlau/Keras-FlappyBird"
"floodsung/DRL-FlappyBird" -> "floodsung/DQN-Atari-Tensorflow"
"floodsung/DRL-FlappyBird" -> "chncyhn/flappybird-qlearning-bot"
"floodsung/DRL-FlappyBird" -> "carpedm20/deep-rl-tensorflow"
"floodsung/DRL-FlappyBird" -> "coreylynch/async-rl"
"floodsung/DRL-FlappyBird" -> "siemanko/tensorflow-deepq"
"floodsung/DRL-FlappyBird" -> "borgwang/reinforce_py"
"floodsung/DRL-FlappyBird" -> "muupan/deep-reinforcement-learning-papers"
"aleju/mario-ai" -> "ehrenbrav/DeepQNetwork"
"aleju/mario-ai" -> "Chrispresso/SuperMarioBros-AI"
"aleju/mario-ai" -> "ppaquette/gym-super-mario"
"aleju/mario-ai" -> "Kautenja/gym-super-mario-bros"
"aleju/mario-ai" -> "rameshvarun/NeuralKart" ["e"=1]
"aleju/mario-ai" -> "coreylynch/async-rl"
"aleju/mario-ai" -> "xbpeng/DeepTerrainRL"
"aleju/mario-ai" -> "nikitasrivatsan/DeepLearningVideoGames"
"aleju/mario-ai" -> "tambetm/simple_dqn"
"aleju/mario-ai" -> "kuz/DeepMind-Atari-Deep-Q-Learner"
"aleju/mario-ai" -> "mam91/neat-genetic-mario"
"aleju/mario-ai" -> "wenkesj/mario"
"aleju/mario-ai" -> "ehrenbrav/FCEUX_Learning_Environment"
"aleju/mario-ai" -> "miyosuda/async_deep_reinforce"
"aleju/mario-ai" -> "siemanko/tensorflow-deepq"
"vmayoral/basic_reinforcement_learning" -> "erlerobot/gym-gazebo"
"vmayoral/basic_reinforcement_learning" -> "rlcode/reinforcement-learning"
"vmayoral/basic_reinforcement_learning" -> "awjuliani/DeepRL-Agents"
"vmayoral/basic_reinforcement_learning" -> "junhyukoh/deep-reinforcement-learning-papers"
"vmayoral/basic_reinforcement_learning" -> "aikorea/awesome-rl"
"vmayoral/basic_reinforcement_learning" -> "rll/rllab"
"vmayoral/basic_reinforcement_learning" -> "carpedm20/deep-rl-tensorflow"
"vmayoral/basic_reinforcement_learning" -> "mpatacchiola/dissecting-reinforcement-learning"
"vmayoral/basic_reinforcement_learning" -> "berkeleydeeprlcourse/homework"
"vmayoral/basic_reinforcement_learning" -> "muupan/deep-reinforcement-learning-papers"
"vmayoral/basic_reinforcement_learning" -> "keras-rl/keras-rl"
"vmayoral/basic_reinforcement_learning" -> "sudharsan13296/Hands-On-Reinforcement-Learning-With-Python"
"vmayoral/basic_reinforcement_learning" -> "devsisters/DQN-tensorflow"
"vmayoral/basic_reinforcement_learning" -> "joschu/modular_rl"
"vmayoral/basic_reinforcement_learning" -> "openai/roboschool"
"instadeepai/og-marl" -> "ling-pan/OMAR"
"instadeepai/og-marl" -> "thu-rllab/CFCQL"
"instadeepai/og-marl" -> "instadeepai/marl-eval"
"instadeepai/og-marl" -> "zbzhu99/madiff" ["e"=1]
"instadeepai/og-marl" -> "instadeepai/flashbax" ["e"=1]
"instadeepai/og-marl" -> "YiqinYang/ICQ"
"instadeepai/og-marl" -> "zzq-bot/offline-marl-framework-offpymarl"
"instadeepai/og-marl" -> "ZhengYinan-AIR/OMIGA"
"instadeepai/og-marl" -> "ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer"
"instadeepai/og-marl" -> "EdanToledo/Stoix" ["e"=1]
"instadeepai/og-marl" -> "TonghanWang/ROMA"
"instadeepai/og-marl" -> "instadeepai/awesome-marl"
"instadeepai/og-marl" -> "instadeepai/Mava"
"instadeepai/og-marl" -> "FLAIROx/JaxMARL" ["e"=1]
"ehrenbrav/DeepQNetwork" -> "ehrenbrav/FCEUX_Learning_Environment"
"ehrenbrav/DeepQNetwork" -> "ppaquette/gym-super-mario"
"pemami4911/deep-rl" -> "liampetti/DDPG"
"pemami4911/deep-rl" -> "stevenpjg/ddpg-aigym"
"pemami4911/deep-rl" -> "rmst/ddpg"
"pemami4911/deep-rl" -> "floodsung/DDPG"
"pemami4911/deep-rl" -> "yukezhu/tensorflow-reinforce"
"pemami4911/deep-rl" -> "yanpanlau/DDPG-Keras-Torcs"
"pemami4911/deep-rl" -> "pat-coady/trpo"
"pemami4911/deep-rl" -> "MG2033/A2C"
"pemami4911/deep-rl" -> "wojzaremba/trpo"
"pemami4911/deep-rl" -> "miyosuda/async_deep_reinforce"
"pemami4911/deep-rl" -> "MOCR/DDPG"
"pemami4911/deep-rl" -> "Damcy/prioritized-experience-replay"
"pemami4911/deep-rl" -> "carpedm20/deep-rl-tensorflow"
"yandexdataschool/AgentNet" -> "yandexdataschool/MLatImperial2017"
"yandexdataschool/AgentNet" -> "miyosuda/async_deep_reinforce"
"VinF/deer" -> "ADGEfficiency/energy-py" ["e"=1]
"VinF/deer" -> "muupan/async-rl"
"VinF/deer" -> "siemanko/tensorflow-deepq"
"VinF/deer" -> "rlpy/rlpy"
"VinF/deer" -> "miyosuda/async_deep_reinforce"
"VinF/deer" -> "spragunr/deep_q_rl"
"VinF/deer" -> "Ardavans/DSR"
"VinF/deer" -> "coreylynch/async-rl"
"VinF/deer" -> "osh/kerlym"
"VinF/deer" -> "joschu/modular_rl"
"farizrahman4u/qlearning4k" -> "osh/kerlym"
"farizrahman4u/qlearning4k" -> "spragunr/deep_q_rl"
"farizrahman4u/qlearning4k" -> "siemanko/tensorflow-deepq"
"farizrahman4u/qlearning4k" -> "kuza55/keras-extras" ["e"=1]
"farizrahman4u/qlearning4k" -> "EderSantana/seya" ["e"=1]
"farizrahman4u/qlearning4k" -> "avisingh599/visual-qa" ["e"=1]
"gliese581gg/DQN_tensorflow" -> "floodsung/DQN-Atari-Tensorflow"
"gliese581gg/DQN_tensorflow" -> "mrkulk/deepQN_tensorflow"
"gliese581gg/DQN_tensorflow" -> "gtoubassi/dqn-atari"
"ngoodger/nle-language-wrapper" -> "upiterbarg/hihack"
"ngoodger/nle-language-wrapper" -> "maciej-sypetkowski/autoascend"
"ntasfi/PyGame-Learning-Environment" -> "lusob/gym-ple"
"ntasfi/PyGame-Learning-Environment" -> "Farama-Foundation/Arcade-Learning-Environment"
"ntasfi/PyGame-Learning-Environment" -> "coreylynch/async-rl"
"ntasfi/PyGame-Learning-Environment" -> "NVlabs/GA3C"
"ntasfi/PyGame-Learning-Environment" -> "muupan/async-rl"
"ntasfi/PyGame-Learning-Environment" -> "carpedm20/deep-rl-tensorflow"
"ntasfi/PyGame-Learning-Environment" -> "rll/rllab"
"ntasfi/PyGame-Learning-Environment" -> "miyosuda/unreal"
"ntasfi/PyGame-Learning-Environment" -> "kidscancode/pygame_tutorials" ["e"=1]
"ntasfi/PyGame-Learning-Environment" -> "devsisters/DQN-tensorflow"
"ntasfi/PyGame-Learning-Environment" -> "VinF/deer"
"ntasfi/PyGame-Learning-Environment" -> "miyosuda/async_deep_reinforce"
"ntasfi/PyGame-Learning-Environment" -> "nikitasrivatsan/DeepLearningVideoGames"
"ntasfi/PyGame-Learning-Environment" -> "Farama-Foundation/Miniworld"
"ntasfi/PyGame-Learning-Environment" -> "Mekire/pygame-samples" ["e"=1]
"amarack/python-rl" -> "armahmood/nonstationary-experiments"
"muupan/async-rl" -> "miyosuda/async_deep_reinforce"
"muupan/async-rl" -> "coreylynch/async-rl"
"muupan/async-rl" -> "NVlabs/GA3C"
"muupan/async-rl" -> "muupan/deep-reinforcement-learning-papers"
"muupan/async-rl" -> "joschu/modular_rl"
"muupan/async-rl" -> "miyosuda/unreal"
"muupan/async-rl" -> "Kaixhin/Atari"
"muupan/async-rl" -> "Ardavans/DSR"
"muupan/async-rl" -> "Jabberwockyll/deep_rl_ale"
"muupan/async-rl" -> "Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning"
"muupan/async-rl" -> "chainer/chainerrl"
"muupan/async-rl" -> "openai/universe-starter-agent"
"muupan/async-rl" -> "carpedm20/deep-rl-tensorflow"
"muupan/async-rl" -> "spragunr/deep_q_rl"
"muupan/async-rl" -> "traai/async-deep-rl"
"osh/kerlym" -> "sherjilozair/dqn"
"osh/kerlym" -> "coreylynch/async-rl"
"osh/kerlym" -> "muupan/async-rl"
"BIT-aerial-robotics/AquaML" -> "kaixindelele/RHER"
"BIT-aerial-robotics/AquaML" -> "yangtao121/AquaRL"
"BIT-aerial-robotics/AquaML" -> "Arya87/RL_draw_seabron"
"facebookresearch/e3b" -> "facebookresearch/impact-driven-exploration"
"facebookresearch/e3b" -> "samlobel/CFN"
"facebookresearch/e3b" -> "tianjunz/NovelD"
"tokb23/dqn" -> "sherjilozair/dqn"
"oxwhirl/smacv2" -> "oxwhirl/smac"
"oxwhirl/smacv2" -> "tjuHaoXiaotian/pymarl3"
"oxwhirl/smacv2" -> "uoe-agents/epymarl"
"oxwhirl/smacv2" -> "hijkzzz/pymarl2"
"oxwhirl/smacv2" -> "FLAIROx/JaxMARL" ["e"=1]
"oxwhirl/smacv2" -> "PKU-MARL/Multi-Agent-Transformer"
"oxwhirl/smacv2" -> "TonghanWang/ROMA"
"oxwhirl/smacv2" -> "PKU-MARL/HARL"
"oxwhirl/smacv2" -> "facebookresearch/BenchMARL"
"oxwhirl/smacv2" -> "schroederdewitt/multiagent_mujoco"
"oxwhirl/smacv2" -> "proroklab/VectorizedMultiAgentSimulator"
"oxwhirl/smacv2" -> "benellis3/pymarl2"
"oxwhirl/smacv2" -> "marlbenchmark/on-policy"
"oxwhirl/smacv2" -> "shariqiqbal2810/REFIL"
"oxwhirl/smacv2" -> "cyanrain7/TRPO-in-MARL"
"algorithmdog/Reinforcement_Learning_Blog" -> "andrewliao11/Deep-Reinforcement-Learning-Survey"
"algorithmdog/Reinforcement_Learning_Blog" -> "floodsung/DRL-FlappyBird"
"algorithmdog/Reinforcement_Learning_Blog" -> "roomai/RoomAI" ["e"=1]
"kaixindelele/RHER" -> "BIT-aerial-robotics/AquaML"
"kaixindelele/RHER" -> "NoneJou072/rl-notebook"
"kaixindelele/RHER" -> "kaixindelele/DRLib"
"chncyhn/flappybird-qlearning-bot" -> "SarvagyaVaish/FlappyBirdRL"
"chncyhn/flappybird-qlearning-bot" -> "mihaibivol/Q-learning-tic-tac-toe"
"chncyhn/flappybird-qlearning-bot" -> "sourabhv/FlapPyBird"
"chncyhn/flappybird-qlearning-bot" -> "kyokin78/rl-flappybird"
"chncyhn/flappybird-qlearning-bot" -> "floodsung/DRL-FlappyBird"
"xbpeng/DeepTerrainRL" -> "xbpeng/DeepLoco"
"xbpeng/DeepTerrainRL" -> "xbpeng/DeepMimic" ["e"=1]
"xbpeng/DeepTerrainRL" -> "Kaixhin/Atari"
"xbpeng/DeepTerrainRL" -> "BartMoyaers/BvhToDeepMimic" ["e"=1]
"tencent-ailab/hok_env" -> "sjtu-marl/malib"
"tencent-ailab/hok_env" -> "sail-sg/envpool" ["e"=1]
"tencent-ailab/hok_env" -> "oxwhirl/smac"
"tencent-ailab/hok_env" -> "FengQuanLi/WZCQ"
"tencent-ailab/hok_env" -> "tencent-ailab/marl-mini"
"tencent-ailab/hok_env" -> "hijkzzz/pymarl2"
"tencent-ailab/hok_env" -> "marlbenchmark/on-policy"
"tencent-ailab/hok_env" -> "myBoris/wzry_ai"
"tencent-ailab/hok_env" -> "PKU-MARL/Multi-Agent-Transformer"
"tencent-ailab/hok_env" -> "lich14/CDS"
"tencent-ailab/hok_env" -> "oxwhirl/pymarl"
"tencent-ailab/hok_env" -> "starry-sky6688/MARL-Algorithms"
"tencent-ailab/hok_env" -> "Replicable-MARL/MARLlib"
"tencent-ailab/hok_env" -> "Farama-Foundation/Metaworld" ["e"=1]
"tencent-ailab/hok_env" -> "tencent-ailab/hokoff"
"gtoubassi/dqn-atari" -> "floodsung/DQN-Atari-Tensorflow"
"gtoubassi/dqn-atari" -> "Jabberwockyll/deep_rl_ale"
"cbfinn/gps" -> "rll/rllab"
"cbfinn/gps" -> "WilsonWangTHU/mbbl" ["e"=1]
"cbfinn/gps" -> "justinjfu/inverse_rl"
"cbfinn/gps" -> "joschu/modular_rl"
"cbfinn/gps" -> "nrontsis/PILCO" ["e"=1]
"cbfinn/gps" -> "kchua/handful-of-trials" ["e"=1]
"cbfinn/gps" -> "carpedm20/NAF-tensorflow"
"cbfinn/gps" -> "rlbayes/rllabplusplus"
"cbfinn/gps" -> "erlerobot/gym-gazebo"
"cbfinn/gps" -> "araffin/robotics-rl-srl" ["e"=1]
"cbfinn/gps" -> "tianheyu927/mil"
"cbfinn/gps" -> "haarnoja/softqlearning"
"cbfinn/gps" -> "cbfinn/maml_rl" ["e"=1]
"cbfinn/gps" -> "avivt/VIN"
"cbfinn/gps" -> "siemanko/guided-policy-search"
"sherjilozair/dqn" -> "osh/kerlym"
"sherjilozair/dqn" -> "tokb23/dqn"
"sherjilozair/dqn" -> "muupan/async-rl"
"sherjilozair/dqn" -> "carpedm20/visual-analogy-tensorflow" ["e"=1]
"sherjilozair/dqn" -> "siemanko/tf-adversarial" ["e"=1]
"sherjilozair/dqn" -> "mokemokechicken/keras_npi" ["e"=1]
"jianzhnie/deep-marl-toolkit" -> "hijkzzz/noisy-mappo"
"heiner/nle" -> "ngoodger/nle-language-wrapper"
"Netease-Games-AI-Lab-Guangzhou/PerfectDou" -> "submit-paper/Doudizhu_plus"
"Netease-Games-AI-Lab-Guangzhou/PerfectDou" -> "Vincentzyx/Douzero_Resnet"
"Netease-Games-AI-Lab-Guangzhou/PerfectDou" -> "Vincentzyx/DouZero_For_HLDDZ_FullAuto"
"Netease-Games-AI-Lab-Guangzhou/PerfectDou" -> "EdwardPooh/douzero-resnet-2.0"
"Netease-Games-AI-Lab-Guangzhou/PerfectDou" -> "datamllab/rlcard-showdown"
"Netease-Games-AI-Lab-Guangzhou/PerfectDou" -> "datamllab/awesome-game-ai"
"vvanirudh/IRL-Toolkit" -> "yl-1993/SpectralMEIRL"
"vvanirudh/IRL-Toolkit" -> "erensezener/aima-based-irl"
"TimeBreaker/Adversarial-Reinforcement-Learning-Papers" -> "TimeBreaker/MARL-resources-collection"
"TimeBreaker/Adversarial-Reinforcement-Learning-Papers" -> "TimeBreaker/Multi-Agent-Reinforcement-Learning-papers"
"Farama-Foundation/MAgent2" -> "geek-ai/MAgent"
"Farama-Foundation/MAgent2" -> "proroklab/VectorizedMultiAgentSimulator"
"Farama-Foundation/MAgent2" -> "Farama-Foundation/Shimmy" ["e"=1]
"Farama-Foundation/MAgent2" -> "FLAIROx/JaxMARL" ["e"=1]
"Farama-Foundation/MAgent2" -> "Farama-Foundation/MicroRTS-Py" ["e"=1]
"Farama-Foundation/MAgent2" -> "uoe-agents/epymarl"
"Farama-Foundation/MAgent2" -> "Replicable-MARL/MARLlib"
"Farama-Foundation/MAgent2" -> "google-deepmind/meltingpot"
"Farama-Foundation/MAgent2" -> "instadeepai/Mava"
"Farama-Foundation/MAgent2" -> "sjtu-marl/malib"
"Farama-Foundation/MAgent2" -> "oxwhirl/smacv2"
"Farama-Foundation/MAgent2" -> "facebookresearch/BenchMARL"
"Farama-Foundation/MAgent2" -> "Farama-Foundation/SuperSuit" ["e"=1]
"thu-rllab/CFCQL" -> "ZhengYinan-AIR/OMIGA"
"thu-rllab/CFCQL" -> "ling-pan/OMAR"
"thu-rllab/CFCQL" -> "zzq-bot/offline-marl-framework-offpymarl"
"openai/pachi-py" -> "openai/pyconfigatron"
"ZhiyeTang/Kaiwu-Paper-List" -> "Unakar/AI_Game_KingGlory"
"openai/doom-py" -> "openai/gym-recording"
"TroddenSpade/Maximum-Entropy-Deep-IRL" -> "tonylitianyu/Preference-Planning-Deep-IRL"
"TroddenSpade/Maximum-Entropy-Deep-IRL" -> "TroddenSpade/GAN-NST"
"openai/dallify-discord-bot" -> "openai/go-alias"
"seermer/HollowKnight_RL" -> "ailec0623/DQN_HollowKnight"
"TroddenSpade/Meta-Reinforcement-Learning" -> "TroddenSpade/Flexible-Conditional-Imitation-Learning"
"TroddenSpade/Meta-Reinforcement-Learning" -> "TroddenSpade/Curiosity-driven-Exploration-in-Navigation"
"TroddenSpade/Meta-Reinforcement-Learning" -> "TroddenSpade/Soccer-Players-Tracking"
"TroddenSpade/Meta-Reinforcement-Learning" -> "TroddenSpade/Exhaustive-Reinforcement-Learning"
"TroddenSpade/Meta-Reinforcement-Learning" -> "TroddenSpade/Decision-Transformer-on-Offline-Reinforcement-Learning"
"TroddenSpade/Meta-Reinforcement-Learning" -> "TroddenSpade/GAN-NST"
"TroddenSpade/Meta-Reinforcement-Learning" -> "TroddenSpade/Federated-DRL"
"ling-pan/OMAR" -> "thu-rllab/CFCQL"
"ling-pan/OMAR" -> "YiqinYang/ICQ"
"ling-pan/OMAR" -> "ZhengYinan-AIR/OMIGA"
"pakoito/MarI-O" -> "mam91/neat-genetic-mario"
"Jabberwockyll/deep_rl_ale" -> "mrkulk/deepQN_tensorflow"
"Ardavans/DSR" -> "EdenMacdonald/h-DQN"
"Ardavans/DSR" -> "erwanbou/sf-deep-rl"
"umd-huang-lab/WocaR-RL" -> "tuomaso/radial_rl_v2"
"benellis3/mappo" -> "benellis3/pymarl2"
"Kinds-of-Intelligence-CFI/animal-ai" -> "mdcrosby/animal-ai"
"facebookresearch/dcd" -> "ucl-dark/paired"
"facebookresearch/dcd" -> "facebookresearch/level-replay"
"facebookresearch/dcd" -> "facebookresearch/minimax" ["e"=1]
"facebookresearch/dcd" -> "DramaCow/jaxued" ["e"=1]
"sash-a/CleanRL.jl" -> "instadeepai/matrax"
"wojzaremba/trpo" -> "ilyasu123/trpo"
"TroddenSpade/Decision-Transformer-on-Offline-Reinforcement-Learning" -> "TroddenSpade/Exhaustive-Reinforcement-Learning"
"TroddenSpade/Decision-Transformer-on-Offline-Reinforcement-Learning" -> "TroddenSpade/GAN-NST"
"TroddenSpade/Decision-Transformer-on-Offline-Reinforcement-Learning" -> "TroddenSpade/Soccer-Players-Tracking"
"TroddenSpade/Decision-Transformer-on-Offline-Reinforcement-Learning" -> "TroddenSpade/Curiosity-driven-Exploration-in-Navigation"
"TroddenSpade/Decision-Transformer-on-Offline-Reinforcement-Learning" -> "TroddenSpade/Flexible-Conditional-Imitation-Learning"
"EdwardPooh/douzero-resnet-2.0" -> "Vincentzyx/Douzero_Resnet"
"EdwardPooh/douzero-resnet-2.0" -> "submit-paper/Doudizhu_plus"
"EdwardPooh/douzero-resnet-2.0" -> "RuBP17/AlphaDou"
"EdwardPooh/douzero-resnet-2.0" -> "1310183534/DouDiZhu"
"jjyyxx/srlnbc" -> "ManUtdMoon/Distributional-Reachability-Policy-Optimization"
"cognitiveailab/TextWorldExpress" -> "microsoft/tdqn"
"WuTheFWasThat/hanabi.rs" -> "Quuxplusone/Hanabi"
"WuTheFWasThat/hanabi.rs" -> "rjtobin/HanSim"
"cognitiveailab/neurosymbolic" -> "microsoft/tdqn"
"reaby/TMModeTemplate" -> "snixtho/clifga"
"frt03/mxt_bench" -> "sunghoonhong/SWAT"
"drdh/Synergy-RL" -> "sunghoonhong/SWAT"
"samjia2000/HSP" -> "ruizhaogit/maximum_entropy_population_based_training"
"samjia2000/HSP" -> "liyang619/COLE-Platform"
"samjia2000/HSP" -> "sjtu-marl/ZSC-Eval"
"facebookresearch/BenchMARL" -> "proroklab/VectorizedMultiAgentSimulator"
"facebookresearch/BenchMARL" -> "FLAIROx/JaxMARL" ["e"=1]
"facebookresearch/BenchMARL" -> "uoe-agents/epymarl"
"facebookresearch/BenchMARL" -> "Replicable-MARL/MARLlib"
"facebookresearch/BenchMARL" -> "oxwhirl/smacv2"
"facebookresearch/BenchMARL" -> "instadeepai/marl-eval"
"facebookresearch/BenchMARL" -> "google-deepmind/meltingpot"
"facebookresearch/BenchMARL" -> "instadeepai/Mava"
"facebookresearch/BenchMARL" -> "PKU-MARL/HARL"
"facebookresearch/BenchMARL" -> "marl-book/codebase"
"facebookresearch/BenchMARL" -> "luchris429/purejaxrl" ["e"=1]
"facebookresearch/BenchMARL" -> "marlbenchmark/on-policy"
"facebookresearch/BenchMARL" -> "hijkzzz/pymarl2"
"facebookresearch/BenchMARL" -> "sjtu-marl/malib"
"facebookresearch/BenchMARL" -> "ffelten/CrazyRL" ["e"=1]
"ShangtongZhang/reinforcement-learning-an-introduction" -> "dennybritz/reinforcement-learning"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "aikorea/awesome-rl"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "openai/baselines"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "ShangtongZhang/DeepRL"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "udacity/deep-reinforcement-learning"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "openai/gym"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "openai/spinningup"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "rlcode/reinforcement-learning"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "google/dopamine"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "yandexdataschool/Practical_RL"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "thu-ml/tianshou"
"ShangtongZhang/reinforcement-learning-an-introduction" -> "keras-rl/keras-rl"
"dennybritz/reinforcement-learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"dennybritz/reinforcement-learning" -> "aikorea/awesome-rl"
"dennybritz/reinforcement-learning" -> "openai/baselines"
"dennybritz/reinforcement-learning" -> "openai/gym"
"dennybritz/reinforcement-learning" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"dennybritz/reinforcement-learning" -> "keras-rl/keras-rl"
"dennybritz/reinforcement-learning" -> "openai/spinningup"
"dennybritz/reinforcement-learning" -> "floodsung/Deep-Learning-Papers-Reading-Roadmap" ["e"=1]
"dennybritz/reinforcement-learning" -> "udacity/deep-reinforcement-learning"
"dennybritz/reinforcement-learning" -> "google/dopamine"
"dennybritz/reinforcement-learning" -> "terryum/awesome-deep-learning-papers" ["e"=1]
"dennybritz/reinforcement-learning" -> "aymericdamien/TensorFlow-Examples" ["e"=1]
"dennybritz/reinforcement-learning" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"dennybritz/reinforcement-learning" -> "yandexdataschool/Practical_RL"
"dennybritz/reinforcement-learning" -> "rlcode/reinforcement-learning"
"keras-rl/keras-rl" -> "tensorforce/tensorforce"
"keras-rl/keras-rl" -> "rll/rllab"
"keras-rl/keras-rl" -> "dennybritz/reinforcement-learning"
"keras-rl/keras-rl" -> "openai/baselines"
"keras-rl/keras-rl" -> "aikorea/awesome-rl"
"keras-rl/keras-rl" -> "google/dopamine"
"keras-rl/keras-rl" -> "rlcode/reinforcement-learning"
"keras-rl/keras-rl" -> "carpedm20/deep-rl-tensorflow"
"keras-rl/keras-rl" -> "awjuliani/DeepRL-Agents"
"keras-rl/keras-rl" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"keras-rl/keras-rl" -> "hill-a/stable-baselines"
"keras-rl/keras-rl" -> "google-deepmind/trfl"
"keras-rl/keras-rl" -> "openai/gym"
"keras-rl/keras-rl" -> "tensorflow/agents"
"keras-rl/keras-rl" -> "devsisters/DQN-tensorflow"
"sisl/MADRL" -> "xuehy/pytorch-maddpg"
"sisl/MADRL" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"sisl/MADRL" -> "shariqiqbal2810/MAAC"
"sisl/MADRL" -> "geek-ai/MAgent"
"sisl/MADRL" -> "openai/maddpg"
"sisl/MADRL" -> "shariqiqbal2810/maddpg-pytorch"
"sisl/MADRL" -> "oxwhirl/pymarl"
"sisl/MADRL" -> "minqi/learning-to-communicate-pytorch"
"sisl/MADRL" -> "starry-sky6688/MARL-Algorithms"
"sisl/MADRL" -> "starry-sky6688/MADDPG"
"sisl/MADRL" -> "koulanurag/ma-gym"
"sisl/MADRL" -> "marlbenchmark/on-policy"
"sisl/MADRL" -> "mlii/mfrl"
"sisl/MADRL" -> "ChenglongChen/pytorch-DRL"
"sisl/MADRL" -> "LantaoYu/MARL-Papers"
"facebookresearch/Pearl" -> "pytorch/rl"
"facebookresearch/Pearl" -> "vwxyzjn/cleanrl"
"facebookresearch/Pearl" -> "luchris429/purejaxrl" ["e"=1]
"facebookresearch/Pearl" -> "facebookresearch/ReAgent"
"facebookresearch/Pearl" -> "DLR-RM/stable-baselines3"
"facebookresearch/Pearl" -> "google-deepmind/acme"
"facebookresearch/Pearl" -> "Farama-Foundation/Gymnasium"
"facebookresearch/Pearl" -> "Farama-Foundation/D4RL" ["e"=1]
"facebookresearch/Pearl" -> "eureka-research/Eureka" ["e"=1]
"facebookresearch/Pearl" -> "RobertTLange/gymnax" ["e"=1]
"facebookresearch/Pearl" -> "facebookresearch/jepa" ["e"=1]
"facebookresearch/Pearl" -> "kzl/decision-transformer" ["e"=1]
"facebookresearch/Pearl" -> "google-deepmind/mctx" ["e"=1]
"facebookresearch/Pearl" -> "danijar/dreamerv3" ["e"=1]
"facebookresearch/Pearl" -> "AgileRL/AgileRL" ["e"=1]
"PKU-MARL/HARL" -> "Replicable-MARL/MARLlib"
"PKU-MARL/HARL" -> "PKU-MARL/Multi-Agent-Transformer"
"PKU-MARL/HARL" -> "marlbenchmark/on-policy"
"PKU-MARL/HARL" -> "cyanrain7/TRPO-in-MARL"
"PKU-MARL/HARL" -> "tinyzqh/light_mappo"
"PKU-MARL/HARL" -> "uoe-agents/epymarl"
"PKU-MARL/HARL" -> "marlbenchmark/off-policy"
"PKU-MARL/HARL" -> "Lizhi-sjtu/MARL-code-pytorch"
"PKU-MARL/HARL" -> "tjuHaoXiaotian/pymarl3"
"PKU-MARL/HARL" -> "agi-brain/xuance"
"PKU-MARL/HARL" -> "liuqh16/LAG" ["e"=1]
"PKU-MARL/HARL" -> "oxwhirl/smacv2"
"PKU-MARL/HARL" -> "hijkzzz/pymarl2"
"PKU-MARL/HARL" -> "oxwhirl/pymarl"
"PKU-MARL/HARL" -> "starry-sky6688/MARL-Algorithms"
"ppaquette/gym-pull" -> "openai/gym-doom"
"pyjarrett/OMSCS_Survival_Guide" -> "kathgironpe/awesome-omscs"
"pyjarrett/OMSCS_Survival_Guide" -> "JonathanTay/CS-7641-assignment-1"
"Linesight-RL/linesight" -> "trackmania-rl/tmrl"
"Linesight-RL/linesight" -> "AndrejGobeX/TrackMania_AI"
"Linesight-RL/linesight" -> "BigBang1112/gbx-net"
"Linesight-RL/linesight" -> "donadigo/TMTrackNN"
"awjuliani/DeepRL-Agents" -> "carpedm20/deep-rl-tensorflow"
"awjuliani/DeepRL-Agents" -> "devsisters/DQN-tensorflow"
"awjuliani/DeepRL-Agents" -> "tensorforce/tensorforce"
"awjuliani/DeepRL-Agents" -> "keras-rl/keras-rl"
"awjuliani/DeepRL-Agents" -> "openai/universe-starter-agent"
"awjuliani/DeepRL-Agents" -> "rll/rllab"
"awjuliani/DeepRL-Agents" -> "rlcode/reinforcement-learning"
"awjuliani/DeepRL-Agents" -> "miyosuda/async_deep_reinforce"
"awjuliani/DeepRL-Agents" -> "junhyukoh/deep-reinforcement-learning-papers"
"awjuliani/DeepRL-Agents" -> "awjuliani/Meta-RL"
"awjuliani/DeepRL-Agents" -> "google-deepmind/trfl"
"awjuliani/DeepRL-Agents" -> "openai/baselines"
"awjuliani/DeepRL-Agents" -> "aikorea/awesome-rl"
"awjuliani/DeepRL-Agents" -> "dennybritz/reinforcement-learning"
"awjuliani/DeepRL-Agents" -> "NVlabs/GA3C"
"erlerobot/gym-gazebo" -> "AcutronicRobotics/gym-gazebo2" ["e"=1]
"erlerobot/gym-gazebo" -> "mit-acl/cadrl_ros" ["e"=1]
"erlerobot/gym-gazebo" -> "vmayoral/basic_reinforcement_learning"
"erlerobot/gym-gazebo" -> "cbfinn/gps"
"erlerobot/gym-gazebo" -> "dusty-nv/jetson-reinforcement" ["e"=1]
"erlerobot/gym-gazebo" -> "openai/roboschool"
"erlerobot/gym-gazebo" -> "vita-epfl/CrowdNav" ["e"=1]
"erlerobot/gym-gazebo" -> "jr-robotics/robo-gym" ["e"=1]
"erlerobot/gym-gazebo" -> "rll/rllab"
"erlerobot/gym-gazebo" -> "araffin/robotics-rl-srl" ["e"=1]
"erlerobot/gym-gazebo" -> "AcutronicRobotics/ros2learn" ["e"=1]
"erlerobot/gym-gazebo" -> "stepjam/PyRep" ["e"=1]
"erlerobot/gym-gazebo" -> "benelot/pybullet-gym"
"erlerobot/gym-gazebo" -> "rail-berkeley/softlearning"
"erlerobot/gym-gazebo" -> "Acmece/rl-collision-avoidance" ["e"=1]
"tensorlayer/TensorLayer" -> "tflearn/tflearn" ["e"=1]
"tensorlayer/TensorLayer" -> "tensorlayer/SRGAN" ["e"=1]
"tensorlayer/TensorLayer" -> "tensorpack/tensorpack" ["e"=1]
"tensorlayer/TensorLayer" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"tensorlayer/TensorLayer" -> "carpedm20/DCGAN-tensorflow" ["e"=1]
"tensorlayer/TensorLayer" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"tensorlayer/TensorLayer" -> "openai/baselines"
"tensorlayer/TensorLayer" -> "dennybritz/reinforcement-learning"
"tensorlayer/TensorLayer" -> "google/dopamine"
"tensorlayer/TensorLayer" -> "apache/mxnet" ["e"=1]
"tensorlayer/TensorLayer" -> "keras-rl/keras-rl"
"tensorlayer/TensorLayer" -> "zhangqianhui/AdversarialNetsPapers" ["e"=1]
"tensorlayer/TensorLayer" -> "google-deepmind/sonnet"
"tensorlayer/TensorLayer" -> "tensorforce/tensorforce"
"tensorlayer/TensorLayer" -> "wiseodd/generative-models" ["e"=1]
"BellmanTimeHut/DIPO" -> "wadx2019/qvpo"
"OpenRL-Lab/openrl" -> "Replicable-MARL/MARLlib"
"OpenRL-Lab/openrl" -> "PKU-MARL/HARL"
"OpenRL-Lab/openrl" -> "sjtu-marl/malib"
"OpenRL-Lab/openrl" -> "PKU-MARL/Multi-Agent-Transformer"
"OpenRL-Lab/openrl" -> "marlbenchmark/on-policy"
"OpenRL-Lab/openrl" -> "marlbenchmark/off-policy"
"OpenRL-Lab/openrl" -> "uoe-agents/epymarl"
"OpenRL-Lab/openrl" -> "FLAIROx/JaxMARL" ["e"=1]
"OpenRL-Lab/openrl" -> "Jingliang-Duan/DSAC-v2"
"OpenRL-Lab/openrl" -> "sail-sg/envpool" ["e"=1]
"OpenRL-Lab/openrl" -> "schroederdewitt/multiagent_mujoco"
"OpenRL-Lab/openrl" -> "Farama-Foundation/D4RL" ["e"=1]
"OpenRL-Lab/openrl" -> "hijkzzz/pymarl2"
"OpenRL-Lab/openrl" -> "ziyanx02/multiagent-quadruped-environment"
"OpenRL-Lab/openrl" -> "PKU-Alignment/safety-gymnasium" ["e"=1]
"Jingliang-Duan/DSAC-v2" -> "Intelligent-Driving-Laboratory/GOPS"
"Jingliang-Duan/DSAC-v2" -> "Jingliang-Duan/DSAC-v1"
"Jingliang-Duan/DSAC-v2" -> "happy-yan/DACER-Diffusion-with-Online-RL"
"Jingliang-Duan/DSAC-v2" -> "PKU-MARL/HARL"
"Jingliang-Duan/DSAC-v2" -> "agi-brain/xuance"
"Jingliang-Duan/DSAC-v2" -> "Lizhi-sjtu/MARL-code-pytorch"
"Jingliang-Duan/DSAC-v2" -> "Lizhi-sjtu/DRL-code-pytorch"
"Jingliang-Duan/DSAC-v2" -> "BY571/Soft-Actor-Critic-and-Extensions"
"Jingliang-Duan/DSAC-v2" -> "Apress/Reinforcement-Learning-for-Sequential-Decision-and-Optimal-Control"
"jwk1rose/RL_Learning" -> "ziwenhahaha/Code-of-RL-Beginning"
"jwk1rose/RL_Learning" -> "SupermanCaozh/The_Coding_Foundation_in_Reinforcement_Learning"
"jwk1rose/RL_Learning" -> "MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning"
"jwk1rose/RL_Learning" -> "DeepRLChinese/DeepRL-Chinese"
"jwk1rose/RL_Learning" -> "lansinuote/More_Simple_Reinforcement_Learning"
"jwk1rose/RL_Learning" -> "XinJingHao/DRL-Pytorch"
"jwk1rose/RL_Learning" -> "boyu-ai/Hands-on-RL"
"jwk1rose/RL_Learning" -> "vincen-github/mlimpl"
"jwk1rose/RL_Learning" -> "10-OASIS-01/minrl"
"jwk1rose/RL_Learning" -> "Ronchy2000/Multi-agent-RL"
"jwk1rose/RL_Learning" -> "lansinuote/Simple_Reinforcement_Learning"
"jwk1rose/RL_Learning" -> "Zhefan-Xu/NavRL" ["e"=1]
"openai/vime" -> "openai/imitation"
"openai/vime" -> "openai/iaf" ["e"=1]
"openai/vime" -> "joschu/modular_rl"
"openai/vime" -> "openai/EPG"
"openai/vime" -> "steveKapturowski/tensorflow-rl"
"openai/vime" -> "alexlee-gk/slac" ["e"=1]
"openai/vime" -> "openai/robosumo"
"openai/vime" -> "openai/neural-gpu"
"openai/vime" -> "openai/universe-starter-agent"
"openai/vime" -> "Ardavans/DSR"
"openai/vime" -> "hi-abhi/tensorflow-value-iteration-networks"
"openai/vime" -> "rll/rllab"
"openai/vime" -> "openai/InfoGAN" ["e"=1]
"openai/vime" -> "miyosuda/unreal"
"openai/vime" -> "openai/large-scale-curiosity"
"agi-brain/xuance" -> "PKU-MARL/HARL"
"agi-brain/xuance" -> "Replicable-MARL/MARLlib"
"agi-brain/xuance" -> "Lizhi-sjtu/MARL-code-pytorch"
"agi-brain/xuance" -> "tinyzqh/light_mappo"
"agi-brain/xuance" -> "XinJingHao/DRL-Pytorch"
"agi-brain/xuance" -> "starry-sky6688/MARL-Algorithms"
"agi-brain/xuance" -> "marlbenchmark/on-policy"
"agi-brain/xuance" -> "Jingliang-Duan/DSAC-v2"
"agi-brain/xuance" -> "starry-sky6688/MADDPG"
"agi-brain/xuance" -> "shariqiqbal2810/maddpg-pytorch"
"agi-brain/xuance" -> "marlbenchmark/off-policy"
"agi-brain/xuance" -> "sjtu-marl/malib"
"agi-brain/xuance" -> "oxwhirl/pymarl"
"agi-brain/xuance" -> "shariqiqbal2810/MAAC"
"agi-brain/xuance" -> "hijkzzz/pymarl2"
"LiSir-HIT/Reinforcement-Learning" -> "Lizhi-sjtu/DRL-code-pytorch"
"openai/requests-for-research" -> "dennybritz/deeplearning-papernotes" ["e"=1]
"openai/requests-for-research" -> "rll/rllab"
"openai/requests-for-research" -> "openai/universe"
"openai/requests-for-research" -> "openai/evolution-strategies-starter"
"openai/requests-for-research" -> "openai/improved-gan" ["e"=1]
"openai/requests-for-research" -> "google-deepmind/learning-to-learn" ["e"=1]
"openai/requests-for-research" -> "openai/universe-starter-agent"
"openai/requests-for-research" -> "openai/roboschool"
"openai/requests-for-research" -> "openai/InfoGAN" ["e"=1]
"openai/requests-for-research" -> "carpedm20/deep-rl-tensorflow"
"openai/requests-for-research" -> "openai/generating-reviews-discovering-sentiment" ["e"=1]
"openai/requests-for-research" -> "tensorflow/fold" ["e"=1]
"openai/requests-for-research" -> "google-deepmind/lab"
"openai/requests-for-research" -> "junhyukoh/deep-reinforcement-learning-papers"
"openai/requests-for-research" -> "harvardnlp/seq2seq-attn" ["e"=1]
"philtabor/Multi-Agent-Reinforcement-Learning" -> "reinshift/MADDPG_Multi_UAV_Roundup"
"philtabor/Multi-Agent-Reinforcement-Learning" -> "Lizhi-sjtu/MARL-code-pytorch"
"philtabor/Multi-Agent-Reinforcement-Learning" -> "Git-123-Hub/maddpg-pettingzoo-pytorch"
"carpedm20/deep-rl-tensorflow" -> "devsisters/DQN-tensorflow"
"carpedm20/deep-rl-tensorflow" -> "rll/rllab"
"carpedm20/deep-rl-tensorflow" -> "junhyukoh/deep-reinforcement-learning-papers"
"carpedm20/deep-rl-tensorflow" -> "coreylynch/async-rl"
"carpedm20/deep-rl-tensorflow" -> "awjuliani/DeepRL-Agents"
"carpedm20/deep-rl-tensorflow" -> "tambetm/simple_dqn"
"carpedm20/deep-rl-tensorflow" -> "siemanko/tensorflow-deepq"
"carpedm20/deep-rl-tensorflow" -> "miyosuda/async_deep_reinforce"
"carpedm20/deep-rl-tensorflow" -> "keras-rl/keras-rl"
"carpedm20/deep-rl-tensorflow" -> "muupan/deep-reinforcement-learning-papers"
"carpedm20/deep-rl-tensorflow" -> "openai/universe-starter-agent"
"carpedm20/deep-rl-tensorflow" -> "muupan/async-rl"
"carpedm20/deep-rl-tensorflow" -> "carpedm20/NTM-tensorflow" ["e"=1]
"carpedm20/deep-rl-tensorflow" -> "hi-abhi/tensorflow-value-iteration-networks"
"carpedm20/deep-rl-tensorflow" -> "rlcode/reinforcement-learning"
"lansinuote/More_Simple_Reinforcement_Learning" -> "lansinuote/Simple_Reinforcement_Learning"
"lansinuote/More_Simple_Reinforcement_Learning" -> "jwk1rose/RL_Learning"
"lansinuote/More_Simple_Reinforcement_Learning" -> "lansinuote/StableBaselines3_SimpleCases"
"lansinuote/More_Simple_Reinforcement_Learning" -> "XinJingHao/DRL-Pytorch"
"david-abel/simple_rl" -> "junhyukoh/value-prediction-network"
"david-abel/simple_rl" -> "ArnaudFickinger/gym-multigrid"
"david-abel/simple_rl" -> "deep-skill-chaining/deep-skill-chaining"
"openai/gym-wikinav" -> "openai/post--example"
"junhyukoh/icml2016-minecraft" -> "floringogianu/categorical-dqn"
"stevenpjg/ddpg-aigym" -> "rmst/ddpg"
"stevenpjg/ddpg-aigym" -> "floodsung/DDPG"
"stevenpjg/ddpg-aigym" -> "pemami4911/deep-rl"
"stevenpjg/ddpg-aigym" -> "jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" ["e"=1]
"stevenpjg/ddpg-aigym" -> "yanpanlau/DDPG-Keras-Torcs"
"stevenpjg/ddpg-aigym" -> "liampetti/DDPG"
"stevenpjg/ddpg-aigym" -> "pat-coady/trpo"
"stevenpjg/ddpg-aigym" -> "stevenpjg/QlearningRobocodeNN"
"stevenpjg/ddpg-aigym" -> "joschu/modular_rl"
"stevenpjg/ddpg-aigym" -> "5vision/deep-reinforcement-learning-networks"
"stevenpjg/ddpg-aigym" -> "carpedm20/NAF-tensorflow"
"carpedm20/NAF-tensorflow" -> "rmst/ddpg"
"carpedm20/NAF-tensorflow" -> "Ardavans/DSR"
"carpedm20/NAF-tensorflow" -> "cbfinn/gps"
"Intelligent-Driving-Laboratory/GOPS" -> "Jingliang-Duan/DSAC-v2"
"Intelligent-Driving-Laboratory/GOPS" -> "Jingliang-Duan/DSAC-v1"
"Intelligent-Driving-Laboratory/GOPS" -> "happy-yan/DACER-Diffusion-with-Online-RL"
"Intelligent-Driving-Laboratory/GOPS" -> "Intelligent-Driving-Laboratory/GOPS_DOC"
"Intelligent-Driving-Laboratory/GOPS" -> "Intelligent-Driving-Laboratory/Reinforcement-Learning-for-Sequential-Decision-and-Optimal-Control"
"Intelligent-Driving-Laboratory/GOPS" -> "TobiasLv/RAD"
"Intelligent-Driving-Laboratory/GOPS" -> "idthanm/mpg"
"Intelligent-Driving-Laboratory/GOPS" -> "ManUtdMoon/Distributional-Reachability-Policy-Optimization"
"jaromiru/AI-blog" -> "Damcy/prioritized-experience-replay"
"jaromiru/AI-blog" -> "miyosuda/async_deep_reinforce"
"jaromiru/AI-blog" -> "NVlabs/GA3C"
"jaromiru/AI-blog" -> "muupan/async-rl"
"jaromiru/AI-blog" -> "pemami4911/deep-rl"
"jaromiru/AI-blog" -> "ikostrikov/pytorch-a3c"
"jaromiru/AI-blog" -> "coreylynch/async-rl"
"jaromiru/AI-blog" -> "Kaixhin/Atari"
"jaromiru/AI-blog" -> "joschu/modular_rl"
"jaromiru/AI-blog" -> "google-deepmind/scalable_agent"
"jaromiru/AI-blog" -> "awjuliani/DeepRL-Agents"
"jaromiru/AI-blog" -> "dgriff777/rl_a3c_pytorch"
"jaromiru/AI-blog" -> "alexis-jacq/Pytorch-DPPO"
"jaromiru/AI-blog" -> "miyosuda/unreal"
"jaromiru/AI-blog" -> "yukezhu/tensorflow-reinforce"
"openai/imitation" -> "openai/vime"
"openai/imitation" -> "andrewliao11/gail-tf"
"openai/imitation" -> "joschu/modular_rl"
"openai/imitation" -> "YunzhuLi/InfoGAIL"
"openai/imitation" -> "yrlu/irl-imitation"
"openai/imitation" -> "justinjfu/inverse_rl"
"openai/imitation" -> "rll/rllab"
"openai/imitation" -> "hi-abhi/tensorflow-value-iteration-networks"
"openai/imitation" -> "openai/InfoGAN" ["e"=1]
"openai/imitation" -> "openai/iaf" ["e"=1]
"openai/imitation" -> "junhyukoh/self-imitation-learning"
"openai/imitation" -> "uidilr/gail_ppo_tf"
"openai/imitation" -> "Khrylx/PyTorch-RL"
"openai/imitation" -> "MatthewJA/Inverse-Reinforcement-Learning"
"openai/imitation" -> "sisl/gail-driver"
"5vision/deep-reinforcement-learning-networks" -> "5vision/DARQN"
"5vision/deep-reinforcement-learning-networks" -> "Ardavans/DSR"
"5vision/deep-reinforcement-learning-networks" -> "isl-org/DirectFuturePrediction"
"itaicaspi/keras-dqn-doom" -> "fhennecker/deepdoom"
"jangirrishabh/toyCarIRL" -> "yrlu/irl-imitation"
"jangirrishabh/toyCarIRL" -> "ahq1993/inverse_rl"
"jangirrishabh/toyCarIRL" -> "sjchoi86/irl_rocks"
"jangirrishabh/toyCarIRL" -> "MatthewJA/Inverse-Reinforcement-Learning"
"jangirrishabh/toyCarIRL" -> "reinforcement-learning-kr/lets-do-irl"
"jangirrishabh/toyCarIRL" -> "qzed/irl-maxent"
"jangirrishabh/toyCarIRL" -> "ermongroup/MA-AIRL"
"jangirrishabh/toyCarIRL" -> "vjg28/Linear-Inverse-RL-algorithms"
"jangirrishabh/toyCarIRL" -> "zacrash/Inverse-RL"
"jangirrishabh/toyCarIRL" -> "yfzhang/vehicle-motion-forecasting"
"jangirrishabh/toyCarIRL" -> "andrewliao11/gail-tf"
"jangirrishabh/toyCarIRL" -> "justinjfu/inverse_rl"
"jangirrishabh/toyCarIRL" -> "neka-nat/inv_rl"
"jangirrishabh/toyCarIRL" -> "dit7ya/awesome-irl"
"jangirrishabh/toyCarIRL" -> "ShivinDass/inverse_rl"
"ppaquette/gym-super-mario" -> "Kautenja/gym-super-mario-bros"
"ppaquette/gym-super-mario" -> "ppaquette/gym-doom"
"ppaquette/gym-super-mario" -> "ehrenbrav/DeepQNetwork"
"ppaquette/gym-super-mario" -> "llSourcell/deep_q_learning"
"ppaquette/gym-super-mario" -> "robmsylvester/Super-Mario-Bros-DQN"
"ppaquette/gym-super-mario" -> "ppaquette/gym-pull"
"ppaquette/gym-super-mario" -> "vivek3141/super-mario-neat"
"ppaquette/gym-super-mario" -> "zuoxingdong/mazelab"
"ppaquette/gym-super-mario" -> "openai/large-scale-curiosity"
"ppaquette/gym-super-mario" -> "chris-chris/mario-rl-tutorial"
"bupticybee/AlphaNLHoldem" -> "AlexKashi/AlphaHoldem"
"ynchuang/DiscoverPath" -> "daochenzha/neuroshard"
"iassael/learning-to-communicate" -> "minqi/learning-to-communicate-pytorch"
"iassael/learning-to-communicate" -> "facebookarchive/CommNet"
"iassael/learning-to-communicate" -> "IC3Net/IC3Net"
"iassael/learning-to-communicate" -> "sisl/MADRL"
"iassael/learning-to-communicate" -> "rhoowd/sched_net"
"iassael/learning-to-communicate" -> "ludc/rltorch" ["e"=1]
"iassael/learning-to-communicate" -> "cts198859/deeprl_network"
"iassael/learning-to-communicate" -> "NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player"
"iassael/learning-to-communicate" -> "xuehy/pytorch-maddpg"
"iassael/learning-to-communicate" -> "Kaixhin/Atari"
"iassael/learning-to-communicate" -> "carpedm20/NAF-tensorflow"
"iassael/learning-to-communicate" -> "facebookarchive/MazeBase"
"iassael/learning-to-communicate" -> "isp1tze/MAProj"
"iassael/learning-to-communicate" -> "wwxFromTju/deepmind_MAS_enviroment"
"iassael/learning-to-communicate" -> "KornbergFresnel/CommNet"
"openai/gym-soccer" -> "LARG/HFO"
"openai/gym-soccer" -> "mhauskn/dqn-hfo"
"openai/gym-soccer" -> "MattChanTK/gym-maze"
"PyAndy/Py3NES" -> "craigthomas/Chip8Python"
"PyAndy/Py3NES" -> "wkcn/pyfcemu"
"liyang619/COLE-Platform" -> "sjtu-marl/ZSC-Eval"
"liyang619/COLE-Platform" -> "LxzGordon/PECAN"
"liyang619/COLE-Platform" -> "samjia2000/HSP"
"EdenMacdonald/h-DQN" -> "mrkulk/hierarchical-deep-RL"
"EdenMacdonald/h-DQN" -> "Ardavans/DSR"
"EdenMacdonald/h-DQN" -> "skumar9876/Hierarchical-DQN"
"binary-husky/vhmap" -> "Arya87/RL_draw_seabron"
"ffelten/MASAC" -> "jaimeluengo/masac"
"ffelten/MASAC" -> "puyuan1996/MARL"
"yanpanlau/Keras-FlappyBird" -> "yanpanlau/DDPG-Keras-Torcs"
"yanpanlau/Keras-FlappyBird" -> "floodsung/DRL-FlappyBird"
"yanpanlau/Keras-FlappyBird" -> "coreylynch/async-rl"
"yanpanlau/Keras-FlappyBird" -> "keon/deep-q-learning"
"yanpanlau/Keras-FlappyBird" -> "farizrahman4u/qlearning4k"
"kengz/openai_lab" -> "steveKapturowski/tensorflow-rl"
"openai/gym-http-api" -> "openai/retro-contest"
"openai/gym-http-api" -> "openai/doom-py"
"openai/robot_controllers" -> "openai/websockify"
"openai/robot_controllers" -> "openai/pyconfigatron"
"NoneJou072/rl-notebook" -> "kaixindelele/RHER"
"Starlight0798/gymRL" -> "NoneJou072/rl-notebook"
"IdreesInc/Cerebrum" -> "IdreesInc/TetNet"
"yao62995/A3C" -> "synpon/prog_nn"
"yao62995/A3C" -> "miyosuda/async_deep_reinforce"
"yao62995/A3C" -> "seann999/progressive_a3c"
"yao62995/A3C" -> "steveKapturowski/tensorflow-rl"
"Farama-Foundation/stable-retro" -> "MatPoliquin/stable-retro"
"Farama-Foundation/stable-retro" -> "MatPoliquin/stable-retro-scripts"
"Farama-Foundation/stable-retro" -> "DarkAutumn/triforce"
"tambetm/gym-minecraft" -> "crowdAI/marLo"
"tambetm/gym-minecraft" -> "vincentberaud/Minecraft-Reinforcement-Learning"
"tambetm/gym-minecraft" -> "muupan/async-rl"
"tambetm/gym-minecraft" -> "junhyukoh/icml2016-minecraft"
"tambetm/gym-minecraft" -> "zuoxingdong/mazelab"
"tambetm/gym-minecraft" -> "pathak22/modular-assemblies"
"tambetm/gym-minecraft" -> "Farama-Foundation/Miniworld"
"tambetm/gym-minecraft" -> "mrkulk/hierarchical-deep-RL"
"tambetm/gym-minecraft" -> "minerllabs/minerl"
"tambetm/gym-minecraft" -> "google-deepmind/pycolab"
"tambetm/gym-minecraft" -> "minerllabs/baselines"
"IdreesInc/TetNet" -> "IdreesInc/Cerebrum"
"SwiftSage/SwiftSage" -> "allenai/ScienceWorld"
"datamllab/ltsm" -> "ynchuang/DiscoverPath"
"datamllab/ltsm" -> "guanchuwang/Taylor-Unswift"
"datamllab/ltsm" -> "henryzhongsc/longctx_bench" ["e"=1]
"datamllab/ltsm" -> "datamllab/BED_main"
"datamllab/ltsm" -> "daochenzha/neuroshard"
"Damcy/prioritized-experience-replay" -> "takoika/PrioritizedExperienceReplay"
"Arya87/RL_draw_seabron" -> "binary-husky/vhmap"
"PKU-Alignment/ProAgent" -> "bic4907/Overcooked-AI"
"openai/neural-gpu" -> "openai/retask"
"openai/neural-gpu" -> "openai/pytorch"
"openai/neural-gpu" -> "openai/gym-wikinav"
"openai/neural-gpu" -> "openai/websockify"
"lusob/gym-tetris" -> "Kautenja/gym-tetris"
"sehoonha/pydart2" -> "sehoonha/pydart"
"jidiai/TaxAI" -> "jidiai/GRF_MARL"
"jidiai/TaxAI" -> "yanxue7/E3T-Overcooked"
"openai/fetch_robots" -> "openai/aws-fluent-plugin-kinesis"
"kvfrans/parallel-trpo" -> "ilyasu123/trpo"
"ocram444/EldenRL" -> "jameszampa/EldenRingAI"
"jidiai/GRF_MARL" -> "Shanghai-Digital-Brain-Laboratory/DB-Football"
"jidiai/GRF_MARL" -> "jidiai/TaxAI"
"OpenRL-Lab/TiZero" -> "Shanghai-Digital-Brain-Laboratory/DB-Football"
"knakamura13/huggingface-dataset-toolkit" -> "knakamura13/mlrose-torch"
"rlcode/reinforcement-learning" -> "keras-rl/keras-rl"
"rlcode/reinforcement-learning" -> "dennybritz/reinforcement-learning"
"rlcode/reinforcement-learning" -> "aikorea/awesome-rl"
"rlcode/reinforcement-learning" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"rlcode/reinforcement-learning" -> "awjuliani/DeepRL-Agents"
"rlcode/reinforcement-learning" -> "udacity/deep-reinforcement-learning"
"rlcode/reinforcement-learning" -> "rll/rllab"
"rlcode/reinforcement-learning" -> "tensorforce/tensorforce"
"rlcode/reinforcement-learning" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"rlcode/reinforcement-learning" -> "ShangtongZhang/DeepRL"
"rlcode/reinforcement-learning" -> "junhyukoh/deep-reinforcement-learning-papers"
"rlcode/reinforcement-learning" -> "yandexdataschool/Practical_RL"
"rlcode/reinforcement-learning" -> "carpedm20/deep-rl-tensorflow"
"rlcode/reinforcement-learning" -> "devsisters/DQN-tensorflow"
"rlcode/reinforcement-learning" -> "openai/baselines"
"openai/universe" -> "google-deepmind/lab"
"openai/universe" -> "openai/universe-starter-agent"
"openai/universe" -> "openai/gym"
"openai/universe" -> "google-deepmind/learning-to-learn" ["e"=1]
"openai/universe" -> "rll/rllab"
"openai/universe" -> "google-deepmind/sonnet"
"openai/universe" -> "openai/retro"
"openai/universe" -> "openai/baselines"
"openai/universe" -> "tflearn/tflearn" ["e"=1]
"openai/universe" -> "google-deepmind/pysc2" ["e"=1]
"openai/universe" -> "commaai/research" ["e"=1]
"openai/universe" -> "dennybritz/reinforcement-learning"
"openai/universe" -> "keras-rl/keras-rl"
"openai/universe" -> "SerpentAI/SerpentAI" ["e"=1]
"openai/universe" -> "magenta/magenta" ["e"=1]
"openai/universe-starter-agent" -> "miyosuda/async_deep_reinforce"
"openai/universe-starter-agent" -> "openai/universe"
"openai/universe-starter-agent" -> "coreylynch/async-rl"
"openai/universe-starter-agent" -> "rll/rllab"
"openai/universe-starter-agent" -> "muupan/async-rl"
"openai/universe-starter-agent" -> "awjuliani/DeepRL-Agents"
"openai/universe-starter-agent" -> "miyosuda/unreal"
"openai/universe-starter-agent" -> "pathak22/noreward-rl"
"openai/universe-starter-agent" -> "joschu/modular_rl"
"openai/universe-starter-agent" -> "carpedm20/deep-rl-tensorflow"
"openai/universe-starter-agent" -> "NVlabs/GA3C"
"openai/universe-starter-agent" -> "openai/evolution-strategies-starter"
"openai/universe-starter-agent" -> "openai/vime"
"openai/universe-starter-agent" -> "google-research/batch-ppo"
"openai/universe-starter-agent" -> "hi-abhi/tensorflow-value-iteration-networks"
"facebookarchive/CommNet" -> "iassael/learning-to-communicate"
"facebookarchive/CommNet" -> "IC3Net/IC3Net"
"facebookarchive/CommNet" -> "rhoowd/sched_net"
"facebookarchive/CommNet" -> "minqi/learning-to-communicate-pytorch"
"facebookarchive/CommNet" -> "0b01/CommNet"
"facebookarchive/CommNet" -> "KornbergFresnel/CommNet"
"facebookarchive/CommNet" -> "wwxFromTju/deepmind_MAS_enviroment"
"facebookarchive/CommNet" -> "PKU-RL/I2C"
"facebookarchive/CommNet" -> "Coac/CommNet-BiCnet"
"facebookarchive/CommNet" -> "tuladhay/ATOC_COMA_PyTorch"
"dbsxdbsx/rl-intro-book-chinese" -> "rl-cn/rl-cn"
"google-deepmind/concordia" -> "google-deepmind/meltingpot"
"google-deepmind/concordia" -> "rstrivedi/Melting-Pot-Contest-2023"
"google-deepmind/concordia" -> "Farama-Foundation/chatarena" ["e"=1]
"google-deepmind/concordia" -> "FLAIROx/JaxMARL" ["e"=1]
"google-deepmind/concordia" -> "AgentTorch/AgentTorch"
"google-deepmind/concordia" -> "facebookresearch/BenchMARL"
"google-deepmind/concordia" -> "eugenevinitsky/sequential_social_dilemma_games"
"google-deepmind/concordia" -> "jbloomAus/SAELens" ["e"=1]
"google-deepmind/concordia" -> "instadeepai/jumanji" ["e"=1]
"google-deepmind/concordia" -> "Stanford-ILIAD/PantheonRL"
"google-deepmind/concordia" -> "callummcdougall/ARENA_3.0" ["e"=1]
"google-deepmind/concordia" -> "marl-book/codebase"
"google-deepmind/concordia" -> "HumanCompatibleAI/overcooked_ai"
"google-deepmind/concordia" -> "facebookresearch/coconut" ["e"=1]
"google-deepmind/concordia" -> "instadeepai/Mava"
"google-deepmind/lab" -> "openai/universe"
"google-deepmind/lab" -> "google-deepmind/learning-to-learn" ["e"=1]
"google-deepmind/lab" -> "google-deepmind/sonnet"
"google-deepmind/lab" -> "rll/rllab"
"google-deepmind/lab" -> "google-deepmind/dm_control"
"google-deepmind/lab" -> "openai/baselines"
"google-deepmind/lab" -> "google-deepmind/pysc2" ["e"=1]
"google-deepmind/lab" -> "google/dopamine"
"google-deepmind/lab" -> "google-deepmind/trfl"
"google-deepmind/lab" -> "google-deepmind/dnc" ["e"=1]
"google-deepmind/lab" -> "openai/gym"
"google-deepmind/lab" -> "microsoft/malmo"
"google-deepmind/lab" -> "keras-rl/keras-rl"
"google-deepmind/lab" -> "dennybritz/reinforcement-learning"
"google-deepmind/lab" -> "aikorea/awesome-rl"
"yandexdataschool/Practical_RL" -> "yandexdataschool/Practical_DL" ["e"=1]
"yandexdataschool/Practical_RL" -> "higgsfield/RL-Adventure"
"yandexdataschool/Practical_RL" -> "aikorea/awesome-rl"
"yandexdataschool/Practical_RL" -> "yandexdataschool/nlp_course" ["e"=1]
"yandexdataschool/Practical_RL" -> "esokolov/ml-course-hse" ["e"=1]
"yandexdataschool/Practical_RL" -> "dennybritz/reinforcement-learning"
"yandexdataschool/Practical_RL" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"yandexdataschool/Practical_RL" -> "higgsfield-ai/higgsfield"
"yandexdataschool/Practical_RL" -> "rlcode/reinforcement-learning"
"yandexdataschool/Practical_RL" -> "udacity/deep-reinforcement-learning"
"yandexdataschool/Practical_RL" -> "keras-rl/keras-rl"
"yandexdataschool/Practical_RL" -> "Yorko/mlcourse.ai" ["e"=1]
"yandexdataschool/Practical_RL" -> "openai/baselines"
"yandexdataschool/Practical_RL" -> "girafe-ai/ml-course" ["e"=1]
"yandexdataschool/Practical_RL" -> "catalyst-team/catalyst" ["e"=1]
"hi-abhi/tensorflow-value-iteration-networks" -> "avivt/VIN"
"hi-abhi/tensorflow-value-iteration-networks" -> "kentsommer/pytorch-value-iteration-networks"
"hi-abhi/tensorflow-value-iteration-networks" -> "zhongwen/predictron"
"hi-abhi/tensorflow-value-iteration-networks" -> "joschu/modular_rl"
"hi-abhi/tensorflow-value-iteration-networks" -> "zuoxingdong/VIN_PyTorch_Visdom"
"hi-abhi/tensorflow-value-iteration-networks" -> "miyosuda/unreal"
"hi-abhi/tensorflow-value-iteration-networks" -> "miyosuda/async_deep_reinforce"
"hi-abhi/tensorflow-value-iteration-networks" -> "openai/imitation"
"hi-abhi/tensorflow-value-iteration-networks" -> "Mostafa-Samir/DNC-tensorflow" ["e"=1]
"hi-abhi/tensorflow-value-iteration-networks" -> "carpedm20/deep-rl-tensorflow"
"hi-abhi/tensorflow-value-iteration-networks" -> "openai/vime"
"hi-abhi/tensorflow-value-iteration-networks" -> "openai/universe-starter-agent"
"hi-abhi/tensorflow-value-iteration-networks" -> "carpedm20/NAF-tensorflow"
"hi-abhi/tensorflow-value-iteration-networks" -> "muupan/async-rl"
"hi-abhi/tensorflow-value-iteration-networks" -> "NVlabs/GA3C"
"marl-book/slides" -> "marl-book/codebase"
"marl-book/slides" -> "LukasSchaefer/marl-book-exercises"
"yanpanlau/DDPG-Keras-Torcs" -> "ugo-nama-kun/gym_torcs" ["e"=1]
"yanpanlau/DDPG-Keras-Torcs" -> "floodsung/DDPG"
"yanpanlau/DDPG-Keras-Torcs" -> "stevenpjg/ddpg-aigym"
"yanpanlau/DDPG-Keras-Torcs" -> "miyosuda/async_deep_reinforce"
"yanpanlau/DDPG-Keras-Torcs" -> "YurongYou/rlTORCS" ["e"=1]
"yanpanlau/DDPG-Keras-Torcs" -> "rmst/ddpg"
"yanpanlau/DDPG-Keras-Torcs" -> "pemami4911/deep-rl"
"yanpanlau/DDPG-Keras-Torcs" -> "germain-hug/Deep-RL-Keras"
"yanpanlau/DDPG-Keras-Torcs" -> "rll/rllab"
"yanpanlau/DDPG-Keras-Torcs" -> "joschu/modular_rl"
"yanpanlau/DDPG-Keras-Torcs" -> "openai/imitation"
"yanpanlau/DDPG-Keras-Torcs" -> "muupan/async-rl"
"yanpanlau/DDPG-Keras-Torcs" -> "yanpanlau/Keras-FlappyBird"
"yanpanlau/DDPG-Keras-Torcs" -> "openai/universe-starter-agent"
"yanpanlau/DDPG-Keras-Torcs" -> "coreylynch/async-rl"
"ziyanx02/multiagent-quadruped-environment" -> "collaborative-mapush/MAPush"
"allenai/discoveryworld" -> "allenai/ScienceWorld"
"allenai/discoveryworld" -> "cognitiveailab/TextWorldExpress"
"zhongwen/predictron" -> "hi-abhi/tensorflow-value-iteration-networks"
"zhongwen/predictron" -> "junhyukoh/value-prediction-network"
"zhongwen/predictron" -> "oxwhirl/treeqn" ["e"=1]
"zhongwen/predictron" -> "avivt/VIN"
"miyosuda/unreal" -> "miyosuda/async_deep_reinforce"
"miyosuda/unreal" -> "NVlabs/GA3C"
"miyosuda/unreal" -> "muupan/async-rl"
"miyosuda/unreal" -> "steveKapturowski/tensorflow-rl"
"miyosuda/unreal" -> "hi-abhi/tensorflow-value-iteration-networks"
"miyosuda/unreal" -> "openai/universe-starter-agent"
"miyosuda/unreal" -> "awjuliani/Meta-RL"
"miyosuda/unreal" -> "Ardavans/DSR"
"miyosuda/unreal" -> "isl-org/DirectFuturePrediction"
"miyosuda/unreal" -> "pathak22/noreward-rl"
"miyosuda/unreal" -> "tgangwani/GA3C-DeepNavigation" ["e"=1]
"miyosuda/unreal" -> "coreylynch/async-rl"
"miyosuda/unreal" -> "rlbayes/rllabplusplus"
"miyosuda/unreal" -> "openai/imitation"
"miyosuda/unreal" -> "davidhershey/feudal_networks"
"wty-yy/KataCR" -> "seermer/HollowKnight_RL"
"leegao/readme2tex" -> "agurod42/github-texify"
"leegao/readme2tex" -> "muupan/async-rl"
"leegao/readme2tex" -> "jwkvam/celluloid" ["e"=1]
"leegao/readme2tex" -> "leegao/float-hacks"
"leegao/readme2tex" -> "hardmaru/sketch-rnn" ["e"=1]
"leegao/readme2tex" -> "coreylynch/async-rl"
"leegao/readme2tex" -> "twitter-archive/torch-autograd" ["e"=1]
"DanielTakeshi/Paper_Notes" -> "DanielTakeshi/rl_algorithms"
"DanielTakeshi/Paper_Notes" -> "rll/rllab"
"DanielTakeshi/Paper_Notes" -> "berkeleydeeprlcourse/homework"
"DanielTakeshi/Paper_Notes" -> "rail-berkeley/softlearning"
"DanielTakeshi/Paper_Notes" -> "dennybritz/deeplearning-papernotes" ["e"=1]
"DanielTakeshi/Paper_Notes" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"DanielTakeshi/Paper_Notes" -> "openai/robosumo"
"DanielTakeshi/Paper_Notes" -> "itaicaspi/mgail"
"DanielTakeshi/Paper_Notes" -> "justinjfu/inverse_rl"
"DanielTakeshi/Paper_Notes" -> "joschu/modular_rl"
"DanielTakeshi/Paper_Notes" -> "cbfinn/gps"
"DanielTakeshi/Paper_Notes" -> "tianheyu927/mil"
"DanielTakeshi/Paper_Notes" -> "rlbayes/rllabplusplus"
"DanielTakeshi/Paper_Notes" -> "haarnoja/softqlearning"
"DanielTakeshi/Paper_Notes" -> "astooke/rlpyt"
"NVlabs/GA3C" -> "miyosuda/async_deep_reinforce"
"NVlabs/GA3C" -> "muupan/async-rl"
"NVlabs/GA3C" -> "miyosuda/unreal"
"NVlabs/GA3C" -> "dgriff777/rl_a3c_pytorch"
"NVlabs/GA3C" -> "ikostrikov/pytorch-a3c"
"NVlabs/GA3C" -> "coreylynch/async-rl"
"NVlabs/GA3C" -> "steveKapturowski/tensorflow-rl"
"NVlabs/GA3C" -> "openai/universe-starter-agent"
"NVlabs/GA3C" -> "google-deepmind/scalable_agent"
"NVlabs/GA3C" -> "pathak22/noreward-rl"
"NVlabs/GA3C" -> "Alfredvc/paac"
"NVlabs/GA3C" -> "hi-abhi/tensorflow-value-iteration-networks"
"NVlabs/GA3C" -> "awjuliani/Meta-RL"
"NVlabs/GA3C" -> "carpedm20/deep-rl-tensorflow"
"NVlabs/GA3C" -> "Kaixhin/Atari"
"mpatacchiola/dissecting-reinforcement-learning" -> "vmayoral/basic_reinforcement_learning"
"mpatacchiola/dissecting-reinforcement-learning" -> "rlcode/reinforcement-learning"
"mpatacchiola/dissecting-reinforcement-learning" -> "awjuliani/oreilly-rl-tutorial" ["e"=1]
"mpatacchiola/dissecting-reinforcement-learning" -> "mimoralea/applied-reinforcement-learning"
"mpatacchiola/dissecting-reinforcement-learning" -> "carpedm20/deep-rl-tensorflow"
"mpatacchiola/dissecting-reinforcement-learning" -> "muupan/deep-reinforcement-learning-papers"
"mpatacchiola/dissecting-reinforcement-learning" -> "awjuliani/DeepRL-Agents"
"mpatacchiola/dissecting-reinforcement-learning" -> "rll/rllab"
"mpatacchiola/dissecting-reinforcement-learning" -> "yrlu/reinforcement_learning"
"mpatacchiola/dissecting-reinforcement-learning" -> "IntelLabs/coach"
"mpatacchiola/dissecting-reinforcement-learning" -> "junhyukoh/deep-reinforcement-learning-papers"
"mpatacchiola/dissecting-reinforcement-learning" -> "TianhongDai/reinforcement-learning-algorithms"
"mpatacchiola/dissecting-reinforcement-learning" -> "yrlu/irl-imitation"
"mpatacchiola/dissecting-reinforcement-learning" -> "tambetm/simple_dqn"
"mpatacchiola/dissecting-reinforcement-learning" -> "sawcordwell/pymdptoolbox" ["e"=1]
"openai/weightnorm" -> "TimSalimans/weight_norm"
"openai/weightnorm" -> "openai/neural-gpu"
"openai/weightnorm" -> "jiamings/fast-weights" ["e"=1]
"openai/weightnorm" -> "zoli333/Weight-Normalization"
"openai/weightnorm" -> "ryankiros/layer-norm" ["e"=1]
"openai/weightnorm" -> "openai/atari-demo"
"jeanharb/option_critic" -> "lweitkamp/option-critic-pytorch"
"jeanharb/option_critic" -> "tdavchev/option-critic"
"jeanharb/option_critic" -> "alversafa/option-critic-arch"
"jeanharb/option_critic" -> "junhyukoh/value-prediction-network"
"jeanharb/option_critic" -> "jeanharb/a2oc_delib"
"jeanharb/option_critic" -> "davidhershey/feudal_networks"
"jeanharb/option_critic" -> "kkhetarpal/ioc"
"jeanharb/option_critic" -> "EdenMacdonald/h-DQN"
"jeanharb/option_critic" -> "mklissa/PPOC"
"cndaqiang/autowzry" -> "myBoris/wzry_ai"
"cndaqiang/autowzry" -> "XRSec/WZRY_AirtestIDE"
"cndaqiang/autowzry" -> "cndaqiang/airtest_mobileauto"
"cndaqiang/autowzry" -> "witmemtech/Algorithm-Deployed-in-WTM2101" ["e"=1]
"cndaqiang/autowzry" -> "tongmume/blockchain_auth"
"cndaqiang/autowzry" -> "AustinIOI/Golaris_Laravel" ["e"=1]
"knakamura13/cs7641-ml-study-materials-2023" -> "knakamura13/mlrose-ky"
"knakamura13/cs7641-ml-study-materials-2023" -> "knakamura13/huggingface-dataset-toolkit"
"knakamura13/cs7641-ml-study-materials-2023" -> "knakamura13/mlrose-torch"
"zond/diplicity" -> "spamguy/dipl.io"
"spamguy/dipl.io" -> "zond/diplicity"
"MattChanTK/gym-maze" -> "MattChanTK/ai-gym"
"MattChanTK/gym-maze" -> "zuoxingdong/mazelab"
"MattChanTK/gym-maze" -> "Farama-Foundation/Miniworld"
"MattChanTK/gym-maze" -> "Farama-Foundation/Minigrid"
"MattChanTK/gym-maze" -> "openai/gym-soccer"
"MattChanTK/gym-maze" -> "tambetm/gym-minecraft"
"MattChanTK/gym-maze" -> "lcswillems/rl-starter-files"
"MattChanTK/gym-maze" -> "kenjyoung/MinAtar" ["e"=1]
"MattChanTK/gym-maze" -> "benelot/pybullet-gym"
"MattChanTK/gym-maze" -> "denisyarats/dmc2gym" ["e"=1]
"openai/websockify" -> "openai/ceph-chef"
"avivt/VIN" -> "hi-abhi/tensorflow-value-iteration-networks"
"avivt/VIN" -> "kentsommer/pytorch-value-iteration-networks"
"avivt/VIN" -> "zuoxingdong/VIN_PyTorch_Visdom"
"avivt/VIN" -> "rlbayes/rllabplusplus"
"avivt/VIN" -> "zuoxingdong/VIN_TensorFlow"
"avivt/VIN" -> "zhongwen/predictron"
"avivt/VIN" -> "miyosuda/async_deep_reinforce"
"avivt/VIN" -> "YunzhuLi/InfoGAIL"
"avivt/VIN" -> "junhyukoh/value-prediction-network"
"avivt/VIN" -> "geek-ai/1m-agents"
"avivt/VIN" -> "RLAgent/gated-path-planning-networks" ["e"=1]
"avivt/VIN" -> "facebookarchive/MazeBase"
"ziwenhahaha/Code-of-RL-Beginning" -> "SupermanCaozh/The_Coding_Foundation_in_Reinforcement_Learning"
"ziwenhahaha/Code-of-RL-Beginning" -> "jwk1rose/RL_Learning"
"ziwenhahaha/Code-of-RL-Beginning" -> "10-OASIS-01/minrl"
"ziwenhahaha/Code-of-RL-Beginning" -> "Ronchy2000/Multi-agent-RL"
"openai/gym-recording" -> "openai/doom-py"
"openai/gym-recording" -> "openai/gym-wikinav"
"rlbayes/rllabplusplus" -> "joschu/modular_rl"
"rlbayes/rllabplusplus" -> "wojzaremba/trpo"
"rlbayes/rllabplusplus" -> "Breakend/DeepReinforcementLearningThatMatters"
"rlbayes/rllabplusplus" -> "DartML/PPO-Stein-Control-Variate"
"rlbayes/rllabplusplus" -> "junhyukoh/self-imitation-learning"
"rlbayes/rllabplusplus" -> "isl-org/DirectFuturePrediction"
"rlbayes/rllabplusplus" -> "zuoxingdong/VIN_TensorFlow"
"openai/go-vncdriver" -> "openai/ceph-chef"
"cookbenjamin/DDPG" -> "rmst/ddpg"
"Unakar/AI_Game_KingGlory" -> "wty-yy/kaiwu2024_taichu"
"myBoris/wzry_ai" -> "tencent-ailab/hok_env"
"myBoris/wzry_ai" -> "cndaqiang/autowzry"
"myBoris/wzry_ai" -> "f200ten/RL_King_of_Glory"
"myBoris/wzry_ai" -> "FengQuanLi/WZCQ"
"myBoris/wzry_ai" -> "Unakar/AI_Game_KingGlory"
"sisl/gail-driver" -> "sisl/ngsim_env" ["e"=1]
"sisl/gail-driver" -> "YunzhuLi/InfoGAIL"
"sisl/gail-driver" -> "MCZhi/Driving-IRL-NGSIM" ["e"=1]
"sisl/gail-driver" -> "andrewliao11/gail-tf"
"sjchoi86/irl_rocks" -> "jangirrishabh/toyCarIRL"
"sjchoi86/irl_rocks" -> "yrlu/irl-imitation"
"sjchoi86/irl_rocks" -> "HumanCompatibleAI/atari-irl"
"sjchoi86/irl_rocks" -> "ahq1993/inverse_rl"
"sjchoi86/irl_rocks" -> "vvanirudh/IRL-Toolkit"
"sjchoi86/irl_rocks" -> "jinming99/DGP-IRL"
"sjchoi86/irl_rocks" -> "makokal/funzo"
"craigthomas/Chip8Python" -> "tobywhughes/Chippy8"
"craigthomas/Chip8Python" -> "craigthomas/Chip8C"
"craigthomas/Chip8Python" -> "AlpacaMax/Python-CHIP8-Emulator"
"ZhengYinan-AIR/OMIGA" -> "thu-rllab/CFCQL"
"ZhengYinan-AIR/OMIGA" -> "maoliyuan/ODICE-Pytorch"
"JKCooper2/rlai-exercises" -> "iamhectorotero/rlai-exercises"
"JKCooper2/rlai-exercises" -> "matteocasolari/reinforcement-learning-an-introduction-solutions"
"crowdAI/crowdai" -> "crowdAI/vizdoom2018-singleplayer-starter-kit"
"openai/mitmproxy" -> "openai/pyconfigatron"
"openai/aws-fluent-plugin-kinesis" -> "openai/pyconfigatron"
"takoika/PrioritizedExperienceReplay" -> "Damcy/prioritized-experience-replay"
"fhennecker/deepdoom" -> "itaicaspi/keras-dqn-doom"
"yandexdataschool/MLatImperial2017" -> "yandexdataschool/MLatGradDays"
"synpon/prog_nn" -> "seann999/progressive_a3c"
"synpon/prog_nn" -> "yao62995/A3C"
"submit-paper/Danzero_plus" -> "AltmanD/guandan_mcc"
"submit-paper/Danzero_plus" -> "samuelgzx/RL_GuanDan"
"submit-paper/Danzero_plus" -> "Vincentzyx/Douzero_Resnet"
"submit-paper/Danzero_plus" -> "submit-paper/Doudizhu_plus"
"seann999/progressive_a3c" -> "synpon/prog_nn"
"upiterbarg/diff_history" -> "upiterbarg/hihack"
"upiterbarg/diff_history" -> "upiterbarg/lintseq"
"ray-project/tutorial" -> "anyscale/academy" ["e"=1]
"ray-project/tutorial" -> "ray-project/xgboost_ray" ["e"=1]
"ray-project/tutorial" -> "google-research/seed_rl"
"ray-project/tutorial" -> "ray-project/rl-experiments"
"ray-project/tutorial" -> "google-deepmind/reverb" ["e"=1]
"ray-project/tutorial" -> "google-deepmind/bsuite"
"ray-project/tutorial" -> "sjtu-marl/malib"
"ray-project/tutorial" -> "google-deepmind/scalable_agent"
"ray-project/tutorial" -> "haarnoja/softqlearning"
"ray-project/tutorial" -> "google-research/dreamer" ["e"=1]
"ray-project/tutorial" -> "araffin/rl-baselines-zoo"
"ray-project/tutorial" -> "shariqiqbal2810/MAAC"
"ray-project/tutorial" -> "oxwhirl/pymarl"
"ray-project/tutorial" -> "Kaixhin/ACER"
"ray-project/tutorial" -> "oxwhirl/smac"
"LantaoYu/MARL-Papers" -> "oxwhirl/pymarl"
"LantaoYu/MARL-Papers" -> "starry-sky6688/MARL-Algorithms"
"LantaoYu/MARL-Papers" -> "openai/multiagent-particle-envs"
"LantaoYu/MARL-Papers" -> "openai/maddpg"
"LantaoYu/MARL-Papers" -> "Farama-Foundation/PettingZoo"
"LantaoYu/MARL-Papers" -> "geek-ai/MAgent"
"LantaoYu/MARL-Papers" -> "marlbenchmark/on-policy"
"LantaoYu/MARL-Papers" -> "oxwhirl/smac"
"LantaoYu/MARL-Papers" -> "openai/baselines"
"LantaoYu/MARL-Papers" -> "tigerneil/awesome-deep-rl"
"LantaoYu/MARL-Papers" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"LantaoYu/MARL-Papers" -> "Replicable-MARL/MARLlib"
"LantaoYu/MARL-Papers" -> "thu-ml/tianshou"
"LantaoYu/MARL-Papers" -> "ShangtongZhang/DeepRL"
"LantaoYu/MARL-Papers" -> "sisl/MADRL"
"berkeleydeeprlcourse/homework" -> "rll/rllab"
"berkeleydeeprlcourse/homework" -> "rail-berkeley/softlearning"
"berkeleydeeprlcourse/homework" -> "rail-berkeley/rlkit"
"berkeleydeeprlcourse/homework" -> "carpedm20/deep-rl-tensorflow"
"berkeleydeeprlcourse/homework" -> "ShangtongZhang/DeepRL"
"berkeleydeeprlcourse/homework" -> "joschu/modular_rl"
"berkeleydeeprlcourse/homework" -> "junhyukoh/deep-reinforcement-learning-papers"
"berkeleydeeprlcourse/homework" -> "astooke/rlpyt"
"berkeleydeeprlcourse/homework" -> "openai/mujoco-py"
"berkeleydeeprlcourse/homework" -> "google-deepmind/trfl"
"berkeleydeeprlcourse/homework" -> "berkeleydeeprlcourse/homework_fall2019"
"berkeleydeeprlcourse/homework" -> "williamFalcon/DeepRLHacks"
"berkeleydeeprlcourse/homework" -> "openai/roboschool"
"berkeleydeeprlcourse/homework" -> "rlworkgroup/garage"
"berkeleydeeprlcourse/homework" -> "cbfinn/gps"
"ShangtongZhang/DeepRL" -> "udacity/deep-reinforcement-learning"
"ShangtongZhang/DeepRL" -> "higgsfield/RL-Adventure"
"ShangtongZhang/DeepRL" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"ShangtongZhang/DeepRL" -> "astooke/rlpyt"
"ShangtongZhang/DeepRL" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"ShangtongZhang/DeepRL" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"ShangtongZhang/DeepRL" -> "rail-berkeley/rlkit"
"ShangtongZhang/DeepRL" -> "rll/rllab"
"ShangtongZhang/DeepRL" -> "openai/baselines"
"ShangtongZhang/DeepRL" -> "higgsfield-ai/higgsfield"
"ShangtongZhang/DeepRL" -> "hill-a/stable-baselines"
"ShangtongZhang/DeepRL" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ShangtongZhang/DeepRL" -> "sfujim/TD3"
"ShangtongZhang/DeepRL" -> "LantaoYu/MARL-Papers"
"ShangtongZhang/DeepRL" -> "Khrylx/PyTorch-RL"
"google-deepmind/sonnet" -> "google-deepmind/learning-to-learn" ["e"=1]
"google-deepmind/sonnet" -> "google-deepmind/dnc" ["e"=1]
"google-deepmind/sonnet" -> "google-deepmind/lab"
"google-deepmind/sonnet" -> "tensorflow/tensor2tensor" ["e"=1]
"google-deepmind/sonnet" -> "pyro-ppl/pyro" ["e"=1]
"google-deepmind/sonnet" -> "PAIR-code/facets" ["e"=1]
"google-deepmind/sonnet" -> "magenta/magenta" ["e"=1]
"google-deepmind/sonnet" -> "horovod/horovod" ["e"=1]
"google-deepmind/sonnet" -> "facebookresearch/fairseq-lua" ["e"=1]
"google-deepmind/sonnet" -> "google-deepmind/pysc2" ["e"=1]
"google-deepmind/sonnet" -> "facebookarchive/caffe2" ["e"=1]
"google-deepmind/sonnet" -> "google/dopamine"
"google-deepmind/sonnet" -> "google/seq2seq" ["e"=1]
"google-deepmind/sonnet" -> "openai/baselines"
"google-deepmind/sonnet" -> "openai/universe"
"ikostrikov/pytorch-a3c" -> "MorvanZhou/pytorch-A3C"
"ikostrikov/pytorch-a3c" -> "dgriff777/rl_a3c_pytorch"
"ikostrikov/pytorch-a3c" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"ikostrikov/pytorch-a3c" -> "jingweiz/pytorch-rl"
"ikostrikov/pytorch-a3c" -> "NVlabs/GA3C"
"ikostrikov/pytorch-a3c" -> "ikostrikov/pytorch-trpo"
"ikostrikov/pytorch-a3c" -> "pathak22/noreward-rl"
"ikostrikov/pytorch-a3c" -> "ShangtongZhang/DeepRL"
"ikostrikov/pytorch-a3c" -> "Kaixhin/Rainbow"
"ikostrikov/pytorch-a3c" -> "ghliu/pytorch-ddpg"
"ikostrikov/pytorch-a3c" -> "haarnoja/sac"
"ikostrikov/pytorch-a3c" -> "rll/rllab"
"ikostrikov/pytorch-a3c" -> "openai/universe-starter-agent"
"ikostrikov/pytorch-a3c" -> "rail-berkeley/rlkit"
"ikostrikov/pytorch-a3c" -> "miyosuda/async_deep_reinforce"
"MushroomRL/mushroom-rl" -> "cpnota/autonomous-learning-library"
"MushroomRL/mushroom-rl" -> "google-deepmind/bsuite"
"MushroomRL/mushroom-rl" -> "rlworkgroup/garage"
"MushroomRL/mushroom-rl" -> "Farama-Foundation/Minigrid"
"MushroomRL/mushroom-rl" -> "astooke/rlpyt"
"MushroomRL/mushroom-rl" -> "robfiras/loco-mujoco" ["e"=1]
"MushroomRL/mushroom-rl" -> "Farama-Foundation/Miniworld"
"MushroomRL/mushroom-rl" -> "Denys88/rl_games" ["e"=1]
"MushroomRL/mushroom-rl" -> "facebookresearch/mbrl-lib" ["e"=1]
"MushroomRL/mushroom-rl" -> "aravindr93/mjrl" ["e"=1]
"MushroomRL/mushroom-rl" -> "google-deepmind/acme"
"MushroomRL/mushroom-rl" -> "Farama-Foundation/Metaworld" ["e"=1]
"MushroomRL/mushroom-rl" -> "clvrai/awesome-rl-envs"
"MushroomRL/mushroom-rl" -> "rail-berkeley/rlkit"
"MushroomRL/mushroom-rl" -> "google-research/realworldrl_suite" ["e"=1]
"chainer/chainerrl" -> "muupan/async-rl"
"chainer/chainerrl" -> "chainer/chainermn" ["e"=1]
"chainer/chainerrl" -> "rll/rllab"
"chainer/chainerrl" -> "chainer/chainercv" ["e"=1]
"chainer/chainerrl" -> "chainer/chainer" ["e"=1]
"chainer/chainerrl" -> "pfnet/pfrl"
"chainer/chainerrl" -> "IntelLabs/coach"
"chainer/chainerrl" -> "joschu/modular_rl"
"chainer/chainerrl" -> "tensorforce/tensorforce"
"chainer/chainerrl" -> "chainer/chainerui" ["e"=1]
"chainer/chainerrl" -> "miyosuda/async_deep_reinforce"
"chainer/chainerrl" -> "astooke/rlpyt"
"chainer/chainerrl" -> "miyosuda/unreal"
"chainer/chainerrl" -> "ShangtongZhang/DeepRL"
"chainer/chainerrl" -> "Kaixhin/Rainbow"
"google-deepmind/dqn" -> "google-deepmind/dqn_zoo" ["e"=1]
"google-deepmind/dqn" -> "devsisters/DQN-tensorflow"
"google-deepmind/dqn" -> "hungtuchen/pytorch-dqn"
"google-deepmind/dqn" -> "tambetm/simple_dqn"
"google-deepmind/dqn" -> "dxyang/DQN_pytorch"
"google-deepmind/dqn" -> "google-deepmind/pycolab"
"google-deepmind/dqn" -> "google-deepmind/dnc" ["e"=1]
"google-deepmind/dqn" -> "google-deepmind/scalable_agent"
"google-deepmind/dqn" -> "Farama-Foundation/Arcade-Learning-Environment"
"google-deepmind/dqn" -> "ikostrikov/pytorch-a3c"
"google-deepmind/dqn" -> "rll/rllab"
"google-deepmind/dqn" -> "gliese581gg/DQN_tensorflow"
"google-deepmind/dqn" -> "ikostrikov/pytorch-trpo"
"google-deepmind/dqn" -> "openai/atari-py"
"google-deepmind/dqn" -> "joschu/modular_rl"
"openai/evolution-strategies-starter" -> "uber-research/deep-neuroevolution" ["e"=1]
"openai/evolution-strategies-starter" -> "atgambardella/pytorch-es" ["e"=1]
"openai/evolution-strategies-starter" -> "hardmaru/estool" ["e"=1]
"openai/evolution-strategies-starter" -> "rll/rllab"
"openai/evolution-strategies-starter" -> "openai/universe-starter-agent"
"openai/evolution-strategies-starter" -> "openai/imitation"
"openai/evolution-strategies-starter" -> "joschu/modular_rl"
"openai/evolution-strategies-starter" -> "miyosuda/async_deep_reinforce"
"openai/evolution-strategies-starter" -> "pathak22/noreward-rl"
"openai/evolution-strategies-starter" -> "williamFalcon/DeepRLHacks"
"openai/evolution-strategies-starter" -> "openai/EPG"
"openai/evolution-strategies-starter" -> "openai/roboschool"
"openai/evolution-strategies-starter" -> "CMA-ES/pycma" ["e"=1]
"openai/evolution-strategies-starter" -> "openai/mlsh"
"openai/evolution-strategies-starter" -> "openai/multiagent-competition"
"pushkar/ABAGAIL" -> "JonathanTay/CS-7641-assignment-1"
"pushkar/ABAGAIL" -> "jmacglashan/burlap"
"pushkar/ABAGAIL" -> "juanjose49/omscs-cs7641-machine-learning-assignment-4"
"pushkar/ABAGAIL" -> "hiive/mlrose"
"pushkar/ABAGAIL" -> "JonathanTay/CS-7641-assignment-2"
"pushkar/ABAGAIL" -> "eternalmothra/ml_cheat_sheet"
"pushkar/ABAGAIL" -> "mjs2600/ML-Final-Exam-Study-Notes"
"pushkar/ABAGAIL" -> "cmaron/CS-7641-assignments"
"pushkar/ABAGAIL" -> "gkhayes/mlrose"
"pushkar/ABAGAIL" -> "cgearhart/students-filters"
"pushkar/ABAGAIL" -> "mjs2600/mimicry"
"pushkar/ABAGAIL" -> "rldm/rldm_tutorials"
"pushkar/ABAGAIL" -> "mimoralea/applied-reinforcement-learning"
"altera-al/project-sid" -> "joonspk-research/genagents" ["e"=1]
"altera-al/project-sid" -> "google-deepmind/concordia"
"altera-al/project-sid" -> "Thytu/Agentarium" ["e"=1]
"altera-al/project-sid" -> "etched-ai/open-oasis" ["e"=1]
"altera-al/project-sid" -> "AgentTorch/AgentTorch"
"altera-al/project-sid" -> "mpaepper/llm_agents" ["e"=1]
"altera-al/project-sid" -> "NousResearch/Open-Reasoning-Tasks" ["e"=1]
"altera-al/project-sid" -> "kolbytn/mindcraft" ["e"=1]
"hungtuchen/pytorch-dqn" -> "jingweiz/pytorch-rl"
"hungtuchen/pytorch-dqn" -> "dxyang/DQN_pytorch"
"hungtuchen/pytorch-dqn" -> "ikostrikov/pytorch-a3c"
"hungtuchen/pytorch-dqn" -> "AndersonJo/dqn-pytorch"
"hungtuchen/pytorch-dqn" -> "dgriff777/rl_a3c_pytorch"
"hungtuchen/pytorch-dqn" -> "google-deepmind/dqn"
"hungtuchen/pytorch-dqn" -> "ikostrikov/pytorch-ddpg-naf"
"hungtuchen/pytorch-dqn" -> "ikostrikov/pytorch-trpo"
"hungtuchen/pytorch-dqn" -> "ShangtongZhang/DeepRL"
"hungtuchen/pytorch-dqn" -> "berkeleydeeprlcourse/homework"
"hungtuchen/pytorch-dqn" -> "ghliu/pytorch-ddpg"
"ghliu/pytorch-ddpg" -> "ikostrikov/pytorch-ddpg-naf"
"ghliu/pytorch-ddpg" -> "floodsung/DDPG"
"ghliu/pytorch-ddpg" -> "ikostrikov/pytorch-a3c"
"ghliu/pytorch-ddpg" -> "xuehy/pytorch-maddpg"
"ghliu/pytorch-ddpg" -> "vy007vikas/PyTorch-ActorCriticRL"
"ghliu/pytorch-ddpg" -> "ikostrikov/pytorch-trpo"
"ghliu/pytorch-ddpg" -> "pranz24/pytorch-soft-actor-critic"
"ghliu/pytorch-ddpg" -> "Khrylx/PyTorch-RL"
"ghliu/pytorch-ddpg" -> "denisyarats/pytorch_sac" ["e"=1]
"ghliu/pytorch-ddpg" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"ghliu/pytorch-ddpg" -> "jingweiz/pytorch-rl"
"ghliu/pytorch-ddpg" -> "ChenglongChen/pytorch-DRL"
"ghliu/pytorch-ddpg" -> "MorvanZhou/pytorch-A3C"
"ghliu/pytorch-ddpg" -> "dgriff777/rl_a3c_pytorch"
"ghliu/pytorch-ddpg" -> "Kaixhin/ACER"
"keon/deep-q-learning" -> "devsisters/DQN-tensorflow"
"keon/deep-q-learning" -> "rlcode/reinforcement-learning"
"keon/deep-q-learning" -> "keras-rl/keras-rl"
"keon/deep-q-learning" -> "germain-hug/Deep-RL-Keras"
"keon/deep-q-learning" -> "awjuliani/DeepRL-Agents"
"keon/deep-q-learning" -> "tambetm/simple_dqn"
"keon/deep-q-learning" -> "carpedm20/deep-rl-tensorflow"
"keon/deep-q-learning" -> "jaromiru/AI-blog"
"keon/deep-q-learning" -> "keon/policy-gradient"
"keon/deep-q-learning" -> "thedimlebowski/Trading-Gym" ["e"=1]
"keon/deep-q-learning" -> "yanpanlau/DDPG-Keras-Torcs"
"keon/deep-q-learning" -> "tensorforce/tensorforce"
"keon/deep-q-learning" -> "deependersingla/deep_trader" ["e"=1]
"keon/deep-q-learning" -> "yanpanlau/Keras-FlappyBird"
"keon/deep-q-learning" -> "tokb23/dqn"
"reinshift/MADDPG_Multi_UAV_Roundup" -> "philtabor/Multi-Agent-Reinforcement-Learning"
"reinshift/MADDPG_Multi_UAV_Roundup" -> "henbudidiao/UAV-path-planning" ["e"=1]
"reinshift/MADDPG_Multi_UAV_Roundup" -> "thu-uav/Multi-UAV-pursuit-evasion"
"reinshift/MADDPG_Multi_UAV_Roundup" -> "LYSJ-feng/mCPP-based-on-MADDPG" ["e"=1]
"tensorforce/tensorforce" -> "IntelLabs/coach"
"tensorforce/tensorforce" -> "rll/rllab"
"tensorforce/tensorforce" -> "keras-rl/keras-rl"
"tensorforce/tensorforce" -> "tensorflow/agents"
"tensorforce/tensorforce" -> "hill-a/stable-baselines"
"tensorforce/tensorforce" -> "google-deepmind/trfl"
"tensorforce/tensorforce" -> "openai/baselines"
"tensorforce/tensorforce" -> "google/dopamine"
"tensorforce/tensorforce" -> "awjuliani/DeepRL-Agents"
"tensorforce/tensorforce" -> "rlcode/reinforcement-learning"
"tensorforce/tensorforce" -> "google-research/batch-ppo"
"tensorforce/tensorforce" -> "facebookresearch/ReAgent"
"tensorforce/tensorforce" -> "google-deepmind/dm_control"
"tensorforce/tensorforce" -> "ShangtongZhang/DeepRL"
"tensorforce/tensorforce" -> "aikorea/awesome-rl"
"ParasGarg/Stevens-Computer-Science-Courses-Materials" -> "dkadyrov/SIT"
"PyPlanet/PyPlanet" -> "EvoEsports/EvoSC"
"PyPlanet/PyPlanet" -> "undeflabs/UASECO-Maniaplanet"
"PyPlanet/PyPlanet" -> "nadeo/trackmania-doc"
"PyPlanet/PyPlanet" -> "Harha/trackmania-server-docker"
"PyPlanet/PyPlanet" -> "codecat/tm-better-chat"
"PyPlanet/PyPlanet" -> "maniaplanet/documentation"
"PyPlanet/PyPlanet" -> "codecat/tm-dashboard"
"jingweiz/pytorch-rl" -> "ikostrikov/pytorch-a3c"
"jingweiz/pytorch-rl" -> "dgriff777/rl_a3c_pytorch"
"jingweiz/pytorch-rl" -> "jingweiz/pytorch-dnc" ["e"=1]
"jingweiz/pytorch-rl" -> "hungtuchen/pytorch-dqn"
"jingweiz/pytorch-rl" -> "ShangtongZhang/DeepRL"
"jingweiz/pytorch-rl" -> "Khrylx/PyTorch-RL"
"jingweiz/pytorch-rl" -> "Kaixhin/ACER"
"jingweiz/pytorch-rl" -> "zuoxingdong/VIN_PyTorch_Visdom"
"jingweiz/pytorch-rl" -> "NVlabs/GA3C"
"jingweiz/pytorch-rl" -> "carpedm20/deep-rl-tensorflow"
"jingweiz/pytorch-rl" -> "junhyukoh/deep-reinforcement-learning-papers"
"jingweiz/pytorch-rl" -> "rll/rllab"
"jingweiz/pytorch-rl" -> "navneet-nmk/pytorch-rl"
"jingweiz/pytorch-rl" -> "williamFalcon/DeepRLHacks"
"jingweiz/pytorch-rl" -> "ghliu/pytorch-ddpg"
"Turing-Project/Black-Myth-Wukong-AI" -> "analoganddigital/DQN_play_sekiro"
"Turing-Project/Black-Myth-Wukong-AI" -> "ocram444/EldenRL"
"Turing-Project/Black-Myth-Wukong-AI" -> "AARG-FAN/Yolo_for_Wukong"
"kentsommer/pytorch-value-iteration-networks" -> "avivt/VIN"
"kentsommer/pytorch-value-iteration-networks" -> "zuoxingdong/VIN_PyTorch_Visdom"
"kentsommer/pytorch-value-iteration-networks" -> "hi-abhi/tensorflow-value-iteration-networks"
"kentsommer/pytorch-value-iteration-networks" -> "junhyukoh/value-prediction-network"
"kentsommer/pytorch-value-iteration-networks" -> "RLAgent/gated-path-planning-networks" ["e"=1]
"kentsommer/pytorch-value-iteration-networks" -> "zuoxingdong/VIN_TensorFlow"
"kentsommer/pytorch-value-iteration-networks" -> "sufengniu/GVIN"
"mimoralea/applied-reinforcement-learning" -> "rldm/rldm_tutorials"
"mimoralea/applied-reinforcement-learning" -> "pushkar/ABAGAIL"
"mimoralea/applied-reinforcement-learning" -> "mimoralea/gdrl"
"sjtu-marl/ZSC-Eval" -> "liyang619/COLE-Platform"
"sjtu-marl/ZSC-Eval" -> "samjia2000/HSP"
"ArztSamuel/Applying_EANNs" -> "ssusnic/Machine-Learning-Flappy-Bird"
"ArztSamuel/Applying_EANNs" -> "greerviau/SnakeAI"
"ArztSamuel/Applying_EANNs" -> "hamuchiwa/AutoRCCar" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "Blueteak/Unity-Neural-Network" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "Sentdex/pygta5" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "tawnkramer/sdsandbox" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "Helpsypoo/primerpython" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "aleju/mario-ai"
"ArztSamuel/Applying_EANNs" -> "techwithtim/NEAT-Flappy-Bird" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "harvitronix/reinforcement-learning-car"
"ArztSamuel/Applying_EANNs" -> "ivanseidel/IAMDinosaur" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "Unity-Technologies/FPSSample" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "red42/HTML5_Genetic_Cars"
"ArztSamuel/Applying_EANNs" -> "udacity/self-driving-car-sim" ["e"=1]
"ArztSamuel/Applying_EANNs" -> "lexfridman/deeptraffic"
"ikostrikov/pytorch-ddpg-naf" -> "ghliu/pytorch-ddpg"
"ikostrikov/pytorch-ddpg-naf" -> "ikostrikov/pytorch-trpo"
"ikostrikov/pytorch-ddpg-naf" -> "dgriff777/rl_a3c_pytorch"
"ikostrikov/pytorch-ddpg-naf" -> "junhyukoh/value-prediction-network"
"ikostrikov/pytorch-ddpg-naf" -> "carpedm20/NAF-tensorflow"
"ikostrikov/pytorch-ddpg-naf" -> "floringogianu/categorical-dqn"
"ikostrikov/pytorch-ddpg-naf" -> "ikostrikov/pytorch-a3c"
"ikostrikov/pytorch-ddpg-naf" -> "jingweiz/pytorch-dnc" ["e"=1]
"ikostrikov/pytorch-ddpg-naf" -> "ikostrikov/pytorch-meta-optimizer" ["e"=1]
"ikostrikov/pytorch-ddpg-naf" -> "adik993/ppo-pytorch"
"awjuliani/dfp" -> "isl-org/DirectFuturePrediction"
"tigerneil/awesome-deep-rl" -> "LantaoYu/MARL-Papers"
"tigerneil/awesome-deep-rl" -> "oxwhirl/pymarl"
"tigerneil/awesome-deep-rl" -> "zhangchuheng123/Reinforcement-Implementation"
"tigerneil/awesome-deep-rl" -> "NeuronDance/DeepRL"
"tigerneil/awesome-deep-rl" -> "aikorea/awesome-rl"
"tigerneil/awesome-deep-rl" -> "geek-ai/MAgent"
"tigerneil/awesome-deep-rl" -> "ShangtongZhang/DeepRL"
"tigerneil/awesome-deep-rl" -> "starry-sky6688/MARL-Algorithms"
"tigerneil/awesome-deep-rl" -> "sisl/MADRL"
"tigerneil/awesome-deep-rl" -> "oxwhirl/smac"
"tigerneil/awesome-deep-rl" -> "junhyukoh/deep-reinforcement-learning-papers"
"tigerneil/awesome-deep-rl" -> "shariqiqbal2810/MAAC"
"tigerneil/awesome-deep-rl" -> "rll/rllab"
"tigerneil/awesome-deep-rl" -> "xuehy/pytorch-maddpg"
"tigerneil/awesome-deep-rl" -> "datamllab/awesome-game-ai"
"siemens/policy_search_bb-alpha" -> "andrewliao11/NoisyNet-DQN"
"mihahauke/VDAIC2017" -> "akolishchak/doom-net-pytorch"
"mihahauke/VDAIC2017" -> "crowdAI/vizdoom2018-singleplayer-starter-kit"
"mihahauke/VDAIC2017" -> "mwydmuch/PyOblige"
"hungtuchen/pytorch-hdqn" -> "gmargo11/hDQN"
"hungtuchen/pytorch-hdqn" -> "nishantgurunath/HRL"
"akolishchak/doom-net-pytorch" -> "mihahauke/VDAIC2017"
"akolishchak/doom-net-pytorch" -> "GoingMyWay/ViZDoomAgents"
"akolishchak/doom-net-pytorch" -> "isl-org/DirectFuturePrediction"
"akolishchak/doom-net-pytorch" -> "mwydmuch/PyOblige"
"akolishchak/doom-net-pytorch" -> "mehdiboubnan/Deep-Reinforcement-Learning-applied-to-DOOM"
"akolishchak/doom-net-pytorch" -> "mihahauke/deep_rl_vizdoom"
"akolishchak/doom-net-pytorch" -> "agiantwhale/NavDoom"
"rcsoccersim/rcssserver" -> "rcsoccersim/rcssmonitor"
"rcsoccersim/rcssserver" -> "helios-base/helios-base"
"rcsoccersim/rcssserver" -> "helios-base/librcsc"
"rcsoccersim/rcssserver" -> "herodrigues/robocup2d-tutorial"
"rcsoccersim/rcssserver" -> "rcsoccersim/rcsslogplayer"
"rcsoccersim/rcssserver" -> "rcsoccersim/manual"
"rcsoccersim/rcssserver" -> "Cyrus2D/Pyrus-SS2D-Base"
"rcsoccersim/rcssserver" -> "wrighteagle2d/wrighteaglebase"
"rcsoccersim/rcssserver" -> "Cyrus2D/Cyrus2DBase"
"rcsoccersim/rcssserver" -> "helios-base/soccerwindow2"
"rcsoccersim/rcssserver" -> "helios-base/fedit2"
"rcsoccersim/rcssserver" -> "LARG/HFO"
"rcsoccersim/rcssserver" -> "Cyrus2D/Pyrus2D"
"steveKapturowski/tensorflow-rl" -> "wojzaremba/trpo"
"steveKapturowski/tensorflow-rl" -> "miyosuda/unreal"
"steveKapturowski/tensorflow-rl" -> "yao62995/A3C"
"steveKapturowski/tensorflow-rl" -> "openai/vime"
"steveKapturowski/tensorflow-rl" -> "Islandman93/reinforcepy"
"steveKapturowski/tensorflow-rl" -> "miyosuda/async_deep_reinforce"
"steveKapturowski/tensorflow-rl" -> "NVlabs/GA3C"
"steveKapturowski/tensorflow-rl" -> "Kaixhin/NoisyNet-A3C"
"steveKapturowski/tensorflow-rl" -> "steveKapturowski/async-deep-rl"
"steveKapturowski/tensorflow-rl" -> "traai/async-deep-rl"
"awjuliani/Meta-RL" -> "achao2013/Learning-To-Reinforcement-Learn" ["e"=1]
"awjuliani/Meta-RL" -> "miyosuda/unreal"
"awjuliani/Meta-RL" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"awjuliani/Meta-RL" -> "cbfinn/maml_rl" ["e"=1]
"awjuliani/Meta-RL" -> "miyosuda/async_deep_reinforce"
"awjuliani/Meta-RL" -> "katerakelly/oyster" ["e"=1]
"awjuliani/Meta-RL" -> "mtrazzi/two-step-task"
"awjuliani/Meta-RL" -> "mwufi/meta-rl-bandits"
"awjuliani/Meta-RL" -> "EdenMacdonald/h-DQN"
"awjuliani/Meta-RL" -> "awjuliani/DeepRL-Agents"
"awjuliani/Meta-RL" -> "NVlabs/GA3C"
"awjuliani/Meta-RL" -> "hi-abhi/tensorflow-value-iteration-networks"
"awjuliani/Meta-RL" -> "jonasrothfuss/ProMP" ["e"=1]
"awjuliani/Meta-RL" -> "ikostrikov/pytorch-ddpg-naf"
"awjuliani/Meta-RL" -> "5vision/deep-reinforcement-learning-networks"
"SupermanCaozh/The_Coding_Foundation_in_Reinforcement_Learning" -> "ziwenhahaha/Code-of-RL-Beginning"
"isl-org/DirectFuturePrediction" -> "akolishchak/doom-net-pytorch"
"isl-org/DirectFuturePrediction" -> "crowdAI/vizdoom2018-singleplayer-starter-kit"
"isl-org/DirectFuturePrediction" -> "rlbayes/rllabplusplus"
"isl-org/DirectFuturePrediction" -> "nsavinov/SPTM" ["e"=1]
"liampetti/DDPG" -> "pemami4911/deep-rl"
"Alfredvc/paac" -> "Kaixhin/NoisyNet-A3C"
"Alfredvc/paac" -> "NVlabs/GA3C"
"YunzhuLi/InfoGAIL" -> "uidilr/gail_ppo_tf"
"YunzhuLi/InfoGAIL" -> "andrewliao11/gail-tf"
"YunzhuLi/InfoGAIL" -> "sisl/hgail"
"YunzhuLi/InfoGAIL" -> "sisl/gail-driver"
"YunzhuLi/InfoGAIL" -> "ermongroup/InfoGAIL"
"YunzhuLi/InfoGAIL" -> "openai/imitation"
"YunzhuLi/InfoGAIL" -> "itaicaspi/mgail"
"YunzhuLi/InfoGAIL" -> "navuboy/gail_gym"
"YunzhuLi/InfoGAIL" -> "bstadie/third_person_im"
"DartEnv/dart-env" -> "DartEnv/gym-dart"
"DartEnv/dart-env" -> "sehoonha/pydart2"
"itaicaspi/mgail" -> "Breakend/OptionGAN"
"rcsoccersim/rcssmonitor" -> "rcsoccersim/rcssserver"
"rcsoccersim/rcssmonitor" -> "rcsoccersim/rcsslogplayer"
"rcsoccersim/rcssmonitor" -> "rcsoccersim/manual"
"rcsoccersim/rcssmonitor" -> "helios-base/soccerwindow2"
"zuoxingdong/VIN_PyTorch_Visdom" -> "kentsommer/pytorch-value-iteration-networks"
"zuoxingdong/VIN_PyTorch_Visdom" -> "avivt/VIN"
"zuoxingdong/VIN_PyTorch_Visdom" -> "zuoxingdong/VIN_TensorFlow"
"zuoxingdong/VIN_PyTorch_Visdom" -> "hi-abhi/tensorflow-value-iteration-networks"
"zuoxingdong/VIN_PyTorch_Visdom" -> "junhyukoh/value-prediction-network"
"zuoxingdong/VIN_PyTorch_Visdom" -> "jingweiz/pytorch-dnc" ["e"=1]
"florensacc/snn4hrl" -> "kpaonaut/HAAR-A-Hierarchical-RL-Algorithm"
"clbaker/BToM" -> "julianje/Bishop"
"clbaker/BToM" -> "stacyste/TheoryOfMindInferenceModels"
"openai/retask" -> "openai/post--example"
"openai/go-alias" -> "openai/ceph-chef"
"collaborative-mapush/MAPush" -> "ziyanx02/multiagent-quadruped-environment"
"wty-yy/kaiwu2024_taichu" -> "Unakar/AI_Game_KingGlory"
"knakamura13/mlrose-ky" -> "knakamura13/mlrose-torch"
"knakamura13/mlrose-ky" -> "jlm429/pyperch"
"thu-uav/Multi-UAV-pursuit-evasion" -> "reinshift/MADDPG_Multi_UAV_Roundup"
"RuBP17/AlphaDou" -> "EdwardPooh/douzero-resnet-2.0"
"RuBP17/AlphaDou" -> "samuelgzx/RL_GuanDan"
"RuBP17/AlphaDou" -> "Vincentzyx/Douzero_Resnet"
"RuBP17/AlphaDou" -> "submit-paper/Doudizhu_plus"
"onlytailei/A3C-PyTorch" -> "onlytailei/pytorch-rl"
"onlytailei/A3C-PyTorch" -> "onlytailei/Value-Iteration-Networks-PyTorch"
"Quuxplusone/Hanabi" -> "WuTheFWasThat/hanabi.rs"
"Quuxplusone/Hanabi" -> "rjtobin/HanSim"
"CLSFramework/py2d" -> "Cyrus2D/SoccerSimulationProxy"
"onlytailei/Value-Iteration-Networks-PyTorch" -> "onlytailei/pytorch-rl"
"rcsoccersim/rcsslogplayer" -> "rcsoccersim/rcssmonitor"
"openai/baselines" -> "openai/gym"
"openai/baselines" -> "openai/spinningup"
"openai/baselines" -> "dennybritz/reinforcement-learning"
"openai/baselines" -> "hill-a/stable-baselines"
"openai/baselines" -> "DLR-RM/stable-baselines3"
"openai/baselines" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"openai/baselines" -> "google/dopamine"
"openai/baselines" -> "MorvanZhou/Reinforcement-learning-with-tensorflow"
"openai/baselines" -> "rll/rllab"
"openai/baselines" -> "ikostrikov/pytorch-a2c-ppo-acktr-gail"
"openai/baselines" -> "aikorea/awesome-rl"
"openai/baselines" -> "thu-ml/tianshou"
"openai/baselines" -> "ray-project/ray" ["e"=1]
"openai/baselines" -> "keras-rl/keras-rl"
"openai/baselines" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "dennybritz/reinforcement-learning"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "openai/baselines"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "datawhalechina/easy-rl"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "thu-ml/tianshou"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "openai/spinningup"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "aikorea/awesome-rl"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "openai/gym"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "princewen/tensorflow_practice" ["e"=1]
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "rlcode/reinforcement-learning"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "keras-rl/keras-rl"
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "MorvanZhou/Tensorflow-Tutorial" ["e"=1]
"MorvanZhou/Reinforcement-learning-with-tensorflow" -> "NeuronDance/DeepRL"
"dgriff777/rl_a3c_pytorch" -> "dgriff777/a3c_continuous"
"dgriff777/rl_a3c_pytorch" -> "ikostrikov/pytorch-a3c"
"dgriff777/rl_a3c_pytorch" -> "NVlabs/GA3C"
"dgriff777/rl_a3c_pytorch" -> "jingweiz/pytorch-rl"
"dgriff777/rl_a3c_pytorch" -> "ikostrikov/pytorch-ddpg-naf"
"dgriff777/rl_a3c_pytorch" -> "miyosuda/async_deep_reinforce"
"dgriff777/rl_a3c_pytorch" -> "Kaixhin/ACER"
"dgriff777/rl_a3c_pytorch" -> "jingweiz/pytorch-dnc" ["e"=1]
"dgriff777/rl_a3c_pytorch" -> "greydanus/baby-a3c"
"dgriff777/rl_a3c_pytorch" -> "pathak22/noreward-rl"
"dgriff777/rl_a3c_pytorch" -> "atgambardella/pytorch-es" ["e"=1]
"dgriff777/rl_a3c_pytorch" -> "onlytailei/A3C-PyTorch"
"dgriff777/rl_a3c_pytorch" -> "miyosuda/unreal"
"dgriff777/rl_a3c_pytorch" -> "joschu/modular_rl"
"dgriff777/rl_a3c_pytorch" -> "google-deepmind/scalable_agent"
"gxnk/reinforcement-learning-code" -> "zhuliquan/reinforcement_learning_basic_book"
"gxnk/reinforcement-learning-code" -> "qqiang00/Reinforce"
"gxnk/reinforcement-learning-code" -> "algorithmdog/Reinforcement_Learning_Blog"
"gxnk/reinforcement-learning-code" -> "GAOYANGAU/DRLPytorch"
"gxnk/reinforcement-learning-code" -> "eyounx/VirtualTaobao" ["e"=1]
"gxnk/reinforcement-learning-code" -> "StepNeverStop/RLs"
"gxnk/reinforcement-learning-code" -> "NovemberChopin/RL_Tutorial"
"gxnk/reinforcement-learning-code" -> "Teacher-Guo/RL_code"
"facebookresearch/ReAgent" -> "google-deepmind/trfl"
"facebookresearch/ReAgent" -> "google/dopamine"
"facebookresearch/ReAgent" -> "astooke/rlpyt"
"facebookresearch/ReAgent" -> "IntelLabs/coach"
"facebookresearch/ReAgent" -> "rll/rllab"
"facebookresearch/ReAgent" -> "rlworkgroup/garage"
"facebookresearch/ReAgent" -> "tensorforce/tensorforce"
"facebookresearch/ReAgent" -> "tensorflow/agents"
"facebookresearch/ReAgent" -> "hill-a/stable-baselines"
"facebookresearch/ReAgent" -> "rail-berkeley/rlkit"
"facebookresearch/ReAgent" -> "ShangtongZhang/DeepRL"
"facebookresearch/ReAgent" -> "openai/baselines"
"facebookresearch/ReAgent" -> "google-deepmind/acme"
"facebookresearch/ReAgent" -> "openai/spinningup"
"facebookresearch/ReAgent" -> "facebookresearch/pytext" ["e"=1]
"openai/roboschool" -> "rll/rllab"
"openai/roboschool" -> "openai/mujoco-py"
"openai/roboschool" -> "google-deepmind/dm_control"
"openai/roboschool" -> "joschu/modular_rl"
"openai/roboschool" -> "benelot/pybullet-gym"
"openai/roboschool" -> "openai/baselines"
"openai/roboschool" -> "erlerobot/gym-gazebo"
"openai/roboschool" -> "openai/universe-starter-agent"
"openai/roboschool" -> "tensorforce/tensorforce"
"openai/roboschool" -> "IntelLabs/coach"
"openai/roboschool" -> "facebookresearch/ELF"
"openai/roboschool" -> "google-deepmind/trfl"
"openai/roboschool" -> "openai/retro"
"openai/roboschool" -> "openai/imitation"
"openai/roboschool" -> "google-deepmind/lab"
"yrlu/irl-imitation" -> "MatthewJA/Inverse-Reinforcement-Learning"
"yrlu/irl-imitation" -> "reinforcement-learning-kr/lets-do-irl"
"yrlu/irl-imitation" -> "jangirrishabh/toyCarIRL"
"yrlu/irl-imitation" -> "justinjfu/inverse_rl"
"yrlu/irl-imitation" -> "qzed/irl-maxent"
"yrlu/irl-imitation" -> "ermongroup/MA-AIRL"
"yrlu/irl-imitation" -> "andrewliao11/gail-tf"
"yrlu/irl-imitation" -> "sjchoi86/irl_rocks"
"yrlu/irl-imitation" -> "ahq1993/inverse_rl"
"yrlu/irl-imitation" -> "neka-nat/inv_rl"
"yrlu/irl-imitation" -> "Kaixhin/imitation-learning"
"yrlu/irl-imitation" -> "openai/imitation"
"yrlu/irl-imitation" -> "toshikwa/gail-airl-ppo.pytorch"
"yrlu/irl-imitation" -> "yfzhang/vehicle-motion-forecasting"
"yrlu/irl-imitation" -> "HumanCompatibleAI/imitation"
"facebookresearch/ELF" -> "pytorch/ELF" ["e"=1]
"facebookresearch/ELF" -> "TorchCraft/TorchCraft" ["e"=1]
"facebookresearch/ELF" -> "rll/rllab"
"facebookresearch/ELF" -> "google-research/batch-ppo"
"facebookresearch/ELF" -> "facebookresearch/darkforestGo" ["e"=1]
"facebookresearch/ELF" -> "openai/universe-starter-agent"
"facebookresearch/ELF" -> "tensorforce/tensorforce"
"facebookresearch/ELF" -> "williamFalcon/DeepRLHacks"
"facebookresearch/ELF" -> "pathak22/noreward-rl"
"facebookresearch/ELF" -> "alibaba/gym-starcraft" ["e"=1]
"facebookresearch/ELF" -> "openai/roboschool"
"facebookresearch/ELF" -> "google-deepmind/lab"
"facebookresearch/ELF" -> "google-deepmind/dnc" ["e"=1]
"facebookresearch/ELF" -> "google-deepmind/trfl"
"facebookresearch/ELF" -> "geek-ai/MAgent"
"qqiang00/Reinforce" -> "NeuronDance/DeepRL"
"qqiang00/Reinforce" -> "zhangchuheng123/Reinforcement-Implementation"
"qqiang00/Reinforce" -> "wwxFromTju/awesome-reinforcement-learning-zh"
"qqiang00/Reinforce" -> "ucla-rlcourse/RLexample"
"qqiang00/Reinforce" -> "zhoubolei/introRL"
"qqiang00/Reinforce" -> "AI4Finance-Foundation/ElegantRL"
"qqiang00/Reinforce" -> "ZhiqingXiao/rl-book"
"qqiang00/Reinforce" -> "starry-sky6688/MARL-Algorithms"
"qqiang00/Reinforce" -> "gxnk/reinforcement-learning-code"
"qqiang00/Reinforce" -> "sweetice/Deep-reinforcement-learning-with-pytorch"
"qqiang00/Reinforce" -> "PacktPublishing/Hands-On-Reinforcement-Learning-with-Python"
"qqiang00/Reinforce" -> "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"
"qqiang00/Reinforce" -> "tigerneil/awesome-deep-rl"
"qqiang00/Reinforce" -> "ShangtongZhang/reinforcement-learning-an-introduction"
"qqiang00/Reinforce" -> "louisnino/RLcode"
"pathak22/noreward-rl" -> "openai/large-scale-curiosity"
"pathak22/noreward-rl" -> "openai/random-network-distillation"
"pathak22/noreward-rl" -> "rll/rllab"
"pathak22/noreward-rl" -> "ikostrikov/pytorch-a3c"
"pathak22/noreward-rl" -> "openai/universe-starter-agent"
"pathak22/noreward-rl" -> "miyosuda/unreal"
"pathak22/noreward-rl" -> "google-deepmind/scalable_agent"
"pathak22/noreward-rl" -> "miyosuda/async_deep_reinforce"
"pathak22/noreward-rl" -> "google-research/batch-ppo"
"pathak22/noreward-rl" -> "NVlabs/GA3C"
"pathak22/noreward-rl" -> "joschu/modular_rl"
"pathak22/noreward-rl" -> "rail-berkeley/softlearning"
"pathak22/noreward-rl" -> "Farama-Foundation/ViZDoom"
"pathak22/noreward-rl" -> "williamFalcon/DeepRLHacks"
"pathak22/noreward-rl" -> "openai/vime"
"ssusnic/Machine-Learning-Flappy-Bird" -> "Code-Bullet/PacNeat"
"ssusnic/Machine-Learning-Flappy-Bird" -> "ArztSamuel/Applying_EANNs"
"ssusnic/Machine-Learning-Flappy-Bird" -> "Code-Bullet/PacmanGame"
"ssusnic/Machine-Learning-Flappy-Bird" -> "cazala/synaptic" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "kaishengtai/neuralart" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "ivanseidel/IAMDinosaur" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "xviniette/FlappyLearning" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "yenchenlin/DeepLearningFlappyBird"
"ssusnic/Machine-Learning-Flappy-Bird" -> "channingbreeze/games" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "nicholas-ochoa/OpenSC2K" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "wb14123/seq2seq-couplet" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "Hopson97/MineCraft-One-Week-Challenge" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "SarvagyaVaish/FlappyBirdRL"
"ssusnic/Machine-Learning-Flappy-Bird" -> "minimaxir/person-blocker" ["e"=1]
"ssusnic/Machine-Learning-Flappy-Bird" -> "MorvanZhou/Evolutionary-Algorithm" ["e"=1]
"xuehy/pytorch-maddpg" -> "shariqiqbal2810/maddpg-pytorch"
"xuehy/pytorch-maddpg" -> "sisl/MADRL"
"xuehy/pytorch-maddpg" -> "starry-sky6688/MADDPG"
"xuehy/pytorch-maddpg" -> "openai/maddpg"
"xuehy/pytorch-maddpg" -> "shariqiqbal2810/MAAC"
"xuehy/pytorch-maddpg" -> "DKuan/MADDPG_torch"
"xuehy/pytorch-maddpg" -> "Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment"
"xuehy/pytorch-maddpg" -> "philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients"
"xuehy/pytorch-maddpg" -> "starry-sky6688/MARL-Algorithms"
"xuehy/pytorch-maddpg" -> "ChenglongChen/pytorch-DRL"
"xuehy/pytorch-maddpg" -> "minqi/learning-to-communicate-pytorch"
"xuehy/pytorch-maddpg" -> "oxwhirl/pymarl"
"xuehy/pytorch-maddpg" -> "Lizhi-sjtu/MARL-code-pytorch"
"xuehy/pytorch-maddpg" -> "openai/multiagent-particle-envs"
"xuehy/pytorch-maddpg" -> "marlbenchmark/on-policy"
"zhozhou/DouDizhuAI" -> "An0nym6/LandlordsEndgame"
"enhuiz/flappybird-ql" -> "SarvagyaVaish/FlappyBirdRL"
"ikostrikov/pytorch-trpo" -> "joschu/modular_rl"
"ikostrikov/pytorch-trpo" -> "mjacar/pytorch-trpo"
"ikostrikov/pytorch-trpo" -> "Khrylx/PyTorch-RL"
"ikostrikov/pytorch-trpo" -> "ikostrikov/pytorch-a3c"
"ikostrikov/pytorch-trpo" -> "ikostrikov/pytorch-ddpg-naf"
"ikostrikov/pytorch-trpo" -> "zuoxingdong/VIN_PyTorch_Visdom"
"ikostrikov/pytorch-trpo" -> "tristandeleu/pytorch-maml-rl" ["e"=1]
"ikostrikov/pytorch-trpo" -> "pat-coady/trpo"
"ikostrikov/pytorch-trpo" -> "pranz24/pytorch-soft-actor-critic"
"ikostrikov/pytorch-trpo" -> "Kaixhin/ACER"
"ikostrikov/pytorch-trpo" -> "wojzaremba/trpo"
"ikostrikov/pytorch-trpo" -> "kimhc6028/pytorch-noreward-rl"
"ikostrikov/pytorch-trpo" -> "reinforcement-learning-kr/pg_travel"
"ikostrikov/pytorch-trpo" -> "TianhongDai/reinforcement-learning-algorithms"
"ikostrikov/pytorch-trpo" -> "sfujim/BCQ" ["e"=1]
"davidhershey/feudal_networks" -> "dnddnjs/feudal-montezuma"
"davidhershey/feudal_networks" -> "tdavchev/option-critic"
"davidhershey/feudal_networks" -> "jeanharb/option_critic"
"davidhershey/feudal_networks" -> "4rChon/NL-FuN"
"davidhershey/feudal_networks" -> "lweitkamp/feudalnets-pytorch"
"davidhershey/feudal_networks" -> "junhyukoh/value-prediction-network"
"davidhershey/feudal_networks" -> "Nat-D/FeatureControlHRL"
"davidhershey/feudal_networks" -> "mrkulk/hierarchical-deep-RL"
"davidhershey/feudal_networks" -> "hoangminhle/hierarchical_IL_RL"
"happy-yan/DACER-Diffusion-with-Online-RL" -> "wadx2019/qvpo"
"happy-yan/DACER-Diffusion-with-Online-RL" -> "Alescontrela/score_matching_rl"
"happy-yan/DACER-Diffusion-with-Online-RL" -> "TobiasLv/RAD"
"happy-yan/DACER-Diffusion-with-Online-RL" -> "BellmanTimeHut/DIPO"
"haarnoja/softqlearning" -> "rail-berkeley/softlearning"
"haarnoja/softqlearning" -> "haarnoja/sac"
"haarnoja/softqlearning" -> "justinjfu/inverse_rl"
"haarnoja/softqlearning" -> "rlbayes/rllabplusplus"
"haarnoja/softqlearning" -> "junhyukoh/value-prediction-network"
"haarnoja/softqlearning" -> "jannerm/mbpo" ["e"=1]
"haarnoja/softqlearning" -> "rll/rllab"
"haarnoja/softqlearning" -> "joschu/modular_rl"
"haarnoja/softqlearning" -> "openai/vime"
"haarnoja/softqlearning" -> "cbfinn/gps"
"haarnoja/softqlearning" -> "vitchyr/multiworld"
"haarnoja/softqlearning" -> "junhyukoh/self-imitation-learning"
"haarnoja/softqlearning" -> "aviralkumar2907/BEAR" ["e"=1]
"haarnoja/softqlearning" -> "openai/robosumo"
"haarnoja/softqlearning" -> "sfujim/BCQ" ["e"=1]
"justinjfu/inverse_rl" -> "ahq1993/inverse_rl"
"justinjfu/inverse_rl" -> "yrlu/irl-imitation"
"justinjfu/inverse_rl" -> "ermongroup/MA-AIRL"
"justinjfu/inverse_rl" -> "MatthewJA/Inverse-Reinforcement-Learning"
"justinjfu/inverse_rl" -> "toshikwa/gail-airl-ppo.pytorch"
"justinjfu/inverse_rl" -> "ermongroup/MetaIRL"
"justinjfu/inverse_rl" -> "reinforcement-learning-kr/lets-do-irl"
"justinjfu/inverse_rl" -> "andrewliao11/gail-tf"
"justinjfu/inverse_rl" -> "vvanirudh/IRL-Toolkit"
"justinjfu/inverse_rl" -> "HuangJiaLian/AIRL_MountainCar"
"justinjfu/inverse_rl" -> "openai/imitation"
"justinjfu/inverse_rl" -> "cbfinn/gps"
"justinjfu/inverse_rl" -> "KamyarGh/rl_swiss"
"justinjfu/inverse_rl" -> "jangirrishabh/toyCarIRL"
"justinjfu/inverse_rl" -> "haarnoja/softqlearning"
"Kaixhin/NoisyNet-A3C" -> "andrewliao11/NoisyNet-DQN"
"Kaixhin/NoisyNet-A3C" -> "floringogianu/categorical-dqn"
"nottombrown/rl-teacher" -> "mrahtz/learning-from-human-preferences"
"nottombrown/rl-teacher" -> "machine-intelligence/rl-teacher-atari"
"nottombrown/rl-teacher" -> "pathak22/zeroshot-imitation"
"nottombrown/rl-teacher" -> "ZachisGit/LearningFromHumanPreferences"
"nottombrown/rl-teacher" -> "google-research/batch-ppo"
"nottombrown/rl-teacher" -> "rlbayes/rllabplusplus"
"nottombrown/rl-teacher" -> "williamFalcon/DeepRLHacks"
"nottombrown/rl-teacher" -> "junhyukoh/self-imitation-learning"
"nottombrown/rl-teacher" -> "openai/evolution-strategies-starter"
"nottombrown/rl-teacher" -> "miyosuda/unreal"
"nottombrown/rl-teacher" -> "sohutv/hotcaffeine" ["e"=1]
"nottombrown/rl-teacher" -> "joschu/modular_rl"
"nottombrown/rl-teacher" -> "stanfordnmbl/osim-rl" ["e"=1]
"nottombrown/rl-teacher" -> "pathak22/noreward-rl"
"nottombrown/rl-teacher" -> "openai/imitation"
"Breakend/gym-extensions" -> "rlbayes/rllabplusplus"
"Breakend/gym-extensions" -> "DartEnv/dart-env"
"Breakend/gym-extensions" -> "junhyukoh/value-prediction-network"
"floringogianu/categorical-dqn" -> "junhyukoh/icml2016-minecraft"
"alexis-jacq/Pytorch-DPPO" -> "TianhongDai/distributed-ppo"
"alexis-jacq/Pytorch-DPPO" -> "Kaixhin/ACER"
"Kaixhin/ACER" -> "alexis-jacq/Pytorch-DPPO"
"Kaixhin/ACER" -> "dchetelat/acer"
"Kaixhin/ACER" -> "mjacar/pytorch-trpo"
"Kaixhin/ACER" -> "Damcy/prioritized-experience-replay"
"Kaixhin/ACER" -> "jingweiz/pytorch-rl"
"Kaixhin/ACER" -> "steveKapturowski/tensorflow-rl"
"tdavchev/option-critic" -> "jeanharb/option_critic"
"tdavchev/option-critic" -> "mklissa/PPOC"
"tdavchev/option-critic" -> "jeanharb/a2oc_delib"
"tdavchev/option-critic" -> "kkhetarpal/ioc"
"tdavchev/option-critic" -> "davidhershey/feudal_networks"
"mjacar/pytorch-trpo" -> "ikostrikov/pytorch-trpo"
"pat-coady/trpo" -> "joschu/modular_rl"
"pat-coady/trpo" -> "wojzaremba/trpo"
"pat-coady/trpo" -> "uidilr/gail_ppo_tf"
"pat-coady/trpo" -> "stevenpjg/ddpg-aigym"
"pat-coady/trpo" -> "pemami4911/deep-rl"
"pat-coady/trpo" -> "ikostrikov/pytorch-trpo"
"pat-coady/trpo" -> "rlbayes/rllabplusplus"
"pat-coady/trpo" -> "DartEnv/dart-env"
"pat-coady/trpo" -> "NVlabs/GA3C"
"pat-coady/trpo" -> "openai/imitation"
"pat-coady/trpo" -> "openai/vime"
"pat-coady/trpo" -> "rmst/ddpg"
"pat-coady/trpo" -> "google-research/batch-ppo"
"pat-coady/trpo" -> "miyosuda/async_deep_reinforce"
"pat-coady/trpo" -> "muupan/async-rl"
"chen0040/java-reinforcement-learning" -> "chen0040/java-reinforcement-learning-flappy-bird"
"chen0040/java-reinforcement-learning" -> "technobium/q-learning-java"
"thuxugang/doudizhu" -> "onestraw/doudizhu"
"thuxugang/doudizhu" -> "zhozhou/DouDizhuAI"
"thuxugang/doudizhu" -> "ZhouWeikuan/DouDiZhu"
"thuxugang/doudizhu" -> "songbaoming/DouDiZhu"
"thuxugang/doudizhu" -> "qq456cvb/doudizhu-C"
"go2sea/C51DQN" -> "Kiwoo/distributional_perspective_on_RL"
"Kiwoo/distributional_perspective_on_RL" -> "go2sea/C51DQN"
"maniaplanet/documentation" -> "maniaplanet/game-modes"
"maniaplanet/game-modes" -> "domino54/title-packs"
"vwxyzjn/cleanrl" ["l"="57.772,18.075"]
"DLR-RM/stable-baselines3" ["l"="57.748,18.051"]
"Farama-Foundation/Gymnasium" ["l"="57.801,18.024"]
"thu-ml/tianshou" ["l"="57.751,18.003"]
"tinkoff-ai/CORL" ["l"="59.433,17.524"]
"Farama-Foundation/PettingZoo" ["l"="57.813,18.218"]
"pytorch/rl" ["l"="57.833,18.103"]
"DLR-RM/rl-baselines3-zoo" ["l"="57.73,18.122"]
"openai/spinningup" ["l"="57.669,17.989"]
"AI4Finance-Foundation/ElegantRL" ["l"="57.809,18.061"]
"p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" ["l"="57.694,18.032"]
"google/brax" ["l"="61.202,16.367"]
"isaac-sim/IsaacGymEnvs" ["l"="61.158,16.434"]
"google-deepmind/acme" ["l"="57.66,18.169"]
"openai/baselines" ["l"="57.585,18.024"]
"Farama-Foundation/D4RL" ["l"="59.425,17.567"]
"alex-petrenko/sample-factory" ["l"="57.604,18.356"]
"sail-sg/envpool" ["l"="59.282,17.489"]
"google-research/rliable" ["l"="59.348,17.497"]
"RobertTLange/gymnax" ["l"="59.311,17.433"]
"google-research/seed_rl" ["l"="57.622,18.311"]
"facebookresearch/torchbeast" ["l"="57.599,18.372"]
"MichaelTMatthews/Craftax" ["l"="59.327,17.422"]
"alex-petrenko/megaverse" ["l"="57.622,18.584"]
"YeWR/EfficientZero" ["l"="59.279,17.526"]
"luchris429/purejaxrl" ["l"="59.323,17.448"]
"instadeepai/jumanji" ["l"="59.287,17.422"]
"astooke/rlpyt" ["l"="57.597,18.191"]
"Farama-Foundation/Miniworld" ["l"="57.538,18.321"]
"openai/procgen" ["l"="57.537,18.351"]
"Code-Bullet/SnakeFusion" ["l"="56.864,17.847"]
"Code-Bullet/Pool_AI" ["l"="56.842,17.823"]
"Code-Bullet/Enigma-Simulator" ["l"="56.847,17.804"]
"Code-Bullet/AsteroidsAI" ["l"="56.827,17.844"]
"Code-Bullet/Smart-Dots-Genetic-Algorithm-Tutorial" ["l"="56.83,17.809"]
"Code-Bullet/Google-Chrome-Dino-Game-AI" ["l"="56.865,17.824"]
"Code-Bullet/PacNeat" ["l"="56.838,17.834"]
"Code-Bullet/WorldsHardestGameAI" ["l"="56.828,17.821"]
"Code-Bullet/Chess-AI" ["l"="56.817,17.835"]
"Code-Bullet/PacmanGame" ["l"="56.844,17.846"]
"Code-Bullet/RubiksCubeAI" ["l"="56.818,17.798"]
"Code-Bullet/minesweeper-AI" ["l"="56.821,17.825"]
"Code-Bullet/Asteroids-with-NEAT" ["l"="56.83,17.832"]
"Code-Bullet/NEAT_Template" ["l"="56.815,17.844"]
"Code-Bullet/2048-AI" ["l"="56.811,17.828"]
"Code-Bullet/Handy-Terminal-Position-Designer-Matrix-Thing" ["l"="56.817,17.821"]
"google/dopamine" ["l"="57.536,17.993"]
"google-deepmind/trfl" ["l"="57.506,18.106"]
"dennybritz/reinforcement-learning" ["l"="57.548,17.917"]
"facebookresearch/ReAgent" ["l"="57.555,18.092"]
"keras-rl/keras-rl" ["l"="57.489,18.005"]
"tensorforce/tensorforce" ["l"="57.5,18.059"]
"hill-a/stable-baselines" ["l"="57.632,18.09"]
"ShangtongZhang/reinforcement-learning-an-introduction" ["l"="57.625,17.931"]
"tensorflow/agents" ["l"="57.578,18.075"]
"rll/rllab" ["l"="57.483,18.133"]
"openai/gym" ["l"="57.598,17.87"]
"aikorea/awesome-rl" ["l"="57.507,17.95"]
"google-deepmind/lab" ["l"="57.451,18.025"]
"google-deepmind/sonnet" ["l"="57.431,17.873"]
"openai/multiagent-particle-envs" ["l"="57.784,18.247"]
"openai/maddpg" ["l"="57.808,18.247"]
"oxwhirl/pymarl" ["l"="57.826,18.262"]
"LantaoYu/MARL-Papers" ["l"="57.768,18.172"]
"marlbenchmark/on-policy" ["l"="57.859,18.239"]
"oxwhirl/smac" ["l"="57.825,18.286"]
"starry-sky6688/MARL-Algorithms" ["l"="57.839,18.215"]
"geek-ai/MAgent" ["l"="57.761,18.277"]
"xuehy/pytorch-maddpg" ["l"="57.785,18.268"]
"shariqiqbal2810/maddpg-pytorch" ["l"="57.846,18.261"]
"shariqiqbal2810/MAAC" ["l"="57.801,18.266"]
"Bigpig4396/Multi-Agent-Reinforcement-Learning-Environment" ["l"="57.808,18.281"]
"sisl/MADRL" ["l"="57.788,18.286"]
"starry-sky6688/MADDPG" ["l"="57.825,18.238"]
"openai/multi-agent-emergence-environments" ["l"="57.716,18.265"]
"google-deepmind/ai-safety-gridworlds" ["l"="57.55,18.296"]
"google-deepmind/pycolab" ["l"="57.517,18.305"]
"openai/safety-gym" ["l"="61.156,15.019"]
"openai/safety-starter-agents" ["l"="61.136,15.02"]
"Farama-Foundation/Minigrid" ["l"="57.587,18.272"]
"befelix/safe_learning" ["l"="61.206,15.041"]
"google-deepmind/bsuite" ["l"="57.591,18.253"]
"PKU-Alignment/safety-gymnasium" ["l"="61.125,15.043"]
"google-deepmind/scalable_agent" ["l"="57.528,18.282"]
"kenjyoung/MinAtar" ["l"="59.33,17.479"]
"chauncygu/Safe-Reinforcement-Learning-Baselines" ["l"="61.145,15.038"]
"mila-iqia/atari-representation-learning" ["l"="59.354,17.557"]
"openai/coinrun" ["l"="57.494,18.368"]
"eleurent/rl-agents" ["l"="62.419,12.31"]
"quanvuong/handful-of-trials-pytorch" ["l"="59.351,17.671"]
"zuoxingdong/mazelab" ["l"="57.452,18.327"]
"google-research/batch-ppo" ["l"="57.422,18.192"]
"junhyukoh/value-prediction-network" ["l"="57.39,18.343"]
"openai/multiagent-competition" ["l"="57.578,18.3"]
"mpSchrader/gym-sokoban" ["l"="57.574,18.336"]
"google-deepmind/dnc" ["l"="46.152,27.91"]
"miyosuda/unreal" ["l"="57.363,18.216"]
"benelot/pybullet-gym" ["l"="57.537,18.233"]
"google-deepmind/lab2d" ["l"="57.706,18.401"]
"lcswillems/rl-starter-files" ["l"="57.551,18.336"]
"sweetice/Deep-reinforcement-learning-with-pytorch" ["l"="57.714,18.07"]
"datawhalechina/easy-rl" ["l"="57.828,17.951"]
"NeuronDance/DeepRL" ["l"="57.772,18.033"]
"opendilab/DI-engine" ["l"="51.293,2.899"]
"MorvanZhou/Reinforcement-learning-with-tensorflow" ["l"="57.663,17.947"]
"PaddlePaddle/PARL" ["l"="57.779,17.985"]
"LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions" ["l"="57.718,17.939"]
"brynhayder/reinforcement_learning_an_introduction" ["l"="57.743,17.788"]
"vojtamolda/reinforcement-learning-an-introduction" ["l"="57.752,17.813"]
"iamhectorotero/rlai-exercises" ["l"="57.727,17.778"]
"ShangtongZhang/DeepRL" ["l"="57.609,18.11"]
"openai/sonic-on-ray" ["l"="57.263,18.563"]
"openai/post--example" ["l"="57.178,18.614"]
"openai/retro-contest" ["l"="57.287,18.537"]
"openai/ceph-chef" ["l"="57.174,18.644"]
"openai/retro-baselines" ["l"="57.35,18.495"]
"openai/mujoco-py" ["l"="57.582,18.111"]
"rlworkgroup/garage" ["l"="57.589,18.148"]
"openai/train-procgen" ["l"="57.504,18.453"]
"pokaxpoka/netrand" ["l"="57.493,18.436"]
"openai/random-network-distillation" ["l"="57.499,18.294"]
"vitchyr/multiworld" ["l"="57.562,18.319"]
"openai/EPG" ["l"="57.398,18.367"]
"openai/large-scale-curiosity" ["l"="57.482,18.313"]
"katerakelly/oyster" ["l"="57.647,19.503"]
"iclavera/learning_to_adapt" ["l"="57.671,19.499"]
"WilsonWangTHU/mbbl" ["l"="59.316,17.67"]
"openai/aws-fluent-plugin-kinesis" ["l"="57.12,18.668"]
"openai/retask" ["l"="57.177,18.588"]
"google-deepmind/dm_control" ["l"="57.558,18.127"]
"google-deepmind/mujoco_menagerie" ["l"="61.196,16.401"]
"google-deepmind/mujoco" ["l"="61.182,16.333"]
"ARISE-Initiative/robosuite" ["l"="59.517,16.465"]
"rail-berkeley/rlkit" ["l"="57.605,18.162"]
"Farama-Foundation/Metaworld" ["l"="59.509,16.488"]
"google-deepmind/mujoco_mpc" ["l"="61.163,16.386"]
"Khrylx/PyTorch-RL" ["l"="57.614,18.23"]
"reinforcement-learning-kr/lets-do-irl" ["l"="57.611,18.339"]
"ikostrikov/pytorch-trpo" ["l"="57.49,18.227"]
"ikostrikov/pytorch-a2c-ppo-acktr-gail" ["l"="57.65,18.107"]
"openai/imitation" ["l"="57.47,18.292"]
"MatthewJA/Inverse-Reinforcement-Learning" ["l"="57.589,18.328"]
"pranz24/pytorch-soft-actor-critic" ["l"="57.645,18.189"]
"Kaixhin/imitation-learning" ["l"="57.633,18.359"]
"quantumiracle/Popular-RL-Algorithms" ["l"="57.706,18.191"]
"yrlu/irl-imitation" ["l"="57.58,18.351"]
"jingweiz/pytorch-rl" ["l"="57.483,18.202"]
"rail-berkeley/softlearning" ["l"="57.56,18.207"]
"araffin/rl-baselines-zoo" ["l"="57.626,18.196"]
"IntelLabs/coach" ["l"="57.526,18.091"]
"stepjam/RLBench" ["l"="59.496,16.518"]
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On" ["l"="57.65,18.02"]
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" ["l"="57.677,18.076"]
"Shmuma/ptan" ["l"="57.54,18.146"]
"udacity/deep-reinforcement-learning" ["l"="57.619,17.986"]
"simoninithomas/Deep_reinforcement_learning_Course" ["l"="57.601,17.958"]
"andri27-ts/Reinforcement-Learning" ["l"="57.589,17.987"]
"PacktPublishing/Deep-Learning-with-Keras" ["l"="47.398,28.471"]
"uber-research/poet" ["l"="57.575,18.546"]
"ucl-dark/paired" ["l"="57.638,18.519"]
"yaricom/goNEAT_NS" ["l"="57.578,18.645"]
"icaros-usc/pyribs" ["l"="59.185,17.309"]
"facebookresearch/dcd" ["l"="57.618,18.511"]
"google-deepmind/open_spiel" ["l"="57.689,18.211"]
"datamllab/rlcard" ["l"="58.627,18.621"]
"werner-duvaud/muzero-general" ["l"="59.209,17.516"]
"suragnair/alpha-zero-general" ["l"="58.533,17.221"]
"google-research/football" ["l"="57.857,18.183"]
"hijkzzz/pymarl2" ["l"="57.903,18.268"]
"BazkieBumpercar/GameplayFootball" ["l"="58.094,18.083"]
"vy007vikas/PyTorch-ActorCriticRL" ["l"="57.611,18.214"]
"ghliu/pytorch-ddpg" ["l"="57.537,18.211"]
"ikostrikov/pytorch-ddpg-naf" ["l"="57.429,18.249"]
"ikostrikov/pytorch-a3c" ["l"="57.503,18.188"]
"MorvanZhou/pytorch-A3C" ["l"="57.578,18.206"]
"ChenglongChen/pytorch-DRL" ["l"="57.75,18.257"]
"dgriff777/a3c_continuous" ["l"="57.497,18.243"]
"nikhilbarhate99/Actor-Critic-PyTorch" ["l"="57.653,18.263"]
"sfujim/TD3" ["l"="57.679,18.109"]
"floodsung/DDPG" ["l"="57.38,18.179"]
"nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2" ["l"="57.9,18.123"]
"dgriff777/rl_a3c_pytorch" ["l"="57.431,18.222"]
"HumanCompatibleAI/overcooked_ai" ["l"="57.87,18.401"]
"HumanCompatibleAI/human_aware_rl" ["l"="57.866,18.465"]
"Stanford-ILIAD/PantheonRL" ["l"="57.889,18.475"]
"rosewang2008/gym-cooking" ["l"="57.857,18.491"]
"uoe-agents/epymarl" ["l"="57.893,18.29"]
"FLAIROx/JaxMARL" ["l"="59.31,17.401"]
"HumanCompatibleAI/overcooked-demo" ["l"="57.874,18.479"]
"google-deepmind/meltingpot" ["l"="57.86,18.364"]
"schroederdewitt/multiagent_mujoco" ["l"="57.934,18.323"]
"oxwhirl/smacv2" ["l"="57.918,18.317"]
"HumanCompatibleAI/imitation" ["l"="57.657,18.308"]
"shivaverma/OpenAIGym" ["l"="57.951,18.098"]
"shivaverma/Orbit" ["l"="58.032,18.079"]
"haarnoja/sac" ["l"="57.544,18.189"]
"sfujim/BCQ" ["l"="59.438,17.61"]
"sfujim/TD3_BC" ["l"="59.47,17.562"]
"wangshusen/DeepLearning" ["l"="57.91,17.912"]
"wangshusen/DRL" ["l"="57.862,17.994"]
"wangshusen/RecommenderSystem" ["l"="58.334,23.42"]
"boyu-ai/Hands-on-RL" ["l"="57.884,18.028"]
"zhoubolei/introRL" ["l"="57.788,17.948"]
"mli/paper-reading" ["l"="50.604,29.465"]
"DA-southampton/NLP_ability" ["l"="53.492,27.077"]
"wangshusen/AdvancedAlgorithms" ["l"="57.971,17.848"]
"youngfish42/Awesome-FL" ["l"="51.559,2.056"]
"innovation-cat/Awesome-Federated-Machine-Learning" ["l"="51.593,2.045"]
"dair-ai/ml-visuals" ["l"="50.669,29.5"]
"datawhalechina/fun-rec" ["l"="58.302,23.426"]
"chaoyanghe/Awesome-Federated-Learning" ["l"="51.546,2.009"]
"takuseno/ppo" ["l"="57.421,18.455"]
"uidilr/ppo_tf" ["l"="57.411,18.499"]
"shareeff/PPO" ["l"="57.397,18.502"]
"rlcode/reinforcement-learning" ["l"="57.516,18.02"]
"yandexdataschool/Practical_RL" ["l"="57.56,17.956"]
"philtabor/Multi-Agent-Deep-Deterministic-Policy-Gradients" ["l"="57.86,18.224"]
"Lizhi-sjtu/MARL-code-pytorch" ["l"="57.89,18.215"]
"jangirrishabh/toyCarIRL" ["l"="57.574,18.367"]
"qzed/irl-maxent" ["l"="57.649,18.355"]
"justinjfu/inverse_rl" ["l"="57.554,18.36"]
"kristery/Awesome-Imitation-Learning" ["l"="57.601,18.398"]
"toshikwa/gail-airl-ppo.pytorch" ["l"="57.615,18.405"]
"ermongroup/MA-AIRL" ["l"="57.629,18.379"]
"MCZhi/Driving-IRL-NGSIM" ["l"="63.405,12.474"]
"ahq1993/inverse_rl" ["l"="57.57,18.397"]
"andrewliao11/gail-tf" ["l"="57.509,18.366"]
"seolhokim/InverseRL-Pytorch" ["l"="57.595,18.428"]
"higgsfield-ai/higgsfield" ["l"="57.619,18.064"]
"higgsfield/RL-Adventure" ["l"="57.606,18.087"]
"seungeunrho/minimalRL" ["l"="57.68,18.133"]
"Kautenja/nes-py" ["l"="57.519,18.406"]
"Kautenja/gym-super-mario-bros" ["l"="57.455,18.259"]
"PyAndy/Py3NES" ["l"="57.425,18.601"]
"nikhilbarhate99/PPO-PyTorch" ["l"="57.752,18.114"]
"ericyangyu/PPO-for-Beginners" ["l"="57.743,18.158"]
"Lizhi-sjtu/DRL-code-pytorch" ["l"="57.846,18.132"]
"vwxyzjn/ppo-implementation-details" ["l"="57.813,18.146"]
"opendilab/PPOxFamily" ["l"="51.279,2.869"]
"Replicable-MARL/MARLlib" ["l"="57.886,18.266"]
"marlbenchmark/off-policy" ["l"="57.889,18.246"]
"DeepReinforcementLearning/DeepReinforcementLearningInAction" ["l"="57.701,18.105"]
"mimoralea/gdrl" ["l"="57.677,18.056"]
"kengz/SLM-Lab" ["l"="57.585,18.172"]
"qfettes/DeepRL-Tutorials" ["l"="57.639,18.162"]
"kengz/awesome-deep-rl" ["l"="57.686,18.253"]
"PacktPublishing/Hands-On-Reinforcement-Learning-with-Python" ["l"="57.713,18.009"]
"Rafael1s/Deep-Reinforcement-Learning-Algorithms" ["l"="57.704,18.146"]
"sudharsan13296/Deep-Reinforcement-Learning-With-Python" ["l"="57.686,18.165"]
"ZhiqingXiao/rl-book" ["l"="57.828,18.03"]
"maxpumperla/deep_learning_and_the_game_of_go" ["l"="58.479,17.299"]
"Curt-Park/rainbow-is-all-you-need" ["l"="57.655,18.136"]
"kathgironpe/awesome-omscs" ["l"="57.16,17.443"]
"pyjarrett/OMSCS_Survival_Guide" ["l"="57.187,17.459"]
"udacity/deep-learning-v2-pytorch" ["l"="47.526,28.476"]
"udacity/deep-learning" ["l"="47.61,28.586"]
"greerviau/SnakeAI" ["l"="57.076,17.94"]
"ArztSamuel/Applying_EANNs" ["l"="57.116,17.909"]
"Chrispresso/SnakeAI" ["l"="57.001,17.974"]
"greerviau/TetrisAI" ["l"="57.154,17.867"]
"chuyangliu/snake" ["l"="48.36,23.671"]
"techwithtim/NEAT-Flappy-Bird" ["l"="59.946,34.328"]
"maurock/snake-ga" ["l"="45.243,19.947"]
"patrickloeber/snake-ai-pytorch" ["l"="38.931,1.834"]
"linyiLYi/snake-ai" ["l"="38.819,1.791"]
"greerviau/DoodleJumpAI" ["l"="57.025,17.928"]
"ssusnic/Machine-Learning-Flappy-Bird" ["l"="57.051,17.888"]
"streamlit/demo-self-driving" ["l"="42.555,-2.701"]
"Chrispresso/SuperMarioBros-AI" ["l"="57.049,18.018"]
"mila-iqia/babyai" ["l"="57.531,18.378"]
"clvrai/awesome-rl-envs" ["l"="57.633,18.269"]
"NVlabs/GA3C" ["l"="57.375,18.196"]
"beyretb/AnimalAI-Olympics" ["l"="57.551,18.378"]
"mdcrosby/animal-ai" ["l"="57.519,18.526"]
"Unity-Technologies/obstacle-tower-env" ["l"="57.459,18.456"]
"seungjaeryanlee/awesome-rl-competitions" ["l"="57.594,18.311"]
"junhyukoh/self-imitation-learning" ["l"="57.416,18.37"]
"kandouss/marlgrid" ["l"="57.73,18.421"]
"MultiAgentLearning/playground" ["l"="57.707,18.34"]
"uber-research/go-explore" ["l"="57.507,18.351"]
"unixpickle/anyrl-py" ["l"="57.468,18.43"]
"MushroomRL/mushroom-rl" ["l"="57.614,18.255"]
"lexfridman/deeptraffic" ["l"="57.438,17.935"]
"lexfridman/mit-deep-learning" ["l"="47.747,28.505"]
"lexfridman/deeptesla" ["l"="53.603,30.601"]
"yenchenlin/DeepLearningFlappyBird" ["l"="57.339,17.946"]
"udacity/self-driving-car-sim" ["l"="61.585,12.485"]
"lengstrom/fast-style-transfer" ["l"="45.585,29.275"]
"udacity/self-driving-car" ["l"="61.599,12.448"]
"ndrplz/self-driving-car" ["l"="61.595,12.517"]
"Farama-Foundation/HighwayEnv" ["l"="62.369,12.278"]
"lgsvl/simulator" ["l"="61.677,12.459"]
"philtabor/Youtube-Code-Repository" ["l"="57.727,18.172"]
"philtabor/Deep-Q-Learning-Paper-To-Code" ["l"="57.761,18.237"]
"philtabor/Actor-Critic-Methods-Paper-To-Code" ["l"="57.757,18.216"]
"pythonlessons/Reinforcement_Learning" ["l"="57.772,18.106"]
"araffin/rl-tutorial-jnrr19" ["l"="57.723,18.15"]
"archsyscall/DeepRL-TensorFlow2" ["l"="57.747,18.144"]
"wwxFromTju/awesome-reinforcement-learning-zh" ["l"="57.76,17.964"]
"applenob/rl_learn" ["l"="57.846,17.861"]
"princewen/tensorflow_practice" ["l"="58.188,23.357"]
"qqiang00/Reinforce" ["l"="57.779,18.01"]
"ucla-rlcourse/RLexample" ["l"="57.812,17.994"]
"tigerneil/awesome-deep-rl" ["l"="57.754,18.187"]
"zhangchuheng123/Reinforcement-Implementation" ["l"="57.809,18.111"]
"rlgraph/rlgraph" ["l"="57.67,18.367"]
"PKU-RL/DGN" ["l"="57.816,18.348"]
"ray-project/rl-experiments" ["l"="57.724,18.394"]
"SurrealAI/surreal" ["l"="59.636,16.405"]
"Officium/RL-Experiments" ["l"="57.703,18.45"]
"medipixel/rl_algorithms" ["l"="57.604,18.29"]
"Unity-Technologies/obstacle-tower-challenge" ["l"="57.428,18.538"]
"Unity-Technologies/obstacle-tower-source" ["l"="57.419,18.558"]
"unixpickle/obs-tower2" ["l"="57.407,18.58"]
"uber-research/ape-x" ["l"="57.499,18.416"]
"Unity-Technologies/marathon-envs" ["l"="57.322,18.788"]
"google-research/planet" ["l"="59.292,17.671"]
"pathak22/noreward-rl" ["l"="57.445,18.234"]
"Kaixhin/PlaNet" ["l"="59.304,17.647"]
"navneet-nmk/pytorch-rl" ["l"="57.524,18.261"]
"MillionIntegrals/vel" ["l"="57.452,18.307"]
"zuoxingdong/lagom" ["l"="57.506,18.278"]
"denisyarats/pytorch_sac_ae" ["l"="59.397,17.601"]
"TianhongDai/hindsight-experience-replay" ["l"="57.631,18.333"]
"higgsfield/Imagination-Augmented-Agents" ["l"="-35.403,-2.641"]
"araffin/robotics-rl-srl" ["l"="59.631,16.375"]
"andrew-j-levy/Hierarchical-Actor-Critc-HAC-" ["l"="57.435,18.416"]
"TianhongDai/reinforcement-learning-algorithms" ["l"="57.665,18.227"]
"kashif/firedup" ["l"="57.273,18.39"]
"HumanCompatibleAI/adversarial-policies" ["l"="57.544,18.602"]
"chenhongge/StateAdvDRL" ["l"="57.554,18.764"]
"huanzhang12/ATLA_robust_RL" ["l"="57.543,18.741"]
"nuwuxian/rl_adv_valuediff" ["l"="57.548,18.643"]
"ugurkanates/awesome-real-world-rl" ["l"="57.663,18.245"]
"brianspiering/awesome-deep-rl" ["l"="57.672,18.199"]
"dongminlee94/deep_rl" ["l"="57.72,18.212"]
"StepNeverStop/RLs" ["l"="57.792,18.168"]
"TimeBreaker/MARL-papers-with-code" ["l"="57.92,18.258"]
"PacktPublishing/Python-Reinforcement-Learning-Projects" ["l"="57.686,18.187"]
"MrSyee/pg-is-all-you-need" ["l"="57.655,18.211"]
"Kaixhin/Rainbow" ["l"="57.564,18.176"]
"Code-Bullet/Flappy-Bird-AI" ["l"="56.8,17.811"]
"Code-Bullet/Storm-The-House-Auto-Clicker" ["l"="56.814,17.812"]
"Code-Bullet/NEAT-Template-JavaScript" ["l"="56.796,17.825"]
"Hanabi-Live/hanabi-live" ["l"="57.778,18.695"]
"hanabi/hanabi.github.io" ["l"="57.779,18.672"]
"WuTheFWasThat/hanabi.rs" ["l"="57.771,18.655"]
"will-hanabi-bot/hanabi-bot" ["l"="57.785,18.729"]
"Quuxplusone/Hanabi" ["l"="57.764,18.609"]
"philtabor/Reinforcement-Learning-In-Motion" ["l"="57.769,18.305"]
"philtabor/Simple-Neural-Network" ["l"="57.771,18.347"]
"chainer/chainerrl" ["l"="57.456,18.153"]
"google-deepmind/hanabi-learning-environment" ["l"="57.704,18.375"]
"tambetm/pommerman-baselines" ["l"="57.737,18.483"]
"mlii/mfrl" ["l"="57.815,18.315"]
"crowdAI/marLo" ["l"="57.499,18.387"]
"minqi/learning-to-communicate-pytorch" ["l"="57.792,18.35"]
"BorealisAI/pommerman-baseline" ["l"="57.738,18.468"]
"PaddlePaddle/MetaGym" ["l"="57.806,17.885"]
"PaddlePaddle/PGL" ["l"="50.034,29.561"]
"astorfi/Deep-Learning-Roadmap" ["l"="47.51,28.638"]
"koulanurag/ma-gym" ["l"="57.805,18.325"]
"CETC-TFAI/MaCA" ["l"="57.949,18.338"]
"SJTUwbl/MaCA" ["l"="63.159,-3.161"]
"liuqh16/LAG" ["l"="63.11,-3.106"]
"sjtu-marl/malib" ["l"="57.879,18.284"]
"wjh720/QPLEX" ["l"="57.989,18.382"]
"AI4Finance-Foundation/FinRL" ["l"="-8.922,12.615"]
"AI4Finance-Foundation/FinRL-Meta" ["l"="-9.6,14.868"]
"kaixindelele/DRLib" ["l"="57.873,18.154"]
"tinyzqh/light_mappo" ["l"="57.884,18.231"]
"openai/vime" ["l"="57.39,18.273"]
"pathak22/exploration-by-disagreement" ["l"="57.375,18.496"]
"ml-jku/baselines-rudder" ["l"="57.463,18.338"]
"openai/mlsh" ["l"="57.429,18.336"]
"google-research/episodic-curiosity" ["l"="60.077,17.353"]
"qiwihui/reinforcement-learning-an-introduction-chinese" ["l"="57.829,17.905"]
"dbsxdbsx/rl-intro-book-chinese" ["l"="57.879,17.823"]
"rl-cn/rl-cn" ["l"="57.898,17.821"]
"GAOYANGAU/DRLPytorch" ["l"="57.809,17.834"]
"finint/RL-Solutions" ["l"="46.592,31.376"]
"Zhenye-Na/reinforcement-learning-stanford" ["l"="57.862,17.663"]
"Skylark0924/Machine-Learning-is-ALL-You-Need" ["l"="57.866,18.084"]
"danijar/dreamer" ["l"="59.319,17.63"]
"danijar/dreamerv2" ["l"="59.293,17.598"]
"rll-research/url_benchmark" ["l"="59.381,17.533"]
"danijar/crafter" ["l"="59.324,17.533"]
"jurgisp/memory-maze" ["l"="59.327,17.501"]
"PaddlePaddle/PaddleRobotics" ["l"="60.88,16.539"]
"openai/robosumo" ["l"="57.443,18.351"]
"hoangminhle/hierarchical_IL_RL" ["l"="57.395,18.391"]
"florensacc/snn4hrl" ["l"="57.323,18.446"]
"jeanharb/option_critic" ["l"="57.356,18.428"]
"openai/evolution-strategies-starter" ["l"="57.409,18.235"]
"cbfinn/maml_rl" ["l"="57.704,19.47"]
"hi-abhi/tensorflow-value-iteration-networks" ["l"="57.361,18.233"]
"Code-Bullet/MarbleCalculator" ["l"="56.807,17.82"]
"Code-Bullet/Hill-Climb-Racing-AI" ["l"="56.806,17.84"]
"Code-Bullet/Piano-Tiles" ["l"="56.806,17.834"]
"eugenevinitsky/sequential_social_dilemma_games" ["l"="57.799,18.378"]
"ArnaudFickinger/gym-multigrid" ["l"="57.786,18.405"]
"oxwhirl/treeqn" ["l"="23.577,14.877"]
"zhongwen/predictron" ["l"="57.323,18.312"]
"kentsommer/pytorch-value-iteration-networks" ["l"="57.341,18.311"]
"Breakend/DeepReinforcementLearningThatMatters" ["l"="57.34,18.331"]
"davidhershey/feudal_networks" ["l"="57.367,18.407"]
"zuoxingdong/VIN_PyTorch_Visdom" ["l"="57.377,18.29"]
"ZhiqingXiao/pytorch-book" ["l"="57.923,17.95"]
"quqixun/RL-Python-Pytorch" ["l"="57.917,17.994"]
"keiohta/tf2rl" ["l"="57.71,18.171"]
"RITCHIEHuang/DeepRL_Algorithms" ["l"="57.771,18.204"]
"anita-hu/TF2-RL" ["l"="57.764,18.137"]
"BY571/Soft-Actor-Critic-and-Extensions" ["l"="57.74,18.219"]
"iffiX/machin" ["l"="57.748,18.303"]
"abhisheksuran/Reinforcement_Learning" ["l"="57.803,18.133"]
"BY571/DQN-Atari-Agents" ["l"="57.723,18.235"]
"JohannesAck/tf2multiagentrl" ["l"="57.952,18.191"]
"alirezakazemipour/DDPG-HER" ["l"="57.839,18.385"]
"microsoft/TextWorld" ["l"="57.486,18.499"]
"microsoft/jericho" ["l"="57.453,18.648"]
"alfworld/alfworld" ["l"="36.654,-2.33"]
"allenai/ScienceWorld" ["l"="57.455,18.672"]
"askforalfred/alfred" ["l"="60.306,17.48"]
"rajammanabrolu/KG-DQN" ["l"="57.426,18.637"]
"IBM/commonsense-rl" ["l"="57.448,18.629"]
"xingdi-eric-yuan/TextWorld-Coin-Collector" ["l"="57.472,18.562"]
"facebookresearch/nle" ["l"="57.589,18.449"]
"cognitiveailab/TextWorldExpress" ["l"="57.466,18.636"]
"tambetm/gym-minecraft" ["l"="57.432,18.37"]
"reinforcement-learning-kr/pg_travel" ["l"="57.499,18.266"]
"pat-coady/trpo" ["l"="57.378,18.226"]
"chagmgang/distributed_reinforcement_learning" ["l"="-4.68,-23.166"]
"chagmgang/pytorch_ppo_rl" ["l"="57.373,18.332"]
"Kaixhin/ACER" ["l"="57.466,18.223"]
"yaricom/goNEAT" ["l"="57.581,18.703"]
"dalmia/David-Silver-Reinforcement-learning" ["l"="57.625,18.03"]
"enggen/DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" ["l"="57.665,17.916"]
"omerbsezer/Reinforcement_learning_tutorial_with_demo" ["l"="57.612,18.134"]
"sudharsan13296/Hands-On-Reinforcement-Learning-With-Python" ["l"="57.646,18.059"]
"berkeleydeeprlcourse/homework" ["l"="57.492,18.083"]
"denisyarats/pytorch_sac" ["l"="59.414,17.595"]
"haarnoja/softqlearning" ["l"="57.477,18.274"]
"joschu/modular_rl" ["l"="57.394,18.195"]
"Stable-Baselines-Team/stable-baselines" ["l"="57.734,18.189"]
"jcwleo/random-network-distillation-pytorch" ["l"="57.368,18.356"]
"jcwleo/curiosity-driven-exploration-pytorch" ["l"="57.336,18.366"]
"wizdom13/RND-Pytorch" ["l"="57.314,18.394"]
"orrivlin/MountainCar_DQN_RND" ["l"="57.299,18.393"]
"alirezakazemipour/PPO-RND" ["l"="57.286,18.446"]
"jcwleo/mario_rl" ["l"="57.378,18.317"]
"Stable-Baselines-Team/stable-baselines3-contrib" ["l"="57.774,18.15"]
"Stable-Baselines-Team/stable-baselines-tf2" ["l"="57.805,18.188"]
"Stable-Baselines-Team/rl-colab-notebooks" ["l"="57.792,18.146"]
"omerbsezer/Generative_Models_Tutorial_with_Demo" ["l"="57.555,18.055"]
"lcswillems/torch-ac" ["l"="57.54,18.414"]
"ikostrikov/jaxrl" ["l"="59.372,17.498"]
"facebookresearch/impact-driven-exploration" ["l"="57.706,18.527"]
"MattChanTK/gym-maze" ["l"="57.414,18.355"]
"red42/HTML5_Genetic_Cars" ["l"="56.987,17.744"]
"pubnub/genetic-car-2" ["l"="56.957,17.728"]
"subprotocol/genetic-js" ["l"="56.906,17.673"]
"TomaszRewak/ML-games" ["l"="56.925,17.707"]
"parano/GeneticAlgorithm-TSP" ["l"="51.052,26.36"]
"illogicz/NeuroEvolution" ["l"="56.984,17.709"]
"thopit/Creatures" ["l"="56.936,17.638"]
"dolphin278/genetic" ["l"="56.949,17.692"]
"schteppe/p2.js" ["l"="35.964,24.607"]
"Farama-Foundation/MAgent2" ["l"="57.869,18.323"]
"JKCooper2/rlai-exercises" ["l"="57.747,17.744"]
"diegoalejogm/Reinforcement-Learning" ["l"="57.734,17.731"]
"matteocasolari/reinforcement-learning-an-introduction-solutions" ["l"="57.751,17.72"]
"mharbuz/rlbook-exercises" ["l"="57.717,17.739"]
"PKU-MARL/HARL" ["l"="57.91,18.246"]
"rlpy/rlpy" ["l"="57.194,17.807"]
"NathanEpstein/reinforce" ["l"="57.19,17.87"]
"jmacglashan/burlap" ["l"="57.212,17.662"]
"amarack/python-rl" ["l"="57.144,17.755"]
"Mononofu/reinforcement-learning" ["l"="57.166,17.773"]
"ARISE-Initiative/robomimic" ["l"="59.444,16.563"]
"takuseno/d3rlpy" ["l"="59.435,17.549"]
"qgallouedec/panda-gym" ["l"="59.546,16.37"]
"mimoralea/applied-reinforcement-learning" ["l"="57.446,17.82"]
"marl-book/codebase" ["l"="57.935,18.304"]
"TikhonJelvis/RL-book" ["l"="-9.012,12.961"]
"ucaiado/QLearning_Trading" ["l"="-9.598,14.704"]
"instadeepai/Mava" ["l"="57.894,18.315"]
"vietnh1009/Super-mario-bros-A3C-pytorch" ["l"="57.561,18.15"]
"vietnh1009/Super-mario-bros-PPO-pytorch" ["l"="57.626,18.141"]
"vietnh1009/Flappy-bird-deep-Q-learning-pytorch" ["l"="57.472,18.048"]
"vietnh1009/QuickDraw" ["l"="50.886,30.511"]
"vietnh1009/Tetris-deep-Q-learning-pytorch" ["l"="57.405,17.962"]
"vietnh1009/Street-fighter-A3C-ICM-pytorch" ["l"="57.476,18.107"]
"williamFalcon/DeepRLHacks" ["l"="57.433,18.15"]
"openai/universe-starter-agent" ["l"="57.388,18.152"]
"facebookresearch/ELF" ["l"="57.451,18.125"]
"miyosuda/async_deep_reinforce" ["l"="57.348,18.157"]
"ppaquette/gym-super-mario" ["l"="57.228,18.297"]
"Farama-Foundation/ViZDoom" ["l"="57.439,18.18"]
"openai/retro" ["l"="57.516,18.132"]
"Farama-Foundation/Arcade-Learning-Environment" ["l"="57.411,18.137"]
"hardmaru/slimevolleygym" ["l"="59.248,17.57"]
"google-deepmind/graph_nets" ["l"="52.964,16.035"]
"tensorflow/adanet" ["l"="45.54,26.002"]
"AndyYue1893/Deep-reinforcement-learning-with-pytorch" ["l"="57.959,18.064"]
"eyounx/VirtualTaobao" ["l"="59.863,23.732"]
"hanjuku-kaso/awesome-offline-rl" ["l"="59.483,17.58"]
"gxywy/rl-plotter" ["l"="57.917,18.208"]
"XinJingHao/TD3-BipedalWalkerHardcore-v2" ["l"="57.903,18.145"]
"huawei-noah/xingtian" ["l"="57.849,18.297"]
"PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition" ["l"="57.738,17.912"]
"pfnet/pfrl" ["l"="57.572,18.238"]
"SarvagyaVaish/FlappyBirdRL" ["l"="57.231,17.989"]
"enhuiz/flappybird-ql" ["l"="57.176,17.964"]
"chncyhn/flappybird-qlearning-bot" ["l"="57.218,17.955"]
"floodsung/DRL-FlappyBird" ["l"="57.287,18.009"]
"kuz/DeepMind-Atari-Deep-Q-Learner" ["l"="57.319,18.074"]
"siemanko/tensorflow-deepq" ["l"="57.297,18.086"]
"nikitasrivatsan/DeepLearningVideoGames" ["l"="57.289,18.06"]
"sourabhv/FlapPyBird" ["l"="57.259,17.974"]
"muupan/deep-reinforcement-learning-papers" ["l"="57.347,18.101"]
"YueDayu/AsmFlappyBird" ["l"="57.147,17.948"]
"TimoWilken/flappy-bird-pygame" ["l"="57.21,17.909"]
"justinmeister/Mario-Level-1" ["l"="59.776,34.175"]
"mx0c/super-mario-python" ["l"="59.79,34.231"]
"kidscancode/pygame_tutorials" ["l"="59.745,34.228"]
"ntasfi/PyGame-Learning-Environment" ["l"="57.348,18.123"]
"justinmeister/The-Stolen-Crown-RPG" ["l"="59.721,34.205"]
"techwithtim/Pygame-Tutorials" ["l"="59.856,34.315"]
"estevaofon/angry-birds-python" ["l"="59.769,34.235"]
"ppizarror/pygame-menu" ["l"="59.685,34.18"]
"Mekire/pygame-samples" ["l"="59.711,34.158"]
"OMSCentral/omscentral-legacy-angular" ["l"="57.224,17.444"]
"cmaron/CS-7641-assignments" ["l"="57.261,17.504"]
"martzcodes/gt-course-surveys" ["l"="57.198,17.424"]
"OMSCentral/omscentral-legacy-react" ["l"="57.217,17.42"]
"orsenthil/coursedocs" ["l"="57.185,17.407"]
"JonathanTay/CS-7641-assignment-1" ["l"="57.239,17.508"]
"iamjakewarner/jdf" ["l"="57.235,17.376"]
"CuriousLearner/jdf-latex" ["l"="57.228,17.404"]
"juanjose49/omscs-cs7641-machine-learning-assignment-4" ["l"="57.253,17.522"]
"M-J-Murray/MAMEToolkit" ["l"="57.511,18.216"]
"alito/mamele" ["l"="57.487,18.253"]
"M-J-Murray/SFAgents" ["l"="57.482,18.243"]
"TorchCraft/TorchCraftAI" ["l"="58.677,18.049"]
"njustesen/botbowl" ["l"="57.416,18.114"]
"BYU-PCCL/holodeck" ["l"="61.72,12.533"]
"inoryy/reaver" ["l"="58.76,18.087"]
"openai/neural-mmo" ["l"="57.738,18.325"]
"NeuralMMO/environment" ["l"="57.74,18.355"]
"NeuralMMO/client" ["l"="57.752,18.389"]
"DKuan/MADDPG_torch" ["l"="57.842,18.28"]
"mam91/neat-genetic-mario" ["l"="57.022,18.106"]
"pakoito/MarI-O" ["l"="56.967,18.106"]
"erwincoumans/pybullet_robots" ["l"="59.565,16.357"]
"hsp-iit/pybullet-robot-envs" ["l"="59.567,16.323"]
"openai/roboschool" ["l"="57.483,18.154"]
"google-research/ravens" ["l"="59.543,16.458"]
"sungyubkim/Deep_RL_with_pytorch" ["l"="57.764,18.437"]
"mbaske/ml-agents-hyperparams" ["l"="57.244,19.02"]
"mbaske/ml-explorer-drone" ["l"="57.269,19.026"]
"mbaske/grid-sensor" ["l"="57.259,19.002"]
"cts198859/deeprl_network" ["l"="57.788,18.323"]
"cts198859/deeprl_signal_control" ["l"="62.265,12.116"]
"AndreaVidali/Deep-QLearning-Agent-for-Traffic-Signal-Control" ["l"="62.239,12.138"]
"docwza/sumolights" ["l"="62.243,12.123"]
"wingsweihua/colight" ["l"="62.257,12.084"]
"isp1tze/MAProj" ["l"="57.853,18.42"]
"mohammadasghari/dqn-multi-agent-rl" ["l"="57.829,18.309"]
"LucasAlegre/sumo-rl" ["l"="62.265,12.138"]
"thesouther/MARL" ["l"="57.978,18.214"]
"shkrwnd/Deep-Reinforcement-Learning-for-Dynamic-Spectrum-Access" ["l"="55.576,4.729"]
"sumitrj/ConnectedQ-Multi-agent-Reinforcement-Learning-Algorithm" ["l"="62.07,13.55"]
"cyoon1729/Multi-agent-reinforcement-learning" ["l"="57.874,18.378"]
"apexrl/Imitation-Learning-Paper-Lists" ["l"="57.638,18.446"]
"kristery/Imitation-Learning-from-Imperfect-Demonstration" ["l"="57.638,18.506"]
"Ericonaldo/ILSwiss" ["l"="57.626,18.429"]
"Div99/IQ-Learn" ["l"="57.609,18.422"]
"siddhanthaldar/ROT" ["l"="57.639,18.48"]
"real-stanford/scalingup" ["l"="59.475,16.568"]
"Observerspy/CS234" ["l"="57.716,17.586"]
"Observerspy/CS294" ["l"="57.648,17.595"]
"zlpure/CS234" ["l"="57.83,17.631"]
"openai/universe" ["l"="57.439,17.977"]
"ucla-rlcourse/DeepRL-Tutorials" ["l"="57.805,17.978"]
"cuhkrlcourse/ierg6130-assignment" ["l"="57.852,17.927"]
"JaeDukSeo/reinforcement-learning-an-introduction" ["l"="58.078,17.869"]
"MJeremy2017/reinforcement-learning-implementation" ["l"="58.026,17.912"]
"Kchu/DeepRL_PyTorch" ["l"="57.784,18.459"]
"nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch" ["l"="57.443,18.438"]
"qq456cvb/doudizhu-C" ["l"="58.253,18.429"]
"charleschen003/doudizhu-rl" ["l"="58.263,18.459"]
"thuxugang/doudizhu" ["l"="58.29,18.433"]
"onestraw/doudizhu" ["l"="58.288,18.45"]
"zhozhou/DouDizhuAI" ["l"="58.318,18.432"]
"ZhouWeikuan/DouDiZhu" ["l"="58.249,18.41"]
"ermongroup/multiagent-gail" ["l"="57.659,18.481"]
"ermongroup/MetaIRL" ["l"="57.585,18.402"]
"twni2016/f-IRL" ["l"="57.639,18.42"]
"SaminYeasar/Off_Policy_Adversarial_Inverse_Reinforcement_Learning" ["l"="57.653,18.432"]
"HuangJiaLian/AIRL_MountainCar" ["l"="57.628,18.41"]
"cpnota/autonomous-learning-library" ["l"="57.636,18.293"]
"jannerm/mbpo" ["l"="59.354,17.637"]
"learnables/cherry" ["l"="57.693,18.428"]
"andyljones/reinforcement-learning-discord-wiki" ["l"="57.682,18.385"]
"fabiopardo/tonic" ["l"="59.32,17.595"]
"Spenhouet/tensorboard-aggregator" ["l"="57.7,18.473"]
"joyiswu/UCL-Deep-learning-ans-Reinforcement-learning" ["l"="57.686,17.813"]
"RylanSchaeffer/ucl-adv-dl-rl" ["l"="57.667,17.819"]
"Zhenye-Na/advanced-deep-learning-and-reinforcement-learning-deepmind" ["l"="57.666,17.803"]
"YidingYu/UCL-DeepMind-Advanced-Deep-Learning-and-Reinforcement-Learning" ["l"="57.665,17.838"]
"mikezhang95/ML_Assignment" ["l"="57.682,17.831"]
"dalmia/Deep-Learning-Book-Chapter-Summaries" ["l"="50.657,28.219"]
"PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python" ["l"="57.721,17.9"]
"sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python" ["l"="57.703,17.989"]
"junhyukoh/deep-reinforcement-learning-papers" ["l"="57.439,18.074"]
"carpedm20/deep-rl-tensorflow" ["l"="57.382,18.103"]
"Lyusungwon/apex_dqn_pytorch" ["l"="57.5,18.644"]
"praveen-palanisamy/Ape-X-DQN" ["l"="57.5,18.593"]
"kristjankorjus/Replicating-DeepMind" ["l"="57.235,18.048"]
"spragunr/deep_q_rl" ["l"="57.3,18.109"]
"muupan/dqn-in-the-caffe" ["l"="57.229,18.092"]
"brian473/neural_rl" ["l"="57.174,18.052"]
"shawntan/neural-turing-machines" ["l"="46.03,27.814"]
"gliese581gg/DQN_tensorflow" ["l"="57.239,18.03"]
"soumith/deepmind-atari" ["l"="45.887,27.724"]
"jbornschein/draw" ["l"="44.914,27.67"]
"TorontoDeepLearning/convnet" ["l"="36.832,0.682"]
"tambetm/simple_dqn" ["l"="57.326,18.099"]
"222464/AILib" ["l"="57.144,18.014"]
"vitruvianscience/OpenDeep" ["l"="44.923,27.641"]
"dmlc/cxxnet" ["l"="57.767,23.488"]
"yandexdataschool/AgentNet" ["l"="57.34,18.016"]
"hungtuchen/pytorch-dqn" ["l"="57.483,18.18"]
"AboudyKreidieh/h-baselines" ["l"="57.483,18.45"]
"011235813/hierarchical-marl" ["l"="57.436,18.487"]
"watakandai/hiro_pytorch" ["l"="57.421,18.481"]
"sudharsan13296/Hands-On-Meta-Learning-With-Python" ["l"="57.786,19.407"]
"vmayoral/basic_reinforcement_learning" ["l"="57.466,18.079"]
"sudharsan13296/Awesome-Meta-Learning" ["l"="57.84,19.394"]
"inoryy/tensorflow2-deep-reinforcement-learning" ["l"="57.816,18.167"]
"Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning" ["l"="57.835,18.153"]
"MJeremy2017/machine-learning-algorithm-implemention" ["l"="58.056,17.883"]
"MJeremy2017/machine-learning-models" ["l"="58.076,17.894"]
"DongChen06/MARL_CAVs" ["l"="62.377,12.309"]
"omerbsezer/LSTM_RNN_Tutorials_with_Demo" ["l"="-9.213,12.66"]
"tristandeleu/pytorch-maml-rl" ["l"="57.739,19.466"]
"sudharsan13296/Word2vec-from-scratch" ["l"="57.707,17.909"]
"sudharsan13296/Getting-Started-with-Google-BERT" ["l"="47.483,26.542"]
"greydanus/visualize_atari" ["l"="57.228,18.323"]
"nikaashpuri/sarfa-saliency" ["l"="57.184,18.347"]
"greydanus/baby-a3c" ["l"="57.302,18.284"]
"JerichoWorld/JerichoWorld" ["l"="57.477,18.686"]
"microsoft/nail_agent" ["l"="57.475,18.707"]
"microsoft/tdqn" ["l"="57.431,18.687"]
"princeton-nlp/XTX" ["l"="57.419,18.686"]
"princeton-nlp/calm-textgame" ["l"="57.433,18.703"]
"xingdi-eric-yuan/GATA-public" ["l"="57.429,18.661"]
"stevens-cs546-cs554/CS-546" ["l"="58.11,17.696"]
"stevens-cs546-cs554/CS-554" ["l"="58.066,17.744"]
"ebonelli/PLaF" ["l"="58.128,17.657"]
"ParasGarg/Stevens-Computer-Science-Courses-Materials" ["l"="58.151,17.669"]
"krasheninnikov/max-causal-ent-irl" ["l"="57.672,18.43"]
"TroddenSpade/Maximum-Entropy-Deep-IRL" ["l"="57.978,18.16"]
"yfzhang/vehicle-motion-forecasting" ["l"="57.635,18.402"]
"ShivinDass/inverse_rl" ["l"="57.625,18.391"]
"nishantkr18/guided-cost-learning" ["l"="57.676,18.414"]
"rjtobin/HanSim" ["l"="57.771,18.64"]
"aviralkumar2907/CQL" ["l"="59.46,17.584"]
"cyoon1729/RLcycle" ["l"="57.521,18.324"]
"cyoon1729/Policy-Gradient-Methods" ["l"="57.5,18.328"]
"cyoon1729/deep-Q-networks" ["l"="57.551,18.255"]
"cyoon1729/distributedRL" ["l"="57.54,18.456"]
"google-research/realworldrl_suite" ["l"="59.388,17.564"]
"glample/Arnold" ["l"="57.285,18.239"]
"akolishchak/doom-net-pytorch" ["l"="57.14,18.276"]
"mihahauke/VDAIC2017" ["l"="57.157,18.282"]
"ruiminshen/yolo2-pytorch" ["l"="50.94,30.134"]
"Breakend/gym-extensions" ["l"="57.27,18.305"]
"itaicaspi/keras-dqn-doom" ["l"="57.161,18.249"]
"mehdiboubnan/Deep-Reinforcement-Learning-applied-to-DOOM" ["l"="57.116,18.269"]
"isl-org/DirectFuturePrediction" ["l"="57.221,18.268"]
"devendrachaplot/DeepRL-Grounding" ["l"="60.153,17.34"]
"cheesecakeufo/topanga" ["l"="40.534,-22.921"]
"catziyan/DRLPytorch-" ["l"="57.83,17.778"]
"Teacher-Guo/RL_code" ["l"="57.799,17.809"]
"gxnk/reinforcement-learning-code" ["l"="57.768,17.89"]
"jaybutera/tetrisRL" ["l"="57.331,17.863"]
"lusob/gym-tetris" ["l"="57.353,17.879"]
"Uglemat/MaTris" ["l"="57.329,17.838"]
"ReinforcementLearning/Tetris" ["l"="57.309,17.825"]
"uidilr/gail_ppo_tf" ["l"="57.489,18.356"]
"YunzhuLi/InfoGAIL" ["l"="57.473,18.35"]
"itaicaspi/mgail" ["l"="57.446,18.386"]
"navuboy/gail_gym" ["l"="57.519,18.422"]
"sisl/gail-driver" ["l"="57.425,18.311"]
"facebookresearch/hanabi_SAD" ["l"="57.792,18.538"]
"facebookresearch/Hanabi_SPARTA" ["l"="57.768,18.55"]
"google-deepmind/spriteworld" ["l"="57.672,18.329"]
"jonasrothfuss/ProMP" ["l"="57.65,19.483"]
"openai/doom-py" ["l"="57.272,18.426"]
"alshedivat/lola" ["l"="57.672,18.498"]
"rlbayes/rllabplusplus" ["l"="57.356,18.273"]
"TianhongDai/self-imitation-learning-pytorch" ["l"="57.371,18.448"]
"pathak22/zeroshot-imitation" ["l"="57.354,18.397"]
"tgangwani/SelfImitationDiverse" ["l"="57.385,18.41"]
"Hwhitetooth/lirpg" ["l"="57.373,18.428"]
"openai/atari-reset" ["l"="57.313,18.476"]
"tianheyu927/mil" ["l"="57.364,18.378"]
"dnddnjs/feudal-montezuma" ["l"="57.398,18.425"]
"lweitkamp/feudalnets-pytorch" ["l"="57.391,18.455"]
"mrkulk/hierarchical-deep-RL" ["l"="57.337,18.402"]
"EdenMacdonald/h-DQN" ["l"="57.319,18.355"]
"skumar9876/Hierarchical-DQN" ["l"="57.313,18.428"]
"flyyufelix/VizDoom-Keras-RL" ["l"="57.078,18.269"]
"avdmitry/rl_3d" ["l"="57.025,18.274"]
"jiechuanjiang/pytorch_DGN" ["l"="57.869,18.43"]
"sumitsk/marl_transfer" ["l"="57.842,18.441"]
"IC3Net/IC3Net" ["l"="57.839,18.402"]
"tegg89/magnet" ["l"="57.823,18.406"]
"CORE-Robotics-Lab/MAGIC" ["l"="57.887,18.428"]
"Farama-Foundation/MAgent" ["l"="57.874,18.348"]
"BorealisAI/mtmfrl" ["l"="57.836,18.362"]
"Code-Bullet/WebsiteTest" ["l"="56.79,17.843"]
"MishaLaskin/curl" ["l"="59.368,17.617"]
"germain-hug/Deep-RL-Keras" ["l"="57.534,18.115"]
"yanpanlau/DDPG-Keras-Torcs" ["l"="57.365,18.146"]
"keon/deep-q-learning" ["l"="57.397,18.043"]
"xiaochus/Deep-Reinforcement-Learning-Practice" ["l"="-8.51,12.779"]
"miroblog/deep_rl_trader" ["l"="-9.549,14.765"]
"inarikami/keras-rl2" ["l"="57.534,18.04"]
"fg91/Deep-Q-Learning" ["l"="57.385,17.836"]
"flyyufelix/C51-DDQN-Keras" ["l"="57.215,18.218"]
"jaromiru/AI-blog" ["l"="57.394,18.169"]
"openai/atari-demo" ["l"="57.208,18.542"]
"reinforcement-learning-kr/rl-montezuma" ["l"="57.279,18.515"]
"vietnh1009/AirGesture" ["l"="50.913,30.59"]
"vivek3141/pacman-ai" ["l"="56.912,18.405"]
"vivek3141/flappy-neat" ["l"="56.922,18.389"]
"vivek3141/learn-c" ["l"="56.925,18.397"]
"vivek3141/learn-js" ["l"="56.915,18.395"]
"vivek3141/tictactoe-minimax" ["l"="56.933,18.394"]
"vivek3141/snake-gym" ["l"="56.91,18.383"]
"minerllabs/minerl" ["l"="57.466,18.366"]
"openai/Video-Pre-Training" ["l"="41.066,-4.565"]
"minerllabs/baselines" ["l"="57.452,18.42"]
"MineDojo/MineDojo" ["l"="41.05,-4.52"]
"minerllabs/competition_submission_template" ["l"="57.48,18.405"]
"microsoft/malmo" ["l"="57.453,18.204"]
"MineDojo/MineCLIP" ["l"="41.025,-4.544"]
"danijar/dreamerv3" ["l"="59.307,17.548"]
"juliusfrost/dreamer-pytorch" ["l"="59.316,17.613"]
"Shalev-Lifshitz/STEVE-1" ["l"="41.019,-4.581"]
"NeuralMMO/baselines" ["l"="57.745,18.436"]
"Bam4d/Griddly" ["l"="57.679,18.45"]
"Farama-Foundation/SuperSuit" ["l"="-1.096,2.966"]
"PufferAI/PufferLib" ["l"="59.256,17.432"]
"Sohojoe/ActiveRagdollStyleTransfer" ["l"="57.297,18.82"]
"Sohojoe/ActiveRagdollAssaultCourse" ["l"="57.309,18.835"]
"Sohojoe/ActiveRagdollControllers" ["l"="57.295,18.849"]
"Code-Bullet/Car-QLearning" ["l"="56.753,17.823"]
"Code-Bullet/Tetris-AI-Javascript" ["l"="56.776,17.823"]
"vitchyr/viskit" ["l"="57.837,18.249"]
"mihdalal/sawyer_control" ["l"="57.538,18.504"]
"mengf1/DHER" ["l"="57.582,18.382"]
"snasiriany/leap" ["l"="57.534,18.436"]
"denisyarats/dmc2gym" ["l"="59.385,17.589"]
"alexis-jacq/LOLA_DiCE" ["l"="57.689,18.554"]
"Nasdin/ReinforcementLearning-AtariGame" ["l"="57.282,18.269"]
"facebookarchive/MazeBase" ["l"="57.399,18.327"]
"rwightman/pytorch-pommerman-rl" ["l"="57.74,18.502"]
"eugene/pommerman" ["l"="57.752,18.495"]
"qian18long/epciclr2020" ["l"="57.876,18.559"]
"YangRui2015/Sparse-Reward-Algorithms" ["l"="57.677,18.347"]
"YuhangSong/DEHRL" ["l"="57.387,18.479"]
"tdavchev/option-critic" ["l"="57.354,18.45"]
"jesbu1/hidio" ["l"="57.399,18.471"]
"florensacc/rllab-curriculum" ["l"="59.361,17.797"]
"vivek3141/super-mario-neat" ["l"="56.953,18.385"]
"vivek3141/super-mario-rl" ["l"="56.903,18.397"]
"vivek3141/ml" ["l"="56.937,18.403"]
"vivek3141/vector-field-visualizer" ["l"="56.926,18.409"]
"vivek3141/forest-fire-predictor" ["l"="56.916,18.414"]
"vivek3141/rubiks-cube-ai" ["l"="56.916,18.37"]
"vivek3141/CommunicationForDeaf" ["l"="56.936,18.367"]
"vivek3141/NavigationForBlind" ["l"="56.949,18.414"]
"vivek3141/streamer-ai" ["l"="56.929,18.379"]
"wwxFromTju/deepmind_MAS_enviroment" ["l"="57.819,18.389"]
"Coac/CommNet-BiCnet" ["l"="57.812,18.436"]
"facebookarchive/CommNet" ["l"="57.805,18.416"]
"TonghanWang/ROMA" ["l"="57.955,18.393"]
"ZhouWeikuan/kuanli_server" ["l"="-23.332,-34.412"]
"songbaoming/DouDiZhu" ["l"="58.312,18.449"]
"qmarliu/ddzlib" ["l"="58.288,18.41"]
"Netease-Games-AI-Lab-Guangzhou/PerfectDou" ["l"="58.165,18.347"]
"yinjimmy/chessAndCard-2dx" ["l"="-23.85,-33.946"]
"fztcjjl/metoo" ["l"="-23.392,-34.421"]
"zhangshiqian1214/skynet-server" ["l"="-23.359,-34.434"]
"yuanfengyun/q_algorithm" ["l"="-23.903,-33.997"]
"yuanfengyun/mj_ai" ["l"="-23.997,-34.064"]
"qipaiprojects/mj_server" ["l"="-23.356,-34.411"]
"PKU-RL/I2C" ["l"="57.886,18.453"]
"rhoowd/sched_net" ["l"="57.826,18.424"]
"saizhang0218/VBC" ["l"="57.859,18.445"]
"apsdehal/ic3net-envs" ["l"="57.847,18.455"]
"tuladhay/ATOC_COMA_PyTorch" ["l"="57.828,18.44"]
"hsvgbkhgbv/SQDDPG" ["l"="57.913,18.455"]
"saizhang0218/TMC" ["l"="57.889,18.442"]
"gmargo11/hDQN" ["l"="57.261,18.469"]
"fedingo/Hierarchical-DQN" ["l"="57.273,18.459"]
"hungtuchen/pytorch-hdqn" ["l"="57.245,18.484"]
"wulfebw/hierarchical_rl" ["l"="57.253,18.455"]
"uber-research/atari-model-zoo" ["l"="57.292,18.578"]
"mohamedameen93/CS-7641-Machine-Learning-Notes" ["l"="57.244,17.478"]
"hiive/mlrose" ["l"="57.265,17.454"]
"gkhayes/mlrose" ["l"="57.286,17.507"]
"pushkar/ABAGAIL" ["l"="57.29,17.589"]
"kylewest520/CS-7641---Machine-Learning" ["l"="57.229,17.495"]
"mohamedameen93/CS-7642-Reinforcement-Learning-Notes" ["l"="57.221,17.47"]
"Farzin-Negahbani/Namira-LogAnalyzer" ["l"="56.851,18.613"]
"Cyrus2D/SoccerSimulationProxy" ["l"="56.881,18.606"]
"pathak22/hierarchical-imitation" ["l"="57.286,18.413"]
"avisingh599/reward-learning-rl" ["l"="59.682,16.361"]
"wyndwarrior/imitation_from_observation" ["l"="57.307,18.413"]
"stepjam/PyRep" ["l"="59.558,16.463"]
"cbfinn/gps" ["l"="57.416,18.276"]
"jangirrishabh/Overcoming-exploration-from-demos" ["l"="57.221,18.453"]
"stepjam/TecNets" ["l"="57.296,18.429"]
"iassael/learning-to-communicate" ["l"="57.723,18.37"]
"social-dilemma/multiagent" ["l"="57.809,18.455"]
"louaaron/CS294_homework" ["l"="57.594,17.618"]
"xuwd11/cs294-112_hws" ["l"="57.564,17.65"]
"RITCHIEHuang/MAGAIL" ["l"="57.667,18.551"]
"facebookresearch/mbrl-lib" ["l"="59.345,17.576"]
"mengf1/CHER" ["l"="49.188,32.613"]
"hemilpanchiwala/Hindsight-Experience-Replay" ["l"="57.655,18.389"]
"facebookresearch/drqv2" ["l"="59.367,17.578"]
"LauraRuis/groundedSCAN" ["l"="46.114,28.39"]
"flowersteam/Grounding_LLMs_with_online_RL" ["l"="36.505,-2.379"]
"tkipf/c-swm" ["l"="48.978,32.44"]
"google-research/clevr_robot_env" ["l"="48.813,32.293"]
"jacobandreas/psketch" ["l"="48.604,32.206"]
"PacktPublishing/Reinforcement-Learning-Algorithms-with-Python" ["l"="57.731,17.862"]
"tinyzqh/awesome-reinforcement-learning" ["l"="57.76,17.852"]
"mbaske/angry-ai" ["l"="57.28,18.966"]
"kressdev/RagdollTrainer" ["l"="57.2,19.006"]
"joanllobera/marathon-envs" ["l"="57.326,18.817"]
"facebookresearch/ScaDiver" ["l"="30.381,28.648"]
"senya-ashukha/quantile-regression-dqn-pytorch" ["l"="57.103,18.227"]
"Silvicek/distributional-dqn" ["l"="57.154,18.23"]
"dannysdeng/dqn-pytorch" ["l"="57.778,18.489"]
"toshikwa/fqf-iqn-qrdqn.pytorch" ["l"="57.82,18.502"]
"Huixxi/CS234-Reinforcement-Learning-Winter-2019" ["l"="57.874,17.599"]
"arowdy98/Stanford-CS234" ["l"="57.862,17.609"]
"changebo/CS234-2020" ["l"="57.88,17.578"]
"tallamjr/stanford-cs234" ["l"="57.883,17.555"]
"ksang/cs234-assignments" ["l"="57.9,17.57"]
"semitable/lb-foraging" ["l"="57.918,18.358"]
"semitable/robotic-warehouse" ["l"="57.947,18.358"]
"lich14/CDS" ["l"="57.979,18.34"]
"uoe-agents/lb-foraging" ["l"="57.991,18.364"]
"RchalYang/torchrl" ["l"="57.777,18.22"]
"Sonkyunghwan/QTRAN" ["l"="58.003,18.42"]
"AnujMahajanOxf/MAVEN" ["l"="57.996,18.453"]
"oxwhirl/wqmix" ["l"="57.985,18.399"]
"xbpeng/DeepLoco" ["l"="57.034,18.156"]
"xbpeng/DeepTerrainRL" ["l"="57.107,18.15"]
"VincentYu68/SymmetryCurriculumLocomotion" ["l"="56.983,18.161"]
"go2sea/C51DQN" ["l"="57.126,18.215"]
"floringogianu/categorical-dqn" ["l"="57.253,18.258"]
"rlworkgroup/dowel" ["l"="57.536,18.687"]
"rlworkgroup/akro" ["l"="57.531,18.639"]
"adik993/ppo-pytorch" ["l"="57.35,18.343"]
"rlcode/per" ["l"="57.635,18.234"]
"alirezamika/evostra" ["l"="59.214,17.85"]
"cardwing/Codes-for-RL-PER" ["l"="57.669,18.272"]
"Jonathan-Pearce/DDPG_PER" ["l"="57.703,18.243"]
"Damcy/prioritized-experience-replay" ["l"="57.402,18.219"]
"Howuhh/prioritized_experience_replay" ["l"="57.66,18.282"]
"xiaobaoonline/pytorch-in-action" ["l"="58.003,17.874"]
"microsoft/malmo-challenge" ["l"="57.451,18.511"]
"crowdAI/marlo-multi-agent-starter-kit" ["l"="57.483,18.424"]
"jmacglashan/burlap_examples" ["l"="57.185,17.631"]
"chen0040/java-reinforcement-learning" ["l"="57.158,17.603"]
"h2r/burlapcraft" ["l"="57.208,17.628"]
"rlpark/rlpark" ["l"="57.184,17.651"]
"samindaa/RLLib" ["l"="53.345,32.992"]
"pathak22/modular-assemblies" ["l"="57.392,18.55"]
"facebookresearch/adversarially-motivated-intrinsic-goals" ["l"="57.548,18.57"]
"jhejna/hierarchical_morphology_transfer" ["l"="57.37,18.589"]
"mrahtz/learning-from-human-preferences" ["l"="57.208,18.358"]
"nottombrown/rl-teacher" ["l"="57.337,18.285"]
"csmile-1006/PreferenceTransformer" ["l"="34.012,31.491"]
"HumanCompatibleAI/learning-from-human-preferences" ["l"="57.166,18.391"]
"ZachisGit/LearningFromHumanPreferences" ["l"="57.261,18.328"]
"machine-intelligence/rl-teacher-atari" ["l"="57.249,18.327"]
"rddy/ReQueST" ["l"="57.139,18.39"]
"rll-research/BPref" ["l"="34.067,31.533"]
"mrahtz/easy-tf-log" ["l"="57.137,18.414"]
"malayandi/DemPrefCode" ["l"="57.18,18.375"]
"mrahtz/gym-moving-dot" ["l"="57.164,18.377"]
"opherlieber/rltime" ["l"="57.582,18.416"]
"kairproject/kair_algorithms_draft" ["l"="57.65,18.335"]
"j-marple-dev/model_compression" ["l"="23.571,14.945"]
"reinforcement-learning-kr/rl_bootcamp" ["l"="-4.766,-23.153"]
"kakaoenterprise/JORLDY" ["l"="-4.731,-23.168"]
"aravindsrinivas/upn" ["l"="57.316,18.588"]
"donadigo/TMTrackNN" ["l"="58.278,19.055"]
"donadigo/pygbx" ["l"="58.31,19.1"]
"hzm2016/Peg_in_hole_assembly" ["l"="58.511,18.892"]
"DengYuelin/vrep_peg_in_hole" ["l"="58.512,18.878"]
"robot0102/rl-peg-in-hole-assembly-webots" ["l"="58.494,18.883"]
"jeongeun980906/peg-in-hole" ["l"="58.501,18.872"]
"SongleChen2015/Peg_in_hole_assembly" ["l"="58.484,18.896"]
"Junzhuodu/ur5_vrep_python" ["l"="58.351,18.772"]
"zanebin/UR5-manipulator-control" ["l"="58.356,18.789"]
"Ianlande/Vrep_yolov3_ddpg_pytorch" ["l"="58.247,18.711"]
"alishbaimran/Robotics-DDPG-HER" ["l"="58.064,18.576"]
"pengzou1/RL_PEG_IN_HOLE" ["l"="58.538,18.882"]
"mtrazzi/two-step-task" ["l"="57.215,18.243"]
"mtrazzi/harlow" ["l"="57.188,18.25"]
"ml3705454/mapr2" ["l"="57.985,18.506"]
"rommeoijcai2019/rommeo" ["l"="58.01,18.541"]
"Sohojoe/ppo-dash" ["l"="57.384,18.614"]
"Kaixhin/spinning-up-basic" ["l"="57.218,18.421"]
"MishaLaskin/torchingup" ["l"="57.236,18.41"]
"zhuang0/BoYaDDZ" ["l"="58.376,18.504"]
"foreverimps/doudizhu" ["l"="58.406,18.52"]
"diplomacy/research" ["l"="57.938,18.628"]
"diplomacy/diplomacy" ["l"="57.927,18.637"]
"rowatc/Diplomacy-AI" ["l"="57.949,18.656"]
"google-deepmind/diplomacy" ["l"="57.932,18.605"]
"facebookresearch/diplomacy_searchbot" ["l"="57.918,18.609"]
"llSourcell/deep_q_learning" ["l"="57.151,18.351"]
"llSourcell/q_learning_demo" ["l"="57.082,18.394"]
"facebookresearch/craftassist" ["l"="57.421,18.395"]
"facebookresearch/fairo" ["l"="59.554,16.499"]
"microsoft/task_oriented_dialogue_as_dataflow_synthesis" ["l"="56.583,29.286"]
"Tigermouthbear/Theia" ["l"="-34.8,-12.969"]
"kami-blue/client" ["l"="-34.804,-13.014"]
"nuno-faria/tetris-ai" ["l"="57.293,17.865"]
"Kautenja/gym-tetris" ["l"="57.339,17.886"]
"LeeYiyuan/tetrisai" ["l"="57.252,17.8"]
"michiel-cox/Tetris-DQN" ["l"="57.327,17.898"]
"fthomasmorel/Tetris-AI" ["l"="57.268,17.831"]
"hrpan/tetris_mcts" ["l"="23.511,14.904"]
"diegoalejogm/deep-q-learning" ["l"="57.743,17.669"]
"zhuliquan/reinforcement_learning_basic_book" ["l"="57.836,17.817"]
"apachecn/stanford-cs234-notes-zh" ["l"="57.856,17.753"]
"mtrazzi/rl-book-challenge" ["l"="23.605,14.915"]
"habanoz/reinforcement-learning-an-introduction" ["l"="57.765,17.762"]
"rajammanabrolu/KG-A2C" ["l"="57.404,18.69"]
"dxyang/DQN_pytorch" ["l"="57.534,18.165"]
"sherjilozair/dqn" ["l"="57.368,18.121"]
"deligentfool/dqn_zoo" ["l"="57.758,18.417"]
"neka-nat/inv_rl" ["l"="57.554,18.43"]
"aravindsiv/irl-lab" ["l"="57.541,18.484"]
"chagmgang/tf2.0_reinforcement_learning" ["l"="57.925,18.138"]
"brianspiering/rl-course" ["l"="47.809,26.8"]
"createamind/DRL" ["l"="58.047,18.059"]
"geyang/ml-logger" ["l"="58.103,18.192"]
"justinjfu/doodad" ["l"="58.003,18.222"]
"janosh/tensorboard-reducer" ["l"="57.716,18.51"]
"JonathanTay/CS-7641-assignment-3" ["l"="57.229,17.526"]
"eternalmothra/ml_cheat_sheet" ["l"="57.262,17.543"]
"JonathanTay/CS-7641-assignment-4" ["l"="57.242,17.531"]
"JonathanTay/CS-7641-assignment-2" ["l"="57.248,17.546"]
"hhexiy/opponent" ["l"="57.686,18.538"]
"stacyste/TheoryOfMindInferenceModels" ["l"="57.955,18.617"]
"clbaker/BToM" ["l"="57.928,18.584"]
"julianje/Bishop" ["l"="57.952,18.602"]
"CILAB-MA/Machine_ToM" ["l"="57.975,18.651"]
"mbaske/ml-drone-collection" ["l"="57.285,19.076"]
"pochl/dqn-uav" ["l"="60.266,14.142"]
"dracolytch/ML-Simplest-Scenario" ["l"="57.287,19.109"]
"IdreesInc/TetNet" ["l"="57.217,17.742"]
"manuelsh/minerl-docker" ["l"="57.46,18.44"]
"amiranas/minerl_imitation_learning" ["l"="57.402,18.488"]
"kimbring2/minecraft_ai" ["l"="57.381,18.522"]
"MichalOp/MineRL2020" ["l"="57.415,18.469"]
"AdrianHsu/breakout-Deep-Q-Network" ["l"="57.337,17.723"]
"sebtheiler/tutorials" ["l"="57.351,17.769"]
"TianhongDai/distributed-ppo" ["l"="57.354,18.084"]
"alexis-jacq/Pytorch-DPPO" ["l"="57.387,18.133"]
"thanard/causal-infogan" ["l"="57.27,18.647"]
"thanard/hallucinative-topological-memory" ["l"="57.252,18.671"]
"ganlumomo/minicheetah-traversability-irl" ["l"="57.656,18.461"]
"xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning" ["l"="57.503,17.73"]
"apachecn/ucb-cs294-112-notes-zh" ["l"="57.877,17.707"]
"apachecn/stanford-cs224n-notes-zh" ["l"="53.659,27.037"]
"EvoEsports/EvoSC" ["l"="58.366,19.222"]
"PyPlanet/PyPlanet" ["l"="58.4,19.242"]
"reaby/TMModeTemplate" ["l"="58.346,19.221"]
"ManiaControl/ManiaControl" ["l"="58.378,19.239"]
"snixtho/clifga" ["l"="58.349,19.235"]
"EvoEsports/gbxclient-node" ["l"="58.351,19.25"]
"EvoEsports/EvoSC-sharp" ["l"="58.365,19.249"]
"ymd-h/cpprb" ["l"="6.181,-41.244"]
"DeepX-inc/machina" ["l"="5.86,-41.171"]
"tensorlayer/RLzoo" ["l"="-55.202,-14.281"]
"hiwonjoon/ICML2019-TREX" ["l"="57.65,18.573"]
"dsbrown1331/CoRL2019-DREX" ["l"="57.645,18.539"]
"google-deepmind/dqn" ["l"="57.418,18.169"]
"devsisters/DQN-tensorflow" ["l"="57.381,18.072"]
"Parsa33033/Deep-Reinforcement-Learning-DQN" ["l"="57.528,18.065"]
"gouxiangchen/dueling-DQN-pytorch" ["l"="57.411,18.008"]
"hiive/hiivemdptoolbox" ["l"="57.286,17.472"]
"lweitkamp/option-critic-pytorch" ["l"="57.364,18.472"]
"martius-lab/HiTS" ["l"="57.371,18.561"]
"manaski/MARL" ["l"="57.966,18.083"]
"mailgyc/doudizhu" ["l"="58.325,18.473"]
"rcsoccersim/manual" ["l"="56.896,18.645"]
"Cyrus2D/Pyrus-SS2D-Base" ["l"="56.908,18.627"]
"rcsoccersim/rcssmonitor" ["l"="56.916,18.649"]
"PacktPublishing/Keras-Reinforcement-Learning-Projects" ["l"="57.71,18.13"]
"BazkieBumpercar/Blunted2" ["l"="58.178,18.048"]
"vi3itor/GameplayFootball" ["l"="58.233,18.028"]
"OussemaMaatouk/Unity_FootBall" ["l"="58.145,18.053"]
"idthanm/env_build" ["l"="58.137,18.189"]
"idthanm/mpg" ["l"="58.087,18.202"]
"mahaitongdae/safe_exp_env" ["l"="58.165,18.18"]
"Kchu/LifelongRL" ["l"="57.794,18.507"]
"BY571/IQN-and-Extensions" ["l"="57.837,18.491"]
"xmfbit/DQN-FlappyBird" ["l"="57.85,17.888"]
"ermongroup/InfoGAIL" ["l"="57.713,18.293"]
"sen-pai/InfoGAIL" ["l"="57.923,18.195"]
"alversafa/option-critic-arch" ["l"="57.327,18.495"]
"mklissa/PPOC" ["l"="57.341,18.472"]
"hzm2016/option-critic-pytorch" ["l"="57.307,18.528"]
"younggyoseo/Ape-X" ["l"="57.511,18.503"]
"jingweiz/pytorch-distributed" ["l"="57.516,18.483"]
"GallagherAiden/footballSimulationEngine" ["l"="58.396,17.941"]
"atas76/openengine" ["l"="58.362,17.967"]
"ZOXEXIVO/open-football" ["l"="58.403,17.959"]
"ElliotJBall/FootballManagerSimulator" ["l"="58.42,17.94"]
"ihofmann/open-websoccer" ["l"="58.439,17.948"]
"bat67/awesome-deep-learning-and-machine-learning-questions" ["l"="50.464,28.342"]
"mengwanglalala/RL-algorithms" ["l"="59.659,15.016"]
"MG2033/A2C" ["l"="57.279,18.167"]
"pemami4911/deep-rl" ["l"="57.334,18.183"]
"mbaske/robot-ants" ["l"="57.28,18.996"]
"mbaske/ml-table-football" ["l"="57.279,19.014"]
"mbaske/ml-eve" ["l"="57.261,19.056"]
"mbaske/ml-audio-sensor" ["l"="57.248,19.037"]
"rlworkgroup/gym-sawyer" ["l"="57.53,18.569"]
"sisl/hgail" ["l"="57.465,18.39"]
"lisidi/SLPlayer" ["l"="0.634,12.297"]
"kanakkabara/Autonomous-Drifting" ["l"="57.119,18.088"]
"caipeide/drift_drl" ["l"="57.044,18.075"]
"harvitronix/reinforcement-learning-car" ["l"="57.258,18.129"]
"angloth/auto-drift" ["l"="57.056,18.063"]
"neka-nat/distributed_rl" ["l"="57.533,18.542"]
"jinbeizame007/pytorch-r2d2-DPG" ["l"="57.523,18.618"]
"naderzare/CYRUS2014" ["l"="56.927,18.637"]
"google-research/batch_rl" ["l"="59.44,17.593"]
"microsoft/MazeExplorer" ["l"="57.032,18.313"]
"agiantwhale/NavDoom" ["l"="57.071,18.304"]
"haje01/distper" ["l"="57.504,18.545"]
"ying-wen/malib_deprecated" ["l"="57.937,18.44"]
"jangirrishabh/HER-learn-InverseKinematics" ["l"="57.193,18.472"]
"cychai1995/DDPGfD" ["l"="57.174,18.471"]
"vivek3141/sql-injection-demo" ["l"="56.878,18.406"]
"ShanHaoYu/Deep-Q-Network-Breakout" ["l"="57.327,17.673"]
"wetliu/dqn_pytorch" ["l"="57.31,17.685"]
"createamind/Distributed-DRL" ["l"="58.11,18.022"]
"mcmachado/count_based_exploration_sr" ["l"="56.994,18.245"]
"bonniesjli/DQN_SR" ["l"="57.041,18.242"]
"fry404006308/-Learning-materials-" ["l"="57.82,18.559"]
"fry404006308/IT_book" ["l"="57.802,18.487"]
"kkhetarpal/ioc" ["l"="57.33,18.464"]
"wisnunugroho21/reinforcement_learning_ppo_rnd" ["l"="57.265,18.484"]
"AllenThomasDev/Football-Simulator" ["l"="58.422,17.967"]
"sadeqa/Super-Mario-Bros-RL" ["l"="57.045,18.266"]
"shakenes/vizdoomgym" ["l"="57.047,18.289"]
"KornbergFresnel/CommNet" ["l"="57.781,18.426"]
"johannah/bootstrap_dqn" ["l"="57.857,18.628"]
"JoungheeKim/bootsrapped-dqn" ["l"="57.866,18.659"]
"jeanharb/a2oc_delib" ["l"="57.333,18.48"]
"ronsailer/A2OC_A2C" ["l"="57.309,18.51"]
"HumanCompatibleAI/overcooked-hAI-exp" ["l"="57.881,18.495"]
"hsvgbkhgbv/shapley-q-learning" ["l"="57.944,18.494"]
"HumanCompatibleAI/population-irl" ["l"="57.562,18.51"]
"tesslerc/ActionRobustRL" ["l"="57.53,18.762"]
"tuomaso/radial_rl_v2" ["l"="57.538,18.774"]
"nikhilbarhate99/Deterministic-GAIL-PyTorch" ["l"="57.494,18.524"]
"jatinarora2702/gail-pytorch" ["l"="57.497,18.475"]
"zoli333/Weight-Normalization" ["l"="57.083,18.613"]
"victorcampos7/weightnorm-init" ["l"="57.061,18.632"]
"Kautenja/playing-mario-with-deep-reinforcement-learning" ["l"="57.071,18.36"]
"robmsylvester/Super-Mario-Bros-DQN" ["l"="57.136,18.333"]
"CORE-Robotics-Lab/SSRR" ["l"="57.64,18.555"]
"pawan47/Fetchreach_gym" ["l"="58.086,18.598"]
"papkov/pommerman-x" ["l"="57.743,18.52"]
"dist1ll/pomcpp" ["l"="57.741,18.532"]
"TheAILearner/Snake-Game-with-Deep-learning" ["l"="56.903,17.951"]
"TheAILearner/Training-Snake-Game-With-Genetic-Algorithm" ["l"="56.939,17.96"]
"mwydmuch/PyOblige" ["l"="57.116,18.292"]
"crowdAI/vizdoom2018-singleplayer-starter-kit" ["l"="57.098,18.305"]
"yunjhongwu/Double-DQN-Breakout" ["l"="57.322,17.645"]
"mohitsharma0690/DirectedInfo-GAIL" ["l"="57.996,18.161"]
"sharma-arjun/DirectedInfo-GAIL" ["l"="58.023,18.152"]
"crowdAI/vizdoom2018-multiplayer-starter-kit" ["l"="57.071,18.324"]
"mihahauke/dqn_vizdoom_theano" ["l"="57.058,18.32"]
"HumanCompatibleAI/atari-irl" ["l"="57.554,18.496"]
"erwanbou/sf-deep-rl" ["l"="57.112,18.242"]
"AndreaTirinzoni/iw-transfer-rl" ["l"="57.078,18.243"]
"mike-gimelfarb/deep-successor-features-for-transfer" ["l"="57.064,18.238"]
"tesslerc/Sparse-IL" ["l"="57.414,18.737"]
"ray-project/ray" ["l"="50.74,29.255"]
"google-deepmind/rlax" ["l"="59.294,17.47"]
"google-deepmind/reverb" ["l"="23.648,14.816"]
"google-deepmind/dm-haiku" ["l"="21.721,14.052"]
"facebookresearch/minihack" ["l"="57.626,18.459"]
"facebookresearch/moolib" ["l"="57.619,18.488"]
"heiner/nle" ["l"="57.598,18.522"]
"eloialonso/iris" ["l"="59.285,17.557"]
"Farama-Foundation/Gymnasium-Robotics" ["l"="59.531,16.42"]
"datawhalechina/leedl-tutorial" ["l"="50.951,28.143"]
"MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning" ["l"="57.881,17.963"]
"datawhalechina/pumpkin-book" ["l"="50.866,28.156"]
"flint-xf-fan/Byzantine-Federated-RL" ["l"="58.614,17.824"]
"siomvas/awesome-federated-reinforcement-learning" ["l"="58.573,17.835"]
"liamhebert/FedFormer" ["l"="58.653,17.812"]
"pengyang7881187/FedRL" ["l"="58.604,17.843"]
"henry-prior/jax-rl" ["l"="57.918,18.512"]
"SforAiDl/genrl" ["l"="-3.761,23.203"]
"toshikwa/rljax" ["l"="57.901,18.492"]
"PWhiddy/PokemonRedExperiments" ["l"="57.965,17.96"]
"Baekalfen/PyBoy" ["l"="57.907,18.086"]
"spdustin/ChatGPT-AutoExpert" ["l"="40.652,0.462"]
"joonspk-research/generative_agents" ["l"="40.913,-4.167"]
"a16z-infra/ai-town" ["l"="40.967,-4.069"]
"MineDojo/Voyager" ["l"="36.62,-2.191"]
"LazyVim/LazyVim" ["l"="-13,-9.434"]
"pagefaultgames/pokerogue" ["l"="-12.124,1.482"]
"roboflow/supervision" ["l"="40.654,-0.062"]
"pret/pokered" ["l"="-15.438,1.6"]
"OpenBMB/ChatDev" ["l"="40.464,0.204"]
"delight-im/OpenSoccer" ["l"="58.467,17.931"]
"louisnino/RLcode" ["l"="57.808,18.091"]
"PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook" ["l"="57.786,18.126"]
"facebookresearch/level-replay" ["l"="57.604,18.538"]
"ngoodger/nle-language-wrapper" ["l"="57.597,18.58"]
"JBLanier/pipeline-psro" ["l"="58.212,18.204"]
"indylab/nxdo" ["l"="58.237,18.189"]
"diversepsro/diverse_psro" ["l"="58.271,18.176"]
"JBLanier/stratego_env" ["l"="58.26,18.195"]
"aicenter/openspiel_reproductions" ["l"="58.169,18.215"]
"indylab/tabular_xdo" ["l"="58.237,18.202"]
"npvoid/OnlineDoubleOracle" ["l"="58.22,18.196"]
"datamllab/awesome-game-ai" ["l"="58.069,18.329"]
"datamllab/rlcard-showdown" ["l"="58.164,18.371"]
"kwai/DouZero" ["l"="58.114,18.27"]
"datamllab/rlcard-tutorial" ["l"="58.131,18.393"]
"EricSteinberger/PokerRL" ["l"="58.688,18.667"]
"happypepper/DeepHoldem" ["l"="58.679,18.68"]
"Agony5757/mahjong" ["l"="-24.083,-34.162"]
"datamllab/autovideo" ["l"="58.165,18.426"]
"daochenzha/rapid" ["l"="58.086,18.429"]
"Turing-Project/WriteGPT" ["l"="58.288,18.298"]
"Morizeyao/GPT2-Chinese" ["l"="53.251,27.165"]
"imcaspar/gpt2-ml" ["l"="53.328,27.202"]
"babysor/MockingBird" ["l"="38.231,1.347"]
"yangjianxin1/GPT2-chitchat" ["l"="53.296,27.144"]
"Turing-Project/EssayTopicPredictV2" ["l"="58.356,18.312"]
"kangvcar/InfoSpider" ["l"="48.231,23.344"]
"brightmart/nlp_chinese_corpus" ["l"="53.296,27.242"]
"menzi11/BullshitGenerator" ["l"="-4.962,17.715"]
"thunlp/WantWords" ["l"="-48.77,12.605"]
"ssssssss-team/spider-flow" ["l"="-3.689,11.552"]
"FengQuanLi/ResnetGPT" ["l"="58.219,18.274"]
"Turing-Project/AntiFraudChatBot" ["l"="38.587,1.84"]
"Baiyuetribe/paper2gui" ["l"="-48.719,12.649"]
"Wechat-ggGitHub/Awesome-GitHub-Repo" ["l"="-4.641,17.539"]
"facebookresearch/rela" ["l"="57.574,18.522"]
"chauby/V-REP-YouBot-Demo" ["l"="58.318,18.766"]
"chauby/PyDMPs_Chauby" ["l"="59.828,16.249"]
"xuhuairuogu/V-REP-Simulation-Projects" ["l"="60.576,11.851"]
"chauby/CoppeliaSimRL" ["l"="58.328,18.797"]
"chauby/BipedalWalkingRobots" ["l"="61.195,16.203"]
"wangshusen/SearchEngine" ["l"="58.346,23.449"]
"DeepRLChinese/DeepRL-Chinese" ["l"="57.93,18.047"]
"XinJingHao/DRL-Pytorch" ["l"="57.884,18.057"]
"vincen-github/mlimpl" ["l"="57.957,18.01"]
"subprotocol/verlet-js" ["l"="35.972,24.583"]
"wagenaartje/neataptic" ["l"="-32.582,-35.95"]
"mkmarek/forex.analytics" ["l"="56.852,17.643"]
"Tom-Alexander/regression-js" ["l"="-5.161,-41.619"]
"gekkowarez/gekkoga" ["l"="-9.576,10.172"]
"panchishin/geneticalgorithm" ["l"="56.883,17.635"]
"lagodiuk/genetic-algorithm" ["l"="49.889,25.843"]
"uoe-agents/robotic-warehouse" ["l"="58.01,18.386"]
"uoe-agents/task-assignment-robotic-warehouse" ["l"="57.971,18.406"]
"Cognitive-AI-Systems/pogema" ["l"="-44.194,26.421"]
"Jiaoyang-Li/RHCR" ["l"="61.955,13.426"]
"proroklab/VectorizedMultiAgentSimulator" ["l"="57.908,18.336"]
"LyapunovJingci/Warehouse_Robot_Path_Planning" ["l"="62.031,13.507"]
"marmotlab/PRIMAL2" ["l"="62.002,13.423"]
"gsartoretti/PRIMAL" ["l"="61.978,13.451"]
"instadeepai/og-marl" ["l"="58.008,18.371"]
"atb033/multi_agent_path_planning" ["l"="61.852,13.456"]
"araffin/sbx" ["l"="59.34,17.444"]
"MarcoMeter/recurrent-ppo-truncated-bptt" ["l"="59.167,17.557"]
"tshrjn/env-zoo" ["l"="57.714,18.311"]
"Allenpandas/Reinforcement-Learning-Papers" ["l"="59.565,17.671"]
"philtabor/Advanced-Replay-Strategies" ["l"="57.747,18.239"]
"philtabor/ProtoRL" ["l"="57.768,18.328"]
"Zuox99/Deep-Reinforcement-Learning-Based-Resource-Provisioning-and-Task-Scheduling-for-Cloud-Service-Provid" ["l"="63.101,-13.35"]
"openai/mujoco-worldgen" ["l"="57.689,18.312"]
"openfootmanager/openfootmanager" ["l"="58.383,17.981"]
"esports-manager/esports-manager" ["l"="58.416,17.989"]
"kieran-murphy/footballgame" ["l"="58.403,17.997"]
"octavio-santiago/Super-Mario-Land-AI" ["l"="57.003,18.012"]
"aleju/mario-ai" ["l"="57.198,18.111"]
"Farama-Foundation/stable-retro" ["l"="56.916,18.013"]
"mdeib/berkeley-deep-RL-pytorch-solutions" ["l"="57.468,17.703"]
"mdeib/berkeley-deep-RL-pytorch-starter" ["l"="57.451,17.722"]
"erfanMhi/Deep-Reinforcement-Learning-CS285-Pytorch" ["l"="57.487,17.712"]
"vamsianumula/cs285-deeprl-ucberkeley-2021" ["l"="57.452,17.67"]
"NVlabs/cule" ["l"="23.679,14.749"]
"coreylynch/async-rl" ["l"="57.327,18.126"]
"muupan/async-rl" ["l"="57.324,18.154"]
"Lasagne/Lasagne" ["l"="47.886,29.002"]
"vikashplus/robohive" ["l"="59.485,16.491"]
"aravindr93/mjrl" ["l"="59.375,17.672"]
"uoe-agents/seac" ["l"="58.027,18.382"]
"BY571/FQF-and-Extensions" ["l"="57.832,18.531"]
"xtma/dsac" ["l"="57.89,18.389"]
"BY571/QR-DQN" ["l"="57.845,18.526"]
"openai/phasic-policy-gradient" ["l"="57.456,18.477"]
"openai/gym3" ["l"="57.52,18.463"]
"dongminlee94/Samsung-DRL-Code" ["l"="-4.756,-23.138"]
"bhairavmehta95/data-efficient-hrl" ["l"="57.528,18.489"]
"CherryPieSexy/imitation_learning" ["l"="57.659,18.414"]
"hcnoh/gail-pytorch" ["l"="57.606,18.448"]
"yunke-wang/WGAIL" ["l"="57.655,18.506"]
"salesforce/ai-economist" ["l"="57.896,18.362"]
"salesforce/warp-drive" ["l"="57.9,18.409"]
"antontarasenko/awesome-economics" ["l"="42.737,26.81"]
"AB-CE/abce" ["l"="40.892,24.112"]
"jofmi/agentpy" ["l"="40.939,24.093"]
"py-why/EconML" ["l"="43.677,25.61"]
"davidrpugh/pyeconomics" ["l"="42.682,26.805"]
"EconForge/dolo.py" ["l"="42.654,26.749"]
"KennethJudd/CompEcon2020" ["l"="42.709,26.784"]
"projectmesa/mesa" ["l"="40.991,24.101"]
"jidiai/TaxAI" ["l"="58.101,18.317"]
"JohannesAck/MATD3implementation" ["l"="58.016,18.169"]
"MJ10/matd3-pytorch" ["l"="58.05,18.157"]
"Skylark0924/Reinforcement-Learning-in-Robotics" ["l"="61.088,16.477"]
"vietnh1009/Contra-PPO-pytorch" ["l"="57.594,18.22"]
"Skylarking/MARL" ["l"="58.046,18.211"]
"verystrongjoe/qmix" ["l"="58.032,18.206"]
"yangchen1997/Multi-Agent-Reinforcement-Learning" ["l"="57.952,18.218"]
"jianzhnie/deep-marl-toolkit" ["l"="58.134,18.161"]
"fry404006308/fry_course_materials" ["l"="57.775,18.375"]
"lansinuote/Simple_Reinforcement_Learning" ["l"="57.981,18.049"]
"bubbliiiing/Siamese-pytorch" ["l"="48.113,21.989"]
"berkeleydeeprlcourse/homework_fall2019" ["l"="57.475,17.839"]
"microsoft/maro" ["l"="57.947,18.239"]
"microsoft/FOST" ["l"="58.029,18.23"]
"hubbs5/or-gym" ["l"="50.96,26.666"]
"ANL-CEEESA/MIPLearn" ["l"="50.915,26.66"]
"ai4co/rl4co" ["l"="51.089,26.609"]
"hongzimao/decima-sim" ["l"="63.176,-13.357"]
"Hanjun-Dai/graph_comb_opt" ["l"="50.963,26.642"]
"ds4dm/ecole" ["l"="50.932,26.643"]
"khalil-research/PyEPO" ["l"="50.846,25.907"]
"pemami4911/neural-combinatorial-rl-pytorch" ["l"="50.991,26.657"]
"huangwl18/modular-rl" ["l"="57.295,18.692"]
"yobibyte/amorpheus" ["l"="57.232,18.778"]
"pythonlessons/RL-Bitcoin-trading-bot" ["l"="-9.501,14.826"]
"pythonlessons/FinRock" ["l"="-9.476,14.856"]
"sergioabreu-g/active-ragdolls" ["l"="57.099,19.21"]
"ashleve/ActiveRagdoll" ["l"="57.065,19.278"]
"dci05049/Active-Ragdoll-Tutorial-Unity" ["l"="57.122,19.206"]
"davidkimighty/TARS" ["l"="57.072,19.251"]
"tiredamage42/DynamicRagdoll" ["l"="57.122,19.235"]
"hairibar/Hairibar.Ragdoll" ["l"="57.09,19.266"]
"Tvtig/UnityLightsaber" ["l"="57.066,19.21"]
"dgreenheck/OpenFracture" ["l"="-26.051,-34.985"]
"m4ttsch/omscs-notes" ["l"="57.163,17.151"]
"m4ttsch/omscs-notes-notes" ["l"="57.171,17.179"]
"trackmania-rl/tmrl" ["l"="58.182,18.902"]
"AndrejGobeX/TrackMania_AI" ["l"="58.205,18.946"]
"yannbouteiller/rtgym" ["l"="58.173,18.935"]
"Linesight-RL/linesight" ["l"="58.241,19"]
"yannbouteiller/vgamepad" ["l"="58.232,18.926"]
"AgileRL/AgileRL" ["l"="59.371,17.441"]
"coax-dev/coax" ["l"="57.989,18.6"]
"learn-to-race/l2r" ["l"="61.841,15.307"]
"222464/ERL" ["l"="57.103,18.001"]
"Oladipupo/Active-Ragdolls-Unity" ["l"="57.045,19.301"]
"TildeAsterisk/Physicanim" ["l"="57.035,19.281"]
"AitorSimona/Traverser" ["l"="57.085,19.298"]
"Sopiro/Unity-Procedural-Animation" ["l"="57.032,19.344"]
"Jingliang-Duan/DSAC-v1" ["l"="57.992,18.247"]
"enjeeneer/sutton_and_barto" ["l"="57.782,17.751"]
"archsyscall/DistRL-TensorFlow2" ["l"="57.869,18.122"]
"Farama-Foundation/MicroRTS-Py" ["l"="58.707,17.928"]
"Farama-Foundation/MicroRTS" ["l"="58.72,17.985"]
"Vincentzyx/Douzero_Resnet" ["l"="58.2,18.362"]
"datamllab/BED_main" ["l"="58.148,18.444"]
"bupticybee/AlphaNLHoldem" ["l"="58.257,18.387"]
"Vincentzyx/DouZero_For_HLDDZ_FullAuto" ["l"="58.185,18.339"]
"munanfan/Mahjong_Game_RLCard_RL" ["l"="-23.921,-34.135"]
"submit-paper/Danzero_plus" ["l"="58.237,18.371"]
"ZDZX-T/cardRecorder" ["l"="58.196,18.383"]
"rraileanu/idaac" ["l"="59.414,17.732"]
"lucidrains/ppo" ["l"="57.445,18.557"]
"KamyarGh/rl_swiss" ["l"="57.616,18.44"]
"jr-robotics/robo-gym" ["l"="59.612,16.347"]
"facebookresearch/jps" ["l"="57.794,18.578"]
"facebookresearch/off-belief-learning" ["l"="57.792,18.593"]
"daochenzha/Meta-AAD" ["l"="58.132,18.434"]
"ynchuang/DiscoverPath" ["l"="58.175,18.463"]
"Ackeraa/snake" ["l"="56.963,17.956"]
"aronsar/hoad" ["l"="57.816,18.596"]
"borninfreedom/DeepLearning" ["l"="57.867,17.785"]
"YJLAugus/Reinforcement-Learning-Notes" ["l"="57.921,17.713"]
"rosewang2008/rmaddpg" ["l"="57.856,18.338"]
"deligentfool/HAVEN" ["l"="57.415,18.527"]
"davide97l/rl-policies-attacks-defenses" ["l"="57.548,18.81"]
"PavelCz/rl-adversarial-attack" ["l"="57.547,18.839"]
"cyanrain7/TRPO-in-MARL" ["l"="57.934,18.269"]
"oxwhirl/facmac" ["l"="57.972,18.316"]
"PKU-MARL/Multi-Agent-Transformer" ["l"="57.92,18.276"]
"PKU-MARL/DexterousHands" ["l"="61.205,16.476"]
"TonghanWang/RODE" ["l"="57.972,18.382"]
"chenhongge/SA_DQN" ["l"="57.553,18.781"]
"huanzhang12/SA_PPO" ["l"="57.574,18.792"]
"liuzuxin/safe-rl-robustness" ["l"="57.571,18.81"]
"umd-huang-lab/WocaR-RL" ["l"="57.537,18.792"]
"jwk1rose/RL_Learning" ["l"="57.981,17.999"]
"QiangLong2017/Deep-Reiforcement-Learning" ["l"="58.016,17.958"]
"fangvv/VN-MADDPG" ["l"="55.551,4.797"]
"ClownW/Reinforcement-learning-with-PyTorch" ["l"="58.043,18.006"]
"lansinuote/More_Simple_Reinforcement_Learning" ["l"="58.001,18.018"]
"The-Firexx/trackmania2020apidocumentation" ["l"="58.441,19.2"]
"breeku/opentrackmania" ["l"="58.46,19.21"]
"breeku/trackmania-api-node" ["l"="58.465,19.194"]
"codecat/tm-dashboard" ["l"="58.399,19.199"]
"BigBang1112/gbx-net" ["l"="58.335,19.143"]
"BigBang1112/nations-converter" ["l"="58.353,19.126"]
"skyslide22/blendermania-addon" ["l"="58.311,19.163"]
"donadigo/TMInterfaceClientPython" ["l"="58.366,19.159"]
"Electron-x/GbxDump" ["l"="58.363,19.142"]
"TwinkieTweaks/Twinkie" ["l"="58.314,19.142"]
"ArkadySK/GbxMapBrowser" ["l"="58.329,19.182"]
"resir014/TMViz" ["l"="58.344,19.164"]
"HosnLS/Hierarchical-Language-Agent" ["l"="57.866,18.514"]
"Stanford-ILIAD/Diverse-Conventions" ["l"="57.879,18.507"]
"Farama-Foundation/Multi-Agent-ALE" ["l"="57.918,18.39"]
"Farama-Foundation/hanabi-learning-environment" ["l"="57.917,18.378"]
"Farama-Foundation/AutoROM" ["l"="57.935,18.391"]
"uoe-agents/seps" ["l"="57.906,18.386"]
"stevenxchung/OMSCS-Notes" ["l"="57.18,17.216"]
"Denys88/rl_games" ["l"="61.175,16.486"]
"microsoft/FQF" ["l"="57.843,18.558"]
"jiayu-ch15/Variational-Automatic-Curriculum-Learning" ["l"="57.886,18.6"]
"facebookresearch/CollaQ" ["l"="57.952,18.429"]
"TonghanWang/DOP" ["l"="57.981,18.418"]
"MAS-anony/ASN" ["l"="57.986,18.477"]
"Git-123-Hub/maddpg-pettingzoo-pytorch" ["l"="57.918,18.222"]
"kngwyu/Rainy" ["l"="57.331,18.527"]
"AmazingAng/WTF-DeepRL" ["l"="57.83,18.184"]
"jmichaux/dqn-pytorch" ["l"="-49.361,13.911"]
"feidieufo/homework" ["l"="57.907,18.165"]
"florist-notes/CS234_RL" ["l"="57.891,17.522"]
"Mauriyin/FLDRL-in-Wireless-Communication" ["l"="58.592,17.804"]
"PeymanTehrani/FDRL-PC-Dyspan" ["l"="58.607,17.784"]
"YidingYu/DLMA" ["l"="55.618,4.76"]
"Code-Bullet/TheBigCB.com" ["l"="56.783,17.805"]
"Jingliang-Duan/DSAC-v2" ["l"="57.954,18.167"]
"openai/kubernetes" ["l"="57.17,18.68"]
"PacktPublishing/Deep-Reinforcement-Learning-with-Python" ["l"="57.87,17.906"]
"Apress/deep-reinforcement-learning-python" ["l"="57.924,17.843"]
"PacktPublishing/Mastering-Reinforcement-Learning-with-Python" ["l"="57.911,17.858"]
"PhilS94/Unity-Procedural-IK-Wall-Walking-Spider" ["l"="57.041,19.377"]
"timi-ty/procedural-animation" ["l"="57.022,19.365"]
"lchaumartin/SpiderProceduralAnimation" ["l"="57.01,19.381"]
"IouJenLiu/CMAE" ["l"="58.057,18.481"]
"yalidu/liir" ["l"="58.027,18.479"]
"TonghanWang/NDQ" ["l"="57.993,18.434"]
"AutodeskRoboticsLab/RLRoboticAssembly" ["l"="58.539,18.908"]
"guodashun/peg-in-hole-gym" ["l"="58.495,18.857"]
"arrival-ltd/catalyst-rl-tutorial" ["l"="58.474,18.868"]
"xieliang555/SFN" ["l"="58.512,18.852"]
"Scitator/catalyst-rl-framework" ["l"="58.454,18.89"]
"The132/franka-panda" ["l"="58.474,18.887"]
"jhaardt/wrs" ["l"="60.628,11.856"]
"Henry1iu/ierg5350_rl_course_project" ["l"="58.498,18.915"]
"manikandan-ravikiran/HCI_Notes" ["l"="57.19,17.243"]
"jgajera/CS6750-Human-Computer-Interaction" ["l"="57.2,17.274"]
"BattleDawnNZ/ProceduralAnimation" ["l"="57.004,19.358"]
"berkeleydeeprlcourse/homework_fall2020" ["l"="57.469,17.777"]
"berkeleydeeprlcourse/homework_fall2021" ["l"="57.438,17.69"]
"silencial/DeepRL" ["l"="57.442,17.788"]
"huawei-noah/SMARTS" ["l"="62.402,12.283"]
"tencent-ailab/TLeague" ["l"="58.073,18.251"]
"tencent-ailab/tleague_projpage" ["l"="58.032,18.282"]
"0b01/CommNet" ["l"="57.83,18.46"]
"ziangqin-stu/rl_hiro" ["l"="57.401,18.522"]
"AIcrowd/neurips2020-procgen-starter-kit" ["l"="57.476,18.543"]
"mantle2048/rlplot" ["l"="57.976,18.187"]
"LxzGordon/Deep-Reinforcement-Learning-with-pytorch" ["l"="57.373,17.919"]
"chscheller/minerl_agent" ["l"="57.364,18.535"]
"cog-isa/forger" ["l"="57.353,18.566"]
"AndyYue1893/Reinforcement-learning-with-tensorflow" ["l"="58.008,18.056"]
"AndyYue1893/Hands-On-Reinforcement-Learning-With-Python" ["l"="58.022,18.047"]
"BY571/Munchausen-RL" ["l"="57.795,18.202"]
"toshikwa/sac-discrete.pytorch" ["l"="59.465,17.544"]
"toshikwa/soft-actor-critic.pytorch" ["l"="59.45,17.557"]
"mhauskn/dqn" ["l"="57.19,18.091"]
"sailorsb/caffe-parallel" ["l"="57.72,23.52"]
"5vision/DARQN" ["l"="57.236,18.157"]
"aiworld/dqn" ["l"="57.173,18.074"]
"wendelinboehmer/dcg" ["l"="57.974,18.436"]
"sisl/DICG" ["l"="57.955,18.457"]
"rocanaan/hanabi-ad-hoc-learning" ["l"="57.826,18.627"]
"jlm429/bettermdptools" ["l"="57.253,17.408"]
"shacklettbp/bps-nav" ["l"="57.635,18.645"]
"shacklettbp/bps3D" ["l"="57.622,18.645"]
"pytorch/workshops" ["l"="57.212,17.378"]
"LucasCJYSDL/HierAIRL" ["l"="57.655,18.446"]
"philtabor/Advanced-Actor-Critic-Methods" ["l"="57.737,18.243"]
"kestasjk/webDiplomacy" ["l"="57.959,18.699"]
"Sleepcap/vDiplomacy" ["l"="57.971,18.729"]
"spamguy/dipl.io" ["l"="57.981,18.747"]
"siemanko/guided-policy-search" ["l"="57.292,18.354"]
"marcino239/gps" ["l"="57.258,18.371"]
"robotsorcerer/gps" ["l"="57.242,18.379"]
"google-deepmind/multi_object_datasets" ["l"="48.923,32.434"]
"analoganddigital/sekiro_tensorflow" ["l"="58.473,18.245"]
"analoganddigital/DQN_play_sekiro" ["l"="58.457,18.265"]
"ricagj/pysekiro_with_RL" ["l"="58.439,18.249"]
"valsamovich/omscs" ["l"="57.156,17.204"]
"hahayonghuming/VDACs" ["l"="58.056,18.44"]
"simsimiSION/pymarl-algorithm-extension-via-starcraft" ["l"="58.084,18.465"]
"deligentfool/policy_based_RL" ["l"="57.767,18.473"]
"XiaoxiaoGuo/rcdqn" ["l"="57.414,18.706"]
"yfletberliac/adversarially-guided-actor-critic" ["l"="57.703,18.598"]
"tianjunz/NovelD" ["l"="57.73,18.588"]
"facebookresearch/e3b" ["l"="57.713,18.582"]
"sparisi/cbet" ["l"="57.73,18.571"]
"tedmoskovitz/TOP" ["l"="57.724,18.602"]
"KristinLague/Mesh-Cutting" ["l"="57.037,19.214"]
"IBM/LOA" ["l"="-1.733,-41.469"]
"IouJenLiu/PIC" ["l"="58.095,18.506"]
"IouJenLiu/HTS-RL" ["l"="58.127,18.532"]
"shariqiqbal2810/REFIL" ["l"="57.999,18.403"]
"QDPP-GitHub/QDPP" ["l"="58.013,18.442"]
"leox1v/FirstTextWorldProblems" ["l"="57.391,18.808"]
"pvl/CogniTextWorldAgent" ["l"="57.397,18.778"]
"karanamrahul/Autonomous-Drifting-using-deep-Reinforcement-Learning" ["l"="57.034,18.057"]
"mcemilg/min-carla-env" ["l"="56.994,18.059"]
"YoungYoung619/reinforcement-learning-based-driving-decision-in-Carla" ["l"="62.481,12.391"]
"caipeide/VTGNet" ["l"="56.998,18.075"]
"jlm429/pyperch" ["l"="57.278,17.421"]
"knakamura13/mlrose-ky" ["l"="57.282,17.397"]
"gouxiangchen/soft-Q-learning" ["l"="57.594,18.475"]
"knagaitsev/tetris-ai" ["l"="57.127,17.838"]
"Cranial-XIX/marl-copa" ["l"="58.036,18.433"]
"zyfsjycc/GoMARL" ["l"="58.037,18.448"]
"pokaxpoka/sunrise" ["l"="57.876,18.696"]
"Miffyli/minecraft-bc" ["l"="57.346,18.554"]
"pranav-s/Stanford_CS234_RL_2021" ["l"="57.903,17.55"]
"openai/prometheus" ["l"="57.059,18.688"]
"openai/robot_controllers" ["l"="57.09,18.67"]
"mbaske/ml-dogfight" ["l"="57.244,19.088"]
"stanford-iprl-lab/multimodal_representation" ["l"="58.512,18.938"]
"princeton-nlp/blindfold-textgame" ["l"="57.428,18.737"]
"wsjeon/multiagent-gail" ["l"="57.679,18.589"]
"yangmuzhi/airl" ["l"="57.667,18.594"]
"MadryLab/implementation-matters" ["l"="57.985,18.532"]
"implementation-matters/code-for-paper" ["l"="58.01,18.569"]
"bmazoure/ppo_jax" ["l"="57.957,18.563"]
"ethanluoyc/magi" ["l"="57.94,18.551"]
"dayekuaipao/ierg6130-assignment" ["l"="57.877,17.882"]
"Aguin/cuhkrlcourse-ierg6130" ["l"="57.884,17.866"]
"LiuShuai26/Distributed-RL" ["l"="58.141,18.003"]
"iassael/torch-bootstrapped-dqn" ["l"="57.222,18.136"]
"rohitrango/BC-regularized-GAIL" ["l"="57.986,18.172"]
"xingdi-eric-yuan/qait_public" ["l"="57.414,18.782"]
"YangRui2015/Modular_HER" ["l"="57.684,18.403"]
"mbaske/ml-hover-bike-race" ["l"="57.244,19.071"]
"cognitiveailab/neurosymbolic" ["l"="57.425,18.715"]
"lukeluocn/dqn-breakout" ["l"="57.296,17.662"]
"mbaske/ml-simple-driver" ["l"="57.261,19.081"]
"TroddenSpade/Samurai-theme-vscode" ["l"="58.266,17.964"]
"TroddenSpade/LoAX" ["l"="58.274,17.94"]
"TroddenSpade/Judge-Cpp" ["l"="58.291,17.941"]
"TroddenSpade/GAN-NST" ["l"="58.233,18.009"]
"facebookresearch/BenchMARL" ["l"="57.911,18.301"]
"google-deepmind/concordia" ["l"="57.928,18.417"]
"rstrivedi/Melting-Pot-Contest-2023" ["l"="57.906,18.437"]
"tianqiraf/DouZero_For_HappyDouDiZhu" ["l"="58.206,18.309"]
"greycodee/wechat-backup" ["l"="45.529,-1.368"]
"Tencent/GameAISDK" ["l"="-0.865,1.026"]
"FengQuanLi/WZCQ" ["l"="58.185,18.249"]
"yqchilde/JDMemberCloseAccount" ["l"="-50.765,15.61"]
"official-pikafish/Pikafish" ["l"="58.627,17.225"]
"ainilili/ratel" ["l"="-3.598,11.763"]
"xfangfang/Macast" ["l"="-48.882,12.869"]
"OpenEthan/SMSBoom" ["l"="-48.542,13.163"]
"Element-Research/rnn" ["l"="46.055,27.692"]
"Kaixhin/Atari" ["l"="57.313,18.173"]
"EdwardPooh/douzero-resnet-2.0" ["l"="58.21,18.37"]
"Vincentzyx/VinXiangQi" ["l"="58.692,17.215"]
"Equim-chan/Mortal" ["l"="-24.078,-34.188"]
"submit-paper/Doudizhu_plus" ["l"="58.213,18.356"]
"RuBP17/AlphaDou" ["l"="58.226,18.359"]
"gbdev/awesome-gbdev" ["l"="-15.603,1.455"]
"mgba-emu/mgba" ["l"="-13.983,1.183"]
"HFO4/gameboy.live" ["l"="-0.56,-26.618"]
"LIJI32/SameBoy" ["l"="-14.062,1.007"]
"kitao/pyxel" ["l"="-50.445,8.648"]
"marblexu/PythonPlantsVsZombies" ["l"="48.217,23.588"]
"chrismaltby/gb-studio" ["l"="-15.586,1.529"]
"grantjenks/free-python-games" ["l"="48.182,23.57"]
"Humpheh/goboy" ["l"="3.617,-5.44"]
"ffelten/MASAC" ["l"="58.012,18.198"]
"jaimeluengo/masac" ["l"="58.005,18.181"]
"facebookresearch/AugLy" ["l"="50.923,29.523"]
"junxiaosong/AlphaZero_Gomoku" ["l"="58.528,17.177"]
"SamLynnEvans/Transformer" ["l"="53.247,25.702"]
"tencent-ailab/hok_env" ["l"="58.018,18.263"]
"IrisRainbowNeko/genshin_auto_fish" ["l"="-54.716,-19.306"]
"alex-damian/pulse" ["l"="45.044,30.834"]
"tobyqin/kog-money" ["l"="-43.967,-34.943"]
"wangshub/RL-Stock" ["l"="-8.367,14.796"]
"myBoris/wzry_ai" ["l"="58.21,18.237"]
"openstf/minitouch" ["l"="-0.844,0.812"]
"ricagj/train_your_own_game_AI" ["l"="58.409,18.255"]
"Turing-Project/Black-Myth-Wukong-AI" ["l"="58.531,18.284"]
"Skaiyin/DQN_play_blood" ["l"="58.471,18.29"]
"ailec0623/DQN_HollowKnight" ["l"="58.502,18.254"]
"ChenWendi2001/alpha-sekiro" ["l"="58.456,18.239"]
"Helen-Cheung/RL-Sekiro" ["l"="58.485,18.28"]
"BAAI-Agents/Cradle" ["l"="36.745,-1.61"]
"AARG-FAN/Yolo_for_Wukong" ["l"="58.507,18.296"]
"XR-stb/DQN_WUKONG" ["l"="58.494,18.228"]
"Changanyue/AI-moba-game" ["l"="58.237,18.242"]
"linyiLYi/street-fighter-ai" ["l"="38.921,1.776"]
"philtabor/Multi-Agent-Reinforcement-Learning" ["l"="57.941,18.204"]
"reinshift/MADDPG_Multi_UAV_Roundup" ["l"="57.989,18.196"]
"tjuHaoXiaotian/pymarl3" ["l"="57.949,18.298"]
"seermer/HollowKnight_RL" ["l"="58.552,18.246"]
"Jacklinkk/Graph_CAVs" ["l"="62.325,12.334"]
"Amanda2024/GCS_aamas337" ["l"="57.917,18.48"]
"WorldDbs/specs-actors" ["l"="52.866,3.061"]
"yl-yue/yue-library" ["l"="52.852,3.056"]
"PercyJon/PercyJon.github.io" ["l"="52.895,3.022"]
"springmonster/RestfulTool-Retrofit" ["l"="52.878,3.047"]
"v2ray-links/v2ray-free" ["l"="52.88,3.066"]
"HeisenbergEmpire/studynote" ["l"="52.897,3.033"]
"admin360bug/bypass" ["l"="52.886,3.039"]
"DataCanvasIO/HyperGBM" ["l"="52.891,3.054"]
"marl-book/slides" ["l"="57.998,18.323"]
"uoe-agents/smaclite" ["l"="58.006,18.34"]
"chrisyrniu/Recent-Advances-in-Multi-Agent-Reinforcement-Learning" ["l"="58.02,18.329"]
"hcnoh/rl-collection-pytorch" ["l"="57.602,18.5"]
"kaixindelele/RHER" ["l"="57.998,18.118"]
"borninfreedom/kuka-reach-drl" ["l"="59.582,16.293"]
"BIT-aerial-robotics/AquaML" ["l"="58.029,18.113"]
"EdanToledo/Stoix" ["l"="59.314,17.418"]
"instadeepai/flashbax" ["l"="59.326,17.409"]
"instadeepai/catx" ["l"="59.252,17.328"]
"adaptive-intelligent-robotics/QDax" ["l"="59.259,17.367"]
"chauncygu/Multi-Agent-Constrained-Policy-Optimisation" ["l"="61.099,15.008"]
"TimeBreaker/MARL-resources-collection" ["l"="57.971,18.294"]
"TimeBreaker/Multi-Agent-Reinforcement-Learning-papers" ["l"="57.965,18.278"]
"TimeBreaker/Adversarial-Reinforcement-Learning-Papers" ["l"="57.988,18.278"]
"notalim/intermediate-statistics-ca-notes" ["l"="58.141,17.635"]
"openai/GPT-3-Encoder" ["l"="57.116,18.628"]
"openai/pytorch" ["l"="57.146,18.59"]
"openai/websockify" ["l"="57.141,18.62"]
"facebookresearch/minimax" ["l"="59.32,17.379"]
"rlchina/RLCN" ["l"="58.115,18.215"]
"jidiai/ai_lib" ["l"="58.203,18.18"]
"LARG/HFO" ["l"="57.003,18.585"]
"openai/gym-soccer" ["l"="57.16,18.498"]
"mhauskn/dqn-hfo" ["l"="57.049,18.558"]
"rcsoccersim/rcssserver" ["l"="56.932,18.624"]
"herodrigues/robocup2d-tutorial" ["l"="56.944,18.643"]
"wrighteagle2d/wrighteaglebase" ["l"="56.954,18.616"]
"ltzheng/pddpg-hfo" ["l"="56.991,18.612"]
"kengz/robocup-soccer" ["l"="56.97,18.596"]
"Cyrus2D/Cyrus2DBase" ["l"="56.923,18.604"]
"bbitmaster/ale_python_interface" ["l"="57.251,18.281"]
"openai/atari-py" ["l"="57.31,18.255"]
"codecat/tm-better-chat" ["l"="58.406,19.221"]
"domino54/title-packs" ["l"="58.427,19.218"]
"laszukdawid/ai-traineree" ["l"="57.711,18.438"]
"bestpredicts/Robocup2dInstall" ["l"="56.936,18.678"]
"wrighteagle2d/autotest2d" ["l"="56.963,18.641"]
"helios-base/helios-base" ["l"="56.919,18.62"]
"bestpredicts/Robocup2DMining" ["l"="56.935,18.664"]
"dark-0ne/RcssAnalyzer" ["l"="56.922,18.672"]
"rcsoccersim/rcsoccersim.github.io" ["l"="56.95,18.671"]
"tavik000/MazeGameAI" ["l"="57.181,19.025"]
"Theohhhu/UPDeT" ["l"="58.04,18.316"]
"mttga/pymarl_transformers" ["l"="58.019,18.309"]
"Coselding/PlaneWar-MFC" ["l"="-20.857,-27.989"]
"dwg255/landlord" ["l"="-24.603,-34.391"]
"donnki/ddz_skynet" ["l"="-23.341,-34.434"]
"ksky521/DouZero" ["l"="58.352,18.471"]
"google-deepmind/dm_env" ["l"="59.353,17.428"]
"google-deepmind/dm_alchemy" ["l"="49.11,32.462"]
"philipjball/TD3_PyTorch" ["l"="57.73,18.639"]
"Kinds-of-Intelligence-CFI/animal-ai" ["l"="57.506,18.57"]
"donadigo/gbxtools" ["l"="58.329,19.097"]
"sjtu-marl/bd_rd_psro" ["l"="58.293,18.165"]
"PacktPublishing/PyTorch-1.x-Reinforcement-Learning-Cookbook" ["l"="57.733,18.096"]
"TorchCraft/TorchCraft" ["l"="58.71,18.07"]
"Ze-Rax/TrackmaniaFlagRush" ["l"="58.336,19.203"]
"manantomar/DSR" ["l"="57.022,18.235"]
"tinyshu/ddz_game" ["l"="-23.868,-33.798"]
"Zhang19910325/nodejs-server-wechat-landLordGame" ["l"="-23.975,-33.787"]
"matchvs/Poke" ["l"="-25.65,-33.269"]
"WhoIsYourBaby/chess" ["l"="-23.856,-33.93"]
"gochenzl/chess" ["l"="-24.624,-34.368"]
"dwg255/fish" ["l"="-24.579,-34.456"]
"nishantkr18/federated-model-averaging-for-DQN" ["l"="58.453,17.894"]
"TroddenSpade/Federated-DRL" ["l"="58.324,17.963"]
"mansicer/MAIC" ["l"="57.979,18.456"]
"Jiwonjeon9603/MASER" ["l"="58.032,18.418"]
"samjia2000/HSP" ["l"="58.003,18.631"]
"HumanCompatibleAI/human_ai_robustness" ["l"="57.934,18.514"]
"jangxx/node-ViGEmClient" ["l"="58.26,18.942"]
"YiqinYang/ICQ" ["l"="58.073,18.402"]
"ling-pan/OMAR" ["l"="58.057,18.417"]
"thu-rllab/CFCQL" ["l"="58.056,18.405"]
"ReinholdM/Offline-Pre-trained-Multi-Agent-Decision-Transformer" ["l"="58.057,18.389"]
"hsvgbkhgbv/Thermostat-assisted-continuously-tempered-Hamiltonian-Monte-Carlo-for-Bayesian-learning" ["l"="57.962,18.523"]
"mbaske/ml-chord-detection" ["l"="57.227,19.055"]
"openai/tabulate" ["l"="57.147,18.653"]
"lchaumartin/HumanoidProceduralAnimation" ["l"="56.988,19.419"]
"pickles976/UnityProceduralBiped" ["l"="56.977,19.441"]
"datamllab/ltsm" ["l"="58.189,18.48"]
"JiabenChen/iQuery" ["l"="58.206,18.452"]
"IBM/action-recognition-pytorch" ["l"="47.838,33.805"]
"VITA-Group/Deep_GCN_Benchmarking" ["l"="58.22,18.475"]
"datamllab/tods" ["l"="52.413,14.799"]
"datamllab/xdeep" ["l"="52.44,14.714"]
"datamllab/AutoRec" ["l"="52.451,14.721"]
"ataulien/blender-trackmania-tools" ["l"="58.295,19.179"]
"helios-base/librcsc" ["l"="56.901,18.617"]
"helios-base/fedit2" ["l"="56.898,18.631"]
"XinZhang525/cGAIL" ["l"="57.969,18.177"]
"hijkzzz/noisy-mappo" ["l"="58.22,18.119"]
"sanmuyang/multi-agent-PPO-on-SMAC" ["l"="58.251,18.103"]
"trzhang0116/HRAC" ["l"="57.344,18.61"]
"junsu-kim97/HIGL" ["l"="57.316,18.64"]
"HeegerGao/CRIL" ["l"="57.333,18.645"]
"fangzai/DeepQLearning" ["l"="57.124,18.039"]
"Cyrus2D/Pyrus2D" ["l"="56.902,18.605"]
"UnrealTracking/ToM2C" ["l"="57.941,18.473"]
"nadeo/trackmania-doc" ["l"="58.423,19.279"]
"openplanet-nl/example-scripts" ["l"="58.434,19.301"]
"EvoEsports/docker-trackmania" ["l"="58.345,19.274"]
"jidiai/Competition_3v3snakes" ["l"="58.251,18.156"]
"CarlossShi/Competition_3v3snakes" ["l"="58.239,18.167"]
"jidiai/SummerCourse2021" ["l"="58.232,18.152"]
"MatPoliquin/stable-retro" ["l"="56.889,18.002"]
"shacklettbp/rlpbr" ["l"="57.628,18.668"]
"donadigo/TMInterfacePublic" ["l"="58.392,19.163"]
"XanderJC/scalable-birl" ["l"="57.568,18.578"]
"ZiyuanMa/R2D2" ["l"="57.523,18.668"]
"kikojay/EMC" ["l"="58.097,18.522"]
"sunghoonhong/SWAT" ["l"="57.201,18.819"]
"helios-base/soccerwindow2" ["l"="56.909,18.639"]
"TroddenSpade/Soccer-Players-Tracking" ["l"="58.294,17.986"]
"TroddenSpade/Curiosity-driven-Exploration-in-Navigation" ["l"="58.308,17.974"]
"TroddenSpade/Exhaustive-Reinforcement-Learning" ["l"="58.31,17.982"]
"TroddenSpade/Flexible-Conditional-Imitation-Learning" ["l"="58.302,17.966"]
"mbarnes1/vehicle-drift" ["l"="57.018,18.049"]
"YunqiuXu/SHA-KG" ["l"="57.404,18.761"]
"src-d/awesome-machine-learning-on-source-code" ["l"="47.666,28.87"]
"keon/awesome-nlp" ["l"="52.863,25.815"]
"GoogleTrends/data" ["l"="47.706,28.91"]
"jbhuang0604/awesome-computer-vision" ["l"="47.724,28.755"]
"igrigorik/decisiontree" ["l"="47.682,28.92"]
"ChristosChristofidis/awesome-deep-learning" ["l"="47.832,28.622"]
"scikit-learn-contrib/lightning" ["l"="47.7,28.936"]
"jtoy/awesome-tensorflow" ["l"="47.814,28.742"]
"nlintz/TensorFlow-Tutorials" ["l"="47.787,28.822"]
"facebookresearch/rlmeta" ["l"="57.625,18.53"]
"facebookresearch/AutoCAT" ["l"="57.639,18.624"]
"clansty/ClassTools" ["l"="-36.577,-12.896"]
"visionsss/recommend_system_version2" ["l"="-1.782,17.084"]
"kzl/decision-transformer" ["l"="59.493,17.609"]
"NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player" ["l"="57.765,18.522"]
"choo8/Tensorflow-DeepMind-Atari-Deep-Q-Learner-2Player" ["l"="57.767,18.57"]
"DorianKodelja/DeepMind-Atari-Deep-Q-Learner-2Player" ["l"="57.784,18.563"]
"agi-brain/xuance" ["l"="57.898,18.193"]
"wwxFromTju/awesome-reinforcement-learning-lib" ["l"="59.405,17.506"]
"sjchoi86/irl_rocks" ["l"="57.567,18.422"]
"tensorflow/skflow" ["l"="47.865,28.864"]
"google/prettytensor" ["l"="47.797,28.994"]
"ericjang/tdb" ["l"="47.77,28.998"]
"amacati/SoulsGym" ["l"="58.658,18.305"]
"amacati/SoulsAI" ["l"="58.682,18.309"]
"ocram444/EldenRL" ["l"="58.604,18.297"]
"yunke-wang/SAIL" ["l"="57.662,18.535"]
"yunke-wang/UID" ["l"="57.669,18.524"]
"agrimgupta92/derl" ["l"="57.119,18.924"]
"agrimgupta92/metamorph" ["l"="57.137,18.9"]
"dennybritz/deeplearning-papernotes" ["l"="47.764,28.803"]
"andrewliao11/Deep-Reinforcement-Learning-Survey" ["l"="57.339,18.14"]
"awjuliani/DeepRL-Agents" ["l"="57.415,18.078"]
"vwxyzjn/invalid-action-masking" ["l"="57.945,18.127"]
"XinJingHao/PPO-Continuous-Pytorch" ["l"="57.876,18.304"]
"EliFUT/android" ["l"="58.498,17.933"]
"glenn89/FederatedRL" ["l"="58.593,17.828"]
"hizircanbayram/FeDQN-A-Federated-Learning-Approach-for-Training-Reinforcement-Learning-Agent-of-Atari-Games" ["l"="58.589,17.852"]
"oli233/corona-analysis" ["l"="50.059,3.353"]
"GaaraZhu/bamboo-on-teams" ["l"="50.043,3.333"]
"vwxyzjn/PPO-Implementation-Deep-Dive" ["l"="57.932,18.367"]
"AltmanD/guandan_mcc" ["l"="58.276,18.371"]
"samuelgzx/RL_GuanDan" ["l"="58.256,18.364"]
"Kaixhin/rlenvs" ["l"="45.936,27.703"]
"twitter-archive/torch-twrl" ["l"="45.986,27.691"]
"ludc/rltorch" ["l"="45.96,27.686"]
"google-deepmind/alewrap" ["l"="46.098,27.389"]
"fmassa/optimize-net" ["l"="45.951,27.65"]
"twitter-archive/torch-autograd" ["l"="45.999,27.666"]
"vivanov879/draw" ["l"="45.965,27.672"]
"yobibyte/atarigrandchallenge" ["l"="57.229,18.176"]
"Ardavans/DSR" ["l"="57.306,18.238"]
"SeanNaren/QlearningExample.torch" ["l"="45.894,27.708"]
"yilunc2020/Attention-DQN" ["l"="60.133,14.253"]
"5vision/deep-reinforcement-learning-networks" ["l"="57.248,18.212"]
"avivt/VIN" ["l"="57.357,18.298"]
"facebookarchive/learningSimpleAlgorithms" ["l"="57.3,18.374"]
"morning9393/HAPPO-HATRPO" ["l"="58.005,18.29"]
"morning9393/Optimal-Baseline-for-Multi-agent-Policy-Gradients" ["l"="58.036,18.25"]
"Code-Bullet/Jump-King" ["l"="56.688,17.794"]
"Code-Bullet/RickAndMortai" ["l"="56.647,17.78"]
"hengyuan-hu/instruct-rl" ["l"="57.803,18.628"]
"SwiftSage/SwiftSage" ["l"="57.456,18.735"]
"allenai/discoveryworld" ["l"="57.458,18.696"]
"microsoft/SmartPlay" ["l"="36.532,-2.323"]
"robocode-dev/tank-royale" ["l"="56.831,18.212"]
"robo-code/robocode" ["l"="56.869,18.222"]
"tonylitianyu/Preference-Planning-Deep-IRL" ["l"="58.009,18.145"]
"bic4907/Overcooked-AI" ["l"="58.046,18.68"]
"liyang619/COLE-Platform" ["l"="58.027,18.664"]
"instadeepai/awesome-marl" ["l"="58.057,18.371"]
"instadeepai/matrax" ["l"="58.081,18.381"]
"instadeepai/marl-eval" ["l"="58.03,18.359"]
"Farama-Foundation/D4RL-Evaluations" ["l"="59.448,17.576"]
"syuntoku14/pytorch-rl-il" ["l"="5.833,-41.21"]
"denisyarats/exorl" ["l"="59.45,17.53"]
"Khrylx/Transform2Act" ["l"="30.412,28.703"]
"frt03/mxt_bench" ["l"="57.176,18.852"]
"ShprAlex/SproutLife" ["l"="56.893,17.573"]
"mila-iqia/spr" ["l"="59.327,17.579"]
"chenf-ai/Multi-Agent-Communication-Considering-Representation-Learning" ["l"="58.009,18.491"]
"TARTRL/TiKick" ["l"="58.361,18.167"]
"OpenRL-Lab/TiZero" ["l"="58.297,18.196"]
"WentseChen/Soft-QMIX" ["l"="58.39,18.156"]
"ling-pan/RES" ["l"="58.081,18.235"]
"ruizhaogit/maximum_entropy_population_based_training" ["l"="58.009,18.655"]
"daochenzha/dreamshard" ["l"="58.247,18.527"]
"daochenzha/autoshard" ["l"="58.233,18.532"]
"daochenzha/neuroshard" ["l"="58.216,18.504"]
"HFrost0/TGC_torch" ["l"="-8.486,15.579"]
"thanhtrunghuynh93/estimate" ["l"="-8.494,15.697"]
"kevinzakka/ibc" ["l"="57.663,18.621"]
"waterhorse1/NAC" ["l"="58.31,18.156"]
"CLSFramework/py2d" ["l"="56.891,18.592"]
"qxtian/Learning-Independent-SKills" ["l"="58.161,18.473"]
"benellis3/pymarl2" ["l"="57.991,18.346"]
"benellis3/mappo" ["l"="58.018,18.352"]
"bourbonut/reinforcedFL" ["l"="58.566,17.816"]
"DesikRengarajan/FEDORA" ["l"="58.677,17.803"]
"AlexKashi/AlphaHoldem" ["l"="58.29,18.39"]
"mrkulk/deepQN_tensorflow" ["l"="57.194,18.034"]
"Jabberwockyll/deep_rl_ale" ["l"="57.204,18.061"]
"maciej-sypetkowski/autoascend" ["l"="57.598,18.621"]
"upiterbarg/hihack" ["l"="57.604,18.645"]
"rasmushaugaard/peg-in-hole-visual-servoing" ["l"="58.539,18.849"]
"mjs2600/ML-Final-Exam-Study-Notes" ["l"="57.276,17.553"]
"1310183534/DouDiZhu" ["l"="58.227,18.382"]
"vzhong/silg" ["l"="57.405,18.722"]
"drdh/Synergy-RL" ["l"="57.193,18.835"]
"isaac-sim/IsaacLab" ["l"="61.185,16.458"]
"keras-team/keras" ["l"="48.067,28.867"]
"tensorflow/models" ["l"="48.134,28.884"]
"pytorch/pytorch" ["l"="48.274,28.966"]
"scikit-learn/scikit-learn" ["l"="48.164,28.819"]
"tensorflow/tensorflow" ["l"="48.211,29.024"]
"facebookresearch/macta" ["l"="57.652,18.676"]
"TianxingChen/Embodied-AI-Guide" ["l"="59.286,16.705"]
"OpenRLHF/OpenRLHF" ["l"="37.161,-0.425"]
"Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning" ["l"="57.215,18.153"]
"yao62995/A3C" ["l"="57.258,18.185"]
"steveKapturowski/tensorflow-rl" ["l"="57.309,18.208"]
"osh/kerlym" ["l"="57.282,18.125"]
"yukezhu/tensorflow-reinforce" ["l"="57.354,18.169"]
"yrlu/reinforcement_learning" ["l"="57.346,18.067"]
"facebookarchive/MIXER" ["l"="46.026,27.713"]
"rmst/ddpg" ["l"="57.341,18.199"]
"wojzaremba/trpo" ["l"="57.337,18.237"]
"rexrex9/reinforcement_torch_pfrl" ["l"="58.063,18.028"]
"ziwenhahaha/Code-of-RL-Beginning" ["l"="58.032,17.982"]
"markriedl/transformer-walkthrough" ["l"="57.21,17.336"]
"google-deepmind/pysc2" ["l"="58.736,18.145"]
"microsoft/CNTK" ["l"="47.95,28.986"]
"Rochester-NRT/RocAlphaGo" ["l"="58.581,17.309"]
"kjw0612/awesome-deep-vision" ["l"="47.831,28.792"]
"carpedm20/DCGAN-tensorflow" ["l"="45.808,29.187"]
"tflearn/tflearn" ["l"="47.841,28.826"]
"algorithmdog/Reinforcement_Learning_Blog" ["l"="57.38,18.001"]
"Kaixhin/NoisyNet-A3C" ["l"="57.225,18.199"]
"0bserver07/Study-Reinforcement-Learning" ["l"="57.195,18.145"]
"alibaba/gym-starcraft" ["l"="58.714,18.105"]
"facebookresearch/diplomacy_cicero" ["l"="57.901,18.528"]
"Farama-Foundation/chatarena" ["l"="41.12,-3.795"]
"google-deepmind/mctx" ["l"="59.235,17.472"]
"allenai/RL4LMs" ["l"="37.17,-0.197"]
"facebookresearch/rebel" ["l"="58.641,18.667"]
"CarperAI/trlx" ["l"="37.133,-0.204"]
"stevenpjg/ddpg-aigym" ["l"="57.314,18.196"]
"MOCR/DDPG" ["l"="57.306,18.185"]
"Shanghai-Digital-Brain-Laboratory/DB-Football" ["l"="58.244,18.222"]
"jidiai/GRF_MARL" ["l"="58.176,18.274"]
"cookbenjamin/DDPG" ["l"="57.287,18.214"]
"carpedm20/NAF-tensorflow" ["l"="57.394,18.257"]
"liampetti/DDPG" ["l"="57.292,18.191"]
"harvitronix/rl-rc-car" ["l"="57.177,18.132"]
"MLJejuCamp2017/DRL_based_SelfDrivingCarControl" ["l"="61.672,12.583"]
"thibo73800/metacar" ["l"="61.745,12.663"]
"songyanho/Reinforcement-Learning-for-Self-Driving-Cars" ["l"="61.65,12.59"]
"ugo-nama-kun/gym_torcs" ["l"="61.76,12.557"]
"kaihuchen/DRL-AutonomousVehicles" ["l"="57.161,18.107"]
"flyyufelix/donkey_rl" ["l"="61.5,12.692"]
"floodsung/DQN-Atari-Tensorflow" ["l"="57.202,18.006"]
"gtoubassi/dqn-atari" ["l"="57.18,18.017"]
"turkishviking/Python-Robocode" ["l"="56.806,18.233"]
"Voidious/Diamond" ["l"="56.837,18.242"]
"stevenpjg/QlearningRobocodeNN" ["l"="57.069,18.21"]
"BigBang1112/randomizer-tmf" ["l"="58.315,19.203"]
"li-haoran/DRL-FlappyBird" ["l"="51.763,33.814"]
"yanpanlau/Keras-FlappyBird" ["l"="57.313,18.043"]
"borgwang/reinforce_py" ["l"="57.239,17.932"]
"ehrenbrav/DeepQNetwork" ["l"="57.178,18.22"]
"rameshvarun/NeuralKart" ["l"="61.394,12.417"]
"wenkesj/mario" ["l"="57.149,18.09"]
"ehrenbrav/FCEUX_Learning_Environment" ["l"="57.158,18.165"]
"erlerobot/gym-gazebo" ["l"="57.463,18.179"]
"mpatacchiola/dissecting-reinforcement-learning" ["l"="57.443,18.096"]
"zbzhu99/madiff" ["l"="59.367,16.829"]
"zzq-bot/offline-marl-framework-offpymarl" ["l"="58.04,18.399"]
"ZhengYinan-AIR/OMIGA" ["l"="58.073,18.418"]
"yandexdataschool/MLatImperial2017" ["l"="57.268,17.917"]
"VinF/deer" ["l"="57.272,18.101"]
"ADGEfficiency/energy-py" ["l"="-12.072,18.657"]
"farizrahman4u/qlearning4k" ["l"="57.244,18.072"]
"kuza55/keras-extras" ["l"="50.297,33.388"]
"EderSantana/seya" ["l"="44.839,27.696"]
"avisingh599/visual-qa" ["l"="48.571,32.1"]
"lusob/gym-ple" ["l"="57.241,18.111"]
"armahmood/nonstationary-experiments" ["l"="57.126,17.735"]
"traai/async-deep-rl" ["l"="57.259,18.169"]
"yangtao121/AquaRL" ["l"="58.071,18.103"]
"Arya87/RL_draw_seabron" ["l"="58.115,18.086"]
"samlobel/CFN" ["l"="57.717,18.627"]
"tokb23/dqn" ["l"="57.339,18.044"]
"roomai/RoomAI" ["l"="58.711,18.712"]
"NoneJou072/rl-notebook" ["l"="58.06,18.088"]
"mihaibivol/Q-learning-tic-tac-toe" ["l"="57.18,17.931"]
"kyokin78/rl-flappybird" ["l"="57.18,17.914"]
"xbpeng/DeepMimic" ["l"="61.158,16.507"]
"BartMoyaers/BvhToDeepMimic" ["l"="30.344,28.635"]
"tencent-ailab/marl-mini" ["l"="58.061,18.265"]
"tencent-ailab/hokoff" ["l"="58.074,18.276"]
"nrontsis/PILCO" ["l"="59.275,17.711"]
"kchua/handful-of-trials" ["l"="59.326,17.68"]
"carpedm20/visual-analogy-tensorflow" ["l"="53.439,26.38"]
"siemanko/tf-adversarial" ["l"="45.938,27.531"]
"mokemokechicken/keras_npi" ["l"="46.027,27.893"]
"vvanirudh/IRL-Toolkit" ["l"="57.551,18.472"]
"yl-1993/SpectralMEIRL" ["l"="57.55,18.524"]
"erensezener/aima-based-irl" ["l"="57.538,18.52"]
"Farama-Foundation/Shimmy" ["l"="59.416,17.407"]
"openai/pachi-py" ["l"="57.049,18.725"]
"openai/pyconfigatron" ["l"="57.081,18.698"]
"ZhiyeTang/Kaiwu-Paper-List" ["l"="58.331,18.243"]
"Unakar/AI_Game_KingGlory" ["l"="58.291,18.241"]
"openai/gym-recording" ["l"="57.219,18.485"]
"openai/dallify-discord-bot" ["l"="57.105,18.755"]
"openai/go-alias" ["l"="57.133,18.711"]
"TroddenSpade/Meta-Reinforcement-Learning" ["l"="58.288,17.975"]
"TroddenSpade/Decision-Transformer-on-Offline-Reinforcement-Learning" ["l"="58.283,17.987"]
"DramaCow/jaxued" ["l"="59.351,17.384"]
"sash-a/CleanRL.jl" ["l"="58.111,18.403"]
"ilyasu123/trpo" ["l"="57.236,18.257"]
"jjyyxx/srlnbc" ["l"="58.186,18.127"]
"ManUtdMoon/Distributional-Reachability-Policy-Optimization" ["l"="58.144,18.141"]
"sjtu-marl/ZSC-Eval" ["l"="58.026,18.648"]
"ffelten/CrazyRL" ["l"="61.876,15.314"]
"floodsung/Deep-Learning-Papers-Reading-Roadmap" ["l"="47.883,28.627"]
"terryum/awesome-deep-learning-papers" ["l"="47.852,28.669"]
"aymericdamien/TensorFlow-Examples" ["l"="47.951,28.706"]
"facebookresearch/Pearl" ["l"="57.842,18.061"]
"eureka-research/Eureka" ["l"="61.224,16.506"]
"facebookresearch/jepa" ["l"="49.119,30.148"]
"ppaquette/gym-pull" ["l"="57.132,18.35"]
"openai/gym-doom" ["l"="57.096,18.37"]
"awjuliani/Meta-RL" ["l"="57.327,18.215"]
"AcutronicRobotics/gym-gazebo2" ["l"="59.682,16.308"]
"mit-acl/cadrl_ros" ["l"="59.709,14.907"]
"dusty-nv/jetson-reinforcement" ["l"="53.264,32.41"]
"vita-epfl/CrowdNav" ["l"="59.727,14.899"]
"AcutronicRobotics/ros2learn" ["l"="59.716,16.277"]
"Acmece/rl-collision-avoidance" ["l"="59.699,14.892"]
"tensorlayer/TensorLayer" ["l"="57.52,17.843"]
"tensorlayer/SRGAN" ["l"="-34.937,21.659"]
"tensorpack/tensorpack" ["l"="50.5,33.165"]
"apache/mxnet" ["l"="47.989,29.03"]
"zhangqianhui/AdversarialNetsPapers" ["l"="45.797,29.146"]
"wiseodd/generative-models" ["l"="45.835,29.145"]
"BellmanTimeHut/DIPO" ["l"="58.173,18.098"]
"wadx2019/qvpo" ["l"="58.155,18.107"]
"OpenRL-Lab/openrl" ["l"="57.955,18.262"]
"ziyanx02/multiagent-quadruped-environment" ["l"="58.059,18.292"]
"Intelligent-Driving-Laboratory/GOPS" ["l"="58.071,18.165"]
"happy-yan/DACER-Diffusion-with-Online-RL" ["l"="58.107,18.125"]
"Apress/Reinforcement-Learning-for-Sequential-Decision-and-Optimal-Control" ["l"="58.057,18.132"]
"SupermanCaozh/The_Coding_Foundation_in_Reinforcement_Learning" ["l"="58.054,17.975"]
"10-OASIS-01/minrl" ["l"="58.054,17.956"]
"Ronchy2000/Multi-agent-RL" ["l"="58.037,17.956"]
"Zhefan-Xu/NavRL" ["l"="60.391,13.926"]
"openai/iaf" ["l"="46.004,29.288"]
"alexlee-gk/slac" ["l"="59.361,17.601"]
"openai/neural-gpu" ["l"="57.187,18.532"]
"openai/InfoGAN" ["l"="45.954,29.255"]
"LiSir-HIT/Reinforcement-Learning" ["l"="57.999,18.088"]
"openai/requests-for-research" ["l"="57.37,18.041"]
"openai/improved-gan" ["l"="45.896,29.177"]
"google-deepmind/learning-to-learn" ["l"="47.672,28.819"]
"openai/generating-reviews-discovering-sentiment" ["l"="53.094,25.499"]
"tensorflow/fold" ["l"="46.192,27.87"]
"harvardnlp/seq2seq-attn" ["l"="46.138,27.753"]
"carpedm20/NTM-tensorflow" ["l"="46.105,27.882"]
"lansinuote/StableBaselines3_SimpleCases" ["l"="58.083,17.985"]
"david-abel/simple_rl" ["l"="57.584,18.492"]
"deep-skill-chaining/deep-skill-chaining" ["l"="57.591,18.558"]
"openai/gym-wikinav" ["l"="57.184,18.558"]
"junhyukoh/icml2016-minecraft" ["l"="57.289,18.32"]
"jimkon/Deep-Reinforcement-Learning-in-Large-Discrete-Action-Spaces" ["l"="60.01,23.73"]
"Intelligent-Driving-Laboratory/GOPS_DOC" ["l"="58.1,18.15"]
"Intelligent-Driving-Laboratory/Reinforcement-Learning-for-Sequential-Decision-and-Optimal-Control" ["l"="58.107,18.161"]
"TobiasLv/RAD" ["l"="58.12,18.141"]
"fhennecker/deepdoom" ["l"="57.129,18.251"]
"vjg28/Linear-Inverse-RL-algorithms" ["l"="57.568,18.44"]
"zacrash/Inverse-RL" ["l"="57.557,18.409"]
"dit7ya/awesome-irl" ["l"="57.558,18.449"]
"ppaquette/gym-doom" ["l"="57.15,18.32"]
"chris-chris/mario-rl-tutorial" ["l"="57.171,18.329"]
"craigthomas/Chip8Python" ["l"="57.363,18.704"]
"wkcn/pyfcemu" ["l"="57.402,18.629"]
"LxzGordon/PECAN" ["l"="58.036,18.694"]
"binary-husky/vhmap" ["l"="58.148,18.075"]
"puyuan1996/MARL" ["l"="58.066,18.192"]
"kengz/openai_lab" ["l"="57.156,18.196"]
"openai/gym-http-api" ["l"="57.236,18.528"]
"Starlight0798/gymRL" ["l"="58.121,18.052"]
"IdreesInc/Cerebrum" ["l"="57.201,17.72"]
"synpon/prog_nn" ["l"="57.202,18.187"]
"seann999/progressive_a3c" ["l"="57.214,18.179"]
"MatPoliquin/stable-retro-scripts" ["l"="56.872,18.013"]
"DarkAutumn/triforce" ["l"="56.886,18.026"]
"vincentberaud/Minecraft-Reinforcement-Learning" ["l"="57.387,18.437"]
"guanchuwang/Taylor-Unswift" ["l"="58.203,18.512"]
"henryzhongsc/longctx_bench" ["l"="38.988,-0.176"]
"takoika/PrioritizedExperienceReplay" ["l"="57.351,18.25"]
"PKU-Alignment/ProAgent" ["l"="58.074,18.718"]
"sehoonha/pydart2" ["l"="57.117,18.332"]
"sehoonha/pydart" ["l"="57.086,18.344"]
"yanxue7/E3T-Overcooked" ["l"="58.128,18.331"]
"openai/fetch_robots" ["l"="57.105,18.691"]
"kvfrans/parallel-trpo" ["l"="57.176,18.274"]
"jameszampa/EldenRingAI" ["l"="58.627,18.305"]
"knakamura13/huggingface-dataset-toolkit" ["l"="57.3,17.343"]
"knakamura13/mlrose-torch" ["l"="57.295,17.369"]
"commaai/research" ["l"="61.55,12.428"]
"SerpentAI/SerpentAI" ["l"="45.087,20.387"]
"magenta/magenta" ["l"="45.441,29.307"]
"AgentTorch/AgentTorch" ["l"="58.029,18.515"]
"jbloomAus/SAELens" ["l"="37.847,-6.926"]
"callummcdougall/ARENA_3.0" ["l"="37.832,-6.919"]
"facebookresearch/coconut" ["l"="37.286,-0.524"]
"yandexdataschool/Practical_DL" ["l"="-44.358,26.065"]
"yandexdataschool/nlp_course" ["l"="52.819,25.863"]
"esokolov/ml-course-hse" ["l"="-44.384,26.054"]
"Yorko/mlcourse.ai" ["l"="47.869,28.553"]
"girafe-ai/ml-course" ["l"="-44.42,26.028"]
"catalyst-team/catalyst" ["l"="50.914,29.869"]
"Mostafa-Samir/DNC-tensorflow" ["l"="46.109,27.904"]
"LukasSchaefer/marl-book-exercises" ["l"="58.04,18.34"]
"YurongYou/rlTORCS" ["l"="61.765,12.582"]
"collaborative-mapush/MAPush" ["l"="58.088,18.3"]
"tgangwani/GA3C-DeepNavigation" ["l"="60.977,33.917"]
"wty-yy/KataCR" ["l"="58.607,18.238"]
"leegao/readme2tex" ["l"="57.134,18.122"]
"agurod42/github-texify" ["l"="57.066,18.103"]
"jwkvam/celluloid" ["l"="23.502,14.937"]
"leegao/float-hacks" ["l"="57.049,18.126"]
"hardmaru/sketch-rnn" ["l"="46.484,7.699"]
"DanielTakeshi/Paper_Notes" ["l"="57.441,18.281"]
"DanielTakeshi/rl_algorithms" ["l"="57.316,18.338"]
"Alfredvc/paac" ["l"="57.27,18.203"]
"awjuliani/oreilly-rl-tutorial" ["l"="46.216,29.193"]
"sawcordwell/pymdptoolbox" ["l"="61.284,13.23"]
"openai/weightnorm" ["l"="57.12,18.578"]
"TimSalimans/weight_norm" ["l"="57.073,18.596"]
"jiamings/fast-weights" ["l"="45.959,27.889"]
"ryankiros/layer-norm" ["l"="45.985,27.719"]
"cndaqiang/autowzry" ["l"="58.317,18.215"]
"XRSec/WZRY_AirtestIDE" ["l"="58.353,18.2"]
"cndaqiang/airtest_mobileauto" ["l"="58.351,18.218"]
"witmemtech/Algorithm-Deployed-in-WTM2101" ["l"="-55.341,-11.105"]
"tongmume/blockchain_auth" ["l"="58.382,18.204"]
"AustinIOI/Golaris_Laravel" ["l"="-55.308,-11.078"]
"knakamura13/cs7641-ml-study-materials-2023" ["l"="57.285,17.355"]
"zond/diplicity" ["l"="57.989,18.77"]
"MattChanTK/ai-gym" ["l"="57.337,18.386"]
"zuoxingdong/VIN_TensorFlow" ["l"="57.322,18.296"]
"geek-ai/1m-agents" ["l"="57.272,18.347"]
"RLAgent/gated-path-planning-networks" ["l"="23.55,14.888"]
"DartML/PPO-Stein-Control-Variate" ["l"="57.292,18.301"]
"openai/go-vncdriver" ["l"="57.149,18.694"]
"wty-yy/kaiwu2024_taichu" ["l"="58.317,18.245"]
"f200ten/RL_King_of_Glory" ["l"="58.257,18.235"]
"sisl/ngsim_env" ["l"="63.489,12.507"]
"jinming99/DGP-IRL" ["l"="57.562,18.488"]
"makokal/funzo" ["l"="57.568,18.476"]
"tobywhughes/Chippy8" ["l"="57.339,18.726"]
"craigthomas/Chip8C" ["l"="57.341,18.743"]
"AlpacaMax/Python-CHIP8-Emulator" ["l"="57.358,18.737"]
"maoliyuan/ODICE-Pytorch" ["l"="58.104,18.446"]
"crowdAI/crowdai" ["l"="57.015,18.336"]
"openai/mitmproxy" ["l"="57.069,18.722"]
"yandexdataschool/MLatGradDays" ["l"="57.236,17.871"]
"upiterbarg/diff_history" ["l"="57.614,18.7"]
"upiterbarg/lintseq" ["l"="57.621,18.734"]
"ray-project/tutorial" ["l"="57.684,18.288"]
"anyscale/academy" ["l"="60.017,23.654"]
"ray-project/xgboost_ray" ["l"="60.165,23.615"]
"google-research/dreamer" ["l"="59.326,17.649"]
"tensorflow/tensor2tensor" ["l"="53.167,25.8"]
"pyro-ppl/pyro" ["l"="45.709,26.076"]
"PAIR-code/facets" ["l"="45.488,25.987"]
"horovod/horovod" ["l"="45.559,25.861"]
"facebookresearch/fairseq-lua" ["l"="53.204,25.644"]
"facebookarchive/caffe2" ["l"="47.886,29.061"]
"google/seq2seq" ["l"="53.179,25.669"]
"robfiras/loco-mujoco" ["l"="61.132,16.452"]
"chainer/chainermn" ["l"="-34.638,20.405"]
"chainer/chainercv" ["l"="50.947,30.086"]
"chainer/chainer" ["l"="47.887,29.112"]
"chainer/chainerui" ["l"="-34.648,20.382"]
"google-deepmind/dqn_zoo" ["l"="59.345,17.468"]
"uber-research/deep-neuroevolution" ["l"="-35.386,-2.787"]
"atgambardella/pytorch-es" ["l"="23.527,14.862"]
"hardmaru/estool" ["l"="59.253,17.759"]
"CMA-ES/pycma" ["l"="45.926,26.292"]
"cgearhart/students-filters" ["l"="57.27,17.574"]
"mjs2600/mimicry" ["l"="57.302,17.563"]
"rldm/rldm_tutorials" ["l"="57.364,17.694"]
"altera-al/project-sid" ["l"="58.056,18.543"]
"joonspk-research/genagents" ["l"="40.995,-4.237"]
"Thytu/Agentarium" ["l"="41.404,0.371"]
"etched-ai/open-oasis" ["l"="46.323,30.652"]
"mpaepper/llm_agents" ["l"="41.354,-3.88"]
"NousResearch/Open-Reasoning-Tasks" ["l"="37.017,-0.398"]
"kolbytn/mindcraft" ["l"="-35.197,-11.686"]
"AndersonJo/dqn-pytorch" ["l"="57.38,18.246"]
"keon/policy-gradient" ["l"="57.3,17.964"]
"thedimlebowski/Trading-Gym" ["l"="-9.585,14.715"]
"deependersingla/deep_trader" ["l"="-9.619,14.707"]
"henbudidiao/UAV-path-planning" ["l"="60.237,14.186"]
"thu-uav/Multi-UAV-pursuit-evasion" ["l"="58.035,18.185"]
"LYSJ-feng/mCPP-based-on-MADDPG" ["l"="62.799,-12.824"]
"dkadyrov/SIT" ["l"="58.171,17.651"]
"undeflabs/UASECO-Maniaplanet" ["l"="58.396,19.27"]
"Harha/trackmania-server-docker" ["l"="58.414,19.262"]
"maniaplanet/documentation" ["l"="58.43,19.249"]
"jingweiz/pytorch-dnc" ["l"="46.166,27.995"]
"sufengniu/GVIN" ["l"="57.289,18.339"]
"hamuchiwa/AutoRCCar" ["l"="61.528,12.493"]
"Blueteak/Unity-Neural-Network" ["l"="-35.084,-2.837"]
"Sentdex/pygta5" ["l"="61.584,12.38"]
"tawnkramer/sdsandbox" ["l"="61.499,12.651"]
"Helpsypoo/primerpython" ["l"="-24.693,-34.94"]
"ivanseidel/IAMDinosaur" ["l"="5.986,-20.351"]
"Unity-Technologies/FPSSample" ["l"="-27.119,-32.816"]
"ikostrikov/pytorch-meta-optimizer" ["l"="57.613,19.387"]
"awjuliani/dfp" ["l"="57.127,18.305"]
"siemens/policy_search_bb-alpha" ["l"="57.047,18.189"]
"andrewliao11/NoisyNet-DQN" ["l"="57.115,18.191"]
"nishantgurunath/HRL" ["l"="57.218,18.506"]
"GoingMyWay/ViZDoomAgents" ["l"="57.105,18.28"]
"mihahauke/deep_rl_vizdoom" ["l"="57.095,18.288"]
"rcsoccersim/rcsslogplayer" ["l"="56.903,18.659"]
"Islandman93/reinforcepy" ["l"="57.232,18.233"]
"steveKapturowski/async-deep-rl" ["l"="57.259,18.226"]
"achao2013/Learning-To-Reinforcement-Learn" ["l"="1.276,12.223"]
"mwufi/meta-rl-bandits" ["l"="57.272,18.224"]
"nsavinov/SPTM" ["l"="60.097,17.389"]
"bstadie/third_person_im" ["l"="57.416,18.428"]
"DartEnv/dart-env" ["l"="57.205,18.3"]
"DartEnv/gym-dart" ["l"="57.167,18.308"]
"Breakend/OptionGAN" ["l"="57.405,18.445"]
"kpaonaut/HAAR-A-Hierarchical-RL-Algorithm" ["l"="57.287,18.492"]
"onlytailei/A3C-PyTorch" ["l"="57.246,18.348"]
"onlytailei/pytorch-rl" ["l"="57.204,18.383"]
"onlytailei/Value-Iteration-Networks-PyTorch" ["l"="57.187,18.398"]
"MorvanZhou/Tensorflow-Tutorial" ["l"="50.52,28.354"]
"NovemberChopin/RL_Tutorial" ["l"="57.803,17.781"]
"facebookresearch/pytext" ["l"="52.976,25.701"]
"pytorch/ELF" ["l"="58.506,17.258"]
"facebookresearch/darkforestGo" ["l"="58.458,17.288"]
"cazala/synaptic" ["l"="-32.63,-35.896"]
"kaishengtai/neuralart" ["l"="45.668,29.433"]
"xviniette/FlappyLearning" ["l"="-32.537,-35.967"]
"channingbreeze/games" ["l"="-24.037,-33.737"]
"nicholas-ochoa/OpenSC2K" ["l"="-11.055,1.608"]
"wb14123/seq2seq-couplet" ["l"="-5.034,17.743"]
"Hopson97/MineCraft-One-Week-Challenge" ["l"="-23.514,-27.268"]
"minimaxir/person-blocker" ["l"="45.494,25.922"]
"MorvanZhou/Evolutionary-Algorithm" ["l"="49.868,26.196"]
"An0nym6/LandlordsEndgame" ["l"="58.365,18.437"]
"mjacar/pytorch-trpo" ["l"="57.469,18.249"]
"kimhc6028/pytorch-noreward-rl" ["l"="57.373,18.266"]
"4rChon/NL-FuN" ["l"="57.333,18.433"]
"Nat-D/FeatureControlHRL" ["l"="57.296,18.465"]
"Alescontrela/score_matching_rl" ["l"="58.139,18.111"]
"aviralkumar2907/BEAR" ["l"="59.474,17.597"]
"sohutv/hotcaffeine" ["l"="-53.055,-12.013"]
"stanfordnmbl/osim-rl" ["l"="61.112,16.056"]
"dchetelat/acer" ["l"="57.418,18.261"]
"chen0040/java-reinforcement-learning-flappy-bird" ["l"="57.149,17.577"]
"technobium/q-learning-java" ["l"="57.129,17.578"]
"Kiwoo/distributional_perspective_on_RL" ["l"="57.093,18.208"]
"maniaplanet/game-modes" ["l"="58.446,19.237"]
}