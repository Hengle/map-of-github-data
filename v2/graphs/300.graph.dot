digraph G {
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "zhjohnchan/awesome-image-captioning"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "ruotianluo/self-critical.pytorch"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "sgrvinod/Deep-Tutorials-for-PyTorch"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "peteanderson80/bottom-up-attention"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "tylin/coco-caption"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "aimagelab/meshed-memory-transformer"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "kelvinxu/arctic-captions"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "DeepRNN/image_captioning"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "rmokady/CLIP_prefix_caption" ["e"=1]
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "husthuaan/AoANet"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "jiasenlu/NeuralBabyTalk"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "yunjey/show-attend-and-tell"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "microsoft/Oscar"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" -> "sgrvinod/a-PyTorch-Tutorial-to-Object-Detection" ["e"=1]
"dabasajay/Image-Caption-Generator" -> "anuragmishracse/caption_generator"
"dabasajay/Image-Caption-Generator" -> "MiteshPuthran/Image-Caption-Generator"
"dabasajay/Image-Caption-Generator" -> "yashk2810/Image-Captioning"
"dabasajay/Image-Caption-Generator" -> "neural-nuts/image-caption-generator"
"dabasajay/Image-Caption-Generator" -> "IBM/MAX-Image-Caption-Generator"
"dabasajay/Image-Caption-Generator" -> "Shobhit20/Image-Captioning"
"dabasajay/Image-Caption-Generator" -> "damminhtien/deep-learning-image-caption-generator"
"dabasajay/Image-Caption-Generator" -> "OpenVisualCloud/Smart-City-Sample" ["e"=1]
"dabasajay/Image-Caption-Generator" -> "saahiluppal/catr"
"dabasajay/Image-Caption-Generator" -> "bhushan2311/image_caption_generator"
"jnhwkim/ban-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"jnhwkim/ban-vqa" -> "MILVLG/mcan-vqa"
"jnhwkim/ban-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"jnhwkim/ban-vqa" -> "Cadene/vqa.pytorch"
"jnhwkim/ban-vqa" -> "yuzcccc/vqa-mfb"
"jnhwkim/ban-vqa" -> "Cyanogenoid/vqa-counting"
"jnhwkim/ban-vqa" -> "Cadene/block.bootstrap.pytorch"
"jnhwkim/ban-vqa" -> "MILVLG/openvqa"
"jnhwkim/ban-vqa" -> "aimbrain/vqa-project"
"jnhwkim/ban-vqa" -> "Cadene/murel.bootstrap.pytorch"
"jnhwkim/ban-vqa" -> "peteanderson80/bottom-up-attention"
"jnhwkim/ban-vqa" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"jnhwkim/ban-vqa" -> "Cyanogenoid/pytorch-vqa"
"jnhwkim/ban-vqa" -> "linjieli222/VQA_ReGAT"
"jnhwkim/ban-vqa" -> "cvlab-tohoku/Dense-CoAttention-Network"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "jnhwkim/ban-vqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cyanogenoid/vqa-counting"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "MILVLG/openvqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "MILVLG/mcan-vqa"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "aimbrain/vqa-project"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cadene/murel.bootstrap.pytorch"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cadene/vqa.pytorch"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cadene/block.bootstrap.pytorch"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "jokieleung/awesome-visual-question-answering"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "ronghanghu/lcgn"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "peteanderson80/bottom-up-attention"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "yuzcccc/vqa-mfb"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "airsplay/lxmert"
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" -> "Cyanogenoid/pytorch-vqa"
"MILVLG/mcan-vqa" -> "MILVLG/openvqa"
"MILVLG/mcan-vqa" -> "jnhwkim/ban-vqa"
"MILVLG/mcan-vqa" -> "facebookresearch/grid-feats-vqa"
"MILVLG/mcan-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"MILVLG/mcan-vqa" -> "airsplay/lxmert"
"MILVLG/mcan-vqa" -> "linjieli222/VQA_ReGAT"
"MILVLG/mcan-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"MILVLG/mcan-vqa" -> "jokieleung/awesome-visual-question-answering"
"MILVLG/mcan-vqa" -> "yuzcccc/vqa-mfb"
"MILVLG/mcan-vqa" -> "peteanderson80/bottom-up-attention"
"MILVLG/mcan-vqa" -> "Cadene/murel.bootstrap.pytorch"
"MILVLG/mcan-vqa" -> "Cadene/block.bootstrap.pytorch"
"MILVLG/mcan-vqa" -> "Cadene/vqa.pytorch"
"MILVLG/mcan-vqa" -> "yanxinzju/CSS-VQA"
"MILVLG/mcan-vqa" -> "jiasenlu/vilbert_beta"
"bckim92/language-evaluation" -> "ctr4si/MMN"
"bckim92/language-evaluation" -> "bckim92/zsh-autoswitch-conda"
"jokieleung/awesome-visual-question-answering" -> "MILVLG/mcan-vqa"
"jokieleung/awesome-visual-question-answering" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"jokieleung/awesome-visual-question-answering" -> "xinke-wang/Awesome-Text-VQA"
"jokieleung/awesome-visual-question-answering" -> "MILVLG/openvqa"
"jokieleung/awesome-visual-question-answering" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"jokieleung/awesome-visual-question-answering" -> "peteanderson80/bottom-up-attention"
"jokieleung/awesome-visual-question-answering" -> "Cadene/vqa.pytorch"
"jokieleung/awesome-visual-question-answering" -> "forence/Awesome-Visual-Captioning"
"jokieleung/awesome-visual-question-answering" -> "facebookresearch/grid-feats-vqa"
"jokieleung/awesome-visual-question-answering" -> "airsplay/lxmert"
"jokieleung/awesome-visual-question-answering" -> "hengyuan-hu/bottom-up-attention-vqa"
"jokieleung/awesome-visual-question-answering" -> "linjieli222/VQA_ReGAT"
"jokieleung/awesome-visual-question-answering" -> "jnhwkim/ban-vqa"
"jokieleung/awesome-visual-question-answering" -> "Cadene/block.bootstrap.pytorch"
"jokieleung/awesome-visual-question-answering" -> "TheShadow29/awesome-grounding"
"codalab/codalab-competitions" -> "codalab/codalab-worksheets"
"codalab/codalab-competitions" -> "codalab/codabench"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Object-Detection" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Transformers"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Text-Classification" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "zhjohnchan/awesome-image-captioning"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "bentrevett/pytorch-seq2seq" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "ruotianluo/ImageCaptioning.pytorch"
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "ahkarami/Deep-Learning-in-Production" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "spro/practical-pytorch" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "dsgiitr/d2l-pytorch" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "ritchieng/the-incredible-pytorch" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "bharathgs/Awesome-pytorch-list" ["e"=1]
"sgrvinod/Deep-Tutorials-for-PyTorch" -> "Atcold/NYU-DLSP20" ["e"=1]
"facebookresearch/mmf" -> "peteanderson80/bottom-up-attention"
"facebookresearch/mmf" -> "pliang279/awesome-multimodal-ml"
"facebookresearch/mmf" -> "facebookresearch/pytext" ["e"=1]
"facebookresearch/mmf" -> "hengyuan-hu/bottom-up-attention-vqa"
"facebookresearch/mmf" -> "microsoft/Oscar"
"facebookresearch/mmf" -> "airsplay/lxmert"
"facebookresearch/mmf" -> "zihangdai/xlnet" ["e"=1]
"facebookresearch/mmf" -> "facebookresearch/vilbert-multi-task"
"facebookresearch/mmf" -> "facebookresearch/XLM" ["e"=1]
"facebookresearch/mmf" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"facebookresearch/mmf" -> "facebookresearch/maskrcnn-benchmark" ["e"=1]
"facebookresearch/mmf" -> "allenai/allennlp" ["e"=1]
"facebookresearch/mmf" -> "facebookresearch/ParlAI" ["e"=1]
"facebookresearch/mmf" -> "facebookresearch/multimodal" ["e"=1]
"facebookresearch/mmf" -> "ChenRocks/UNITER"
"computationalmedia/semstyle" -> "kacky24/stylenet"
"computationalmedia/semstyle" -> "yiyang92/caption-stylenet_tensorflow"
"computationalmedia/semstyle" -> "andyweizhao/Multitask_Image_Captioning"
"pliang279/awesome-multimodal-ml" -> "Eurus-Holmes/Awesome-Multimodal-Research"
"pliang279/awesome-multimodal-ml" -> "BradyFU/Awesome-Multimodal-Large-Language-Models" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "jason718/awesome-self-supervised-learning" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "facebookresearch/mmf"
"pliang279/awesome-multimodal-ml" -> "salesforce/LAVIS" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"pliang279/awesome-multimodal-ml" -> "diff-usion/Awesome-Diffusion-Models" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "openai/CLIP" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "salesforce/ALBEF"
"pliang279/awesome-multimodal-ml" -> "microsoft/unilm" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "thunlp/PromptPapers" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "A2Zadeh/CMU-MultimodalSDK" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "mlfoundations/open_clip" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "lucidrains/vit-pytorch" ["e"=1]
"pliang279/awesome-multimodal-ml" -> "facebookresearch/moco" ["e"=1]
"Cadene/bootstrap.pytorch" -> "Cadene/block.bootstrap.pytorch"
"Cadene/bootstrap.pytorch" -> "Cadene/murel.bootstrap.pytorch"
"Cadene/bootstrap.pytorch" -> "ThomasRobertFr/gpu-monitor"
"Cadene/bootstrap.pytorch" -> "emited/gantk2"
"Cadene/bootstrap.pytorch" -> "cdancette/rubi.bootstrap.pytorch"
"Cadene/bootstrap.pytorch" -> "cdancette/detect-shortcuts"
"google-deepmind/abstract-reasoning-matrices" -> "WellyZhang/RAVEN"
"google-deepmind/abstract-reasoning-matrices" -> "Fen9/WReN"
"google-deepmind/abstract-reasoning-matrices" -> "WellyZhang/CoPINet"
"google-deepmind/abstract-reasoning-matrices" -> "husheng12345/SRAN"
"WellyZhang/RAVEN" -> "Fen9/WReN"
"WellyZhang/RAVEN" -> "zkcys001/distracting_feature"
"WellyZhang/RAVEN" -> "google-deepmind/abstract-reasoning-matrices"
"WellyZhang/RAVEN" -> "WellyZhang/CoPINet"
"WellyZhang/RAVEN" -> "husheng12345/SRAN"
"WellyZhang/RAVEN" -> "WellyZhang/PrAE"
"WellyZhang/RAVEN" -> "dhh1995/SCL"
"WellyZhang/RAVEN" -> "mjedmonds/OpenLock" ["e"=1]
"facebookresearch/phyre" -> "HaozhiQi/RPIN"
"facebookresearch/phyre" -> "WellyZhang/RAVEN"
"facebookresearch/phyre" -> "k-r-allen/tool-games" ["e"=1]
"facebookresearch/phyre" -> "facebookresearch/clevr-dataset-gen"
"facebookresearch/phyre" -> "Fen9/WReN"
"facebookresearch/phyre" -> "phy-q/benchmark"
"facebookresearch/phyre" -> "lvis-dataset/lvis-api" ["e"=1]
"facebookresearch/phyre" -> "facebookresearch/fair_self_supervision_benchmark" ["e"=1]
"facebookresearch/phyre" -> "rr-learning/CausalWorld"
"wookayin/gpustat-web" -> "wookayin/dotfiles"
"wookayin/gpustat-web" -> "wookayin/gpustat" ["e"=1]
"wookayin/gpustat-web" -> "fgaim/gpuview"
"ethanjperez/film" -> "facebookresearch/clevr-iep" ["e"=1]
"ethanjperez/film" -> "caffeinism/FiLM-pytorch"
"ethanjperez/film" -> "stanfordnlp/mac-network"
"ethanjperez/film" -> "devendrachaplot/DeepRL-Grounding" ["e"=1]
"ethanjperez/film" -> "davidmascharka/tbd-nets"
"ethanjperez/film" -> "facebookresearch/clevr-dataset-gen"
"ethanjperez/film" -> "Cadene/murel.bootstrap.pytorch"
"ethanjperez/film" -> "jnhwkim/ban-vqa"
"zhjohnchan/awesome-image-captioning" -> "ruotianluo/self-critical.pytorch"
"zhjohnchan/awesome-image-captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"zhjohnchan/awesome-image-captioning" -> "forence/Awesome-Visual-Captioning"
"zhjohnchan/awesome-image-captioning" -> "husthuaan/AoANet"
"zhjohnchan/awesome-image-captioning" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"zhjohnchan/awesome-image-captioning" -> "peteanderson80/bottom-up-attention"
"zhjohnchan/awesome-image-captioning" -> "aimagelab/meshed-memory-transformer"
"zhjohnchan/awesome-image-captioning" -> "fengyang0317/unsupervised_captioning"
"zhjohnchan/awesome-image-captioning" -> "jiasenlu/NeuralBabyTalk"
"zhjohnchan/awesome-image-captioning" -> "JDAI-CV/image-captioning"
"zhjohnchan/awesome-image-captioning" -> "tylin/coco-caption"
"zhjohnchan/awesome-image-captioning" -> "aimagelab/show-control-and-tell"
"zhjohnchan/awesome-image-captioning" -> "yangxuntu/SGAE"
"zhjohnchan/awesome-image-captioning" -> "yahoo/object_relation_transformer"
"zhjohnchan/awesome-image-captioning" -> "luo3300612/image-captioning-DLCT"
"hengyuan-hu/bottom-up-attention-vqa" -> "peteanderson80/bottom-up-attention"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cadene/vqa.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "jnhwkim/ban-vqa"
"hengyuan-hu/bottom-up-attention-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "MILVLG/mcan-vqa"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cyanogenoid/pytorch-vqa"
"hengyuan-hu/bottom-up-attention-vqa" -> "peteanderson80/Up-Down-Captioner"
"hengyuan-hu/bottom-up-attention-vqa" -> "GT-Vision-Lab/VQA"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cadene/block.bootstrap.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "markdtw/vqa-winner-cvprw-2017"
"hengyuan-hu/bottom-up-attention-vqa" -> "airsplay/lxmert"
"hengyuan-hu/bottom-up-attention-vqa" -> "yuzcccc/vqa-mfb"
"hengyuan-hu/bottom-up-attention-vqa" -> "ruotianluo/self-critical.pytorch"
"hengyuan-hu/bottom-up-attention-vqa" -> "Cyanogenoid/vqa-counting"
"hengyuan-hu/bottom-up-attention-vqa" -> "chingyaoc/awesome-vqa"
"aimagelab/show-control-and-tell" -> "fengyang0317/unsupervised_captioning"
"aimagelab/show-control-and-tell" -> "yangxuntu/SGAE"
"aimagelab/show-control-and-tell" -> "husthuaan/AoANet"
"aimagelab/show-control-and-tell" -> "cshizhe/asg2cap"
"aimagelab/show-control-and-tell" -> "JDAI-CV/image-captioning"
"aimagelab/show-control-and-tell" -> "yahoo/object_relation_transformer"
"aimagelab/show-control-and-tell" -> "jiasenlu/NeuralBabyTalk"
"aimagelab/show-control-and-tell" -> "ruotianluo/DiscCaptioning"
"aimagelab/show-control-and-tell" -> "aimagelab/meshed-memory-transformer"
"aimagelab/show-control-and-tell" -> "poojahira/image-captioning-bottom-up-top-down"
"aimagelab/show-control-and-tell" -> "ruotianluo/self-critical.pytorch"
"aimagelab/show-control-and-tell" -> "peteanderson80/Up-Down-Captioner"
"aimagelab/show-control-and-tell" -> "luo3300612/image-captioning-DLCT"
"aimagelab/show-control-and-tell" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"aimagelab/show-control-and-tell" -> "tsenghungchen/show-adapt-and-tell"
"MILVLG/openvqa" -> "MILVLG/mcan-vqa"
"MILVLG/openvqa" -> "yuzcccc/vqa-mfb"
"MILVLG/openvqa" -> "facebookresearch/grid-feats-vqa"
"MILVLG/openvqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"MILVLG/openvqa" -> "jnhwkim/ban-vqa"
"MILVLG/openvqa" -> "asdf0982/vqa-mfb.pytorch"
"MILVLG/openvqa" -> "jokieleung/awesome-visual-question-answering"
"MILVLG/openvqa" -> "airsplay/lxmert"
"MILVLG/openvqa" -> "linjieli222/VQA_ReGAT"
"MILVLG/openvqa" -> "Cyanogenoid/vqa-counting"
"MILVLG/openvqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"MILVLG/openvqa" -> "yanxinzju/CSS-VQA"
"MILVLG/openvqa" -> "Cadene/murel.bootstrap.pytorch"
"MILVLG/openvqa" -> "cvlab-tohoku/Dense-CoAttention-Network"
"MILVLG/openvqa" -> "Cadene/block.bootstrap.pytorch"
"google-research-datasets/conceptual-captions" -> "google-research-datasets/conceptual-12m"
"google-research-datasets/conceptual-captions" -> "fartashf/vsepp" ["e"=1]
"google-research-datasets/conceptual-captions" -> "igorbrigadir/DownloadConceptualCaptions"
"google-research-datasets/conceptual-captions" -> "lichengunc/refer"
"google-research-datasets/conceptual-captions" -> "facebookresearch/vilbert-multi-task"
"google-research-datasets/conceptual-captions" -> "google-research-datasets/wit"
"google-research-datasets/conceptual-captions" -> "microsoft/Oscar"
"google-research-datasets/conceptual-captions" -> "peteanderson80/bottom-up-attention"
"google-research-datasets/conceptual-captions" -> "jiasenlu/NeuralBabyTalk"
"google-research-datasets/conceptual-captions" -> "jiasenlu/vilbert_beta"
"google-research-datasets/conceptual-captions" -> "peteanderson80/Up-Down-Captioner"
"google-research-datasets/conceptual-captions" -> "yahoo/object_relation_transformer"
"google-research-datasets/conceptual-captions" -> "ruotianluo/self-critical.pytorch"
"google-research-datasets/conceptual-captions" -> "rowanz/r2c" ["e"=1]
"google-research-datasets/conceptual-captions" -> "ranjaykrishna/visual_genome_python_driver" ["e"=1]
"foamliu/Image-Captioning-PyTorch" -> "foamliu/Image-Captioning"
"foamliu/Image-Captioning-PyTorch" -> "HughChi/Image-Caption"
"foamliu/Image-Captioning-PyTorch" -> "showkeyjar/chinese_im2text.pytorch"
"foamliu/Image-Captioning-PyTorch" -> "ruotianluo/Image_Captioning_AI_Challenger"
"stanfordnlp/mac-network" -> "rosinality/mac-network-pytorch"
"stanfordnlp/mac-network" -> "kexinyi/ns-vqa"
"stanfordnlp/mac-network" -> "hengyuan-hu/bottom-up-attention-vqa"
"stanfordnlp/mac-network" -> "ceyzaguirre4/NSM"
"stanfordnlp/mac-network" -> "jnhwkim/ban-vqa"
"stanfordnlp/mac-network" -> "facebookresearch/clevr-iep" ["e"=1]
"stanfordnlp/mac-network" -> "vacancy/NSCL-PyTorch-Release" ["e"=1]
"stanfordnlp/mac-network" -> "Cadene/murel.bootstrap.pytorch"
"stanfordnlp/mac-network" -> "ronghanghu/n2nmn"
"stanfordnlp/mac-network" -> "davidmascharka/tbd-nets"
"stanfordnlp/mac-network" -> "peteanderson80/bottom-up-attention"
"stanfordnlp/mac-network" -> "MILVLG/mcan-vqa"
"stanfordnlp/mac-network" -> "facebookresearch/clevr-dataset-gen"
"stanfordnlp/mac-network" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"stanfordnlp/mac-network" -> "Cadene/vqa.pytorch"
"TheShadow29/awesome-grounding" -> "lichengunc/refer"
"TheShadow29/awesome-grounding" -> "ashkamath/mdetr"
"TheShadow29/awesome-grounding" -> "djiajunustc/TransVG"
"TheShadow29/awesome-grounding" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"TheShadow29/awesome-grounding" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"TheShadow29/awesome-grounding" -> "lichengunc/MAttNet" ["e"=1]
"TheShadow29/awesome-grounding" -> "microsoft/GLIP" ["e"=1]
"TheShadow29/awesome-grounding" -> "jokieleung/awesome-visual-question-answering"
"TheShadow29/awesome-grounding" -> "facebookresearch/grounded-video-description" ["e"=1]
"TheShadow29/awesome-grounding" -> "jianzongwu/Awesome-Open-Vocabulary" ["e"=1]
"TheShadow29/awesome-grounding" -> "vacancy/SceneGraphParser" ["e"=1]
"TheShadow29/awesome-grounding" -> "iworldtong/Awesome-Temporal-Sentence-Grounding-in-Videos" ["e"=1]
"TheShadow29/awesome-grounding" -> "zyang-ur/onestage_grounding"
"TheShadow29/awesome-grounding" -> "microsoft/Oscar"
"TheShadow29/awesome-grounding" -> "peteanderson80/bottom-up-attention"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "pliang279/awesome-multimodal-ml"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "A2Zadeh/CMU-MultimodalSDK" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "yaohungt/Multimodal-Transformer" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "declare-lab/multimodal-deep-learning" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "microsoft/Oscar"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "forence/Awesome-Visual-Captioning"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "salesforce/ALBEF"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "dandelin/ViLT"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "soujanyaporia/multimodal-sentiment-analysis" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "jokieleung/awesome-visual-question-answering"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "thuiar/MMSA" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "danieljf24/awesome-video-text-retrieval" ["e"=1]
"Eurus-Holmes/Awesome-Multimodal-Research" -> "facebookresearch/mmf"
"Eurus-Holmes/Awesome-Multimodal-Research" -> "jayleicn/ClipBERT"
"AIChallenger/AI_Challenger_2017" -> "AIChallenger/AI_Challenger_2018" ["e"=1]
"AIChallenger/AI_Challenger_2017" -> "ruotianluo/Image_Captioning_AI_Challenger"
"AIChallenger/AI_Challenger_2017" -> "bearpaw/PyraNet" ["e"=1]
"AIChallenger/AI_Challenger_2017" -> "matteorr/coco-analyze" ["e"=1]
"AIChallenger/AI_Challenger_2017" -> "TuSimple/mx-maskrcnn" ["e"=1]
"AIChallenger/AI_Challenger_2017" -> "ruotianluo/ImageCaptioning.pytorch"
"AIChallenger/AI_Challenger_2017" -> "ruotianluo/self-critical.pytorch"
"AIChallenger/AI_Challenger_2017" -> "HouJP/kaggle-quora-question-pairs" ["e"=1]
"AIChallenger/AI_Challenger_2017" -> "chenyuntc/scene-baseline" ["e"=1]
"AIChallenger/AI_Challenger_2017" -> "HughChi/Image-Caption"
"AIChallenger/AI_Challenger_2017" -> "gujiuxiang/chinese_im2text.pytorch"
"AIChallenger/AI_Challenger_2017" -> "MVIG-SJTU/RMPE" ["e"=1]
"salaniz/pycocoevalcap" -> "tylin/coco-caption"
"salaniz/pycocoevalcap" -> "ramavedantam/cider"
"salaniz/pycocoevalcap" -> "yahoo/object_relation_transformer"
"salaniz/pycocoevalcap" -> "ruotianluo/self-critical.pytorch"
"salaniz/pycocoevalcap" -> "wangleihitcs/CaptionMetrics"
"salaniz/pycocoevalcap" -> "jiasenlu/NeuralBabyTalk"
"salaniz/pycocoevalcap" -> "forence/Awesome-Visual-Captioning"
"salaniz/pycocoevalcap" -> "husthuaan/AoANet"
"salaniz/pycocoevalcap" -> "zhjohnchan/awesome-image-captioning"
"salaniz/pycocoevalcap" -> "facebookresearch/grounded-video-description" ["e"=1]
"salaniz/pycocoevalcap" -> "yangxuntu/SGAE"
"salaniz/pycocoevalcap" -> "cshizhe/asg2cap"
"salaniz/pycocoevalcap" -> "salesforce/densecap" ["e"=1]
"salaniz/pycocoevalcap" -> "ruotianluo/ImageCaptioning.pytorch"
"salaniz/pycocoevalcap" -> "krasserm/fairseq-image-captioning"
"furkanbiten/GoodNews" -> "alasdairtran/transform-and-tell"
"furkanbiten/GoodNews" -> "AndresPMD/StacMR"
"furkanbiten/GoodNews" -> "AndresPMD/Fine_Grained_Clf"
"furkanbiten/GoodNews" -> "FuxiaoLiu/VisualNews-Repository" ["e"=1]
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "yz93/LAVT-RIS"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "DerrickWang005/CRIS.pytorch"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "lichengunc/refer"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "TheShadow29/awesome-grounding"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "wjn922/ReferFormer"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "amazon-science/polygon-transformer"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "kkakkkka/ETRIS"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "lxtGH/Awesome-Segmentation-With-Transformer" ["e"=1]
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "henghuiding/Vision-Language-Transformer" ["e"=1]
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "Qinying-Liu/Awesome-Open-Vocabulary-Semantic-Segmentation" ["e"=1]
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "dvlab-research/LISA" ["e"=1]
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "henghuiding/ReLA" ["e"=1]
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "JerryX1110/awesome-rvos"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "Seonghoon-Yu/Zero-shot-RIS"
"MarkMoHR/Awesome-Referring-Image-Segmentation" -> "henghuiding/gRefCOCO" ["e"=1]
"necla-ml/SNLI-VE" -> "maximek3/e-ViL"
"necla-ml/SNLI-VE" -> "BryanPlummer/flickr30k_entities"
"hmorioka/TCL" -> "siamakz/iVAE"
"ilkhem/iVAE" -> "ilkhem/icebeem"
"ilkhem/iVAE" -> "phlippe/CITRIS"
"njchoma/transformer_image_caption" -> "poojahira/image-captioning-bottom-up-top-down"
"njchoma/transformer_image_caption" -> "yahoo/object_relation_transformer"
"jiasenlu/NeuralBabyTalk" -> "ruotianluo/self-critical.pytorch"
"jiasenlu/NeuralBabyTalk" -> "aimagelab/show-control-and-tell"
"jiasenlu/NeuralBabyTalk" -> "jiasenlu/AdaptiveAttention"
"jiasenlu/NeuralBabyTalk" -> "ruotianluo/ImageCaptioning.pytorch"
"jiasenlu/NeuralBabyTalk" -> "zhjohnchan/awesome-image-captioning"
"jiasenlu/NeuralBabyTalk" -> "peteanderson80/Up-Down-Captioner"
"jiasenlu/NeuralBabyTalk" -> "fengyang0317/unsupervised_captioning"
"jiasenlu/NeuralBabyTalk" -> "peteanderson80/bottom-up-attention"
"jiasenlu/NeuralBabyTalk" -> "husthuaan/AoANet"
"jiasenlu/NeuralBabyTalk" -> "yangxuntu/SGAE"
"jiasenlu/NeuralBabyTalk" -> "aditya12agd5/convcap"
"jiasenlu/NeuralBabyTalk" -> "poojahira/image-captioning-bottom-up-top-down"
"jiasenlu/NeuralBabyTalk" -> "aimagelab/meshed-memory-transformer"
"jiasenlu/NeuralBabyTalk" -> "gujiuxiang/Stack-Captioning"
"jiasenlu/NeuralBabyTalk" -> "ruotianluo/Transformer_Captioning"
"doubledaibo/gancaption_iccv2017" -> "rakshithShetty/captionGAN"
"doubledaibo/gancaption_iccv2017" -> "doubledaibo/clcaption_nips2017"
"doubledaibo/gancaption_iccv2017" -> "yiyang92/vae_captioning"
"doubledaibo/gancaption_iccv2017" -> "ruotianluo/DiscCaptioning"
"doubledaibo/gancaption_iccv2017" -> "zhegan27/Semantic_Compositional_Nets"
"aditya12agd5/convcap" -> "chenxinpeng/ARNet"
"aditya12agd5/convcap" -> "aimagelab/show-control-and-tell"
"aditya12agd5/convcap" -> "daqingliu/CAVP"
"aditya12agd5/convcap" -> "gujiuxiang/Stack-Captioning"
"aditya12agd5/convcap" -> "lukemelas/image-paragraph-captioning"
"aditya12agd5/convcap" -> "ruotianluo/DiscCaptioning"
"aditya12agd5/convcap" -> "ruotianluo/Transformer_Captioning"
"aditya12agd5/convcap" -> "jiasenlu/NeuralBabyTalk"
"aditya12agd5/convcap" -> "s-gupta/visual-concepts"
"aditya12agd5/convcap" -> "fengyang0317/unsupervised_captioning"
"aditya12agd5/convcap" -> "andyweizhao/Multitask_Image_Captioning"
"aditya12agd5/convcap" -> "cswhjiang/Recurrent_Fusion_Network"
"rakshithShetty/captionGAN" -> "doubledaibo/gancaption_iccv2017"
"rakshithShetty/captionGAN" -> "yiyang92/vae_captioning"
"rakshithShetty/captionGAN" -> "andyweizhao/Multitask_Image_Captioning"
"rakshithShetty/captionGAN" -> "zhegan27/Semantic_Compositional_Nets"
"rakshithShetty/captionGAN" -> "cswhjiang/Recurrent_Fusion_Network"
"ChenyunWu/PhraseCutDataset" -> "spyflying/CMPC-Refseg"
"ChenyunWu/PhraseCutDataset" -> "Seonghoon-Yu/Zero-shot-RIS"
"ChenyunWu/PhraseCutDataset" -> "ccvl/iep-ref"
"ajamjoom/Image-Captions" -> "yahoo/object_relation_transformer"
"ajamjoom/Image-Captions" -> "RoyalSkye/Image-Caption"
"ruotianluo/DiscCaptioning" -> "richardaecn/cvpr18-caption-eval"
"ruotianluo/DiscCaptioning" -> "gujiuxiang/Stack-Captioning"
"ruotianluo/DiscCaptioning" -> "cswhjiang/Recurrent_Fusion_Network"
"ruotianluo/DiscCaptioning" -> "fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning"
"ruotianluo/DiscCaptioning" -> "ruotianluo/Transformer_Captioning"
"ruotianluo/DiscCaptioning" -> "doubledaibo/clcaption_nips2017"
"ruotianluo/DiscCaptioning" -> "doubledaibo/gancaption_iccv2017"
"foamliu/Image-Captioning" -> "HughChi/Image-Caption"
"foamliu/Image-Captioning" -> "foamliu/Image-Captioning-PyTorch"
"foamliu/Image-Captioning" -> "showkeyjar/chinese_im2text.pytorch"
"foamliu/Image-Captioning" -> "TimoYoung/im2txt_Chinese"
"peteanderson80/Up-Down-Captioner" -> "peteanderson80/bottom-up-attention"
"peteanderson80/Up-Down-Captioner" -> "poojahira/image-captioning-bottom-up-top-down"
"peteanderson80/Up-Down-Captioner" -> "hengyuan-hu/bottom-up-attention-vqa"
"peteanderson80/Up-Down-Captioner" -> "ruotianluo/self-critical.pytorch"
"peteanderson80/Up-Down-Captioner" -> "aimagelab/show-control-and-tell"
"peteanderson80/Up-Down-Captioner" -> "jiasenlu/NeuralBabyTalk"
"peteanderson80/Up-Down-Captioner" -> "fengyang0317/unsupervised_captioning"
"peteanderson80/Up-Down-Captioner" -> "yangxuntu/SGAE"
"peteanderson80/Up-Down-Captioner" -> "jiasenlu/AdaptiveAttention"
"peteanderson80/Up-Down-Captioner" -> "husthuaan/AoANet"
"peteanderson80/Up-Down-Captioner" -> "ruotianluo/DiscCaptioning"
"peteanderson80/Up-Down-Captioner" -> "chenxinpeng/ARNet"
"peteanderson80/Up-Down-Captioner" -> "JDAI-CV/image-captioning"
"peteanderson80/Up-Down-Captioner" -> "violetteshev/bottom-up-features"
"peteanderson80/Up-Down-Captioner" -> "gujiuxiang/Stack-Captioning"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "aimbrain/vqa-project"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "abhshkdz/neural-vqa-attention"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "jialinwu17/self_critical_vqa"
"SinghJasdeep/Attention-on-Attention-for-VQA" -> "markdtw/vqa-winner-cvprw-2017"
"violetteshev/bottom-up-features" -> "poojahira/image-captioning-bottom-up-top-down"
"tbmoon/basic_vqa" -> "Cyanogenoid/pytorch-vqa"
"tbmoon/basic_vqa" -> "noagarcia/awesome-vqa-pytorch"
"li-xirong/coco-cn" -> "li-xirong/cross-lingual-cap"
"li-xirong/coco-cn" -> "BAAI-WuDao/BriVL" ["e"=1]
"li-xirong/coco-cn" -> "ruotianluo/Image_Captioning_AI_Challenger"
"li-xirong/coco-cn" -> "weiyuk/fluent-cap"
"kexinyi/ns-vqa" -> "vacancy/NSCL-PyTorch-Release" ["e"=1]
"kexinyi/ns-vqa" -> "facebookresearch/clevr-dataset-gen"
"kexinyi/ns-vqa" -> "chuangg/CLEVRER"
"kexinyi/ns-vqa" -> "stanfordnlp/mac-network"
"kexinyi/ns-vqa" -> "facebookresearch/clevr-iep" ["e"=1]
"kexinyi/ns-vqa" -> "nerdimite/neuro-symbolic-ai-soc"
"kexinyi/ns-vqa" -> "kdexd/probnmn-clevr"
"kexinyi/ns-vqa" -> "vacancy/Jacinle" ["e"=1]
"kexinyi/ns-vqa" -> "ronghanghu/snmn"
"kexinyi/ns-vqa" -> "google-research/clevr_robot_env"
"kexinyi/ns-vqa" -> "ceyzaguirre4/NSM"
"kexinyi/ns-vqa" -> "satwikkottur/clevr-dialog"
"kexinyi/ns-vqa" -> "ronghanghu/n2nmn"
"kexinyi/ns-vqa" -> "microsoft/DFOL-VQA"
"kexinyi/ns-vqa" -> "HobbitLong/shape2prog" ["e"=1]
"asdf0982/vqa-mfb.pytorch" -> "yuzcccc/vqa-mfb"
"caffeinism/FiLM-pytorch" -> "caffeinism/StyleGAN-pytorch" ["e"=1]
"zyang-ur/onestage_grounding" -> "zyang-ur/ReSC"
"zyang-ur/onestage_grounding" -> "svip-lab/LBYLNet"
"zyang-ur/onestage_grounding" -> "nku-shengzheliu/Pytorch-TransVG"
"zyang-ur/onestage_grounding" -> "ChopinSharp/ref-nms"
"zyang-ur/onestage_grounding" -> "lichengunc/MAttNet" ["e"=1]
"zyang-ur/onestage_grounding" -> "djiajunustc/TransVG"
"zyang-ur/onestage_grounding" -> "zjunlp/HVPNeT" ["e"=1]
"zyang-ur/onestage_grounding" -> "BigRedT/info-ground"
"zyang-ur/onestage_grounding" -> "yangli18/VLTVG"
"zyang-ur/onestage_grounding" -> "luogen1996/MCN"
"zyang-ur/onestage_grounding" -> "thecharm/Mega" ["e"=1]
"markdtw/vqa-winner-cvprw-2017" -> "Cadene/vqa.pytorch"
"markdtw/vqa-winner-cvprw-2017" -> "hengyuan-hu/bottom-up-attention-vqa"
"markdtw/vqa-winner-cvprw-2017" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"markdtw/vqa-winner-cvprw-2017" -> "Cyanogenoid/pytorch-vqa"
"markdtw/vqa-winner-cvprw-2017" -> "asdf0982/vqa-mfb.pytorch"
"markdtw/vqa-winner-cvprw-2017" -> "aimbrain/vqa-project"
"markdtw/vqa-winner-cvprw-2017" -> "abhshkdz/neural-vqa-attention"
"markdtw/vqa-winner-cvprw-2017" -> "yuzcccc/vqa-mfb"
"markdtw/vqa-winner-cvprw-2017" -> "chingyaoc/awesome-vqa"
"markdtw/vqa-winner-cvprw-2017" -> "jnhwkim/ban-vqa"
"davidmascharka/tbd-nets" -> "endernewton/iter-reason" ["e"=1]
"davidmascharka/tbd-nets" -> "rsokl/MyGrad"
"davidmascharka/tbd-nets" -> "stanfordnlp/mac-network"
"davidmascharka/tbd-nets" -> "ronghanghu/n2nmn"
"davidmascharka/tbd-nets" -> "rsokl/noggin"
"davidmascharka/tbd-nets" -> "facebookresearch/clevr-iep" ["e"=1]
"davidmascharka/tbd-nets" -> "Cadene/murel.bootstrap.pytorch"
"davidmascharka/tbd-nets" -> "Cyanogenoid/vqa-counting"
"davidmascharka/tbd-nets" -> "ronghanghu/snmn"
"davidmascharka/tbd-nets" -> "facebookresearch/clevr-dataset-gen"
"e2crawfo/auto_yolo" -> "stelzner/supair"
"e2crawfo/auto_yolo" -> "Abishekpras/Attend-Infer-Repeat---Pytorch"
"e2crawfo/auto_yolo" -> "aakhundov/tf-attend-infer-repeat"
"Cadene/block.bootstrap.pytorch" -> "Cadene/vqa.pytorch"
"Cadene/block.bootstrap.pytorch" -> "Cadene/murel.bootstrap.pytorch"
"Cadene/block.bootstrap.pytorch" -> "jnhwkim/ban-vqa"
"Cadene/block.bootstrap.pytorch" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cadene/block.bootstrap.pytorch" -> "Cadene/bootstrap.pytorch"
"Cadene/block.bootstrap.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cadene/block.bootstrap.pytorch" -> "MILVLG/mcan-vqa"
"Cadene/block.bootstrap.pytorch" -> "linjieli222/VQA_ReGAT"
"Cadene/block.bootstrap.pytorch" -> "Cyanogenoid/vqa-counting"
"Cadene/block.bootstrap.pytorch" -> "Cyanogenoid/pytorch-vqa"
"Cadene/block.bootstrap.pytorch" -> "aimbrain/vqa-project"
"Cadene/block.bootstrap.pytorch" -> "chingyaoc/awesome-vqa"
"Cadene/block.bootstrap.pytorch" -> "GT-Vision-Lab/VQA"
"Cadene/block.bootstrap.pytorch" -> "jokieleung/awesome-visual-question-answering"
"Cadene/block.bootstrap.pytorch" -> "peteanderson80/bottom-up-attention"
"Yu-Wu/Decoupled-Novel-Object-Captioner" -> "LisaAnne/DCC"
"zhixuan-lin/IODINE" -> "zhixuan-lin/SPACE"
"zhixuan-lin/IODINE" -> "baudm/MONet-pytorch"
"zhixuan-lin/IODINE" -> "applied-ai-lab/genesis"
"zhixuan-lin/IODINE" -> "stelzner/monet"
"zhixuan-lin/IODINE" -> "zhixuan-lin/G-SWM"
"zhixuan-lin/IODINE" -> "Abishekpras/Attend-Infer-Repeat---Pytorch"
"ctr4si/MMN" -> "bckim92/zsh-autoswitch-conda"
"ctr4si/MMN" -> "bckim92/language-evaluation"
"ctr4si/MMN" -> "ucfnlp/summarization-sing-pair-mix" ["e"=1]
"li-xirong/cross-lingual-cap" -> "HughChi/Image-Caption"
"li-xirong/cross-lingual-cap" -> "li-xirong/coco-cn"
"li-xirong/cross-lingual-cap" -> "weiyuk/fluent-cap"
"HughChi/Image-Caption" -> "foamliu/Image-Captioning"
"HughChi/Image-Caption" -> "foamliu/Image-Captioning-PyTorch"
"HughChi/Image-Caption" -> "li-xirong/cross-lingual-cap"
"HughChi/Image-Caption" -> "showkeyjar/chinese_im2text.pytorch"
"HughChi/Image-Caption" -> "ruotianluo/Image_Captioning_AI_Challenger"
"HughChi/Image-Caption" -> "lxtGH/AI_challenger_Chinese_Caption"
"HughChi/Image-Caption" -> "weiyuk/fluent-cap"
"rsokl/Learning_Python" -> "rsokl/MyGrad"
"siamakz/iVAE" -> "ilkhem/icebeem"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "jiasenlu/visDial.pytorch"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "batra-mlp-lab/visdial-rl"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "yuleiniu/rva"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "gicheonkang/dan-visdial"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "vmurahari3/visdial-bert"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "batra-mlp-lab/visdial"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "zilongzheng/visdial-gnn"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "taesunwhang/MVAN-VisDial"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge" ["e"=1]
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "facebookresearch/corefnmn"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "shubhamagarwal92/visdial_conv"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "simpleshinobu/visdial-principles"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "jnhwkim/ban-vqa"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "naver/aqm-plus"
"batra-mlp-lab/visdial-challenge-starter-pytorch" -> "idansc/mrr-ndcg"
"shijx12/XNM-Net" -> "ronghanghu/snmn"
"shijx12/XNM-Net" -> "ronghanghu/lcgn"
"aimbrain/vqa-project" -> "Cadene/murel.bootstrap.pytorch"
"aimbrain/vqa-project" -> "linjieli222/VQA_ReGAT"
"aimbrain/vqa-project" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"aimbrain/vqa-project" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"aimbrain/vqa-project" -> "jialinwu17/self_critical_vqa"
"aimbrain/vqa-project" -> "jnhwkim/ban-vqa"
"aimbrain/vqa-project" -> "Cyanogenoid/pytorch-vqa"
"aimbrain/vqa-project" -> "Cyanogenoid/vqa-counting"
"aimbrain/vqa-project" -> "gicheonkang/dan-visdial"
"aimbrain/vqa-project" -> "hengyuan-hu/bottom-up-attention-vqa"
"BryanPlummer/flickr30k_entities" -> "lichengunc/refer"
"BryanPlummer/flickr30k_entities" -> "necla-ml/SNLI-VE"
"BryanPlummer/flickr30k_entities" -> "zyang-ur/ReSC"
"BryanPlummer/flickr30k_entities" -> "djiajunustc/TransVG"
"BryanPlummer/flickr30k_entities" -> "zyang-ur/onestage_grounding"
"BryanPlummer/flickr30k_entities" -> "yangli18/VLTVG"
"Shobhit20/Image-Captioning" -> "anuragmishracse/caption_generator"
"Shobhit20/Image-Captioning" -> "vinaybysani/Image-Captioning-System--Deep-learning"
"Shobhit20/Image-Captioning" -> "damminhtien/deep-learning-image-caption-generator"
"cdancette/rubi.bootstrap.pytorch" -> "jialinwu17/self_critical_vqa"
"cdancette/rubi.bootstrap.pytorch" -> "chrisc36/debias"
"cdancette/rubi.bootstrap.pytorch" -> "yanxinzju/CSS-VQA"
"cdancette/rubi.bootstrap.pytorch" -> "chrisc36/bottom-up-attention-vqa"
"cdancette/rubi.bootstrap.pytorch" -> "cdancette/vqa-cp-leaderboard"
"wangleihitcs/CaptionMetrics" -> "harpribot/nlp-metrics" ["e"=1]
"wangleihitcs/CaptionMetrics" -> "salaniz/pycocoevalcap"
"kacky24/stylenet" -> "yiyang92/caption-stylenet_tensorflow"
"kacky24/stylenet" -> "computationalmedia/semstyle"
"kacky24/stylenet" -> "andyweizhao/Multitask_Image_Captioning"
"kacky24/stylenet" -> "cswhjiang/Recurrent_Fusion_Network"
"lxtGH/AI_challenger_Chinese_Caption" -> "Wind-Ward/Image_Caption_Competition"
"ruotianluo/Transformer_Captioning" -> "zhegan27/Semantic_Compositional_Nets"
"ruotianluo/Transformer_Captioning" -> "ruotianluo/DiscCaptioning"
"ruotianluo/Transformer_Captioning" -> "chenxinpeng/im2p"
"ruotianluo/Transformer_Captioning" -> "andyweizhao/Multitask_Image_Captioning"
"ruotianluo/Transformer_Captioning" -> "rakshithShetty/captionGAN"
"ruotianluo/Transformer_Captioning" -> "gujiuxiang/Stack-Captioning"
"ruotianluo/Transformer_Captioning" -> "yiyang92/vae_captioning"
"ruotianluo/Transformer_Captioning" -> "cswhjiang/Recurrent_Fusion_Network"
"ruotianluo/Transformer_Captioning" -> "tsenghungchen/show-adapt-and-tell"
"ruotianluo/Transformer_Captioning" -> "ruotianluo/self-critical.pytorch"
"ruotianluo/Transformer_Captioning" -> "fengyang0317/unsupervised_captioning"
"andyweizhao/Multitask_Image_Captioning" -> "cswhjiang/Recurrent_Fusion_Network"
"andyweizhao/Multitask_Image_Captioning" -> "yiyang92/caption-stylenet_tensorflow"
"batra-mlp-lab/visdial-rl" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"batra-mlp-lab/visdial-rl" -> "jiasenlu/visDial.pytorch"
"batra-mlp-lab/visdial-rl" -> "batra-mlp-lab/visdial"
"batra-mlp-lab/visdial-rl" -> "batra-mlp-lab/visdial-amt-chat"
"batra-mlp-lab/visdial-rl" -> "satwikkottur/clevr-dialog"
"batra-mlp-lab/visdial-rl" -> "naver/aqm-plus"
"batra-mlp-lab/visdial-rl" -> "vmurahari3/visdial-diversity"
"batra-mlp-lab/visdial-rl" -> "yuleiniu/rva"
"batra-mlp-lab/visdial-rl" -> "zilongzheng/visdial-gnn"
"batra-mlp-lab/visdial-rl" -> "GuessWhatGame/guesswhat"
"batra-mlp-lab/visdial-rl" -> "facebookresearch/corefnmn"
"lukemelas/image-paragraph-captioning" -> "bupt-mmai/CNN-Caption"
"lukemelas/image-paragraph-captioning" -> "chenxinpeng/im2p"
"lukemelas/image-paragraph-captioning" -> "daqingliu/CAVP"
"forence/Awesome-Visual-Captioning" -> "tgc1997/Awesome-Video-Captioning" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "jayleicn/recurrent-transformer" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "terry-r123/Awesome-Captioning"
"forence/Awesome-Visual-Captioning" -> "tgc1997/RMN" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "JDAI-CV/image-captioning"
"forence/Awesome-Visual-Captioning" -> "yangxuntu/SGAE"
"forence/Awesome-Visual-Captioning" -> "yangbang18/Non-Autoregressive-Video-Captioning" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "zhjohnchan/awesome-image-captioning"
"forence/Awesome-Visual-Captioning" -> "facebookresearch/grounded-video-description" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "SydCaption/SAAT" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "luo3300612/image-captioning-DLCT"
"forence/Awesome-Visual-Captioning" -> "hobincar/SGN" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "aimagelab/meshed-memory-transformer"
"forence/Awesome-Visual-Captioning" -> "vsislab/Controllable_XGating" ["e"=1]
"forence/Awesome-Visual-Captioning" -> "husthuaan/AoANet"
"fengyang0317/unsupervised_captioning" -> "aimagelab/show-control-and-tell"
"fengyang0317/unsupervised_captioning" -> "husthuaan/AoANet"
"fengyang0317/unsupervised_captioning" -> "yangxuntu/SGAE"
"fengyang0317/unsupervised_captioning" -> "yahoo/object_relation_transformer"
"fengyang0317/unsupervised_captioning" -> "zhjohnchan/awesome-image-captioning"
"fengyang0317/unsupervised_captioning" -> "gujiuxiang/Stack-Captioning"
"fengyang0317/unsupervised_captioning" -> "luo3300612/image-captioning-DLCT"
"fengyang0317/unsupervised_captioning" -> "ruotianluo/DiscCaptioning"
"fengyang0317/unsupervised_captioning" -> "ruotianluo/Transformer_Captioning"
"fengyang0317/unsupervised_captioning" -> "ruotianluo/self-critical.pytorch"
"fengyang0317/unsupervised_captioning" -> "jiasenlu/NeuralBabyTalk"
"fengyang0317/unsupervised_captioning" -> "fawazsammani/show-edit-tell"
"fengyang0317/unsupervised_captioning" -> "peteanderson80/Up-Down-Captioner"
"fengyang0317/unsupervised_captioning" -> "tsenghungchen/show-adapt-and-tell"
"fengyang0317/unsupervised_captioning" -> "JDAI-CV/image-captioning"
"yangxuntu/SGAE" -> "husthuaan/AoANet"
"yangxuntu/SGAE" -> "YiwuZhong/Sub-GC"
"yangxuntu/SGAE" -> "cshizhe/asg2cap"
"yangxuntu/SGAE" -> "rowanz/neural-motifs" ["e"=1]
"yangxuntu/SGAE" -> "yahoo/object_relation_transformer"
"yangxuntu/SGAE" -> "aimagelab/show-control-and-tell"
"yangxuntu/SGAE" -> "ruotianluo/self-critical.pytorch"
"yangxuntu/SGAE" -> "forence/Awesome-Visual-Captioning"
"yangxuntu/SGAE" -> "fengyang0317/unsupervised_captioning"
"yangxuntu/SGAE" -> "JDAI-CV/image-captioning"
"yangxuntu/SGAE" -> "vacancy/SceneGraphParser" ["e"=1]
"yangxuntu/SGAE" -> "peteanderson80/SPICE" ["e"=1]
"yangxuntu/SGAE" -> "yikang-li/MSDN" ["e"=1]
"yangxuntu/SGAE" -> "Dong-JinKim/DenseRelationalCaptioning"
"yangxuntu/SGAE" -> "aimagelab/meshed-memory-transformer"
"poojahira/image-captioning-bottom-up-top-down" -> "peteanderson80/Up-Down-Captioner"
"poojahira/image-captioning-bottom-up-top-down" -> "njchoma/transformer_image_caption"
"poojahira/image-captioning-bottom-up-top-down" -> "violetteshev/bottom-up-features"
"poojahira/image-captioning-bottom-up-top-down" -> "husthuaan/AoANet"
"poojahira/image-captioning-bottom-up-top-down" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"poojahira/image-captioning-bottom-up-top-down" -> "yahoo/object_relation_transformer"
"poojahira/image-captioning-bottom-up-top-down" -> "aimagelab/show-control-and-tell"
"poojahira/image-captioning-bottom-up-top-down" -> "yangxuntu/SGAE"
"poojahira/image-captioning-bottom-up-top-down" -> "aimagelab/meshed-memory-transformer"
"poojahira/image-captioning-bottom-up-top-down" -> "cshizhe/asg2cap"
"poojahira/image-captioning-bottom-up-top-down" -> "YiwuZhong/Sub-GC"
"poojahira/image-captioning-bottom-up-top-down" -> "ezeli/BUTD_model"
"poojahira/image-captioning-bottom-up-top-down" -> "peteanderson80/bottom-up-attention"
"poojahira/image-captioning-bottom-up-top-down" -> "hengyuan-hu/bottom-up-attention-vqa"
"poojahira/image-captioning-bottom-up-top-down" -> "ruotianluo/self-critical.pytorch"
"Cyanogenoid/vqa-counting" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cyanogenoid/vqa-counting" -> "jnhwkim/ban-vqa"
"Cyanogenoid/vqa-counting" -> "Cyanogenoid/pytorch-vqa"
"Cyanogenoid/vqa-counting" -> "yuzcccc/vqa-mfb"
"Cyanogenoid/vqa-counting" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cyanogenoid/vqa-counting" -> "linjieli222/VQA_ReGAT"
"Cyanogenoid/vqa-counting" -> "aimbrain/vqa-project"
"Cyanogenoid/vqa-counting" -> "Cadene/vqa.pytorch"
"Cyanogenoid/vqa-counting" -> "Cadene/murel.bootstrap.pytorch"
"Cyanogenoid/vqa-counting" -> "Cadene/block.bootstrap.pytorch"
"Cyanogenoid/vqa-counting" -> "asdf0982/vqa-mfb.pytorch"
"Cyanogenoid/vqa-counting" -> "MILVLG/openvqa"
"Cyanogenoid/vqa-counting" -> "chingyaoc/awesome-vqa"
"Cyanogenoid/vqa-counting" -> "facebookresearch/grid-feats-vqa"
"Cyanogenoid/vqa-counting" -> "ronghanghu/lcgn"
"ruotianluo/Image_Captioning_AI_Challenger" -> "showkeyjar/chinese_im2text.pytorch"
"ruotianluo/Image_Captioning_AI_Challenger" -> "ruotianluo/bottom-up-attention-ai-challenger"
"ruotianluo/Image_Captioning_AI_Challenger" -> "Wind-Ward/Image_Caption_Competition"
"ruotianluo/Image_Captioning_AI_Challenger" -> "HughChi/Image-Caption"
"ruotianluo/Image_Captioning_AI_Challenger" -> "peteanderson80/Up-Down-Captioner"
"ruotianluo/Image_Captioning_AI_Challenger" -> "foamliu/Image-Captioning"
"ruotianluo/Image_Captioning_AI_Challenger" -> "jiasenlu/NeuralBabyTalk"
"ruotianluo/Image_Captioning_AI_Challenger" -> "ruotianluo/self-critical.pytorch"
"ruotianluo/Image_Captioning_AI_Challenger" -> "lxtGH/AI_challenger_Chinese_Caption"
"ruotianluo/Image_Captioning_AI_Challenger" -> "AIChallenger/AI_Challenger_2017"
"ruotianluo/Image_Captioning_AI_Challenger" -> "cai-lw/image-captioning-chinese"
"ruotianluo/Image_Captioning_AI_Challenger" -> "ruotianluo/ImageCaptioning.pytorch"
"ruotianluo/Image_Captioning_AI_Challenger" -> "husthuaan/AoANet"
"ruotianluo/Image_Captioning_AI_Challenger" -> "li-xirong/coco-cn"
"ruotianluo/Image_Captioning_AI_Challenger" -> "fengyang0317/unsupervised_captioning"
"liqing-ustc/VizWiz_LSTM_CNN_Attention" -> "DenisDsh/VizWiz-VQA-PyTorch"
"ccvl/clevr-refplus-dataset-gen" -> "ccvl/iep-ref"
"davidnvq/visdial" -> "shubhamagarwal92/visdial_conv"
"davidnvq/visdial" -> "gicheonkang/dan-visdial"
"davidnvq/visdial" -> "wh0330/CAG_VisDial"
"davidnvq/visdial" -> "vmurahari3/visdial-bert"
"yuleiniu/rva" -> "zilongzheng/visdial-gnn"
"yuleiniu/rva" -> "shubhamagarwal92/visdial_conv"
"yuleiniu/rva" -> "naver/aqm-plus"
"yuleiniu/rva" -> "davidnvq/visdial"
"yuleiniu/rva" -> "vmurahari3/visdial-diversity"
"yuleiniu/rva" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"yuleiniu/rva" -> "vmurahari3/visdial-bert"
"yuleiniu/rva" -> "satwikkottur/clevr-dialog"
"yuleiniu/rva" -> "jiasenlu/visDial.pytorch"
"Fen9/WReN" -> "WellyZhang/CoPINet"
"Fen9/WReN" -> "zkcys001/distracting_feature"
"yufengm/Adaptive" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"yufengm/Adaptive" -> "jiasenlu/AdaptiveAttention"
"yufengm/Adaptive" -> "ruotianluo/DiscCaptioning"
"yufengm/Adaptive" -> "s1879281/Image-Captioning-with-Adaptive-Attention"
"yufengm/Adaptive" -> "zjuchenlong/sca-cnn.cvpr17"
"yufengm/Adaptive" -> "qingzwang/GHA-ImageCaptioning"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "yufengm/Adaptive"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "cswhjiang/Recurrent_Fusion_Network"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "gujiuxiang/Stack-Captioning"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "fawazsammani/show-edit-tell"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "jiasenlu/AdaptiveAttention"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "husthuaan/AAT"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "yiyang92/caption-stylenet_tensorflow"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "yahoo/object_relation_transformer"
"fawazsammani/knowing-when-to-look-adaptive-attention" -> "poojahira/image-captioning-bottom-up-top-down"
"satwikkottur/clevr-dialog" -> "zilongzheng/visdial-gnn"
"satwikkottur/clevr-dialog" -> "idansc/mrr-ndcg"
"Cadene/murel.bootstrap.pytorch" -> "aimbrain/vqa-project"
"Cadene/murel.bootstrap.pytorch" -> "Cadene/block.bootstrap.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "cdancette/rubi.bootstrap.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "linjieli222/VQA_ReGAT"
"Cadene/murel.bootstrap.pytorch" -> "MILVLG/mcan-vqa"
"Cadene/murel.bootstrap.pytorch" -> "Cadene/bootstrap.pytorch"
"Cadene/murel.bootstrap.pytorch" -> "jnhwkim/ban-vqa"
"Cadene/murel.bootstrap.pytorch" -> "ronghanghu/lcgn"
"Cadene/murel.bootstrap.pytorch" -> "Cyanogenoid/vqa-counting"
"Cadene/murel.bootstrap.pytorch" -> "yanxinzju/CSS-VQA"
"Cadene/murel.bootstrap.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cadene/murel.bootstrap.pytorch" -> "Cadene/vqa.pytorch"
"hlamba28/Automatic-Image-Captioning" -> "yashk2810/Image-Captioning"
"jiasenlu/visDial.pytorch" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"jiasenlu/visDial.pytorch" -> "batra-mlp-lab/visdial-rl"
"jiasenlu/visDial.pytorch" -> "facebookresearch/corefnmn"
"jiasenlu/visDial.pytorch" -> "yuleiniu/rva"
"jiasenlu/visDial.pytorch" -> "gicheonkang/dan-visdial"
"jiasenlu/visDial.pytorch" -> "zilongzheng/visdial-gnn"
"jiasenlu/visDial.pytorch" -> "batra-mlp-lab/visdial"
"jiasenlu/visDial.pytorch" -> "naver/aqm-plus"
"jiasenlu/visDial.pytorch" -> "vmurahari3/visdial-bert"
"jiasenlu/visDial.pytorch" -> "shubhamagarwal92/visdial_conv"
"jiasenlu/visDial.pytorch" -> "vmurahari3/visdial-diversity"
"yiyang92/vae_captioning" -> "rakshithShetty/captionGAN"
"yiyang92/vae_captioning" -> "visinf/cos-cvae"
"naver/aqm-plus" -> "yuleiniu/rva"
"lichengunc/mask-faster-rcnn" -> "lichengunc/speaker_listener_reinforcer"
"lichengunc/mask-faster-rcnn" -> "xh-liu/CM-Erase-REG"
"lichengunc/mask-faster-rcnn" -> "mikittt/easy-to-understand-REG"
"Cloud-CV/EvalAI-Starters" -> "Cloud-CV/evalai-cli"
"richardaecn/cvpr18-caption-eval" -> "ruotianluo/DiscCaptioning"
"richardaecn/cvpr18-caption-eval" -> "JonghwanMun/TextguidedATT"
"richardaecn/cvpr18-caption-eval" -> "cswhjiang/Recurrent_Fusion_Network"
"richardaecn/cvpr18-caption-eval" -> "ck0123/improved-bertscore-for-image-captioning-evaluation"
"ronghanghu/snmn" -> "shijx12/XNM-Net"
"ruotianluo/bottom-up-attention-ai-challenger" -> "Wind-Ward/Image_Caption_Competition"
"tommccoy1/hans" -> "BIU-NLP/Breaking_NLI"
"tommccoy1/hans" -> "UKPLab/emnlp2020-debiasing-unknown"
"tommccoy1/hans" -> "hhexiy/debiased"
"tommccoy1/hans" -> "decompositional-semantics-initiative/DNC"
"ronghanghu/lcgn" -> "linjieli222/VQA_ReGAT"
"ronghanghu/lcgn" -> "shijx12/XNM-Net"
"ronghanghu/lcgn" -> "daqingliu/NMTree"
"ronghanghu/lcgn" -> "youngfly11/LCMCG-PyTorch"
"facebookresearch/corefnmn" -> "shubhamagarwal92/visdial_conv"
"facebookresearch/corefnmn" -> "gicheonkang/dan-visdial"
"facebookresearch/corefnmn" -> "idansc/mrr-ndcg"
"facebookresearch/corefnmn" -> "simpleshinobu/visdial-principles"
"facebookresearch/corefnmn" -> "zilongzheng/visdial-gnn"
"facebookresearch/corefnmn" -> "jiasenlu/visDial.pytorch"
"facebookresearch/corefnmn" -> "agakshat/visualdialog-pytorch"
"baudm/MONet-pytorch" -> "stelzner/monet"
"baudm/MONet-pytorch" -> "zhixuan-lin/IODINE"
"baudm/MONet-pytorch" -> "google-deepmind/multi_object_datasets"
"BCV-Uniandes/DMS" -> "liruiyu/referseg_rrn"
"BCV-Uniandes/DMS" -> "chenxi116/TF-phrasecut-public"
"BCV-Uniandes/DMS" -> "zyang-ur/ReSC"
"BCV-Uniandes/DMS" -> "spyflying/CMPC-Refseg"
"BCV-Uniandes/DMS" -> "ronghanghu/text_objseg"
"cai-lw/image-captioning-chinese" -> "yuanx520/chinese_image_captioning"
"InnerPeace-Wu/densecap-tensorflow" -> "linjieyangsc/densecap"
"InnerPeace-Wu/densecap-tensorflow" -> "InnerPeace-Wu/im2p-tensorflow"
"InnerPeace-Wu/densecap-tensorflow" -> "chenxinpeng/im2p"
"cvlab-tohoku/Dense-CoAttention-Network" -> "jnhwkim/ban-vqa"
"bckim92/zsh-autoswitch-conda" -> "bckim92/colloquial-claims"
"stelzner/monet" -> "baudm/MONet-pytorch"
"stelzner/monet" -> "zhixuan-lin/IODINE"
"stelzner/monet" -> "dfdazac/monet"
"stelzner/monet" -> "ecker-lab/object-centric-representation-benchmark"
"vsubhashini/noc" -> "LisaAnne/DCC"
"vsubhashini/noc" -> "wenhuchen/Semi-Supervised-Image-Captioning"
"liruiyu/referseg_rrn" -> "chenxi116/TF-phrasecut-public"
"liruiyu/referseg_rrn" -> "spyflying/CMPC-Refseg"
"DenisDsh/VizWiz-VQA-PyTorch" -> "liqing-ustc/VizWiz_LSTM_CNN_Attention"
"rosinality/mac-network-pytorch" -> "ronghanghu/snmn"
"rosinality/mac-network-pytorch" -> "bhpfelix/Compositional-Attention-Networks-for-Machine-Reasoning-PyTorch"
"rosinality/mac-network-pytorch" -> "ronilp/mac-network-pytorch-gqa"
"rosinality/mac-network-pytorch" -> "stanfordnlp/mac-network"
"rosinality/mac-network-pytorch" -> "ceyzaguirre4/NSM"
"daqingliu/CAVP" -> "cswhjiang/Recurrent_Fusion_Network"
"neural-nuts/Cam2Caption" -> "neural-nuts/image-caption-generator"
"chenxinpeng/ARNet" -> "aditya12agd5/convcap"
"chenxinpeng/ARNet" -> "gujiuxiang/Stack-Captioning"
"chenxinpeng/ARNet" -> "cswhjiang/Recurrent_Fusion_Network"
"ruotianluo/coco-caption" -> "cshizhe/asg2cap"
"HaleyPei/Implement-of-SCA-CNN" -> "stevehuanghe/image_captioning"
"lluisgomez/single-shot-str" -> "AndresPMD/Pytorch-yolo-phoc"
"zilongzheng/visdial-gnn" -> "satwikkottur/clevr-dialog"
"zilongzheng/visdial-gnn" -> "shubhamagarwal92/visdial_conv"
"QUVA-Lab/lang-tracker" -> "fredfung007/snlt"
"distkv-project/distkv" -> "distkv-project/drpc"
"Cloud-CV/EvalAI-ngx" -> "aka-jain/Drango"
"ZiliangWang0505/AI-Challenger-Caption-Competition" -> "Wind-Ward/Image_Caption_Competition"
"ruizhaogit/EnergyBasedPrioritization" -> "ruizhaogit/mep"
"ronghanghu/mmf" -> "ZephyrZhuQi/ssbaseline"
"ronghanghu/mmf" -> "guanghuixu/AnchorCaptioner"
"ronghanghu/mmf" -> "yashkant/sam-textvqa"
"ronghanghu/mmf" -> "microsoft/TAP"
"ronghanghu/mmf" -> "ChenyuGAO-CS/SMA"
"ronghanghu/mmf" -> "guanghuixu/CRN_tvqa"
"cswhjiang/Recurrent_Fusion_Network" -> "andyweizhao/Multitask_Image_Captioning"
"cswhjiang/Recurrent_Fusion_Network" -> "gujiuxiang/Stack-Captioning"
"cswhjiang/Recurrent_Fusion_Network" -> "fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning"
"cswhjiang/Recurrent_Fusion_Network" -> "yiyang92/caption-stylenet_tensorflow"
"gujiuxiang/Stack-Captioning" -> "cswhjiang/Recurrent_Fusion_Network"
"gujiuxiang/Stack-Captioning" -> "andyweizhao/Multitask_Image_Captioning"
"gujiuxiang/Stack-Captioning" -> "ruotianluo/DiscCaptioning"
"showkeyjar/chinese_im2text.pytorch" -> "Wind-Ward/Image_Caption_Competition"
"tgGuo15/PriorImageCaption" -> "cswhjiang/Recurrent_Fusion_Network"
"jialinwu17/self_critical_vqa" -> "erobic/negative_analysis_of_grounding"
"jialinwu17/self_critical_vqa" -> "chrisc36/bottom-up-attention-vqa"
"jialinwu17/self_critical_vqa" -> "yanxinzju/CSS-VQA"
"jialinwu17/self_critical_vqa" -> "cdancette/rubi.bootstrap.pytorch"
"davidmascharka/MyNN" -> "rsokl/noggin"
"rsokl/noggin" -> "davidmascharka/MyNN"
"rsokl/noggin" -> "rsokl/MyGrad"
"AndresPMD/Pytorch-yolo-phoc" -> "AndresPMD/Fine_Grained_Clf"
"airsplay/lxmert" -> "facebookresearch/vilbert-multi-task"
"airsplay/lxmert" -> "ChenRocks/UNITER"
"airsplay/lxmert" -> "jackroos/VL-BERT"
"airsplay/lxmert" -> "uclanlp/visualbert"
"airsplay/lxmert" -> "jiasenlu/vilbert_beta"
"airsplay/lxmert" -> "peteanderson80/bottom-up-attention"
"airsplay/lxmert" -> "MILVLG/mcan-vqa"
"airsplay/lxmert" -> "microsoft/Oscar"
"airsplay/lxmert" -> "hengyuan-hu/bottom-up-attention-vqa"
"airsplay/lxmert" -> "LuoweiZhou/VLP"
"airsplay/lxmert" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"airsplay/lxmert" -> "facebookresearch/grid-feats-vqa"
"airsplay/lxmert" -> "airsplay/py-bottom-up-attention"
"airsplay/lxmert" -> "MILVLG/openvqa"
"airsplay/lxmert" -> "yikuan8/Transformers-VQA"
"tbmoon/kalman_filter" -> "DonghoonPark12/Book_KalmanFilter"
"tbmoon/kalman_filter" -> "tbmoon/basic_vqa"
"gyy8426/Computer_Vision_primer" -> "zchoi/S2-Transformer"
"gyy8426/Computer_Vision_primer" -> "NovaMind-Z/PTSN"
"gyy8426/Computer_Vision_primer" -> "AceCoooool/interview-computer-vision"
"gyy8426/Computer_Vision_primer" -> "zchoi/PKOL"
"jackroos/VL-BERT" -> "facebookresearch/vilbert-multi-task"
"jackroos/VL-BERT" -> "ChenRocks/UNITER"
"jackroos/VL-BERT" -> "airsplay/lxmert"
"jackroos/VL-BERT" -> "uclanlp/visualbert"
"jackroos/VL-BERT" -> "jiasenlu/vilbert_beta"
"jackroos/VL-BERT" -> "LuoweiZhou/VLP"
"jackroos/VL-BERT" -> "microsoft/Oscar"
"jackroos/VL-BERT" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"jackroos/VL-BERT" -> "peteanderson80/bottom-up-attention"
"jackroos/VL-BERT" -> "jayleicn/ClipBERT"
"jackroos/VL-BERT" -> "airsplay/py-bottom-up-attention"
"jackroos/VL-BERT" -> "facebookresearch/grid-feats-vqa"
"jackroos/VL-BERT" -> "ruotianluo/self-critical.pytorch"
"jackroos/VL-BERT" -> "jokieleung/awesome-visual-question-answering"
"jackroos/VL-BERT" -> "forence/Awesome-Visual-Captioning"
"ChenRocks/UNITER" -> "jackroos/VL-BERT"
"ChenRocks/UNITER" -> "airsplay/lxmert"
"ChenRocks/UNITER" -> "facebookresearch/vilbert-multi-task"
"ChenRocks/UNITER" -> "microsoft/Oscar"
"ChenRocks/UNITER" -> "uclanlp/visualbert"
"ChenRocks/UNITER" -> "jiasenlu/vilbert_beta"
"ChenRocks/UNITER" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"ChenRocks/UNITER" -> "LuoweiZhou/VLP"
"ChenRocks/UNITER" -> "peteanderson80/bottom-up-attention"
"ChenRocks/UNITER" -> "dandelin/ViLT"
"ChenRocks/UNITER" -> "linjieli222/HERO" ["e"=1]
"ChenRocks/UNITER" -> "salesforce/ALBEF"
"ChenRocks/UNITER" -> "MILVLG/bottom-up-attention.pytorch"
"ChenRocks/UNITER" -> "kuanghuei/SCAN" ["e"=1]
"ChenRocks/UNITER" -> "jayleicn/ClipBERT"
"kdexd/virtex" -> "facebookresearch/grid-feats-vqa"
"kdexd/virtex" -> "ChenRocks/UNITER"
"kdexd/virtex" -> "microsoft/Oscar"
"kdexd/virtex" -> "mbanani/unsupervisedRR" ["e"=1]
"kdexd/virtex" -> "jayleicn/ClipBERT"
"kdexd/virtex" -> "airsplay/lxmert"
"kdexd/virtex" -> "pzzhang/VinVL"
"kdexd/virtex" -> "jiasenlu/vilbert_beta"
"kdexd/virtex" -> "researchmm/soho"
"kdexd/virtex" -> "clip-vil/CLIP-ViL"
"kdexd/virtex" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"kdexd/virtex" -> "kuanghuei/SCAN" ["e"=1]
"kdexd/virtex" -> "fengyang0317/unsupervised_captioning"
"kdexd/virtex" -> "LuoweiZhou/VLP"
"kdexd/virtex" -> "ashkamath/mdetr"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "sangminwoo/awesome-vision-and-language"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "ChenRocks/UNITER"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jackroos/VL-BERT"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "airsplay/lxmert"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "microsoft/Oscar"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "forence/Awesome-Visual-Captioning"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jiasenlu/vilbert_beta"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jokieleung/awesome-visual-question-answering"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "TheShadow29/awesome-grounding"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "facebookresearch/vilbert-multi-task"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "jayleicn/ClipBERT"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "LuoweiZhou/VLP"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "dandelin/ViLT"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "salesforce/ALBEF"
"yuewang-cuhk/awesome-vision-language-pretraining-papers" -> "uclanlp/visualbert"
"lucidrains/slot-attention" -> "evelinehong/slot-attention-pytorch"
"lucidrains/slot-attention" -> "imbue-ai/slot_attention" ["e"=1]
"lucidrains/slot-attention" -> "tkipf/c-swm"
"lucidrains/slot-attention" -> "google-research/slot-attention-video"
"lucidrains/slot-attention" -> "google-deepmind/multi_object_datasets"
"lucidrains/slot-attention" -> "amazon-science/object-centric-learning-framework"
"lucidrains/slot-attention" -> "singhgautam/slate"
"lucidrains/slot-attention" -> "zhixuan-lin/IODINE"
"lucidrains/slot-attention" -> "addtt/object-centric-library"
"lucidrains/slot-attention" -> "baudm/MONet-pytorch"
"lucidrains/slot-attention" -> "amazon-science/AdaSlot"
"lucidrains/slot-attention" -> "ajabri/videowalk" ["e"=1]
"lucidrains/slot-attention" -> "stelzner/monet"
"lucidrains/slot-attention" -> "applied-ai-lab/genesis"
"lucidrains/slot-attention" -> "facebookresearch/clevr-dataset-gen"
"tkipf/c-swm" -> "lucidrains/slot-attention"
"tkipf/c-swm" -> "applied-ai-lab/genesis"
"tkipf/c-swm" -> "google-deepmind/multi_object_datasets"
"tkipf/c-swm" -> "mila-iqia/atari-representation-learning" ["e"=1]
"tkipf/c-swm" -> "jcoreyes/OP3"
"tkipf/c-swm" -> "stelzner/monet"
"tkipf/c-swm" -> "danijar/dreamer" ["e"=1]
"tkipf/c-swm" -> "baudm/MONet-pytorch"
"tkipf/c-swm" -> "JindongJiang/SCALOR"
"tkipf/c-swm" -> "dido1998/Recurrent-Independent-Mechanisms"
"tkipf/c-swm" -> "dido1998/CausalMBRL"
"tkipf/c-swm" -> "floodsung/Deep-Reasoning-Papers"
"tkipf/c-swm" -> "anirudh9119/RIMs"
"sgrvinod/a-PyTorch-Tutorial-to-Transformers" -> "devjwsong/transformer-translator-pytorch"
"sgrvinod/a-PyTorch-Tutorial-to-Transformers" -> "sgrvinod/Deep-Tutorials-for-PyTorch"
"BierOne/bottom-up-attention-vqa" -> "ezeli/BUTD_model"
"aimagelab/meshed-memory-transformer" -> "husthuaan/AoANet"
"aimagelab/meshed-memory-transformer" -> "luo3300612/image-captioning-DLCT"
"aimagelab/meshed-memory-transformer" -> "yahoo/object_relation_transformer"
"aimagelab/meshed-memory-transformer" -> "ruotianluo/self-critical.pytorch"
"aimagelab/meshed-memory-transformer" -> "JDAI-CV/image-captioning"
"aimagelab/meshed-memory-transformer" -> "krasserm/fairseq-image-captioning"
"aimagelab/meshed-memory-transformer" -> "davidnvq/grit"
"aimagelab/meshed-memory-transformer" -> "forence/Awesome-Visual-Captioning"
"aimagelab/meshed-memory-transformer" -> "ruotianluo/ImageCaptioning.pytorch"
"aimagelab/meshed-memory-transformer" -> "facebookresearch/grid-feats-vqa"
"aimagelab/meshed-memory-transformer" -> "zhangxuying1004/RSTNet"
"aimagelab/meshed-memory-transformer" -> "aimagelab/show-control-and-tell"
"aimagelab/meshed-memory-transformer" -> "yangxuntu/SGAE"
"aimagelab/meshed-memory-transformer" -> "zhjohnchan/awesome-image-captioning"
"aimagelab/meshed-memory-transformer" -> "poojahira/image-captioning-bottom-up-top-down"
"linjieli222/VQA_ReGAT" -> "ronghanghu/lcgn"
"linjieli222/VQA_ReGAT" -> "aimbrain/vqa-project"
"linjieli222/VQA_ReGAT" -> "MILVLG/mcan-vqa"
"linjieli222/VQA_ReGAT" -> "codexxxl/GraphVQA"
"linjieli222/VQA_ReGAT" -> "Cadene/murel.bootstrap.pytorch"
"linjieli222/VQA_ReGAT" -> "yangxuntu/SGAE"
"linjieli222/VQA_ReGAT" -> "Cadene/block.bootstrap.pytorch"
"linjieli222/VQA_ReGAT" -> "CrossmodalGroup/GSMN" ["e"=1]
"linjieli222/VQA_ReGAT" -> "facebookresearch/grid-feats-vqa"
"linjieli222/VQA_ReGAT" -> "Cyanogenoid/vqa-counting"
"linjieli222/VQA_ReGAT" -> "cshizhe/asg2cap"
"linjieli222/VQA_ReGAT" -> "jnhwkim/ban-vqa"
"linjieli222/VQA_ReGAT" -> "KunpengLi1994/VSRN" ["e"=1]
"linjieli222/VQA_ReGAT" -> "MILVLG/openvqa"
"catty-project/catty" -> "ck0123/improved-bertscore-for-image-captioning-evaluation"
"catty-project/catty" -> "distkv-project/drpc"
"sangminwoo/awesome-vision-and-language" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"sangminwoo/awesome-vision-and-language" -> "phellonchen/awesome-Vision-and-Language-Pre-training"
"sangminwoo/awesome-vision-and-language" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"sangminwoo/awesome-vision-and-language" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" ["e"=1]
"sangminwoo/awesome-vision-and-language" -> "jokieleung/awesome-visual-question-answering"
"sangminwoo/awesome-vision-and-language" -> "EdisonLeeeee/Awesome-Masked-Autoencoders" ["e"=1]
"sangminwoo/awesome-vision-and-language" -> "ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["e"=1]
"sangminwoo/awesome-vision-and-language" -> "TheShadow29/awesome-grounding"
"sangminwoo/awesome-vision-and-language" -> "ChenRocks/UNITER"
"sangminwoo/awesome-vision-and-language" -> "danieljf24/awesome-video-text-retrieval" ["e"=1]
"sangminwoo/awesome-vision-and-language" -> "LuoweiZhou/VLP"
"sangminwoo/awesome-vision-and-language" -> "sfikas/medical-imaging-datasets" ["e"=1]
"sangminwoo/awesome-vision-and-language" -> "clip-vil/CLIP-ViL"
"sangminwoo/awesome-vision-and-language" -> "jackroos/VL-BERT"
"sangminwoo/awesome-vision-and-language" -> "Yangzhangcst/Transformer-in-Computer-Vision" ["e"=1]
"microsoft/Oscar" -> "pzzhang/VinVL"
"microsoft/Oscar" -> "ChenRocks/UNITER"
"microsoft/Oscar" -> "peteanderson80/bottom-up-attention"
"microsoft/Oscar" -> "facebookresearch/vilbert-multi-task"
"microsoft/Oscar" -> "LuoweiZhou/VLP"
"microsoft/Oscar" -> "airsplay/lxmert"
"microsoft/Oscar" -> "jackroos/VL-BERT"
"microsoft/Oscar" -> "microsoft/scene_graph_benchmark" ["e"=1]
"microsoft/Oscar" -> "facebookresearch/grid-feats-vqa"
"microsoft/Oscar" -> "ruotianluo/self-critical.pytorch"
"microsoft/Oscar" -> "airsplay/py-bottom-up-attention"
"microsoft/Oscar" -> "aimagelab/meshed-memory-transformer"
"microsoft/Oscar" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"microsoft/Oscar" -> "dandelin/ViLT"
"microsoft/Oscar" -> "jayleicn/ClipBERT"
"floodsung/Deep-Reasoning-Papers" -> "WellyZhang/RAVEN"
"floodsung/Deep-Reasoning-Papers" -> "tkipf/c-swm"
"floodsung/Deep-Reasoning-Papers" -> "WellyZhang/CoPINet"
"saahiluppal/catr" -> "krasserm/fairseq-image-captioning"
"saahiluppal/catr" -> "aimagelab/meshed-memory-transformer"
"saahiluppal/catr" -> "RoyalSkye/Image-Caption"
"saahiluppal/catr" -> "luo3300612/image-captioning-DLCT"
"saahiluppal/catr" -> "zarzouram/image_captioning_with_transformers"
"saahiluppal/catr" -> "yahoo/object_relation_transformer"
"saahiluppal/catr" -> "JDAI-CV/image-captioning"
"saahiluppal/catr" -> "davidnvq/grit"
"saahiluppal/catr" -> "facebookresearch/grid-feats-vqa"
"saahiluppal/catr" -> "wtliao/ImageTransformer"
"saahiluppal/catr" -> "YiwuZhong/Sub-GC"
"saahiluppal/catr" -> "bearcatt/LaBERT"
"facebookresearch/mmbt" -> "johnarevalo/gmu-mmimdb"
"facebookresearch/mmbt" -> "WasifurRahman/BERT_multimodal_transformer" ["e"=1]
"facebookresearch/mmbt" -> "facebookresearch/vilbert-multi-task"
"JDAI-CV/image-captioning" -> "husthuaan/AoANet"
"JDAI-CV/image-captioning" -> "luo3300612/image-captioning-DLCT"
"JDAI-CV/image-captioning" -> "cshizhe/asg2cap"
"JDAI-CV/image-captioning" -> "aimagelab/meshed-memory-transformer"
"JDAI-CV/image-captioning" -> "yahoo/object_relation_transformer"
"JDAI-CV/image-captioning" -> "forence/Awesome-Visual-Captioning"
"JDAI-CV/image-captioning" -> "facebookresearch/grid-feats-vqa"
"JDAI-CV/image-captioning" -> "ruotianluo/self-critical.pytorch"
"JDAI-CV/image-captioning" -> "zhangxuying1004/RSTNet"
"JDAI-CV/image-captioning" -> "yangxuntu/SGAE"
"JDAI-CV/image-captioning" -> "aimagelab/show-control-and-tell"
"JDAI-CV/image-captioning" -> "fawazsammani/show-edit-tell"
"JDAI-CV/image-captioning" -> "YuanEZhou/Grounded-Image-Captioning"
"JDAI-CV/image-captioning" -> "ruotianluo/coco-caption"
"JDAI-CV/image-captioning" -> "zhjohnchan/awesome-image-captioning"
"codalab/codalab-worksheets" -> "codalab/codalab-worksheets-old"
"codalab/codalab-worksheets" -> "codalab/worksheets-examples"
"ilkhem/icebeem" -> "ilkhem/iVAE"
"ilkhem/icebeem" -> "siamakz/iVAE"
"ilkhem/icebeem" -> "slachapelle/disentanglement_via_mechanism_sparsity"
"ilkhem/icebeem" -> "vislearn/GIN"
"ilkhem/icebeem" -> "HHalva/hmnlica"
"ilkhem/icebeem" -> "HHalva/snica"
"jiasenlu/vilbert_beta" -> "facebookresearch/vilbert-multi-task"
"jiasenlu/vilbert_beta" -> "uclanlp/visualbert"
"jiasenlu/vilbert_beta" -> "airsplay/lxmert"
"jiasenlu/vilbert_beta" -> "jackroos/VL-BERT"
"jiasenlu/vilbert_beta" -> "ChenRocks/UNITER"
"jiasenlu/vilbert_beta" -> "peteanderson80/bottom-up-attention"
"jiasenlu/vilbert_beta" -> "microsoft/Oscar"
"jiasenlu/vilbert_beta" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"jiasenlu/vilbert_beta" -> "LuoweiZhou/VLP"
"jiasenlu/vilbert_beta" -> "MILVLG/mcan-vqa"
"jiasenlu/vilbert_beta" -> "kuanghuei/SCAN" ["e"=1]
"jiasenlu/vilbert_beta" -> "KunpengLi1994/VSRN" ["e"=1]
"jiasenlu/vilbert_beta" -> "jnhwkim/ban-vqa"
"jiasenlu/vilbert_beta" -> "facebookresearch/grid-feats-vqa"
"jiasenlu/vilbert_beta" -> "jayleicn/ClipBERT"
"facebookresearch/vilbert-multi-task" -> "jiasenlu/vilbert_beta"
"facebookresearch/vilbert-multi-task" -> "jackroos/VL-BERT"
"facebookresearch/vilbert-multi-task" -> "airsplay/lxmert"
"facebookresearch/vilbert-multi-task" -> "ChenRocks/UNITER"
"facebookresearch/vilbert-multi-task" -> "uclanlp/visualbert"
"facebookresearch/vilbert-multi-task" -> "microsoft/Oscar"
"facebookresearch/vilbert-multi-task" -> "LuoweiZhou/VLP"
"facebookresearch/vilbert-multi-task" -> "peteanderson80/bottom-up-attention"
"facebookresearch/vilbert-multi-task" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"facebookresearch/vilbert-multi-task" -> "dandelin/ViLT"
"facebookresearch/vilbert-multi-task" -> "airsplay/py-bottom-up-attention"
"facebookresearch/vilbert-multi-task" -> "jayleicn/ClipBERT"
"facebookresearch/vilbert-multi-task" -> "clip-vil/CLIP-ViL"
"facebookresearch/vilbert-multi-task" -> "facebookresearch/mmf"
"facebookresearch/vilbert-multi-task" -> "ruotianluo/self-critical.pytorch"
"JXZe/DualVD" -> "vmurahari3/visdial-bert"
"JXZe/DualVD" -> "wh0330/CAG_VisDial"
"JXZe/DualVD" -> "gicheonkang/dan-visdial"
"JXZe/DualVD" -> "taesunwhang/MVAN-VisDial"
"JXZe/DualVD" -> "shubhamagarwal92/visdial_conv"
"JXZe/DualVD" -> "simpleshinobu/visdial-principles"
"JXZe/DualVD" -> "davidnvq/visdial"
"facebookresearch/grid-feats-vqa" -> "luo3300612/image-captioning-DLCT"
"facebookresearch/grid-feats-vqa" -> "MILVLG/mcan-vqa"
"facebookresearch/grid-feats-vqa" -> "MILVLG/openvqa"
"facebookresearch/grid-feats-vqa" -> "JDAI-CV/image-captioning"
"facebookresearch/grid-feats-vqa" -> "zhangxuying1004/RSTNet"
"facebookresearch/grid-feats-vqa" -> "aimagelab/meshed-memory-transformer"
"facebookresearch/grid-feats-vqa" -> "MILVLG/bottom-up-attention.pytorch"
"facebookresearch/grid-feats-vqa" -> "shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome"
"facebookresearch/grid-feats-vqa" -> "microsoft/Oscar"
"facebookresearch/grid-feats-vqa" -> "pzzhang/VinVL"
"facebookresearch/grid-feats-vqa" -> "yahoo/object_relation_transformer"
"facebookresearch/grid-feats-vqa" -> "forence/Awesome-Visual-Captioning"
"facebookresearch/grid-feats-vqa" -> "linjieli222/VQA_ReGAT"
"facebookresearch/grid-feats-vqa" -> "airsplay/py-bottom-up-attention"
"facebookresearch/grid-feats-vqa" -> "LuoweiZhou/VLP"
"MILVLG/bottom-up-attention.pytorch" -> "airsplay/py-bottom-up-attention"
"MILVLG/bottom-up-attention.pytorch" -> "peteanderson80/bottom-up-attention"
"MILVLG/bottom-up-attention.pytorch" -> "facebookresearch/grid-feats-vqa"
"MILVLG/bottom-up-attention.pytorch" -> "cshizhe/asg2cap"
"MILVLG/bottom-up-attention.pytorch" -> "ChenRocks/UNITER"
"MILVLG/bottom-up-attention.pytorch" -> "yangxuntu/SGAE"
"MILVLG/bottom-up-attention.pytorch" -> "poojahira/image-captioning-bottom-up-top-down"
"MILVLG/bottom-up-attention.pytorch" -> "microsoft/Oscar"
"MILVLG/bottom-up-attention.pytorch" -> "MILVLG/rosita"
"MILVLG/bottom-up-attention.pytorch" -> "MILVLG/mcan-vqa"
"MILVLG/bottom-up-attention.pytorch" -> "CrossmodalGroup/GSMN" ["e"=1]
"MILVLG/bottom-up-attention.pytorch" -> "ruotianluo/self-critical.pytorch"
"MILVLG/bottom-up-attention.pytorch" -> "microsoft/scene_graph_benchmark" ["e"=1]
"MILVLG/bottom-up-attention.pytorch" -> "luo3300612/image-captioning-DLCT"
"MILVLG/bottom-up-attention.pytorch" -> "forence/Awesome-Visual-Captioning"
"pytorch/hydra-torch" -> "romesco/hydra-lightning"
"pytorch/hydra-torch" -> "mit-ll-responsible-ai/hydra-zen"
"eddieantonio/imgcat" -> "wookayin/python-imgcat"
"eddieantonio/imgcat" -> "olivere/iterm2-imagetools"
"bearcatt/LaBERT" -> "visinf/cos-cvae"
"bearcatt/LaBERT" -> "Gitsamshi/WeakVRD-Captioning"
"airsplay/py-bottom-up-attention" -> "MILVLG/bottom-up-attention.pytorch"
"airsplay/py-bottom-up-attention" -> "yikuan8/Transformers-VQA"
"airsplay/py-bottom-up-attention" -> "pzzhang/VinVL"
"airsplay/py-bottom-up-attention" -> "microsoft/Oscar"
"airsplay/py-bottom-up-attention" -> "facebookresearch/grid-feats-vqa"
"airsplay/py-bottom-up-attention" -> "airsplay/lxmert"
"airsplay/py-bottom-up-attention" -> "peteanderson80/bottom-up-attention"
"airsplay/py-bottom-up-attention" -> "poojahira/image-captioning-bottom-up-top-down"
"airsplay/py-bottom-up-attention" -> "ChenRocks/UNITER"
"airsplay/py-bottom-up-attention" -> "YuanEZhou/Grounded-Image-Captioning"
"airsplay/py-bottom-up-attention" -> "linjieli222/VQA_ReGAT"
"airsplay/py-bottom-up-attention" -> "shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome"
"airsplay/py-bottom-up-attention" -> "jackroos/VL-BERT"
"airsplay/py-bottom-up-attention" -> "yangxuntu/SGAE"
"airsplay/py-bottom-up-attention" -> "jiasenlu/vilbert_beta"
"anirudh9119/RIMs" -> "dido1998/Recurrent-Independent-Mechanisms"
"anirudh9119/RIMs" -> "anirudh9119/neural_production_systems"
"chuangg/CLEVRER" -> "zfchenUnique/DCL-Release" ["e"=1]
"chuangg/CLEVRER" -> "dingmyu/VRDP" ["e"=1]
"chuangg/CLEVRER" -> "kexinyi/ns-vqa"
"chuangg/CLEVRER" -> "YunzhuLi/PropNet" ["e"=1]
"applied-ai-lab/genesis" -> "zhixuan-lin/IODINE"
"applied-ai-lab/genesis" -> "google-deepmind/multi_object_datasets"
"applied-ai-lab/genesis" -> "stelzner/monet"
"applied-ai-lab/genesis" -> "JindongJiang/SCALOR"
"applied-ai-lab/genesis" -> "e2crawfo/auto_yolo"
"applied-ai-lab/genesis" -> "hyenal/relate"
"applied-ai-lab/genesis" -> "zhixuan-lin/SPACE"
"rr-learning/CausalWorld" -> "dido1998/CausalMBRL"
"rr-learning/CausalWorld" -> "martius-lab/cid-in-rl"
"rr-learning/CausalWorld" -> "wangzizhao/CausalDynamicsLearning"
"rr-learning/CausalWorld" -> "phlippe/CITRIS"
"rr-learning/CausalWorld" -> "anirudh9119/RIMs"
"lwye/CMSA-Net" -> "spyflying/CMPC-Refseg"
"uclanlp/visualbert" -> "jackroos/VL-BERT"
"uclanlp/visualbert" -> "jiasenlu/vilbert_beta"
"uclanlp/visualbert" -> "airsplay/lxmert"
"uclanlp/visualbert" -> "ChenRocks/UNITER"
"uclanlp/visualbert" -> "facebookresearch/vilbert-multi-task"
"uclanlp/visualbert" -> "yikuan8/Transformers-VQA"
"uclanlp/visualbert" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"uclanlp/visualbert" -> "LuoweiZhou/VLP"
"uclanlp/visualbert" -> "microsoft/Oscar"
"uclanlp/visualbert" -> "peteanderson80/bottom-up-attention"
"uclanlp/visualbert" -> "airsplay/py-bottom-up-attention"
"uclanlp/visualbert" -> "husthuaan/AoANet"
"uclanlp/visualbert" -> "fartashf/vsepp" ["e"=1]
"uclanlp/visualbert" -> "e-bug/volta"
"uclanlp/visualbert" -> "jnhwkim/ban-vqa"
"anisha2102/docvqa" -> "mineshmathew/DocVQA"
"anisha2102/docvqa" -> "allanj/LayoutLMv3-DocVQA"
"xinke-wang/Awesome-Text-VQA" -> "yashkant/sam-textvqa"
"xinke-wang/Awesome-Text-VQA" -> "microsoft/TAP"
"xinke-wang/Awesome-Text-VQA" -> "ronghanghu/mmf"
"xinke-wang/Awesome-Text-VQA" -> "ZephyrZhuQi/ssbaseline"
"xinke-wang/Awesome-Text-VQA" -> "uakarsh/latr"
"xinke-wang/Awesome-Text-VQA" -> "jokieleung/awesome-visual-question-answering"
"xinke-wang/Awesome-Text-VQA" -> "mineshmathew/DocVQA"
"xinke-wang/Awesome-Text-VQA" -> "ronghanghu/lcgn"
"xinke-wang/Awesome-Text-VQA" -> "xiaojino/RUArt"
"xinke-wang/Awesome-Text-VQA" -> "ZeningLin/ViBERTgrid-PyTorch"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "facebookresearch/grid-feats-vqa"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "yahoo/object_relation_transformer"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "KunpengLi1994/VSRN" ["e"=1]
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "peteanderson80/bottom-up-attention"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "airsplay/py-bottom-up-attention"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "forence/Awesome-Visual-Captioning"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "MILVLG/bottom-up-attention.pytorch"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "yangxuntu/SGAE"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "JDAI-CV/image-captioning"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "peteanderson80/Up-Down-Captioner"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "vacancy/SceneGraphParser" ["e"=1]
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "jackroos/VL-BERT"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch" ["e"=1]
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "husthuaan/AoANet"
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" -> "facebookresearch/grounded-video-description" ["e"=1]
"cdancette/vqa-cp-leaderboard" -> "tejas-gokhale/vqa_mutant"
"cdancette/vqa-cp-leaderboard" -> "cdancette/detect-shortcuts"
"AndresPMD/GCN_classification" -> "AndresPMD/Fine_Grained_Clf"
"AndresPMD/GCN_classification" -> "AndresPMD/StacMR"
"AndresPMD/GCN_classification" -> "AndresPMD/Pytorch-yolo-phoc"
"AndresPMD/GCN_classification" -> "MCLAB-OCR/KnowledgeMiningWithSceneText"
"LuoweiZhou/VLP" -> "microsoft/Oscar"
"LuoweiZhou/VLP" -> "ChenRocks/UNITER"
"LuoweiZhou/VLP" -> "jackroos/VL-BERT"
"LuoweiZhou/VLP" -> "facebookresearch/grid-feats-vqa"
"LuoweiZhou/VLP" -> "airsplay/lxmert"
"LuoweiZhou/VLP" -> "husthuaan/AoANet"
"LuoweiZhou/VLP" -> "facebookresearch/vilbert-multi-task"
"LuoweiZhou/VLP" -> "cshizhe/asg2cap"
"LuoweiZhou/VLP" -> "aimagelab/meshed-memory-transformer"
"LuoweiZhou/VLP" -> "jiasenlu/vilbert_beta"
"LuoweiZhou/VLP" -> "yahoo/object_relation_transformer"
"LuoweiZhou/VLP" -> "ruotianluo/self-critical.pytorch"
"LuoweiZhou/VLP" -> "JDAI-CV/image-captioning"
"LuoweiZhou/VLP" -> "yangxuntu/SGAE"
"LuoweiZhou/VLP" -> "pzzhang/VinVL"
"ezeli/BUTD_model" -> "ezeli/bottom_up_features_extract"
"ezeli/BUTD_model" -> "ezeli/Transformer_model"
"ezeli/BUTD_model" -> "BierOne/bottom-up-attention-vqa"
"spitis/mrl" -> "wangzizhao/CausalDynamicsLearning"
"spitis/mrl" -> "martius-lab/cid-in-rl"
"spitis/mrl" -> "Stilwell-Git/Hindsight-Goal-Generation"
"alasdairtran/transform-and-tell" -> "furkanbiten/GoodNews"
"alasdairtran/transform-and-tell" -> "fenglinliu98/MIA"
"husthuaan/AoANet" -> "yahoo/object_relation_transformer"
"husthuaan/AoANet" -> "JDAI-CV/image-captioning"
"husthuaan/AoANet" -> "yangxuntu/SGAE"
"husthuaan/AoANet" -> "aimagelab/meshed-memory-transformer"
"husthuaan/AoANet" -> "ruotianluo/self-critical.pytorch"
"husthuaan/AoANet" -> "fengyang0317/unsupervised_captioning"
"husthuaan/AoANet" -> "luo3300612/image-captioning-DLCT"
"husthuaan/AoANet" -> "aimagelab/show-control-and-tell"
"husthuaan/AoANet" -> "husthuaan/AAT"
"husthuaan/AoANet" -> "cshizhe/asg2cap"
"husthuaan/AoANet" -> "fawazsammani/show-edit-tell"
"husthuaan/AoANet" -> "zhjohnchan/awesome-image-captioning"
"husthuaan/AoANet" -> "poojahira/image-captioning-bottom-up-top-down"
"husthuaan/AoANet" -> "forence/Awesome-Visual-Captioning"
"husthuaan/AoANet" -> "ruotianluo/ImageCaptioning.pytorch"
"ck0123/improved-bertscore-for-image-captioning-evaluation" -> "catty-project/catty"
"ck0123/improved-bertscore-for-image-captioning-evaluation" -> "hwanheelee1993/ViLBERTScore"
"google-deepmind/multi_object_datasets" -> "applied-ai-lab/genesis"
"google-deepmind/multi_object_datasets" -> "zhixuan-lin/IODINE"
"google-deepmind/multi_object_datasets" -> "stelzner/monet"
"google-deepmind/multi_object_datasets" -> "zhixuan-lin/SPACE"
"google-deepmind/multi_object_datasets" -> "baudm/MONet-pytorch"
"google-deepmind/multi_object_datasets" -> "singhgautam/slate"
"google-deepmind/multi_object_datasets" -> "facebookresearch/clevr-dataset-gen"
"google-deepmind/multi_object_datasets" -> "lucidrains/slot-attention"
"google-deepmind/multi_object_datasets" -> "tkipf/c-swm"
"google-deepmind/multi_object_datasets" -> "JindongJiang/SCALOR"
"google-deepmind/multi_object_datasets" -> "amazon-science/object-centric-learning-framework"
"google-deepmind/multi_object_datasets" -> "e2crawfo/auto_yolo"
"google-deepmind/multi_object_datasets" -> "google-research/slot-attention-video"
"google-deepmind/multi_object_datasets" -> "google-deepmind/dsprites-dataset" ["e"=1]
"google-deepmind/multi_object_datasets" -> "ecker-lab/object-centric-representation-benchmark"
"krasserm/fairseq-image-captioning" -> "yahoo/object_relation_transformer"
"krasserm/fairseq-image-captioning" -> "aimagelab/meshed-memory-transformer"
"krasserm/fairseq-image-captioning" -> "saahiluppal/catr"
"krasserm/fairseq-image-captioning" -> "ruotianluo/self-critical.pytorch"
"krasserm/fairseq-image-captioning" -> "RoyalSkye/Image-Caption"
"krasserm/fairseq-image-captioning" -> "JDAI-CV/image-captioning"
"krasserm/fairseq-image-captioning" -> "husthuaan/AoANet"
"krasserm/fairseq-image-captioning" -> "luo3300612/image-captioning-DLCT"
"krasserm/fairseq-image-captioning" -> "poojahira/image-captioning-bottom-up-top-down"
"krasserm/fairseq-image-captioning" -> "fawazsammani/show-edit-tell"
"krasserm/fairseq-image-captioning" -> "cshizhe/asg2cap"
"krasserm/fairseq-image-captioning" -> "LuoweiZhou/VLP"
"krasserm/fairseq-image-captioning" -> "aimagelab/show-control-and-tell"
"krasserm/fairseq-image-captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"krasserm/fairseq-image-captioning" -> "wtliao/ImageTransformer"
"BigRedT/info-ground" -> "qinzzz/Multimodal-Alignment-Framework"
"BigRedT/info-ground" -> "zyang-ur/ReSC"
"BigRedT/info-ground" -> "youngfly11/ReIR-WeaklyGrounding.pytorch"
"BigRedT/info-ground" -> "GingL/ARN"
"BigRedT/info-ground" -> "ChopinSharp/ref-nms"
"BigRedT/info-ground" -> "svip-lab/LBYLNet"
"luogen1996/MCN" -> "spyflying/CMPC-Refseg"
"luogen1996/MCN" -> "lichengunc/MAttNet" ["e"=1]
"luogen1996/MCN" -> "zyang-ur/ReSC"
"luogen1996/MCN" -> "luogen1996/Real-time-Global-Inference-Network"
"luogen1996/MCN" -> "zyang-ur/onestage_grounding"
"luogen1996/MCN" -> "BCV-Uniandes/DMS"
"luogen1996/MCN" -> "sibeiyang/sgmn"
"luogen1996/MCN" -> "xh-liu/CM-Erase-REG"
"luogen1996/MCN" -> "daqingliu/NMTree"
"luogen1996/MCN" -> "fengguang94/BRINet"
"yikuan8/Transformers-VQA" -> "airsplay/py-bottom-up-attention"
"yikuan8/Transformers-VQA" -> "uclanlp/visualbert"
"yikuan8/Transformers-VQA" -> "abachaa/VQA-Med-2019" ["e"=1]
"yikuan8/Transformers-VQA" -> "airsplay/lxmert"
"yikuan8/Transformers-VQA" -> "HimariO/HatefulMemesChallenge"
"yikuan8/Transformers-VQA" -> "Awenbocc/med-vqa" ["e"=1]
"yikuan8/Transformers-VQA" -> "aioz-ai/MICCAI21_MMQ" ["e"=1]
"yikuan8/Transformers-VQA" -> "VirajBagal/MMBERT" ["e"=1]
"yikuan8/Transformers-VQA" -> "UCSD-AI4H/PathVQA" ["e"=1]
"yikuan8/Transformers-VQA" -> "ChenRocks/UNITER"
"gicheonkang/dan-visdial" -> "gicheonkang/sglkt-visdial"
"gicheonkang/dan-visdial" -> "shubhamagarwal92/visdial_conv"
"gicheonkang/dan-visdial" -> "vmurahari3/visdial-bert"
"gicheonkang/dan-visdial" -> "wh0330/CAG_VisDial"
"gicheonkang/dan-visdial" -> "taesunwhang/MVAN-VisDial"
"gicheonkang/dan-visdial" -> "facebookresearch/corefnmn"
"WellyZhang/CoPINet" -> "zkcys001/distracting_feature"
"WellyZhang/CoPINet" -> "Fen9/WReN"
"wangzheng17/awesome-causal-vision" -> "Wangt-CN/VC-R-CNN" ["e"=1]
"wangzheng17/awesome-causal-vision" -> "Wangt-CN/Awesome-Causality-in-CV" ["e"=1]
"wangzheng17/awesome-causal-vision" -> "yangxuntu/lxmertcatt"
"daqingliu/awesome-rec" -> "daqingliu/NMTree"
"RoyalSkye/Image-Caption" -> "aravindvarier/Image-Captioning-Pytorch"
"RoyalSkye/Image-Caption" -> "senadkurtisi/pytorch-image-captioning"
"RoyalSkye/Image-Caption" -> "saahiluppal/catr"
"RoyalSkye/Image-Caption" -> "zarzouram/image_captioning_with_transformers"
"kapilks/Download-youtube-video" -> "kapilks/Basic-Kernel"
"JindongJiang/SCALOR" -> "zhixuan-lin/G-SWM"
"JindongJiang/SCALOR" -> "martius-lab/SMORL"
"spyflying/CMPC-Refseg" -> "liruiyu/referseg_rrn"
"spyflying/CMPC-Refseg" -> "fengguang94/BRINet"
"spyflying/CMPC-Refseg" -> "spyflying/LSCM-Refseg"
"spyflying/CMPC-Refseg" -> "lwye/CMSA-Net"
"zyang-ur/ReSC" -> "zyang-ur/onestage_grounding"
"zyang-ur/ReSC" -> "ChopinSharp/ref-nms"
"zyang-ur/ReSC" -> "BigRedT/info-ground"
"zyang-ur/ReSC" -> "svip-lab/LBYLNet"
"zyang-ur/ReSC" -> "qinzzz/Multimodal-Alignment-Framework"
"zyang-ur/ReSC" -> "yangli18/VLTVG"
"zyang-ur/ReSC" -> "djiajunustc/TransVG"
"zyang-ur/ReSC" -> "nku-shengzheliu/Pytorch-TransVG"
"zyang-ur/ReSC" -> "LukeForeverYoung/QRNet"
"YiwuZhong/Sub-GC" -> "cshizhe/asg2cap"
"YiwuZhong/Sub-GC" -> "yangxuntu/SGAE"
"YiwuZhong/Sub-GC" -> "yahoo/object_relation_transformer"
"YiwuZhong/Sub-GC" -> "Kien085/SG2Caps"
"zhixuan-lin/SPACE" -> "zhixuan-lin/IODINE"
"zhixuan-lin/SPACE" -> "JindongJiang/SCALOR"
"zhixuan-lin/SPACE" -> "google-deepmind/multi_object_datasets"
"zhixuan-lin/SPACE" -> "zhixuan-lin/G-SWM"
"HKUST-KnowComp/Visual_PCR" -> "ZihaoW123/UniMM"
"HKUST-KnowComp/Visual_PCR" -> "shubhamagarwal92/visdial_conv"
"AndresPMD/Fine_Grained_Clf" -> "MCLAB-OCR/KnowledgeMiningWithSceneText"
"AndresPMD/Fine_Grained_Clf" -> "AndresPMD/Pytorch-yolo-phoc"
"AndresPMD/Fine_Grained_Clf" -> "AndresPMD/StacMR"
"AndresPMD/Fine_Grained_Clf" -> "AndresPMD/GCN_classification"
"cshizhe/asg2cap" -> "YiwuZhong/Sub-GC"
"cshizhe/asg2cap" -> "yangxuntu/SGAE"
"cshizhe/asg2cap" -> "JDAI-CV/image-captioning"
"cshizhe/asg2cap" -> "yahoo/object_relation_transformer"
"cshizhe/asg2cap" -> "aimagelab/show-control-and-tell"
"cshizhe/asg2cap" -> "luo3300612/image-captioning-DLCT"
"cshizhe/asg2cap" -> "fawazsammani/show-edit-tell"
"cshizhe/asg2cap" -> "husthuaan/AoANet"
"cshizhe/asg2cap" -> "ruotianluo/coco-caption"
"cshizhe/asg2cap" -> "YuanEZhou/Grounded-Image-Captioning"
"cshizhe/asg2cap" -> "jayleicn/recurrent-transformer" ["e"=1]
"cshizhe/asg2cap" -> "forence/Awesome-Visual-Captioning"
"cshizhe/asg2cap" -> "poojahira/image-captioning-bottom-up-top-down"
"cshizhe/asg2cap" -> "SydCaption/SAAT" ["e"=1]
"cshizhe/asg2cap" -> "aimagelab/meshed-memory-transformer"
"mengf1/CHER" -> "ruizhaogit/EnergyBasedPrioritization"
"mengf1/CHER" -> "YangRui2015/AWGCSL"
"mengf1/CHER" -> "henrycharlesworth/PlanGAN"
"mengf1/CHER" -> "Stilwell-Git/Hindsight-Goal-Generation"
"chrisc36/debias" -> "UKPLab/acl2020-confidence-regularization"
"chrisc36/debias" -> "UKPLab/emnlp2020-debiasing-unknown"
"chrisc36/debias" -> "cdancette/rubi.bootstrap.pytorch"
"chrisc36/debias" -> "chrisc36/bottom-up-attention-vqa"
"chrisc36/debias" -> "jialinwu17/self_critical_vqa"
"idansc/mrr-ndcg" -> "simpleshinobu/visdial-principles"
"idansc/mrr-ndcg" -> "salesforce/VD-BERT"
"jack1yang/image-paragraph-captioning" -> "bupt-mmai/CNN-Caption"
"yahoo/object_relation_transformer" -> "husthuaan/AoANet"
"yahoo/object_relation_transformer" -> "aimagelab/meshed-memory-transformer"
"yahoo/object_relation_transformer" -> "zhangxuying1004/RSTNet"
"yahoo/object_relation_transformer" -> "YiwuZhong/Sub-GC"
"yahoo/object_relation_transformer" -> "yangxuntu/SGAE"
"yahoo/object_relation_transformer" -> "JDAI-CV/image-captioning"
"yahoo/object_relation_transformer" -> "luo3300612/image-captioning-DLCT"
"yahoo/object_relation_transformer" -> "cshizhe/asg2cap"
"yahoo/object_relation_transformer" -> "krasserm/fairseq-image-captioning"
"yahoo/object_relation_transformer" -> "fawazsammani/show-edit-tell"
"yahoo/object_relation_transformer" -> "fenglinliu98/MIA"
"yahoo/object_relation_transformer" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"yahoo/object_relation_transformer" -> "aimagelab/show-control-and-tell"
"yahoo/object_relation_transformer" -> "poojahira/image-captioning-bottom-up-top-down"
"yahoo/object_relation_transformer" -> "gujiuxiang/Stack-Captioning"
"zkcys001/distracting_feature" -> "WellyZhang/CoPINet"
"zkcys001/distracting_feature" -> "Fen9/WReN"
"zkcys001/distracting_feature" -> "husheng12345/SRAN"
"husthuaan/AAT" -> "JonghwanMun/TextguidedATT"
"husthuaan/AAT" -> "andyweizhao/Multitask_Image_Captioning"
"dido1998/Recurrent-Independent-Mechanisms" -> "anirudh9119/RIMs"
"fengguang94/BRINet" -> "spyflying/CMPC-Refseg"
"qinzzz/Multimodal-Alignment-Framework" -> "youngfly11/ReIR-WeaklyGrounding.pytorch"
"qinzzz/Multimodal-Alignment-Framework" -> "BigRedT/info-ground"
"qinzzz/Multimodal-Alignment-Framework" -> "zyang-ur/ReSC"
"vmurahari3/visdial-bert" -> "gicheonkang/dan-visdial"
"vmurahari3/visdial-bert" -> "salesforce/VD-BERT"
"vmurahari3/visdial-bert" -> "idansc/mrr-ndcg"
"vmurahari3/visdial-bert" -> "shubhamagarwal92/visdial_conv"
"vmurahari3/visdial-bert" -> "simpleshinobu/visdial-principles"
"vmurahari3/visdial-bert" -> "davidnvq/visdial"
"vmurahari3/visdial-bert" -> "taesunwhang/MVAN-VisDial"
"vmurahari3/visdial-bert" -> "JXZe/DualVD"
"vmurahari3/visdial-bert" -> "wh0330/CAG_VisDial"
"vmurahari3/visdial-bert" -> "yuleiniu/rva"
"vmurahari3/visdial-bert" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"GingL/ARN" -> "GingL/KPRN"
"GingL/ARN" -> "insomnia94/DTWREG"
"fawazsammani/show-edit-tell" -> "YuanEZhou/Grounded-Image-Captioning"
"Gitsamshi/WeakVRD-Captioning" -> "visinf/cos-cvae"
"Gitsamshi/WeakVRD-Captioning" -> "fawazsammani/look-and-modify"
"miriambellver/refvos" -> "skynbe/Refer-Youtube-VOS"
"sibeiyang/sgmn" -> "lichengunc/MAttNet" ["e"=1]
"sibeiyang/sgmn" -> "ronghanghu/lcgn"
"sibeiyang/sgmn" -> "yuleiniu/vc"
"sibeiyang/sgmn" -> "zyang-ur/ReSC"
"sibeiyang/sgmn" -> "GingL/ARN"
"simpleshinobu/visdial-principles" -> "idansc/mrr-ndcg"
"simpleshinobu/visdial-principles" -> "shubhamagarwal92/visdial_conv"
"simpleshinobu/visdial-principles" -> "vmurahari3/visdial-bert"
"shubhamagarwal92/visdial_conv" -> "wh0330/CAG_VisDial"
"shubhamagarwal92/visdial_conv" -> "gicheonkang/dan-visdial"
"shubhamagarwal92/visdial_conv" -> "facebookresearch/corefnmn"
"shubhamagarwal92/visdial_conv" -> "davidnvq/visdial"
"yanxinzju/CSS-VQA" -> "chrisc36/bottom-up-attention-vqa"
"yanxinzju/CSS-VQA" -> "jialinwu17/self_critical_vqa"
"yanxinzju/CSS-VQA" -> "cdancette/vqa-cp-leaderboard"
"yanxinzju/CSS-VQA" -> "cdancette/rubi.bootstrap.pytorch"
"yanxinzju/CSS-VQA" -> "gqa-ood/GQA-OOD"
"yanxinzju/CSS-VQA" -> "yuleiniu/cfvqa"
"yanxinzju/CSS-VQA" -> "GeraldHan/GGE"
"yanxinzju/CSS-VQA" -> "CrossmodalGroup/SSL-VQA" ["e"=1]
"ronghanghu/vqa-maskrcnn-benchmark-m4c" -> "ChenyuGAO-CS/SMA"
"zhixuan-lin/G-SWM" -> "JindongJiang/SCALOR"
"skynbe/Refer-Youtube-VOS" -> "miriambellver/refvos"
"visiontao/evar" -> "visiontao/dcnet"
"chrisc36/bottom-up-attention-vqa" -> "yanxinzju/CSS-VQA"
"chrisc36/bottom-up-attention-vqa" -> "jialinwu17/self_critical_vqa"
"insomnia94/DTWREG" -> "insomnia94/ISREG"
"taesunwhang/MVAN-VisDial" -> "gicheonkang/dan-visdial"
"taesunwhang/MVAN-VisDial" -> "shubhamagarwal92/visdial_conv"
"taesunwhang/MVAN-VisDial" -> "wh0330/CAG_VisDial"
"taesunwhang/MVAN-VisDial" -> "vmurahari3/visdial-bert"
"youngfly11/LCMCG-PyTorch" -> "daqingliu/NMTree"
"UKPLab/acl2020-confidence-regularization" -> "PluviophileYU/CVC-QA"
"erobic/negative_analysis_of_grounding" -> "jialinwu17/self_critical_vqa"
"guanghuixu/CRN_tvqa" -> "guanghuixu/AnchorCaptioner"
"HLR/Cross_Modality_Relevance" -> "wh0330/CAG_VisDial"
"GingL/KPRN" -> "GingL/ARN"
"distkv-project/drpc" -> "distkv-project/distkv"
"distkv-project/drpc" -> "catty-project/catty"
"wh0330/CAG_VisDial" -> "shubhamagarwal92/visdial_conv"
"bupt-mmai/CNN-Caption" -> "jack1yang/image-paragraph-captioning"
"salesforce/ALBEF" -> "dandelin/ViLT"
"salesforce/ALBEF" -> "salesforce/BLIP" ["e"=1]
"salesforce/ALBEF" -> "ChenRocks/UNITER"
"salesforce/ALBEF" -> "uta-smile/TCL"
"salesforce/ALBEF" -> "salesforce/LAVIS" ["e"=1]
"salesforce/ALBEF" -> "microsoft/Oscar"
"salesforce/ALBEF" -> "OFA-Sys/OFA" ["e"=1]
"salesforce/ALBEF" -> "zdou0830/METER"
"salesforce/ALBEF" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"salesforce/ALBEF" -> "KaiyangZhou/CoOp" ["e"=1]
"salesforce/ALBEF" -> "microsoft/GLIP" ["e"=1]
"salesforce/ALBEF" -> "airsplay/lxmert"
"salesforce/ALBEF" -> "zengyan-97/X-VLM"
"salesforce/ALBEF" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"salesforce/ALBEF" -> "jayleicn/ClipBERT"
"zhegan27/VILLA" -> "zhegan27/LXMERT-AdvTrain"
"Muennighoff/vilio" -> "HimariO/HatefulMemesChallenge"
"Muennighoff/vilio" -> "Nithin-Holla/meme_challenge"
"HimariO/HatefulMemesChallenge" -> "rizavelioglu/hateful_memes-hate_detectron"
"HimariO/HatefulMemesChallenge" -> "Muennighoff/vilio"
"HimariO/HatefulMemesChallenge" -> "Nithin-Holla/meme_challenge"
"google-research-datasets/wit" -> "microsoft/Oscar"
"google-research-datasets/wit" -> "ChenRocks/UNITER"
"google-research-datasets/wit" -> "facebookresearch/vilbert-multi-task"
"google-research-datasets/wit" -> "google-research-datasets/conceptual-captions"
"google-research-datasets/wit" -> "allenai/mmc4" ["e"=1]
"google-research-datasets/wit" -> "clip-vil/CLIP-ViL"
"google-research-datasets/wit" -> "salesforce/ALBEF"
"google-research-datasets/wit" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"google-research-datasets/wit" -> "jiasenlu/vilbert_beta"
"google-research-datasets/wit" -> "FreddeFrallan/Multilingual-CLIP" ["e"=1]
"google-research-datasets/wit" -> "LuoweiZhou/VLP"
"google-research-datasets/wit" -> "kakaobrain/coyo-dataset" ["e"=1]
"google-research-datasets/wit" -> "microsoft/UniCL"
"google-research-datasets/wit" -> "ashkamath/mdetr"
"google-research-datasets/wit" -> "rom1504/img2dataset" ["e"=1]
"tylin/coco-caption" -> "ruotianluo/self-critical.pytorch"
"tylin/coco-caption" -> "ruotianluo/ImageCaptioning.pytorch"
"tylin/coco-caption" -> "salaniz/pycocoevalcap"
"tylin/coco-caption" -> "peteanderson80/bottom-up-attention"
"tylin/coco-caption" -> "kelvinxu/arctic-captions"
"tylin/coco-caption" -> "zhjohnchan/awesome-image-captioning"
"tylin/coco-caption" -> "yunjey/show-attend-and-tell"
"tylin/coco-caption" -> "Maluuba/nlg-eval" ["e"=1]
"tylin/coco-caption" -> "aimagelab/meshed-memory-transformer"
"tylin/coco-caption" -> "jiasenlu/NeuralBabyTalk"
"tylin/coco-caption" -> "xiadingZ/video-caption.pytorch" ["e"=1]
"tylin/coco-caption" -> "husthuaan/AoANet"
"tylin/coco-caption" -> "jiasenlu/AdaptiveAttention"
"tylin/coco-caption" -> "DeepRNN/image_captioning"
"tylin/coco-caption" -> "forence/Awesome-Visual-Captioning"
"kelvinxu/arctic-captions" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"kelvinxu/arctic-captions" -> "yunjey/show-attend-and-tell"
"kelvinxu/arctic-captions" -> "tylin/coco-caption"
"kelvinxu/arctic-captions" -> "DeepRNN/image_captioning"
"kelvinxu/arctic-captions" -> "ruotianluo/ImageCaptioning.pytorch"
"kelvinxu/arctic-captions" -> "Element-Research/rnn" ["e"=1]
"kelvinxu/arctic-captions" -> "jcjohnson/densecap"
"kelvinxu/arctic-captions" -> "jiasenlu/AdaptiveAttention"
"kelvinxu/arctic-captions" -> "ryankiros/visual-semantic-embedding" ["e"=1]
"kelvinxu/arctic-captions" -> "mansimov/text2image" ["e"=1]
"kelvinxu/arctic-captions" -> "ruotianluo/self-critical.pytorch"
"kelvinxu/arctic-captions" -> "jiasenlu/NeuralBabyTalk"
"kelvinxu/arctic-captions" -> "karpathy/neuraltalk" ["e"=1]
"kelvinxu/arctic-captions" -> "jazzsaxmafia/show_and_tell.tensorflow"
"kelvinxu/arctic-captions" -> "nyu-dl/dl4mt-tutorial" ["e"=1]
"mjhucla/mRNN-CR" -> "mjhucla/TF-mRNN"
"dandelin/ViLT" -> "salesforce/ALBEF"
"dandelin/ViLT" -> "ChenRocks/UNITER"
"dandelin/ViLT" -> "microsoft/Oscar"
"dandelin/ViLT" -> "airsplay/lxmert"
"dandelin/ViLT" -> "zdou0830/METER"
"dandelin/ViLT" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"dandelin/ViLT" -> "facebookresearch/vilbert-multi-task"
"dandelin/ViLT" -> "salesforce/BLIP" ["e"=1]
"dandelin/ViLT" -> "OFA-Sys/OFA" ["e"=1]
"dandelin/ViLT" -> "ashkamath/mdetr"
"dandelin/ViLT" -> "jackroos/VL-BERT"
"dandelin/ViLT" -> "researchmm/soho"
"dandelin/ViLT" -> "zengyan-97/X-VLM"
"dandelin/ViLT" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"dandelin/ViLT" -> "KaiyangZhou/CoOp" ["e"=1]
"evelinehong/slot-attention-pytorch" -> "lucidrains/slot-attention"
"evelinehong/slot-attention-pytorch" -> "amazon-science/object-centric-learning-framework"
"evelinehong/slot-attention-pytorch" -> "amazon-science/AdaSlot"
"evelinehong/slot-attention-pytorch" -> "google-research/slot-attention-video"
"djiajunustc/TransVG" -> "zyang-ur/ReSC"
"djiajunustc/TransVG" -> "nku-shengzheliu/Pytorch-TransVG"
"djiajunustc/TransVG" -> "yangli18/VLTVG"
"djiajunustc/TransVG" -> "linhuixiao/CLIP-VG"
"djiajunustc/TransVG" -> "seanzhuh/SeqTR"
"djiajunustc/TransVG" -> "LukeForeverYoung/QRNet"
"djiajunustc/TransVG" -> "zyang-ur/onestage_grounding"
"djiajunustc/TransVG" -> "LeapLabTHU/Pseudo-Q" ["e"=1]
"djiajunustc/TransVG" -> "djiajunustc/H-23D_R-CNN"
"djiajunustc/TransVG" -> "ubc-vision/RefTR"
"djiajunustc/TransVG" -> "lichengunc/refer"
"djiajunustc/TransVG" -> "TheShadow29/awesome-grounding"
"djiajunustc/TransVG" -> "Dmmm1997/SimVG"
"djiajunustc/TransVG" -> "ZhanYang-nwpu/RSVG-pytorch" ["e"=1]
"djiajunustc/TransVG" -> "BryanPlummer/flickr30k_entities"
"ZephyrZhuQi/ssbaseline" -> "ronghanghu/mmf"
"ZephyrZhuQi/ssbaseline" -> "ChenyuGAO-CS/SMA"
"ZephyrZhuQi/ssbaseline" -> "yashkant/sam-textvqa"
"ZephyrZhuQi/ssbaseline" -> "microsoft/TAP"
"GT-Vision-Lab/VQA" -> "jiasenlu/HieCoAttenVQA"
"GT-Vision-Lab/VQA" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"GT-Vision-Lab/VQA" -> "Cadene/vqa.pytorch"
"GT-Vision-Lab/VQA" -> "hengyuan-hu/bottom-up-attention-vqa"
"GT-Vision-Lab/VQA" -> "chingyaoc/VQA-tensorflow"
"GT-Vision-Lab/VQA" -> "chingyaoc/awesome-vqa"
"GT-Vision-Lab/VQA" -> "Cadene/block.bootstrap.pytorch"
"GT-Vision-Lab/VQA" -> "jnhwkim/ban-vqa"
"GT-Vision-Lab/VQA" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"GT-Vision-Lab/VQA" -> "MILVLG/mcan-vqa"
"GT-Vision-Lab/VQA" -> "akirafukui/vqa-mcb"
"GT-Vision-Lab/VQA" -> "iamaaditya/VQA_Demo"
"GT-Vision-Lab/VQA" -> "Cyanogenoid/pytorch-vqa"
"GT-Vision-Lab/VQA" -> "zhoubolei/VQAbaseline"
"GT-Vision-Lab/VQA" -> "stanfordnlp/mac-network"
"codexxxl/GraphVQA" -> "jialinwu17/MAVEX"
"codexxxl/GraphVQA" -> "jlian2/mucko"
"ashkamath/mdetr" -> "microsoft/GLIP" ["e"=1]
"ashkamath/mdetr" -> "TheShadow29/awesome-grounding"
"ashkamath/mdetr" -> "lichengunc/refer"
"ashkamath/mdetr" -> "facebookresearch/Detic" ["e"=1]
"ashkamath/mdetr" -> "microsoft/Oscar"
"ashkamath/mdetr" -> "jayleicn/ClipBERT"
"ashkamath/mdetr" -> "wjn922/ReferFormer"
"ashkamath/mdetr" -> "alirezazareian/ovr-cnn" ["e"=1]
"ashkamath/mdetr" -> "djiajunustc/TransVG"
"ashkamath/mdetr" -> "ChenRocks/UNITER"
"ashkamath/mdetr" -> "OFA-Sys/OFA" ["e"=1]
"ashkamath/mdetr" -> "salesforce/ALBEF"
"ashkamath/mdetr" -> "mmaaz60/mvits_for_class_agnostic_od" ["e"=1]
"ashkamath/mdetr" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"ashkamath/mdetr" -> "clip-vil/CLIP-ViL"
"jialinwu17/MAVEX" -> "guoyang9/UnifER"
"jialinwu17/MAVEX" -> "alirezasalemi7/DEDR-MM-FiD"
"nku-shengzheliu/Pytorch-TransVG" -> "djiajunustc/TransVG"
"nku-shengzheliu/Pytorch-TransVG" -> "youngfly11/ReIR-WeaklyGrounding.pytorch"
"nku-shengzheliu/Pytorch-TransVG" -> "zyang-ur/ReSC"
"nku-shengzheliu/Pytorch-TransVG" -> "usr922/vgtr"
"google-research-datasets/conceptual-12m" -> "google-research-datasets/conceptual-captions"
"google-research-datasets/conceptual-12m" -> "zhegan27/VILLA"
"pzzhang/VinVL" -> "microsoft/scene_graph_benchmark" ["e"=1]
"pzzhang/VinVL" -> "microsoft/Oscar"
"pzzhang/VinVL" -> "airsplay/py-bottom-up-attention"
"pzzhang/VinVL" -> "facebookresearch/grid-feats-vqa"
"pzzhang/VinVL" -> "ChenRocks/UNITER"
"pzzhang/VinVL" -> "clip-vil/CLIP-ViL"
"pzzhang/VinVL" -> "LuoweiZhou/VLP"
"pzzhang/VinVL" -> "microsoft/TAP"
"pzzhang/VinVL" -> "researchmm/soho"
"pzzhang/VinVL" -> "peteanderson80/bottom-up-attention"
"pzzhang/VinVL" -> "yuleiniu/cfvqa"
"pzzhang/VinVL" -> "linjieli222/VQA_ReGAT"
"pzzhang/VinVL" -> "KaihuaTang/Scene-Graph-Benchmark.pytorch" ["e"=1]
"pzzhang/VinVL" -> "airsplay/lxmert"
"pzzhang/VinVL" -> "jokieleung/awesome-visual-question-answering"
"guanghuixu/AnchorCaptioner" -> "guanghuixu/CRN_tvqa"
"rizavelioglu/hateful_memes-hate_detectron" -> "HimariO/HatefulMemesChallenge"
"rizavelioglu/hateful_memes-hate_detectron" -> "gokulkarthik/hateclipper"
"s-gupta/visual-concepts" -> "chapternewscu/image-captioning-with-semantic-attention"
"s-gupta/visual-concepts" -> "kimiyoung/review_net"
"s-gupta/visual-concepts" -> "zhegan27/Semantic_Compositional_Nets"
"s-gupta/visual-concepts" -> "fenglinliu98/MIA"
"s-gupta/visual-concepts" -> "andyweizhao/Multitask_Image_Captioning"
"s-gupta/visual-concepts" -> "mjhucla/mRNN-CR"
"s-gupta/visual-concepts" -> "gujiuxiang/MIL.pytorch" ["e"=1]
"s-gupta/visual-concepts" -> "ruotianluo/Transformer_Captioning"
"s-gupta/visual-concepts" -> "yahoo/object_relation_transformer"
"s-gupta/visual-concepts" -> "yiyang92/vae_captioning"
"s-gupta/visual-concepts" -> "aditya12agd5/convcap"
"jayleicn/ClipBERT" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"jayleicn/ClipBERT" -> "linjieli222/HERO" ["e"=1]
"jayleicn/ClipBERT" -> "gabeur/mmt" ["e"=1]
"jayleicn/ClipBERT" -> "CryhanFang/CLIP2Video" ["e"=1]
"jayleicn/ClipBERT" -> "m-bain/frozen-in-time" ["e"=1]
"jayleicn/ClipBERT" -> "danieljf24/awesome-video-text-retrieval" ["e"=1]
"jayleicn/ClipBERT" -> "microsoft/UniVL" ["e"=1]
"jayleicn/ClipBERT" -> "jayleicn/singularity" ["e"=1]
"jayleicn/ClipBERT" -> "tsujuifu/pytorch_violet" ["e"=1]
"jayleicn/ClipBERT" -> "microsoft/Oscar"
"jayleicn/ClipBERT" -> "albanie/collaborative-experts" ["e"=1]
"jayleicn/ClipBERT" -> "jayleicn/moment_detr" ["e"=1]
"jayleicn/ClipBERT" -> "antoine77340/MIL-NCE_HowTo100M" ["e"=1]
"jayleicn/ClipBERT" -> "ChenRocks/UNITER"
"jayleicn/ClipBERT" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"luo3300612/image-captioning-DLCT" -> "zhangxuying1004/RSTNet"
"luo3300612/image-captioning-DLCT" -> "aimagelab/meshed-memory-transformer"
"luo3300612/image-captioning-DLCT" -> "JDAI-CV/image-captioning"
"luo3300612/image-captioning-DLCT" -> "facebookresearch/grid-feats-vqa"
"luo3300612/image-captioning-DLCT" -> "husthuaan/AoANet"
"luo3300612/image-captioning-DLCT" -> "yahoo/object_relation_transformer"
"luo3300612/image-captioning-DLCT" -> "232525/PureT"
"luo3300612/image-captioning-DLCT" -> "davidnvq/grit"
"luo3300612/image-captioning-DLCT" -> "cshizhe/asg2cap"
"luo3300612/image-captioning-DLCT" -> "forence/Awesome-Visual-Captioning"
"luo3300612/image-captioning-DLCT" -> "luo3300612/Transformer-Captioning"
"luo3300612/image-captioning-DLCT" -> "terry-r123/Awesome-Captioning"
"luo3300612/image-captioning-DLCT" -> "aimagelab/show-control-and-tell"
"luo3300612/image-captioning-DLCT" -> "fengyang0317/unsupervised_captioning"
"luo3300612/image-captioning-DLCT" -> "yangxuntu/SGAE"
"clip-vil/CLIP-ViL" -> "YicongHong/Recurrent-VLN-BERT" ["e"=1]
"clip-vil/CLIP-ViL" -> "airsplay/R2R-EnvDrop" ["e"=1]
"clip-vil/CLIP-ViL" -> "cshizhe/VLN-HAMT" ["e"=1]
"clip-vil/CLIP-ViL" -> "cshizhe/VLN-DUET" ["e"=1]
"clip-vil/CLIP-ViL" -> "weituo12321/PREVALENT" ["e"=1]
"clip-vil/CLIP-ViL" -> "jialuli-luka/EnvEdit" ["e"=1]
"clip-vil/CLIP-ViL" -> "facebookresearch/grid-feats-vqa"
"clip-vil/CLIP-ViL" -> "microsoft/Oscar"
"clip-vil/CLIP-ViL" -> "YicongHong/Discrete-Continuous-VLN" ["e"=1]
"clip-vil/CLIP-ViL" -> "pzzhang/VinVL"
"clip-vil/CLIP-ViL" -> "zdou0830/METER"
"clip-vil/CLIP-ViL" -> "YicongHong/Fine-Grained-R2R" ["e"=1]
"clip-vil/CLIP-ViL" -> "arjunmajum/vln-bert" ["e"=1]
"clip-vil/CLIP-ViL" -> "MarSaKi/VLN-BEVBert" ["e"=1]
"clip-vil/CLIP-ViL" -> "ronghanghu/speaker_follower" ["e"=1]
"MILVLG/rosita" -> "MILVLG/mt-captioning"
"Vision-CAIR/VisualGPT" -> "luo3300612/image-captioning-DLCT"
"Vision-CAIR/VisualGPT" -> "aimagelab/meshed-memory-transformer"
"Vision-CAIR/VisualGPT" -> "Jhhuangkay/DeepOpht-Medical-Report-Generation-for-Retinal-Images-via-Deep-Models-and-Visual-Explanation"
"Vision-CAIR/VisualGPT" -> "yahoo/object_relation_transformer"
"Vision-CAIR/VisualGPT" -> "omar-mohamed/GPT2-Chest-X-Ray-Report-Generation" ["e"=1]
"Vision-CAIR/VisualGPT" -> "GT-RIPL/Xmodal-Ctx"
"Vision-CAIR/VisualGPT" -> "forence/Awesome-Visual-Captioning"
"Vision-CAIR/VisualGPT" -> "davidnvq/grit"
"Vision-CAIR/VisualGPT" -> "cshizhe/asg2cap"
"lanfeng4659/STR-TDSL" -> "AndresPMD/Pytorch-yolo-phoc"
"yuleiniu/cfvqa" -> "yanxinzju/CSS-VQA"
"yuleiniu/cfvqa" -> "yangxuntu/lxmertcatt"
"yuleiniu/cfvqa" -> "cdancette/rubi.bootstrap.pytorch"
"yuleiniu/cfvqa" -> "simpleshinobu/visdial-principles"
"yuleiniu/cfvqa" -> "chrisc36/debias"
"yuleiniu/cfvqa" -> "cdancette/vqa-cp-leaderboard"
"mit-ll-responsible-ai/hydra-zen" -> "pytorch/hydra-torch"
"mit-ll-responsible-ai/hydra-zen" -> "rsokl/MyGrad"
"mit-ll-responsible-ai/hydra-zen" -> "mit-ll-responsible-ai/responsible-ai-toolbox"
"mit-ll-responsible-ai/hydra-zen" -> "patrick-kidger/jaxtyping" ["e"=1]
"djiajunustc/H-23D_R-CNN" -> "caiqi/Cascasde-3D"
"jlian2/mucko" -> "wh0330/CAG_VisDial"
"jlian2/mucko" -> "China-UK-ZSL/ZS-F-VQA"
"dido1998/CausalMBRL" -> "wangzizhao/CausalDynamicsLearning"
"dido1998/CausalMBRL" -> "GilgameshD/GRADER"
"Dantekk/Image-Captioning" -> "senadkurtisi/pytorch-image-captioning"
"Dantekk/Image-Captioning" -> "zarzouram/image_captioning_with_transformers"
"Dantekk/Image-Captioning" -> "Dantekk/PokeGAN"
"husheng12345/SRAN" -> "zkcys001/distracting_feature"
"husheng12345/SRAN" -> "visiontao/dcnet"
"husheng12345/SRAN" -> "WellyZhang/PrAE"
"husheng12345/SRAN" -> "visiontao/ncd"
"WellyZhang/PrAE" -> "WellyZhang/ALANS"
"WellyZhang/PrAE" -> "husheng12345/SRAN"
"ThalesGroup/ConceptBERT" -> "jlian2/mucko"
"ThalesGroup/ConceptBERT" -> "jialinwu17/MAVEX"
"ThalesGroup/ConceptBERT" -> "China-UK-ZSL/ZS-F-VQA"
"sumedh7/CausalCuriosity" -> "wangzizhao/CausalDynamicsLearning"
"China-UK-ZSL/ZS-F-VQA" -> "jlian2/mucko"
"China-UK-ZSL/ZS-F-VQA" -> "ThalesGroup/ConceptBERT"
"China-UK-ZSL/ZS-F-VQA" -> "AndersonStra/MuKEA"
"JindongJiang/GNM" -> "WellyZhang/ALANS"
"microsoft/TAP" -> "uakarsh/latr"
"microsoft/TAP" -> "yashkant/sam-textvqa"
"microsoft/TAP" -> "ChenyuGAO-CS/SMA"
"microsoft/TAP" -> "ZephyrZhuQi/ssbaseline"
"microsoft/TAP" -> "ronghanghu/mmf"
"microsoft/TAP" -> "xinke-wang/Awesome-Text-VQA"
"microsoft/TAP" -> "guanghuixu/CRN_tvqa"
"microsoft/TAP" -> "HenryJunW/TAG"
"ncs-jss/HTTP_200" -> "spyshiv/dummytextjs"
"ncs-jss/HTTP_200" -> "aka-jain/Drango"
"zhangxuying1004/RSTNet" -> "luo3300612/image-captioning-DLCT"
"zhangxuying1004/RSTNet" -> "yahoo/object_relation_transformer"
"zhangxuying1004/RSTNet" -> "luo3300612/Transformer-Captioning"
"zhangxuying1004/RSTNet" -> "davidnvq/grit"
"zhangxuying1004/RSTNet" -> "mrwu-mac/DIFNet"
"zhangxuying1004/RSTNet" -> "GT-RIPL/Xmodal-Ctx"
"zhangxuying1004/RSTNet" -> "JDAI-CV/image-captioning"
"zhangxuying1004/RSTNet" -> "xmu-xiaoma666/LSTNet"
"zhangxuying1004/RSTNet" -> "232525/PureT"
"zhangxuying1004/RSTNet" -> "Gitsamshi/WeakVRD-Captioning"
"zhangxuying1004/RSTNet" -> "facebookresearch/grid-feats-vqa"
"researchmm/soho" -> "facebookresearch/grid-feats-vqa"
"researchmm/soho" -> "microsoft/Oscar"
"researchmm/soho" -> "zdou0830/METER"
"researchmm/soho" -> "e-bug/volta"
"researchmm/soho" -> "uta-smile/TCL"
"google-deepmind/dm_alchemy" -> "dido1998/CausalMBRL"
"google-deepmind/dm_alchemy" -> "rr-learning/CausalWorld"
"Cloud-CV/CloudCV-Old" -> "Cloud-CV/origami-lib"
"Cloud-CV/CloudCV-Old" -> "aka-jain/Drango"
"youngfly11/ReIR-WeaklyGrounding.pytorch" -> "qinzzz/Multimodal-Alignment-Framework"
"youngfly11/ReIR-WeaklyGrounding.pytorch" -> "BigRedT/info-ground"
"AndresPMD/StacMR" -> "AndresPMD/semantic_adaptive_margin" ["e"=1]
"AndresPMD/StacMR" -> "AndresPMD/Fine_Grained_Clf"
"AndresPMD/StacMR" -> "AndresPMD/Pytorch-yolo-phoc"
"salesforce/VD-BERT" -> "idansc/mrr-ndcg"
"salesforce/VD-BERT" -> "vmurahari3/visdial-bert"
"salesforce/VD-BERT" -> "simpleshinobu/visdial-principles"
"salesforce/VD-BERT" -> "wh0330/CAG_VisDial"
"yangxuntu/lxmertcatt" -> "simpleshinobu/visdial-principles"
"yangxuntu/lxmertcatt" -> "yuleiniu/cfvqa"
"romesco/hydra-lightning" -> "pytorch/hydra-torch"
"yashkant/sam-textvqa" -> "microsoft/TAP"
"yashkant/sam-textvqa" -> "uakarsh/latr"
"yashkant/sam-textvqa" -> "ZephyrZhuQi/ssbaseline"
"yashkant/sam-textvqa" -> "ronghanghu/mmf"
"yashkant/sam-textvqa" -> "guanghuixu/CRN_tvqa"
"yashkant/sam-textvqa" -> "xinke-wang/Awesome-Text-VQA"
"terry-r123/Awesome-Captioning" -> "forence/Awesome-Visual-Captioning"
"terry-r123/Awesome-Captioning" -> "GT-RIPL/Xmodal-Ctx"
"terry-r123/Awesome-Captioning" -> "luo3300612/image-captioning-DLCT"
"terry-r123/Awesome-Captioning" -> "yangbang18/Non-Autoregressive-Video-Captioning" ["e"=1]
"terry-r123/Awesome-Captioning" -> "YuanEZhou/CBTrans"
"maximek3/e-ViL" -> "SkrighYZ/FGVE"
"YangRui2015/AWGCSL" -> "YangRui2015/GOAT"
"ChenyuGAO-CS/SMA" -> "ronghanghu/vqa-maskrcnn-benchmark-m4c"
"svip-lab/LBYLNet" -> "zyang-ur/ReSC"
"svip-lab/LBYLNet" -> "ChopinSharp/ref-nms"
"wangxiao5791509/TNL2K_evaluation_toolkit" -> "lizhou-cs/JointNLT"
"insomnia94/ISREG" -> "insomnia94/DTWREG"
"anirudh9119/shared_workspace" -> "anirudh9119/neural_production_systems"
"GeraldHan/GGE" -> "chojw/genb"
"GeraldHan/GGE" -> "yuleiniu/introd"
"visinf/cos-cvae" -> "AnnikaLindh/Diverse_and_Specific_Image_Captioning"
"visinf/cos-cvae" -> "Gitsamshi/WeakVRD-Captioning"
"ChopinSharp/ref-nms" -> "zyang-ur/ReSC"
"visiontao/dcnet" -> "visiontao/ncd"
"abhshkdz/neural-vqa" -> "iamaaditya/VQA_Demo"
"abhshkdz/neural-vqa" -> "zhoubolei/VQAbaseline"
"abhshkdz/neural-vqa" -> "avisingh599/visual-qa"
"abhshkdz/neural-vqa" -> "paarthneekhara/neural-vqa-tensorflow"
"abhshkdz/neural-vqa" -> "jiasenlu/HieCoAttenVQA"
"abhshkdz/neural-vqa" -> "Element-Research/rnn" ["e"=1]
"abhshkdz/neural-vqa" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"abhshkdz/neural-vqa" -> "kaishengtai/torch-ntm" ["e"=1]
"abhshkdz/neural-vqa" -> "akirafukui/vqa-mcb"
"abhshkdz/neural-vqa" -> "Cadene/vqa.pytorch"
"abhshkdz/neural-vqa" -> "Element-Research/dpnn" ["e"=1]
"abhshkdz/neural-vqa" -> "chingyaoc/VQA-tensorflow"
"abhshkdz/neural-vqa" -> "carpedm20/visual-analogy-tensorflow" ["e"=1]
"abhshkdz/neural-vqa" -> "ericjang/tdb" ["e"=1]
"abhshkdz/neural-vqa" -> "GT-Vision-Lab/VQA"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "yunjey/show-attend-and-tell"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "kelvinxu/arctic-captions"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "jazzsaxmafia/show_and_tell.tensorflow"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "DeepRNN/image_captioning"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "jazzsaxmafia/video_to_sequence" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "mosessoh/CNN-LSTM-Caption-Generator"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "ry/tensorflow-vgg16" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "mansimov/text2image" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "TensorFlowKR/awesome_tensorflow_implementations" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "facebookarchive/MIXER" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "paarthneekhara/neural-vqa-tensorflow"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "kimiyoung/review_net"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "avisingh599/visual-qa"
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "tsenghungchen/SA-tensorflow" ["e"=1]
"jazzsaxmafia/show_attend_and_tell.tensorflow" -> "jiasenlu/NeuralBabyTalk"
"avisingh599/visual-qa" -> "iamaaditya/VQA_Demo"
"avisingh599/visual-qa" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"avisingh599/visual-qa" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"avisingh599/visual-qa" -> "jiasenlu/HieCoAttenVQA"
"avisingh599/visual-qa" -> "abhshkdz/neural-vqa"
"avisingh599/visual-qa" -> "zhoubolei/VQAbaseline"
"avisingh599/visual-qa" -> "codekansas/keras-language-modeling" ["e"=1]
"avisingh599/visual-qa" -> "nicolas-ivanov/debug_seq2seq" ["e"=1]
"avisingh599/visual-qa" -> "siemanko/tf-adversarial" ["e"=1]
"avisingh599/visual-qa" -> "dblN/stochastic_depth_keras"
"avisingh599/visual-qa" -> "akirafukui/vqa-mcb"
"avisingh599/visual-qa" -> "vinhkhuc/MemN2N-babi-python" ["e"=1]
"avisingh599/visual-qa" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"avisingh599/visual-qa" -> "mila-iqia/summerschool2015" ["e"=1]
"avisingh599/visual-qa" -> "allenai/deep_qa" ["e"=1]
"mttr2021/MTTR" -> "wjn922/ReferFormer"
"mttr2021/MTTR" -> "JerryX1110/awesome-rvos"
"mttr2021/MTTR" -> "yz93/LAVT-RIS"
"mttr2021/MTTR" -> "hkchengrex/STCN" ["e"=1]
"mttr2021/MTTR" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"mttr2021/MTTR" -> "DerrickWang005/CRIS.pytorch"
"mttr2021/MTTR" -> "henghuiding/Vision-Language-Transformer" ["e"=1]
"mttr2021/MTTR" -> "gaomingqi/Awesome-Video-Object-Segmentation" ["e"=1]
"mttr2021/MTTR" -> "ashkamath/mdetr"
"mttr2021/MTTR" -> "FoundationVision/VNext" ["e"=1]
"mttr2021/MTTR" -> "dzh19990407/LBDT"
"mttr2021/MTTR" -> "seoungwugoh/STM" ["e"=1]
"mttr2021/MTTR" -> "yoxu515/aot-benchmark" ["e"=1]
"mttr2021/MTTR" -> "wjf5203/SeqFormer" ["e"=1]
"mttr2021/MTTR" -> "xmlyqing00/AFB-URR" ["e"=1]
"jacobandreas/nmn2" -> "ronghanghu/n2nmn"
"jacobandreas/nmn2" -> "zhoubolei/VQAbaseline"
"jacobandreas/nmn2" -> "jiasenlu/HieCoAttenVQA"
"jacobandreas/nmn2" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"jacobandreas/nmn2" -> "akirafukui/vqa-mcb"
"jacobandreas/nmn2" -> "facebookresearch/clevr-iep" ["e"=1]
"jacobandreas/nmn2" -> "jnhwkim/cbp"
"jacobandreas/nmn2" -> "ilyasu123/rlntm" ["e"=1]
"jacobandreas/nmn2" -> "kaishengtai/torch-ntm" ["e"=1]
"jacobandreas/nmn2" -> "jacobandreas/psketch"
"jazzsaxmafia/show_and_tell.tensorflow" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"jazzsaxmafia/show_and_tell.tensorflow" -> "chapternewscu/image-captioning-with-semantic-attention"
"jazzsaxmafia/show_and_tell.tensorflow" -> "zhoubolei/VQAbaseline"
"jazzsaxmafia/show_and_tell.tensorflow" -> "mjhucla/mRNN-CR"
"jazzsaxmafia/show_and_tell.tensorflow" -> "mosessoh/CNN-LSTM-Caption-Generator"
"jazzsaxmafia/show_and_tell.tensorflow" -> "kelvinxu/arctic-captions"
"jazzsaxmafia/show_and_tell.tensorflow" -> "ry/tensorflow-vgg16" ["e"=1]
"jazzsaxmafia/show_and_tell.tensorflow" -> "ruotianluo/neuraltalk2-tensorflow"
"jazzsaxmafia/show_and_tell.tensorflow" -> "yunjey/show-attend-and-tell"
"jazzsaxmafia/show_and_tell.tensorflow" -> "woodrush/neural-art-tf" ["e"=1]
"jazzsaxmafia/show_and_tell.tensorflow" -> "s-gupta/visual-concepts"
"jazzsaxmafia/show_and_tell.tensorflow" -> "mjhucla/TF-mRNN"
"zhoubolei/VQAbaseline" -> "chingyaoc/VQA-tensorflow"
"zhoubolei/VQAbaseline" -> "jiasenlu/HieCoAttenVQA"
"zhoubolei/VQAbaseline" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"zhoubolei/VQAbaseline" -> "akirafukui/vqa-mcb"
"zhoubolei/VQAbaseline" -> "iamaaditya/VQA_Demo"
"zhoubolei/VQAbaseline" -> "yukezhu/visual7w-qa-models"
"zhoubolei/VQAbaseline" -> "yukezhu/visual7w-toolkit"
"zhoubolei/VQAbaseline" -> "chingyaoc/awesome-vqa"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "jiasenlu/HieCoAttenVQA"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "akirafukui/vqa-mcb"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "chingyaoc/VQA-tensorflow"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "GT-Vision-Lab/VQA"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "iamaaditya/VQA_Demo"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "zhoubolei/VQAbaseline"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "avisingh599/visual-qa"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "chingyaoc/awesome-vqa"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "Cadene/vqa.pytorch"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "abhshkdz/neural-vqa-attention"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "yukezhu/visual7w-qa-models"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "paarthneekhara/neural-vqa-tensorflow"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "jacobandreas/nmn2"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "HyeonwooNoh/DPPnet"
"GT-Vision-Lab/VQA_LSTM_CNN" -> "zcyang/imageqa-san"
"HyeonwooNoh/DPPnet" -> "zcyang/imageqa-san"
"google-research/slot-attention-video" -> "pairlab/SlotFormer"
"google-research/slot-attention-video" -> "singhgautam/steve"
"google-research/slot-attention-video" -> "amazon-science/object-centric-learning-framework"
"google-research/slot-attention-video" -> "evelinehong/slot-attention-pytorch"
"google-research/slot-attention-video" -> "Wuziyi616/SlotDiffusion"
"google-research/slot-attention-video" -> "YuLiu-LY/BO-QSA"
"google-research/slot-attention-video" -> "singhgautam/slate"
"google-research/slot-attention-video" -> "gkakogeorgiou/spot"
"google-research/slot-attention-video" -> "junkeun-yi/SAVi-pytorch"
"google-research/slot-attention-video" -> "martius-lab/videosaur"
"google-research/slot-attention-video" -> "gorkaydemir/SOLV"
"google-research/slot-attention-video" -> "karazijal/clevrtex-generation"
"google-research/slot-attention-video" -> "amazon-science/AdaSlot"
"google-research/slot-attention-video" -> "gorkaydemir/DINOSAUR"
"DerrickWang005/CRIS.pytorch" -> "yz93/LAVT-RIS"
"DerrickWang005/CRIS.pytorch" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"DerrickWang005/CRIS.pytorch" -> "zichengsaber/LAVT-pytorch"
"DerrickWang005/CRIS.pytorch" -> "amazon-science/polygon-transformer"
"DerrickWang005/CRIS.pytorch" -> "wjn922/ReferFormer"
"DerrickWang005/CRIS.pytorch" -> "JerryX1110/awesome-rvos"
"DerrickWang005/CRIS.pytorch" -> "kkakkkka/ETRIS"
"DerrickWang005/CRIS.pytorch" -> "lichengunc/refer"
"DerrickWang005/CRIS.pytorch" -> "chongzhou96/MaskCLIP" ["e"=1]
"DerrickWang005/CRIS.pytorch" -> "henghuiding/Vision-Language-Transformer" ["e"=1]
"DerrickWang005/CRIS.pytorch" -> "toggle1995/RIS-DMMI"
"DerrickWang005/CRIS.pytorch" -> "Seonghoon-Yu/Zero-shot-RIS"
"DerrickWang005/CRIS.pytorch" -> "yangli18/VLTVG"
"DerrickWang005/CRIS.pytorch" -> "linyq2117/CLIP-ES" ["e"=1]
"DerrickWang005/CRIS.pytorch" -> "isl-org/lang-seg" ["e"=1]
"allenai/reclip" -> "ylingfeng/FGVP"
"allenai/reclip" -> "Show-han/Zeroshot_REC"
"zhjohnchan/awesome-vision-and-language-pretraining" -> "wangxidong06/CS224N"
"microsoft/UniCL" -> "researchmm/soho"
"microsoft/UniCL" -> "microsoft/X-Decoder" ["e"=1]
"microsoft/UniCL" -> "pzzhang/VinVL"
"microsoft/UniCL" -> "facebookresearch/SLIP" ["e"=1]
"jcjohnson/densecap" -> "karpathy/neuraltalk2" ["e"=1]
"jcjohnson/densecap" -> "ruotianluo/ImageCaptioning.pytorch"
"jcjohnson/densecap" -> "kelvinxu/arctic-captions"
"jcjohnson/densecap" -> "DeepRNN/image_captioning"
"jcjohnson/densecap" -> "yueatsprograms/Stochastic_Depth" ["e"=1]
"jcjohnson/densecap" -> "peteanderson80/bottom-up-attention"
"jcjohnson/densecap" -> "facebookarchive/fb.resnet.torch" ["e"=1]
"jcjohnson/densecap" -> "ruotianluo/self-critical.pytorch"
"jcjohnson/densecap" -> "facebookarchive/torchnet" ["e"=1]
"jcjohnson/densecap" -> "mansimov/text2image" ["e"=1]
"jcjohnson/densecap" -> "Element-Research/rnn" ["e"=1]
"jcjohnson/densecap" -> "jcjohnson/torch-rnn" ["e"=1]
"jcjohnson/densecap" -> "karpathy/neuraltalk" ["e"=1]
"jcjohnson/densecap" -> "tylin/coco-caption"
"jcjohnson/densecap" -> "DmitryUlyanov/texture_nets" ["e"=1]
"phellonchen/awesome-Vision-and-Language-Pre-training" -> "sangminwoo/awesome-vision-and-language"
"phellonchen/awesome-Vision-and-Language-Pre-training" -> "zhjohnchan/awesome-vision-and-language-pretraining"
"phellonchen/awesome-Vision-and-Language-Pre-training" -> "zdou0830/METER"
"phellonchen/awesome-Vision-and-Language-Pre-training" -> "yuewang-cuhk/awesome-vision-language-pretraining-papers"
"zdou0830/METER" -> "microsoft/BridgeTower"
"zdou0830/METER" -> "zengyan-97/X-VLM"
"zdou0830/METER" -> "clip-vil/CLIP-ViL"
"zdou0830/METER" -> "dandelin/ViLT"
"zdou0830/METER" -> "researchmm/soho"
"zdou0830/METER" -> "salesforce/ALBEF"
"zdou0830/METER" -> "uta-smile/TCL"
"zdou0830/METER" -> "airsplay/lxmert"
"zdou0830/METER" -> "ylsung/VL_adapter" ["e"=1]
"zdou0830/METER" -> "linjieli222/VQA_ReGAT"
"zdou0830/METER" -> "microsoft/Oscar"
"zdou0830/METER" -> "facebookresearch/grid-feats-vqa"
"zengyan-97/X-VLM" -> "zengyan-97/X2-VLM"
"zengyan-97/X-VLM" -> "uta-smile/TCL"
"zengyan-97/X-VLM" -> "zdou0830/METER"
"zengyan-97/X-VLM" -> "salesforce/ALBEF"
"zengyan-97/X-VLM" -> "ChenRocks/UNITER"
"zengyan-97/X-VLM" -> "mertyg/vision-language-models-are-bows" ["e"=1]
"zengyan-97/X-VLM" -> "Paranioar/SGRAF" ["e"=1]
"zengyan-97/X-VLM" -> "OFA-Sys/OFA" ["e"=1]
"zengyan-97/X-VLM" -> "MichaelZhouwang/VLUE"
"zengyan-97/X-VLM" -> "dandelin/ViLT"
"zengyan-97/X-VLM" -> "ioanacroi/qb-norm" ["e"=1]
"zengyan-97/X-VLM" -> "microsoft/Oscar"
"zengyan-97/X-VLM" -> "zengyan-97/CCLM" ["e"=1]
"zengyan-97/X-VLM" -> "TencentARC/MCQ" ["e"=1]
"zengyan-97/X-VLM" -> "sail-sg/ptp"
"uta-smile/TCL" -> "zengyan-97/X-VLM"
"uta-smile/TCL" -> "salesforce/ALBEF"
"uta-smile/TCL" -> "researchmm/soho"
"uta-smile/TCL" -> "ioanacroi/qb-norm" ["e"=1]
"uta-smile/TCL" -> "zdou0830/METER"
"uta-smile/TCL" -> "adversarial-for-goodness/Co-Attack" ["e"=1]
"guoyang9/UnifER" -> "alirezasalemi7/DEDR-MM-FiD"
"guilk/KAT" -> "PhoebusSi/Thinking-while-Observing"
"guilk/KAT" -> "guoyang9/UnifER"
"guilk/KAT" -> "jialinwu17/MAVEX"
"yz93/LAVT-RIS" -> "zichengsaber/LAVT-pytorch"
"yz93/LAVT-RIS" -> "DerrickWang005/CRIS.pytorch"
"yz93/LAVT-RIS" -> "wjn922/ReferFormer"
"yz93/LAVT-RIS" -> "toggle1995/RIS-DMMI"
"yz93/LAVT-RIS" -> "lsa1997/CARIS"
"yz93/LAVT-RIS" -> "henghuiding/Vision-Language-Transformer" ["e"=1]
"yz93/LAVT-RIS" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"yz93/LAVT-RIS" -> "kkakkkka/ETRIS"
"yz93/LAVT-RIS" -> "fawnliu/TRIS"
"yz93/LAVT-RIS" -> "amazon-science/polygon-transformer"
"yz93/LAVT-RIS" -> "Seonghoon-Yu/Zero-shot-RIS"
"yz93/LAVT-RIS" -> "jianzongwu/robust-ref-seg"
"yz93/LAVT-RIS" -> "SooLab/CGFormer"
"yz93/LAVT-RIS" -> "ubc-vision/RefTR"
"yz93/LAVT-RIS" -> "lichengunc/refer"
"yangli18/VLTVG" -> "zyang-ur/ReSC"
"yangli18/VLTVG" -> "LukeForeverYoung/QRNet"
"yangli18/VLTVG" -> "djiajunustc/TransVG"
"yangli18/VLTVG" -> "svip-lab/LBYLNet"
"yangli18/VLTVG" -> "youngfly11/ReIR-WeaklyGrounding.pytorch"
"yangli18/VLTVG" -> "yangli18/PDNet"
"yangli18/VLTVG" -> "qinzzz/Multimodal-Alignment-Framework"
"phlippe/CITRIS" -> "Qualcomm-AI-research/weakly-supervised-causal-representation-learning"
"phlippe/CITRIS" -> "ilkhem/iVAE"
"wjn922/ReferFormer" -> "JerryX1110/awesome-rvos"
"wjn922/ReferFormer" -> "mttr2021/MTTR"
"wjn922/ReferFormer" -> "yz93/LAVT-RIS"
"wjn922/ReferFormer" -> "henghuiding/Vision-Language-Transformer" ["e"=1]
"wjn922/ReferFormer" -> "wjf5203/SeqFormer" ["e"=1]
"wjn922/ReferFormer" -> "bo-miao/SgMg"
"wjn922/ReferFormer" -> "DerrickWang005/CRIS.pytorch"
"wjn922/ReferFormer" -> "skynbe/Refer-Youtube-VOS"
"wjn922/ReferFormer" -> "leonnnop/Locater"
"wjn922/ReferFormer" -> "FoundationVision/VNext" ["e"=1]
"wjn922/ReferFormer" -> "FoundationVision/UniRef"
"wjn922/ReferFormer" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"wjn922/ReferFormer" -> "hustvl/TeViT" ["e"=1]
"wjn922/ReferFormer" -> "zichengsaber/LAVT-pytorch"
"wjn922/ReferFormer" -> "Surrey-UP-Lab/RegionSpot" ["e"=1]
"seanzhuh/SeqTR" -> "djiajunustc/TransVG"
"seanzhuh/SeqTR" -> "amazon-science/polygon-transformer"
"seanzhuh/SeqTR" -> "luogen1996/SimREC"
"mjhucla/Google_Refexp_toolbox" -> "lichengunc/refer"
"mjhucla/Google_Refexp_toolbox" -> "lichengunc/MAttNet" ["e"=1]
"mjhucla/Google_Refexp_toolbox" -> "jnhwkim/cbp"
"mjhucla/Google_Refexp_toolbox" -> "apple2373/chainer_caption_generation"
"mjhucla/Google_Refexp_toolbox" -> "ronghanghu/text_objseg"
"mjhucla/Google_Refexp_toolbox" -> "lichengunc/speaker_listener_reinforcer"
"mjhucla/Google_Refexp_toolbox" -> "varun-nagaraja/referring-expressions"
"milkymap/transformer-image-captioning" -> "jsoft88/cptr-vision-transformer"
"NovaMind-Z/PTSN" -> "zchoi/PKOL"
"NovaMind-Z/PTSN" -> "zchoi/S2-Transformer"
"zchoi/S2-Transformer" -> "zchoi/PKOL"
"zchoi/S2-Transformer" -> "NovaMind-Z/PTSN"
"singhgautam/slate" -> "amazon-science/object-centric-learning-framework"
"singhgautam/slate" -> "karazijal/clevrtex-generation"
"singhgautam/slate" -> "singhgautam/steve"
"singhgautam/slate" -> "pairlab/SlotFormer"
"gokulkarthik/hateclipper" -> "rizavelioglu/hateful_memes-hate_detectron"
"aioz-ai/CFR_VQA" -> "AndersonStra/MuKEA"
"ronghanghu/natural-language-object-retrieval" -> "andrewliao11/Natural-Language-Object-Retrieval-tensorflow"
"ronghanghu/natural-language-object-retrieval" -> "ronghanghu/text_objseg"
"AndersonStra/MuKEA" -> "aioz-ai/CFR_VQA"
"AndersonStra/MuKEA" -> "jlian2/mucko"
"AndersonStra/MuKEA" -> "hackerchenzhuo/LaKo"
"AndersonStra/MuKEA" -> "China-UK-ZSL/ZS-F-VQA"
"AndersonStra/MuKEA" -> "jialinwu17/MAVEX"
"AndersonStra/MuKEA" -> "ThalesGroup/ConceptBERT"
"martius-lab/cid-in-rl" -> "wangzizhao/CausalDynamicsLearning"
"GT-RIPL/Xmodal-Ctx" -> "232525/PureT"
"GT-RIPL/Xmodal-Ctx" -> "jacobswan1/ViTCAP"
"GT-RIPL/Xmodal-Ctx" -> "SjokerLily/awesome-image-captioning"
"spyshiv/dummytextjs" -> "aka-jain/Drango"
"karazijal/clevrtex-generation" -> "karazijal/probable-motion"
"karazijal/clevrtex-generation" -> "karazijal/guess-what-moves"
"senadkurtisi/pytorch-image-captioning" -> "Dantekk/Image-Captioning"
"kapilks/Basic-Kernel" -> "kapilks/Download-youtube-video"
"zarzouram/image_captioning_with_transformers" -> "Dantekk/Image-Captioning"
"zarzouram/image_captioning_with_transformers" -> "milkymap/transformer-image-captioning"
"MCLAB-OCR/KnowledgeMiningWithSceneText" -> "AndresPMD/Fine_Grained_Clf"
"MCLAB-OCR/KnowledgeMiningWithSceneText" -> "AndresPMD/StacMR"
"232525/PureT" -> "GT-RIPL/Xmodal-Ctx"
"232525/PureT" -> "davidnvq/grit"
"232525/PureT" -> "luo3300612/image-captioning-DLCT"
"232525/PureT" -> "NovaMind-Z/PTSN"
"232525/PureT" -> "YuanEZhou/CBTrans"
"uakarsh/latr" -> "microsoft/TAP"
"uakarsh/latr" -> "yashkant/sam-textvqa"
"uakarsh/latr" -> "furkanbiten/stvqa_amazon_ocr"
"uakarsh/latr" -> "xiaojino/RUArt"
"uakarsh/latr" -> "ChenyuGAO-CS/SMA"
"yukezhu/visual7w-toolkit" -> "yukezhu/visual7w-qa-models"
"codalab/codalab-worksheets-old" -> "codalab/codalab-worksheets"
"JerryX1110/awesome-rvos" -> "wjn922/ReferFormer"
"JerryX1110/awesome-rvos" -> "lxa9867/R2VOS"
"JerryX1110/awesome-rvos" -> "dzh19990407/LBDT"
"JerryX1110/awesome-rvos" -> "miriambellver/refvos"
"JerryX1110/awesome-rvos" -> "wudongming97/OnlineRefer"
"JerryX1110/awesome-rvos" -> "zichengsaber/LAVT-pytorch"
"JerryX1110/awesome-rvos" -> "JerryX1110/RPCMVOS" ["e"=1]
"JerryX1110/awesome-rvos" -> "leonnnop/Locater"
"ZeningLin/ViBERTgrid-PyTorch" -> "SCUT-DLVCLab/RFUND" ["e"=1]
"ZeningLin/ViBERTgrid-PyTorch" -> "sciencefictionlab/chargrid-pytorch"
"furkanbiten/idl_data" -> "uakarsh/latr"
"furkanbiten/idl_data" -> "furkanbiten/stvqa_amazon_ocr"
"furkanbiten/idl_data" -> "biswassanket/synth_doc_generation" ["e"=1]
"furkanbiten/idl_data" -> "AndresPMD/Fine_Grained_Clf"
"zchoi/PKOL" -> "zchoi/S2-Transformer"
"zchoi/PKOL" -> "NovaMind-Z/PTSN"
"mit-ll-responsible-ai/responsible-ai-toolbox" -> "mit-ll-responsible-ai/equine"
"luogen1996/SimREC" -> "kingthreestones/RefCLIP"
"luogen1996/SimREC" -> "Disguiser15/RefTeacher"
"LukeForeverYoung/QRNet" -> "yangli18/VLTVG"
"zichengsaber/LAVT-pytorch" -> "yz93/LAVT-RIS"
"zichengsaber/LAVT-pytorch" -> "fengguang94/CEFNet"
"zichengsaber/LAVT-pytorch" -> "JerryX1110/awesome-rvos"
"gicheonkang/sglkt-visdial" -> "gicheonkang/gst-visdial"
"gicheonkang/sglkt-visdial" -> "gicheonkang/dan-visdial"
"microsoft/act" -> "microsoft/azfuse"
"Zhiquan-Wen/D-VQA" -> "GeraldHan/GGE"
"visiontao/ncd" -> "visiontao/dcnet"
"yukezhu/visual7w-qa-models" -> "yukezhu/visual7w-toolkit"
"microsoft/GenerativeImage2Text" -> "microsoft/TAP"
"microsoft/GenerativeImage2Text" -> "microsoft/SwinBERT" ["e"=1]
"microsoft/GenerativeImage2Text" -> "davidnvq/grit"
"microsoft/GenerativeImage2Text" -> "microsoft/azfuse"
"microsoft/GenerativeImage2Text" -> "OFA-Sys/OFA" ["e"=1]
"microsoft/GenerativeImage2Text" -> "microsoft/Oscar"
"microsoft/GenerativeImage2Text" -> "JialianW/GRiT" ["e"=1]
"microsoft/GenerativeImage2Text" -> "TXH-mercury/VALOR" ["e"=1]
"microsoft/GenerativeImage2Text" -> "microsoft/UniVL" ["e"=1]
"microsoft/GenerativeImage2Text" -> "lucidrains/CoCa-pytorch" ["e"=1]
"microsoft/GenerativeImage2Text" -> "m-bain/frozen-in-time" ["e"=1]
"microsoft/GenerativeImage2Text" -> "yuweihao/MM-Vet" ["e"=1]
"microsoft/GenerativeImage2Text" -> "zdou0830/METER"
"microsoft/GenerativeImage2Text" -> "clip-vil/CLIP-ViL"
"microsoft/GenerativeImage2Text" -> "kdexd/virtex"
"zcyang/imageqa-san" -> "HyeonwooNoh/DPPnet"
"zcyang/imageqa-san" -> "jiasenlu/HieCoAttenVQA"
"zcyang/imageqa-san" -> "abhshkdz/neural-vqa-attention"
"zcyang/imageqa-san" -> "akirafukui/vqa-mcb"
"zcyang/imageqa-san" -> "chingyaoc/san-torch"
"zcyang/imageqa-san" -> "chingyaoc/VQA-tensorflow"
"zcyang/imageqa-san" -> "yukezhu/visual7w-qa-models"
"microsoft/azfuse" -> "microsoft/act"
"davidnvq/grit" -> "232525/PureT"
"davidnvq/grit" -> "luo3300612/image-captioning-DLCT"
"davidnvq/grit" -> "zhangxuying1004/RSTNet"
"davidnvq/grit" -> "GT-RIPL/Xmodal-Ctx"
"davidnvq/grit" -> "aimagelab/meshed-memory-transformer"
"davidnvq/grit" -> "jacobswan1/ViTCAP"
"davidnvq/grit" -> "NovaMind-Z/PTSN"
"davidnvq/grit" -> "zarzouram/image_captioning_with_transformers"
"davidnvq/grit" -> "husthuaan/AoANet"
"davidnvq/grit" -> "zchoi/S2-Transformer"
"davidnvq/grit" -> "facebookresearch/grid-feats-vqa"
"iamaaditya/VQA_Demo" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"iamaaditya/VQA_Demo" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"iamaaditya/VQA_Demo" -> "avisingh599/visual-qa"
"iamaaditya/VQA_Demo" -> "jiasenlu/HieCoAttenVQA"
"iamaaditya/VQA_Demo" -> "iamaaditya/VQA_Keras"
"iamaaditya/VQA_Demo" -> "chingyaoc/awesome-vqa"
"iamaaditya/VQA_Demo" -> "zhoubolei/VQAbaseline"
"iamaaditya/VQA_Demo" -> "chingyaoc/VQA-tensorflow"
"iamaaditya/VQA_Demo" -> "akirafukui/vqa-mcb"
"iamaaditya/VQA_Demo" -> "abhshkdz/neural-vqa"
"iamaaditya/VQA_Demo" -> "Cadene/vqa.pytorch"
"iamaaditya/VQA_Demo" -> "anujshah1003/VQA-Demo-GUI"
"iamaaditya/VQA_Demo" -> "GT-Vision-Lab/VQA"
"iamaaditya/VQA_Demo" -> "VedantYadav/VQA"
"iamaaditya/VQA_Demo" -> "Cyanogenoid/vqa-counting"
"chingyaoc/VQA-tensorflow" -> "paarthneekhara/neural-vqa-tensorflow"
"chingyaoc/VQA-tensorflow" -> "zhoubolei/VQAbaseline"
"chingyaoc/VQA-tensorflow" -> "shmsw25/mcb-model-for-vqa"
"chingyaoc/VQA-tensorflow" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"chingyaoc/VQA-tensorflow" -> "akirafukui/vqa-mcb"
"chingyaoc/VQA-tensorflow" -> "liuzhi136/Visual-Question-Answering"
"chingyaoc/VQA-tensorflow" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"chingyaoc/VQA-tensorflow" -> "chingyaoc/awesome-vqa"
"chingyaoc/VQA-tensorflow" -> "jiasenlu/HieCoAttenVQA"
"chingyaoc/VQA-tensorflow" -> "jnhwkim/MulLowBiVQA"
"chingyaoc/VQA-tensorflow" -> "GT-Vision-Lab/VQA"
"jiasenlu/HieCoAttenVQA" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"jiasenlu/HieCoAttenVQA" -> "akirafukui/vqa-mcb"
"jiasenlu/HieCoAttenVQA" -> "GT-Vision-Lab/VQA"
"jiasenlu/HieCoAttenVQA" -> "chingyaoc/awesome-vqa"
"jiasenlu/HieCoAttenVQA" -> "zhoubolei/VQAbaseline"
"jiasenlu/HieCoAttenVQA" -> "zcyang/imageqa-san"
"jiasenlu/HieCoAttenVQA" -> "chingyaoc/VQA-tensorflow"
"jiasenlu/HieCoAttenVQA" -> "iamaaditya/VQA_Demo"
"jiasenlu/HieCoAttenVQA" -> "Cadene/vqa.pytorch"
"jiasenlu/HieCoAttenVQA" -> "hengyuan-hu/bottom-up-attention-vqa"
"jiasenlu/HieCoAttenVQA" -> "paarthneekhara/neural-vqa-tensorflow"
"jiasenlu/HieCoAttenVQA" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"jiasenlu/HieCoAttenVQA" -> "MILVLG/mcan-vqa"
"jiasenlu/HieCoAttenVQA" -> "jacobandreas/nmn2"
"jiasenlu/HieCoAttenVQA" -> "abhshkdz/neural-vqa-attention"
"chingyaoc/awesome-vqa" -> "Cadene/vqa.pytorch"
"chingyaoc/awesome-vqa" -> "jiasenlu/HieCoAttenVQA"
"chingyaoc/awesome-vqa" -> "chingyaoc/VQA-tensorflow"
"chingyaoc/awesome-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"chingyaoc/awesome-vqa" -> "akirafukui/vqa-mcb"
"chingyaoc/awesome-vqa" -> "iamaaditya/VQA_Demo"
"chingyaoc/awesome-vqa" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"chingyaoc/awesome-vqa" -> "GT-Vision-Lab/VQA"
"chingyaoc/awesome-vqa" -> "abhshkdz/neural-vqa-attention"
"chingyaoc/awesome-vqa" -> "Cadene/block.bootstrap.pytorch"
"chingyaoc/awesome-vqa" -> "Cyanogenoid/vqa-counting"
"chingyaoc/awesome-vqa" -> "zhoubolei/VQAbaseline"
"chingyaoc/awesome-vqa" -> "jnhwkim/ban-vqa"
"chingyaoc/awesome-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"chingyaoc/awesome-vqa" -> "peteanderson80/bottom-up-attention"
"wookayin/dotfiles" -> "bckim92/zsh-autoswitch-conda"
"wookayin/dotfiles" -> "cesc-park/attend2u"
"wookayin/dotfiles" -> "wookayin/gpustat-web"
"wookayin/dotfiles" -> "wookayin/tensorflow-plot"
"wookayin/dotfiles" -> "ctr4si/MMN"
"wookayin/dotfiles" -> "wookayin/TensorFlowKR-2017-talk-bestpractice" ["e"=1]
"wookayin/dotfiles" -> "wookayin/python-imgcat"
"wookayin/dotfiles" -> "bckim92/language-evaluation"
"wookayin/dotfiles" -> "YunseokJANG/tgif-qa" ["e"=1]
"wookayin/dotfiles" -> "krta2/awesome-nonsan" ["e"=1]
"karazijal/guess-what-moves" -> "karazijal/probable-motion"
"zengyan-97/X2-VLM" -> "zengyan-97/X-VLM"
"mlpc-ucsd/BoundaryFormer" -> "amazon-science/polygon-transformer"
"Cloud-CV/Fabrik" -> "Cloud-CV/Origami"
"Cloud-CV/Fabrik" -> "Cloud-CV/visual-chatbot"
"Cloud-CV/Fabrik" -> "Cloud-CV/EvalAI"
"Cloud-CV/Fabrik" -> "Cloud-CV/GSoC-Ideas"
"Cloud-CV/Fabrik" -> "Cloud-CV/CloudCV"
"Cloud-CV/Fabrik" -> "Cloud-CV/CloudCV-Old"
"Cloud-CV/Fabrik" -> "joeddav/devol" ["e"=1]
"Cloud-CV/Fabrik" -> "keplr-io/quiver" ["e"=1]
"Cloud-CV/Fabrik" -> "reiinakano/xcessiv" ["e"=1]
"Cloud-CV/Fabrik" -> "MrNothing/AI-Blocks" ["e"=1]
"Cloud-CV/Fabrik" -> "nmhkahn/deep_learning_tutorial" ["e"=1]
"Cloud-CV/Fabrik" -> "GunhoChoi/PyTorch-FastCampus" ["e"=1]
"Cloud-CV/Fabrik" -> "Cloud-CV/origami-lib"
"Cloud-CV/Fabrik" -> "batra-mlp-lab/visdial"
"Cloud-CV/Fabrik" -> "MagNet-DL/magnet" ["e"=1]
"MILVLG/prophet" -> "microsoft/PICa"
"MILVLG/prophet" -> "AndersonStra/MuKEA"
"MILVLG/prophet" -> "Yushi-Hu/PromptCap"
"MILVLG/prophet" -> "guilk/KAT"
"MILVLG/prophet" -> "yuanze-lin/REVIVE"
"MILVLG/prophet" -> "aioz-ai/CFR_VQA"
"MILVLG/prophet" -> "hackerchenzhuo/LaKo"
"MILVLG/prophet" -> "MILVLG/rosita"
"MILVLG/prophet" -> "zhangxi1997/VQACL" ["e"=1]
"MILVLG/prophet" -> "YuJungHeo/kbvqa-public"
"microsoft/BridgeTower" -> "zdou0830/METER"
"pairlab/SlotFormer" -> "Wuziyi616/SlotDiffusion"
"pairlab/SlotFormer" -> "martius-lab/videosaur"
"pairlab/SlotFormer" -> "google-research/slot-attention-video"
"pairlab/SlotFormer" -> "singhgautam/steve"
"pairlab/SlotFormer" -> "amazon-science/object-centric-learning-framework"
"pairlab/SlotFormer" -> "singhgautam/slate"
"pairlab/SlotFormer" -> "singhgautam/sysbinder"
"pairlab/SlotFormer" -> "Wuziyi616/nerv"
"pairlab/SlotFormer" -> "gkakogeorgiou/spot"
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" -> "LinWeizheDragon/FLMR"
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" -> "Go2Heart/EchoSight" ["e"=1]
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" -> "yuanze-lin/REVIVE"
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" -> "AndersonStra/MuKEA"
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" -> "PhoebusSi/Thinking-while-Observing"
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" -> "guilk/KAT"
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" -> "Yushi-Hu/PromptCap"
"lichengunc/refer" -> "lichengunc/MAttNet" ["e"=1]
"lichengunc/refer" -> "TheShadow29/awesome-grounding"
"lichengunc/refer" -> "mjhucla/Google_Refexp_toolbox"
"lichengunc/refer" -> "MarkMoHR/Awesome-Referring-Image-Segmentation"
"lichengunc/refer" -> "BryanPlummer/flickr30k_entities"
"lichengunc/refer" -> "djiajunustc/TransVG"
"lichengunc/refer" -> "yz93/LAVT-RIS"
"lichengunc/refer" -> "ashkamath/mdetr"
"lichengunc/refer" -> "zyang-ur/ReSC"
"lichengunc/refer" -> "luogen1996/MCN"
"lichengunc/refer" -> "zyang-ur/onestage_grounding"
"lichengunc/refer" -> "DerrickWang005/CRIS.pytorch"
"lichengunc/refer" -> "henghuiding/Vision-Language-Transformer" ["e"=1]
"lichengunc/refer" -> "allenai/reclip"
"lichengunc/refer" -> "lichengunc/refer-parser2"
"Cloud-CV/Origami" -> "Cloud-CV/CloudCV"
"Cloud-CV/Origami" -> "Cloud-CV/visual-chatbot"
"Cloud-CV/Origami" -> "Cloud-CV/origami-lib"
"Cloud-CV/Origami" -> "Cloud-CV/evalai-cli"
"Cloud-CV/Origami" -> "Cloud-CV/GSoC-Ideas"
"Cloud-CV/Origami" -> "Cloud-CV/CloudCV-Old"
"Cloud-CV/Origami" -> "Cloud-CV/Fabrik"
"karazijal/probable-motion" -> "karazijal/guess-what-moves"
"allanj/LayoutLMv3-DocVQA" -> "redthing1/layoutlm_experiments"
"ronghanghu/text_objseg" -> "Seth-Park/text_objseg_caffe"
"ronghanghu/text_objseg" -> "andrewliao11/Natural-Language-Object-Retrieval-tensorflow"
"ronghanghu/text_objseg" -> "chenxi116/TF-phrasecut-public"
"ronghanghu/text_objseg" -> "QUVA-Lab/lang-tracker"
"Cloud-CV/diverse-beam-search" -> "Cloud-CV/origami-lib"
"lxa9867/R2VOS" -> "lxa9867/QSD"
"wangzizhao/CausalDynamicsLearning" -> "martius-lab/cid-in-rl"
"wangzizhao/CausalDynamicsLearning" -> "dido1998/CausalMBRL"
"wangzizhao/CausalDynamicsLearning" -> "sumedh7/CausalCuriosity"
"HenryJunW/TAG" -> "ChenyuGAO-CS/SMA"
"singhgautam/steve" -> "JindongJiang/latent-slot-diffusion"
"singhgautam/steve" -> "zhixuan-lin/G-SWM"
"jianzongwu/robust-ref-seg" -> "lsa1997/CARIS"
"DeepRNN/image_captioning" -> "yunjey/show-attend-and-tell"
"DeepRNN/image_captioning" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"DeepRNN/image_captioning" -> "ruotianluo/ImageCaptioning.pytorch"
"DeepRNN/image_captioning" -> "kelvinxu/arctic-captions"
"DeepRNN/image_captioning" -> "yashk2810/Image-Captioning"
"DeepRNN/image_captioning" -> "anuragmishracse/caption_generator"
"DeepRNN/image_captioning" -> "mosessoh/CNN-LSTM-Caption-Generator"
"DeepRNN/image_captioning" -> "jcjohnson/densecap"
"DeepRNN/image_captioning" -> "tylin/coco-caption"
"DeepRNN/image_captioning" -> "jiasenlu/AdaptiveAttention"
"DeepRNN/image_captioning" -> "zhjohnchan/awesome-image-captioning"
"DeepRNN/image_captioning" -> "ruotianluo/self-critical.pytorch"
"DeepRNN/image_captioning" -> "jiasenlu/NeuralBabyTalk"
"DeepRNN/image_captioning" -> "mansimov/text2image" ["e"=1]
"DeepRNN/image_captioning" -> "zjuchenlong/sca-cnn.cvpr17"
"yunjey/show-attend-and-tell" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"yunjey/show-attend-and-tell" -> "DeepRNN/image_captioning"
"yunjey/show-attend-and-tell" -> "kelvinxu/arctic-captions"
"yunjey/show-attend-and-tell" -> "tylin/coco-caption"
"yunjey/show-attend-and-tell" -> "jiasenlu/AdaptiveAttention"
"yunjey/show-attend-and-tell" -> "ruotianluo/self-critical.pytorch"
"yunjey/show-attend-and-tell" -> "ruotianluo/ImageCaptioning.pytorch"
"yunjey/show-attend-and-tell" -> "peteanderson80/bottom-up-attention"
"yunjey/show-attend-and-tell" -> "jiasenlu/NeuralBabyTalk"
"yunjey/show-attend-and-tell" -> "tsenghungchen/show-adapt-and-tell"
"yunjey/show-attend-and-tell" -> "jazzsaxmafia/show_and_tell.tensorflow"
"yunjey/show-attend-and-tell" -> "zhjohnchan/awesome-image-captioning"
"yunjey/show-attend-and-tell" -> "mosessoh/CNN-LSTM-Caption-Generator"
"yunjey/show-attend-and-tell" -> "anuragmishracse/caption_generator"
"yunjey/show-attend-and-tell" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"fawnliu/TRIS" -> "Huntersxsx/RIS-Learning-List"
"fawnliu/TRIS" -> "kdwonn/SaG" ["e"=1]
"fawnliu/TRIS" -> "toggle1995/RIS-DMMI"
"linhuixiao/CLIP-VG" -> "linhuixiao/HiVG"
"linhuixiao/CLIP-VG" -> "djiajunustc/TransVG"
"linhuixiao/CLIP-VG" -> "linhuixiao/Awesome-Visual-Grounding"
"linhuixiao/CLIP-VG" -> "LukeForeverYoung/QRNet"
"linhuixiao/CLIP-VG" -> "chenwei746/EEVG"
"linhuixiao/CLIP-VG" -> "Dmmm1997/SimVG"
"linhuixiao/CLIP-VG" -> "MightXiong/FedMIT" ["e"=1]
"linhuixiao/CLIP-VG" -> "LANMNG/LQVG" ["e"=1]
"mosessoh/CNN-LSTM-Caption-Generator" -> "zsdonghao/Image-Captioning"
"mosessoh/CNN-LSTM-Caption-Generator" -> "DeepRNN/image_captioning"
"mosessoh/CNN-LSTM-Caption-Generator" -> "anuragmishracse/caption_generator"
"mosessoh/CNN-LSTM-Caption-Generator" -> "jazzsaxmafia/show_and_tell.tensorflow"
"mosessoh/CNN-LSTM-Caption-Generator" -> "jazzsaxmafia/show_attend_and_tell.tensorflow"
"mosessoh/CNN-LSTM-Caption-Generator" -> "mjhucla/TF-mRNN"
"mosessoh/CNN-LSTM-Caption-Generator" -> "yunjey/show-attend-and-tell"
"mosessoh/CNN-LSTM-Caption-Generator" -> "deepsemantic/image_captioning"
"mjhucla/TF-mRNN" -> "mjhucla/mRNN-CR"
"akirafukui/vqa-mcb" -> "jiasenlu/HieCoAttenVQA"
"akirafukui/vqa-mcb" -> "yuzcccc/vqa-mfb"
"akirafukui/vqa-mcb" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"akirafukui/vqa-mcb" -> "jnhwkim/MulLowBiVQA"
"akirafukui/vqa-mcb" -> "chingyaoc/VQA-tensorflow"
"akirafukui/vqa-mcb" -> "shmsw25/mcb-model-for-vqa"
"akirafukui/vqa-mcb" -> "zcyang/imageqa-san"
"akirafukui/vqa-mcb" -> "chingyaoc/awesome-vqa"
"akirafukui/vqa-mcb" -> "zhoubolei/VQAbaseline"
"akirafukui/vqa-mcb" -> "jnhwkim/cbp"
"akirafukui/vqa-mcb" -> "iamaaditya/VQA_Demo"
"akirafukui/vqa-mcb" -> "Cadene/vqa.pytorch"
"akirafukui/vqa-mcb" -> "asdf0982/vqa-mfb.pytorch"
"akirafukui/vqa-mcb" -> "Cyanogenoid/vqa-counting"
"akirafukui/vqa-mcb" -> "jnhwkim/ban-vqa"
"TonyLianLong/RCF-UnsupVideoSeg" -> "karazijal/guess-what-moves"
"vvvb-github/AVSegFormer" -> "jinxiang-liu/anno-free-AVS"
"vvvb-github/AVSegFormer" -> "yannqi/COMBO-AVS"
"vvvb-github/AVSegFormer" -> "GeWu-Lab/Ref-AVS"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "iamaaditya/VQA_Demo"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "iamaaditya/VQA_Keras"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "chingyaoc/VQA-tensorflow"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "avisingh599/visual-qa"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "abhshkdz/neural-vqa-attention"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "jiasenlu/HieCoAttenVQA"
"anantzoid/VQA-Keras-Visual-Question-Answering" -> "paarthneekhara/neural-vqa-tensorflow"
"lizhou-cs/JointNLT" -> "wangxiao5791509/TNL2K_evaluation_toolkit"
"lizhou-cs/JointNLT" -> "OpenSpaceAI/UVLTrack" ["e"=1]
"lizhou-cs/JointNLT" -> "fredfung007/snlt"
"chojw/genb" -> "GeraldHan/GGE"
"amazon-science/polygon-transformer" -> "mlpc-ucsd/BoundaryFormer"
"amazon-science/polygon-transformer" -> "yz93/LAVT-RIS"
"amazon-science/polygon-transformer" -> "seanzhuh/SeqTR"
"amazon-science/polygon-transformer" -> "DerrickWang005/CRIS.pytorch"
"amazon-science/polygon-transformer" -> "fawnliu/TRIS"
"amazon-science/polygon-transformer" -> "SooLab/CGFormer"
"amazon-science/polygon-transformer" -> "lsa1997/CARIS"
"amazon-science/polygon-transformer" -> "Seonghoon-Yu/Zero-shot-RIS"
"amazon-science/polygon-transformer" -> "zorzi-s/PolyWorldPretrainedNetwork" ["e"=1]
"paarthneekhara/neural-vqa-tensorflow" -> "chingyaoc/VQA-tensorflow"
"paarthneekhara/neural-vqa-tensorflow" -> "paarthneekhara/Weather-From-Map"
"paarthneekhara/neural-vqa-tensorflow" -> "jiasenlu/HieCoAttenVQA"
"paarthneekhara/neural-vqa-tensorflow" -> "kapilks/Basic-Kernel"
"paarthneekhara/neural-vqa-tensorflow" -> "paarthneekhara/convolutional-vqa"
"paarthneekhara/neural-vqa-tensorflow" -> "adiitya/p2pstream" ["e"=1]
"paarthneekhara/neural-vqa-tensorflow" -> "kapilks/Download-youtube-video"
"paarthneekhara/neural-vqa-tensorflow" -> "anantzoid/VQA-Keras-Visual-Question-Answering"
"paarthneekhara/neural-vqa-tensorflow" -> "GT-Vision-Lab/VQA_LSTM_CNN"
"paarthneekhara/neural-vqa-tensorflow" -> "DeepRNN/visual_question_answering" ["e"=1]
"paarthneekhara/neural-vqa-tensorflow" -> "abhshkdz/neural-vqa"
"paarthneekhara/neural-vqa-tensorflow" -> "chingyaoc/awesome-vqa"
"paarthneekhara/neural-vqa-tensorflow" -> "paarthneekhara/byteNet-tensorflow" ["e"=1]
"kimiyoung/review_net" -> "s-gupta/visual-concepts"
"kimiyoung/review_net" -> "chapternewscu/image-captioning-with-semantic-attention"
"kimiyoung/review_net" -> "zhegan27/Semantic_Compositional_Nets"
"toggle1995/RIS-DMMI" -> "fgirbal/segment-select-correct"
"toggle1995/RIS-DMMI" -> "kdwonn/SaG" ["e"=1]
"toggle1995/RIS-DMMI" -> "lsa1997/CARIS"
"bo-miao/SgMg" -> "heshuting555/DsHmp" ["e"=1]
"bo-miao/SgMg" -> "lxa9867/R2VOS"
"bo-miao/SgMg" -> "bo-miao/RefHuman"
"bo-miao/SgMg" -> "RobertLuo1/NeurIPS2023_SOC"
"bo-miao/SgMg" -> "wudongming97/OnlineRefer"
"bo-miao/SgMg" -> "cilinyan/ReVOS-api" ["e"=1]
"LisaAnne/DCC" -> "Yu-Wu/Decoupled-Novel-Object-Captioner"
"LisaAnne/DCC" -> "vsubhashini/noc"
"LisaAnne/DCC" -> "wenhuchen/Semi-Supervised-Image-Captioning"
"gicheonkang/gst-visdial" -> "gicheonkang/sglkt-visdial"
"kkakkkka/ETRIS" -> "kkakkkka/MambaTalk"
"kkakkkka/ETRIS" -> "jiaqihuang01/DETRIS"
"kkakkkka/ETRIS" -> "liuting20/MaPPER"
"kkakkkka/ETRIS" -> "toggle1995/RIS-DMMI"
"kkakkkka/ETRIS" -> "kkakkkka/HunyuanPortrait" ["e"=1]
"kkakkkka/ETRIS" -> "jianzongwu/robust-ref-seg"
"kkakkkka/ETRIS" -> "SooLab/CGFormer"
"amazon-science/object-centric-learning-framework" -> "martius-lab/videosaur"
"amazon-science/object-centric-learning-framework" -> "gorkaydemir/DINOSAUR"
"amazon-science/object-centric-learning-framework" -> "singhgautam/steve"
"amazon-science/object-centric-learning-framework" -> "evelinehong/slot-attention-pytorch"
"amazon-science/object-centric-learning-framework" -> "gkakogeorgiou/spot"
"amazon-science/object-centric-learning-framework" -> "singhgautam/slate"
"amazon-science/object-centric-learning-framework" -> "amazon-science/AdaSlot"
"amazon-science/object-centric-learning-framework" -> "JindongJiang/latent-slot-diffusion"
"amazon-science/object-centric-learning-framework" -> "google-research/slot-attention-video"
"amazon-science/object-centric-learning-framework" -> "Wuziyi616/SlotDiffusion"
"amazon-science/object-centric-learning-framework" -> "pairlab/SlotFormer"
"amazon-science/object-centric-learning-framework" -> "zadaianchuk/comus"
"jinxiang-liu/anno-free-AVS" -> "GeWu-Lab/Generalizable-Audio-Visual-Segmentation"
"RobertLuo1/NeurIPS2023_SOC" -> "RobertLuo1/CoHD"
"SooLab/Free-Bloom" -> "cvlab-kaist/DirecT2V" ["e"=1]
"SooLab/Free-Bloom" -> "SooLab/REP-ERU"
"SooLab/Free-Bloom" -> "SooLab/CGFormer"
"White-Link/gpm" -> "emited/gantk2"
"LuoweiZhou/e2e-gLSTM-sc" -> "fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning"
"LuoweiZhou/e2e-gLSTM-sc" -> "JonghwanMun/TextguidedATT"
"Seonghoon-Yu/Zero-shot-RIS" -> "SooLab/CGFormer"
"Seonghoon-Yu/Zero-shot-RIS" -> "lsa1997/CARIS"
"Seonghoon-Yu/Zero-shot-RIS" -> "kkakkkka/ETRIS"
"Seonghoon-Yu/Zero-shot-RIS" -> "Yeon07/RISCLIP" ["e"=1]
"Seonghoon-Yu/Zero-shot-RIS" -> "allenai/reclip"
"Seonghoon-Yu/Zero-shot-RIS" -> "kdwonn/SaG" ["e"=1]
"Seonghoon-Yu/Zero-shot-RIS" -> "yz93/LAVT-RIS"
"Seonghoon-Yu/Zero-shot-RIS" -> "toggle1995/RIS-DMMI"
"Seonghoon-Yu/Zero-shot-RIS" -> "JerryX1110/awesome-rvos"
"Seonghoon-Yu/Zero-shot-RIS" -> "ChenyunWu/PhraseCutDataset"
"kingthreestones/RefCLIP" -> "luogen1996/SimREC"
"SooLab/CGFormer" -> "lsa1997/CARIS"
"edchengg/oven_eval" -> "edchengg/infoseek_eval"
"edchengg/oven_eval" -> "open-vision-language/oven"
"edchengg/oven_eval" -> "MrZilinXiao/AutoVER"
"aka-jain/Drango" -> "spyshiv/dummytextjs"
"Huntersxsx/RIS-Learning-List" -> "fgirbal/segment-select-correct"
"jnhwkim/cbp" -> "jnhwkim/MulLowBiVQA"
"jnhwkim/cbp" -> "gy20073/compact_bilinear_pooling" ["e"=1]
"varun-nagaraja/referring-expressions" -> "yuleiniu/vc"
"aspirinone/CATR.github.io" -> "GeWu-Lab/Generalizable-Audio-Visual-Segmentation"
"Wuziyi616/SlotDiffusion" -> "JindongJiang/latent-slot-diffusion"
"Wuziyi616/SlotDiffusion" -> "martius-lab/videosaur"
"Wuziyi616/SlotDiffusion" -> "pairlab/SlotFormer"
"Wuziyi616/SlotDiffusion" -> "gorkaydemir/DINOSAUR"
"Wuziyi616/SlotDiffusion" -> "amazon-science/object-centric-learning-framework"
"open-vision-language/oven" -> "edchengg/oven_eval"
"open-vision-language/oven" -> "MrZilinXiao/AutoVER"
"open-vision-language/infoseek" -> "edchengg/infoseek_eval"
"open-vision-language/infoseek" -> "open-vision-language/oven"
"lsa1997/CARIS" -> "jianzongwu/robust-ref-seg"
"lsa1997/CARIS" -> "Huntersxsx/RIS-Learning-List"
"martius-lab/videosaur" -> "gorkaydemir/SOLV"
"martius-lab/videosaur" -> "shvdiwnkozbw/SMTC"
"martius-lab/videosaur" -> "gorkaydemir/DINOSAUR"
"martius-lab/videosaur" -> "gkakogeorgiou/spot"
"singhgautam/sysbinder" -> "ThomasMrY/VCT"
"ChengShiest/Zip-Your-CLIP" -> "SooLab/REP-ERU"
"edchengg/infoseek_eval" -> "edchengg/oven_eval"
"edchengg/infoseek_eval" -> "open-vision-language/infoseek"
"alirezasalemi7/DEDR-MM-FiD" -> "guoyang9/UnifER"
"alirezasalemi7/DEDR-MM-FiD" -> "PhoebusSi/Thinking-while-Observing"
"zsdonghao/Image-Captioning" -> "mosessoh/CNN-LSTM-Caption-Generator"
"zsdonghao/Image-Captioning" -> "tsenghungchen/show-adapt-and-tell"
"linhuixiao/HiVG" -> "linhuixiao/OneRef"
"linhuixiao/HiVG" -> "linhuixiao/CLIP-VG"
"linhuixiao/HiVG" -> "chenwei746/EEVG"
"linhuixiao/HiVG" -> "Dmmm1997/SimVG"
"jnhwkim/MulLowBiVQA" -> "jnhwkim/cbp"
"Cloud-CV/EvalAI" -> "Cloud-CV/Fabrik"
"Cloud-CV/EvalAI" -> "Cloud-CV/Origami"
"Cloud-CV/EvalAI" -> "Cloud-CV/GSoC-Ideas"
"Cloud-CV/EvalAI" -> "Cloud-CV/EvalAI-Starters"
"Cloud-CV/EvalAI" -> "Cloud-CV/visual-chatbot"
"Cloud-CV/EvalAI" -> "Cloud-CV/CloudCV"
"Cloud-CV/EvalAI" -> "Cloud-CV/evalai-cli"
"Cloud-CV/EvalAI" -> "Cloud-CV/EvalAI-ngx"
"Cloud-CV/EvalAI" -> "facebookresearch/mmf"
"Cloud-CV/EvalAI" -> "coala/coala" ["e"=1]
"Cloud-CV/EvalAI" -> "codalab/codalab-competitions"
"Cloud-CV/EvalAI" -> "reiinakano/xcessiv" ["e"=1]
"Cloud-CV/EvalAI" -> "replicate/keepsake" ["e"=1]
"Cloud-CV/EvalAI" -> "Cloud-CV/CloudCV-Old"
"Cloud-CV/EvalAI" -> "adeshpande3/Machine-Learning-Links-And-Lessons-Learned" ["e"=1]
"zjuchenlong/sca-cnn.cvpr17" -> "jiasenlu/AdaptiveAttention"
"zjuchenlong/sca-cnn.cvpr17" -> "stevehuanghe/image_captioning"
"zjuchenlong/sca-cnn.cvpr17" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"zjuchenlong/sca-cnn.cvpr17" -> "husthuaan/AoANet"
"zjuchenlong/sca-cnn.cvpr17" -> "HaleyPei/Implement-of-SCA-CNN"
"zjuchenlong/sca-cnn.cvpr17" -> "yufengm/Adaptive"
"zjuchenlong/sca-cnn.cvpr17" -> "gujiuxiang/Stack-Captioning"
"zjuchenlong/sca-cnn.cvpr17" -> "aditya12agd5/convcap"
"zjuchenlong/sca-cnn.cvpr17" -> "cswhjiang/Recurrent_Fusion_Network"
"zjuchenlong/sca-cnn.cvpr17" -> "poojahira/image-captioning-bottom-up-top-down"
"zjuchenlong/sca-cnn.cvpr17" -> "richardaecn/cvpr18-caption-eval"
"zjuchenlong/sca-cnn.cvpr17" -> "cesc-park/attend2u"
"zjuchenlong/sca-cnn.cvpr17" -> "peteanderson80/SPICE" ["e"=1]
"zjuchenlong/sca-cnn.cvpr17" -> "daqingliu/CAVP"
"chapternewscu/image-captioning-with-semantic-attention" -> "s-gupta/visual-concepts"
"chapternewscu/image-captioning-with-semantic-attention" -> "LuoweiZhou/e2e-gLSTM-sc"
"FoundationVision/UniRef" -> "wjn922/ReferFormer"
"FoundationVision/UniRef" -> "CVMI-Lab/CoDet" ["e"=1]
"FoundationVision/UniRef" -> "Surrey-UP-Lab/RegionSpot" ["e"=1]
"FoundationVision/UniRef" -> "zhang-tao-whu/DVIS_Plus" ["e"=1]
"liuting20/DARA" -> "liuting20/MaPPER"
"liuting20/DARA" -> "liuting20/Sparse-Tuning"
"liuting20/DARA" -> "liuting20/SwimVG"
"batra-mlp-lab/visdial-amt-chat" -> "btgraham/Batchwise-Dropout" ["e"=1]
"batra-mlp-lab/visdial-amt-chat" -> "batra-mlp-lab/visdial"
"batra-mlp-lab/visdial-amt-chat" -> "GunhoChoi/Deep-Learning-For-Beginners"
"batra-mlp-lab/visdial-amt-chat" -> "6004x/jade" ["e"=1]
"familyld/Awesome-Causal-RL" -> "libo-huang/Awesome-Causal-Reinforcement-Learning"
"familyld/Awesome-Causal-RL" -> "wangzizhao/CausalDynamicsLearning"
"abhshkdz/neural-vqa-attention" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"abhshkdz/neural-vqa-attention" -> "zcyang/imageqa-san"
"apple2373/chainer-caption" -> "apple2373/chainer_caption_generation"
"JindongJiang/latent-slot-diffusion" -> "Wuziyi616/SlotDiffusion"
"JindongJiang/latent-slot-diffusion" -> "singhgautam/steve"
"JindongJiang/latent-slot-diffusion" -> "amazon-science/object-centric-learning-framework"
"ThomasMrY/DisDiff" -> "wuancong/FDAE"
"wuancong/FDAE" -> "ThomasMrY/VCT"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/origami-lib"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/CloudCV"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/Origami"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/EvalAI-ngx"
"Cloud-CV/GSoC-Ideas" -> "Cloud-CV/visual-chatbot"
"ThomasRobertFr/gpu-monitor" -> "emited/gantk2"
"LinWeizheDragon/FLMR" -> "LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering"
"LinWeizheDragon/FLMR" -> "edchengg/infoseek_eval"
"LinWeizheDragon/FLMR" -> "open-vision-language/infoseek"
"neural-nuts/image-caption-generator" -> "neural-nuts/Cam2Caption"
"neural-nuts/image-caption-generator" -> "anuragmishracse/caption_generator"
"LemonATsu/Keras-Image-Caption" -> "amaiasalvador/imcap_keras"
"gkakogeorgiou/spot" -> "martius-lab/videosaur"
"gkakogeorgiou/spot" -> "amazon-science/AdaSlot"
"gkakogeorgiou/spot" -> "gorkaydemir/SOLV"
"gkakogeorgiou/spot" -> "amazon-science/object-centric-learning-framework"
"yannqi/COMBO-AVS" -> "GeWu-Lab/Generalizable-Audio-Visual-Segmentation"
"yannqi/COMBO-AVS" -> "vvvb-github/AVSegFormer"
"GunhoChoi/Deep-Learning-For-Beginners" -> "btgraham/Batchwise-Dropout" ["e"=1]
"GunhoChoi/Deep-Learning-For-Beginners" -> "batra-mlp-lab/visdial-amt-chat"
"GunhoChoi/Deep-Learning-For-Beginners" -> "6004x/jade" ["e"=1]
"gorkaydemir/SOLV" -> "martius-lab/videosaur"
"gorkaydemir/SOLV" -> "gorkaydemir/DINOSAUR"
"gorkaydemir/DINOSAUR" -> "martius-lab/videosaur"
"gorkaydemir/DINOSAUR" -> "gorkaydemir/SOLV"
"JonghwanMun/TextguidedATT" -> "fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning"
"wenhuchen/Semi-Supervised-Image-Captioning" -> "fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning"
"lxa9867/QSD" -> "lxa9867/r2bench"
"lichengunc/speaker_listener_reinforcer" -> "mikittt/easy-to-understand-REG"
"anuragmishracse/caption_generator" -> "yashk2810/Image-Captioning"
"anuragmishracse/caption_generator" -> "neural-nuts/image-caption-generator"
"anuragmishracse/caption_generator" -> "LemonATsu/Keras-Image-Caption"
"anuragmishracse/caption_generator" -> "DeepRNN/image_captioning"
"anuragmishracse/caption_generator" -> "oarriaga/neural_image_captioning"
"anuragmishracse/caption_generator" -> "mosessoh/CNN-LSTM-Caption-Generator"
"anuragmishracse/caption_generator" -> "zsdonghao/Image-Captioning"
"anuragmishracse/caption_generator" -> "Shobhit20/Image-Captioning"
"anuragmishracse/caption_generator" -> "amaiasalvador/imcap_keras"
"anuragmishracse/caption_generator" -> "Div99/Image-Captioning"
"anuragmishracse/caption_generator" -> "danieljl/keras-image-captioning"
"anuragmishracse/caption_generator" -> "dabasajay/Image-Caption-Generator"
"anuragmishracse/caption_generator" -> "boluoyu/ImageCaption"
"anuragmishracse/caption_generator" -> "apple2373/chainer-caption"
"anuragmishracse/caption_generator" -> "fregu856/CS224n_project"
"ruotianluo/ImageCaptioning.pytorch" -> "ruotianluo/self-critical.pytorch"
"ruotianluo/ImageCaptioning.pytorch" -> "zhjohnchan/awesome-image-captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "peteanderson80/bottom-up-attention"
"ruotianluo/ImageCaptioning.pytorch" -> "tylin/coco-caption"
"ruotianluo/ImageCaptioning.pytorch" -> "sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "aimagelab/meshed-memory-transformer"
"ruotianluo/ImageCaptioning.pytorch" -> "husthuaan/AoANet"
"ruotianluo/ImageCaptioning.pytorch" -> "jiasenlu/NeuralBabyTalk"
"ruotianluo/ImageCaptioning.pytorch" -> "DeepRNN/image_captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "jiasenlu/AdaptiveAttention"
"ruotianluo/ImageCaptioning.pytorch" -> "JDAI-CV/image-captioning"
"ruotianluo/ImageCaptioning.pytorch" -> "jcjohnson/densecap"
"ruotianluo/ImageCaptioning.pytorch" -> "yangxuntu/SGAE"
"ruotianluo/ImageCaptioning.pytorch" -> "kelvinxu/arctic-captions"
"ruotianluo/ImageCaptioning.pytorch" -> "fengyang0317/unsupervised_captioning"
"amaiasalvador/imcap_keras" -> "LemonATsu/Keras-Image-Caption"
"jiasenlu/AdaptiveAttention" -> "yufengm/Adaptive"
"jiasenlu/AdaptiveAttention" -> "fawazsammani/knowing-when-to-look-adaptive-attention"
"jiasenlu/AdaptiveAttention" -> "ruotianluo/self-critical.pytorch"
"jiasenlu/AdaptiveAttention" -> "ruotianluo/DiscCaptioning"
"jiasenlu/AdaptiveAttention" -> "jiasenlu/NeuralBabyTalk"
"jiasenlu/AdaptiveAttention" -> "zjuchenlong/sca-cnn.cvpr17"
"jiasenlu/AdaptiveAttention" -> "aimagelab/show-control-and-tell"
"jiasenlu/AdaptiveAttention" -> "peteanderson80/Up-Down-Captioner"
"jiasenlu/AdaptiveAttention" -> "ruotianluo/ImageCaptioning.pytorch"
"jiasenlu/AdaptiveAttention" -> "yunjey/show-attend-and-tell"
"jiasenlu/AdaptiveAttention" -> "gujiuxiang/Stack-Captioning"
"jiasenlu/AdaptiveAttention" -> "peteanderson80/bottom-up-attention"
"jiasenlu/AdaptiveAttention" -> "fengyang0317/unsupervised_captioning"
"jiasenlu/AdaptiveAttention" -> "husthuaan/AoANet"
"jiasenlu/AdaptiveAttention" -> "poojahira/image-captioning-bottom-up-top-down"
"wookayin/tensorflow-plot" -> "shaohua0116/Activation-Visualization-Histogram" ["e"=1]
"wookayin/tensorflow-plot" -> "cesc-park/attend2u"
"wookayin/tensorflow-plot" -> "wookayin/TensorFlowKR-2017-talk-bestpractice" ["e"=1]
"wookayin/tensorflow-plot" -> "wookayin/dotfiles"
"GeWu-Lab/Stepping-Stones" -> "GeWu-Lab/Ref-AVS"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/Origami"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/CloudCV"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/origami-lib"
"Cloud-CV/visual-chatbot" -> "batra-mlp-lab/visdial"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/VQA"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/EvalAI-ngx"
"Cloud-CV/visual-chatbot" -> "jiasenlu/visDial.pytorch"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/GSoC-Ideas"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/CloudCV-Old"
"Cloud-CV/visual-chatbot" -> "Cloud-CV/Fabrik"
"amazon-science/AdaSlot" -> "martius-lab/videosaur"
"amazon-science/AdaSlot" -> "gorkaydemir/SOLV"
"amazon-science/AdaSlot" -> "gkakogeorgiou/spot"
"yannqi/Draw-an-Audio-Code" -> "yannqi/COMBO-AVS"
"cesc-park/attend2u" -> "tsenghungchen/show-adapt-and-tell"
"cesc-park/attend2u" -> "jiasenlu/AdaptiveAttention"
"cesc-park/attend2u" -> "chapternewscu/image-captioning-with-semantic-attention"
"cesc-park/attend2u" -> "s-gupta/visual-concepts"
"cesc-park/attend2u" -> "zhegan27/Semantic_Compositional_Nets"
"cesc-park/attend2u" -> "computationalmedia/semstyle"
"cesc-park/attend2u" -> "kacky24/stylenet"
"oarriaga/neural_image_captioning" -> "anuragmishracse/caption_generator"
"oarriaga/neural_image_captioning" -> "danieljl/keras-image-captioning"
"oarriaga/neural_image_captioning" -> "LemonATsu/Keras-Image-Caption"
"appletea233/AL-Ref-SAM2" -> "GeWu-Lab/Ref-AVS"
"appletea233/AL-Ref-SAM2" -> "spyflying/CMPC-Refseg"
"linhuixiao/Awesome-Visual-Grounding" -> "linhuixiao/CLIP-VG"
"linhuixiao/Awesome-Visual-Grounding" -> "linhuixiao/HiVG"
"linhuixiao/Awesome-Visual-Grounding" -> "linhuixiao/OneRef"
"linhuixiao/Awesome-Visual-Grounding" -> "Dmmm1997/SimVG"
"linhuixiao/Awesome-Visual-Grounding" -> "LANMNG/LQVG" ["e"=1]
"liuting20/Sparse-Tuning" -> "liuting20/DARA"
"liuting20/Sparse-Tuning" -> "liuting20/MaPPER"
"GeWu-Lab/Ref-AVS" -> "GeWu-Lab/Stepping-Stones"
"GeWu-Lab/Ref-AVS" -> "GeWu-Lab/Generalizable-Audio-Visual-Segmentation"
"batra-mlp-lab/visdial" -> "batra-mlp-lab/visdial-amt-chat"
"batra-mlp-lab/visdial" -> "jiasenlu/visDial.pytorch"
"batra-mlp-lab/visdial" -> "batra-mlp-lab/visdial-rl"
"batra-mlp-lab/visdial" -> "batra-mlp-lab/visdial-challenge-starter-pytorch"
"batra-mlp-lab/visdial" -> "gicheonkang/dan-visdial"
"batra-mlp-lab/visdial" -> "facebookresearch/corefnmn"
"batra-mlp-lab/visdial" -> "Cloud-CV/visual-chatbot"
"linhuixiao/OneRef" -> "Mr-Bigworth/MMCA"
"nini0919/SemiRES" -> "nini0919/DiffPNG"
"nini0919/SemiRES" -> "Aria-Zhangjl/E3-FaceNet"
"chenxinpeng/im2p" -> "InnerPeace-Wu/im2p-tensorflow"
"chenxinpeng/im2p" -> "jack1yang/image-paragraph-captioning"
"chenxinpeng/im2p" -> "Wentong-DST/im2p"
"zhegan27/Semantic_Compositional_Nets" -> "zhegan27/SCN_for_video_captioning" ["e"=1]
"zhegan27/Semantic_Compositional_Nets" -> "yiyang92/vae_captioning"
"zhegan27/Semantic_Compositional_Nets" -> "ruotianluo/Transformer_Captioning"
"zhegan27/Semantic_Compositional_Nets" -> "andyweizhao/Multitask_Image_Captioning"
"Dmmm1997/SimVG" -> "Dmmm1997/DRL" ["e"=1]
"Dmmm1997/SimVG" -> "linhuixiao/HiVG"
"Dmmm1997/SimVG" -> "chenwei746/EEVG"
"Dmmm1997/SimVG" -> "linhuixiao/OneRef"
"Dmmm1997/SimVG" -> "om-ai-lab/GroundVLP"
"kkakkkka/MambaTalk" -> "kkakkkka/ETRIS"
"kkakkkka/MambaTalk" -> "andypinxinliu/GestureLSM" ["e"=1]
"kkakkkka/MambaTalk" -> "jiaqihuang01/DETRIS"
"kkakkkka/MambaTalk" -> "liuting20/MaPPER"
"lluisgomez/TextTopicNet" -> "AndresPMD/Fine_Grained_Clf"
"liuting20/MaPPER" -> "liuting20/SwimVG"
"liuting20/MaPPER" -> "liuting20/DARA"
"nini0919/DiffPNG" -> "nini0919/SemiRES"
"nini0919/DiffPNG" -> "Disguiser15/RefTeacher"
"Mr-Bigworth/MMCA" -> "linhuixiao/OneRef"
"liuting20/SwimVG" -> "liuting20/MaPPER"
"liuting20/SwimVG" -> "liuting20/DARA"
"yashk2810/Image-Captioning" -> "anuragmishracse/caption_generator"
"yashk2810/Image-Captioning" -> "DeepRNN/image_captioning"
"yashk2810/Image-Captioning" -> "danieljl/keras-image-captioning"
"yashk2810/Image-Captioning" -> "hlamba28/Automatic-Image-Captioning"
"yashk2810/Image-Captioning" -> "Div99/Image-Captioning"
"yashk2810/Image-Captioning" -> "neural-nuts/image-caption-generator"
"yashk2810/Image-Captioning" -> "Shobhit20/Image-Captioning"
"yashk2810/Image-Captioning" -> "jiasenlu/AdaptiveAttention"
"yashk2810/Image-Captioning" -> "oarriaga/neural_image_captioning"
"yashk2810/Image-Captioning" -> "yunjey/show-attend-and-tell"
"yashk2810/Image-Captioning" -> "yufengm/Adaptive"
"yashk2810/Image-Captioning" -> "fengyang0317/unsupervised_captioning"
"yashk2810/Image-Captioning" -> "LemonATsu/Keras-Image-Caption"
"yashk2810/Image-Captioning" -> "peteanderson80/Up-Down-Captioner"
"yashk2810/Image-Captioning" -> "ruotianluo/Transformer_Captioning"
"ruotianluo/self-critical.pytorch" -> "ruotianluo/ImageCaptioning.pytorch"
"ruotianluo/self-critical.pytorch" -> "peteanderson80/bottom-up-attention"
"ruotianluo/self-critical.pytorch" -> "zhjohnchan/awesome-image-captioning"
"ruotianluo/self-critical.pytorch" -> "husthuaan/AoANet"
"ruotianluo/self-critical.pytorch" -> "aimagelab/meshed-memory-transformer"
"ruotianluo/self-critical.pytorch" -> "yangxuntu/SGAE"
"ruotianluo/self-critical.pytorch" -> "jiasenlu/NeuralBabyTalk"
"ruotianluo/self-critical.pytorch" -> "tylin/coco-caption"
"ruotianluo/self-critical.pytorch" -> "JDAI-CV/image-captioning"
"ruotianluo/self-critical.pytorch" -> "peteanderson80/Up-Down-Captioner"
"ruotianluo/self-critical.pytorch" -> "jiasenlu/AdaptiveAttention"
"ruotianluo/self-critical.pytorch" -> "aimagelab/show-control-and-tell"
"ruotianluo/self-critical.pytorch" -> "yahoo/object_relation_transformer"
"ruotianluo/self-critical.pytorch" -> "forence/Awesome-Visual-Captioning"
"ruotianluo/self-critical.pytorch" -> "microsoft/Oscar"
"Cadene/vqa.pytorch" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cadene/vqa.pytorch" -> "Cadene/block.bootstrap.pytorch"
"Cadene/vqa.pytorch" -> "Cyanogenoid/pytorch-vqa"
"Cadene/vqa.pytorch" -> "chingyaoc/awesome-vqa"
"Cadene/vqa.pytorch" -> "jnhwkim/ban-vqa"
"Cadene/vqa.pytorch" -> "GT-Vision-Lab/VQA"
"Cadene/vqa.pytorch" -> "peteanderson80/bottom-up-attention"
"Cadene/vqa.pytorch" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cadene/vqa.pytorch" -> "jiasenlu/HieCoAttenVQA"
"Cadene/vqa.pytorch" -> "markdtw/vqa-winner-cvprw-2017"
"Cadene/vqa.pytorch" -> "MILVLG/mcan-vqa"
"Cadene/vqa.pytorch" -> "Cyanogenoid/vqa-counting"
"Cadene/vqa.pytorch" -> "yuzcccc/vqa-mfb"
"Cadene/vqa.pytorch" -> "akirafukui/vqa-mcb"
"Cadene/vqa.pytorch" -> "Cadene/murel.bootstrap.pytorch"
"peteanderson80/bottom-up-attention" -> "hengyuan-hu/bottom-up-attention-vqa"
"peteanderson80/bottom-up-attention" -> "ruotianluo/self-critical.pytorch"
"peteanderson80/bottom-up-attention" -> "peteanderson80/Up-Down-Captioner"
"peteanderson80/bottom-up-attention" -> "airsplay/lxmert"
"peteanderson80/bottom-up-attention" -> "microsoft/Oscar"
"peteanderson80/bottom-up-attention" -> "ruotianluo/ImageCaptioning.pytorch"
"peteanderson80/bottom-up-attention" -> "kuanghuei/SCAN" ["e"=1]
"peteanderson80/bottom-up-attention" -> "MILVLG/bottom-up-attention.pytorch"
"peteanderson80/bottom-up-attention" -> "zhjohnchan/awesome-image-captioning"
"peteanderson80/bottom-up-attention" -> "Cadene/vqa.pytorch"
"peteanderson80/bottom-up-attention" -> "ChenRocks/UNITER"
"peteanderson80/bottom-up-attention" -> "MILVLG/mcan-vqa"
"peteanderson80/bottom-up-attention" -> "husthuaan/AoANet"
"peteanderson80/bottom-up-attention" -> "tylin/coco-caption"
"peteanderson80/bottom-up-attention" -> "aimagelab/meshed-memory-transformer"
"chenxi116/TF-phrasecut-public" -> "liruiyu/referseg_rrn"
"chenxi116/TF-phrasecut-public" -> "fengguang94/BRINet"
"facebookresearch/clevr-dataset-gen" -> "facebookresearch/clevr-iep" ["e"=1]
"facebookresearch/clevr-dataset-gen" -> "kexinyi/ns-vqa"
"facebookresearch/clevr-dataset-gen" -> "google-deepmind/multi_object_datasets"
"facebookresearch/clevr-dataset-gen" -> "stanfordnlp/mac-network"
"facebookresearch/clevr-dataset-gen" -> "vacancy/NSCL-PyTorch-Release" ["e"=1]
"facebookresearch/clevr-dataset-gen" -> "ronghanghu/n2nmn"
"facebookresearch/clevr-dataset-gen" -> "lucidrains/slot-attention"
"facebookresearch/clevr-dataset-gen" -> "chuangg/CLEVRER"
"facebookresearch/clevr-dataset-gen" -> "google-deepmind/dsprites-dataset" ["e"=1]
"facebookresearch/clevr-dataset-gen" -> "WellyZhang/RAVEN"
"facebookresearch/clevr-dataset-gen" -> "danielgordon10/thor-iqa-cvpr-2018" ["e"=1]
"facebookresearch/clevr-dataset-gen" -> "facebookresearch/phyre"
"facebookresearch/clevr-dataset-gen" -> "davidmascharka/tbd-nets"
"facebookresearch/clevr-dataset-gen" -> "danfeiX/scene-graph-TF-release" ["e"=1]
"facebookresearch/clevr-dataset-gen" -> "kimhc6028/relational-networks" ["e"=1]
"yuzcccc/vqa-mfb" -> "asdf0982/vqa-mfb.pytorch"
"yuzcccc/vqa-mfb" -> "MILVLG/openvqa"
"yuzcccc/vqa-mfb" -> "akirafukui/vqa-mcb"
"yuzcccc/vqa-mfb" -> "jnhwkim/ban-vqa"
"yuzcccc/vqa-mfb" -> "MILVLG/mcan-vqa"
"yuzcccc/vqa-mfb" -> "jnhwkim/MulLowBiVQA"
"yuzcccc/vqa-mfb" -> "Cyanogenoid/vqa-counting"
"yuzcccc/vqa-mfb" -> "shtechair/vqa-sva"
"yuzcccc/vqa-mfb" -> "hengyuan-hu/bottom-up-attention-vqa"
"yuzcccc/vqa-mfb" -> "Cadene/vqa.pytorch"
"yuzcccc/vqa-mfb" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"yuzcccc/vqa-mfb" -> "aimbrain/vqa-project"
"yuzcccc/vqa-mfb" -> "SinghJasdeep/Attention-on-Attention-for-VQA"
"yuzcccc/vqa-mfb" -> "gicheonkang/dan-visdial"
"yuzcccc/vqa-mfb" -> "markdtw/vqa-winner-cvprw-2017"
"rsokl/MyGrad" -> "rsokl/noggin"
"rsokl/MyGrad" -> "davidmascharka/MyNN"
"rsokl/MyGrad" -> "davidmascharka/tbd-nets"
"rsokl/MyGrad" -> "PTNobel/AutoDiff"
"rsokl/MyGrad" -> "mit-ll-responsible-ai/hydra-zen"
"Cyanogenoid/pytorch-vqa" -> "Cadene/vqa.pytorch"
"Cyanogenoid/pytorch-vqa" -> "hengyuan-hu/bottom-up-attention-vqa"
"Cyanogenoid/pytorch-vqa" -> "Cyanogenoid/vqa-counting"
"Cyanogenoid/pytorch-vqa" -> "aimbrain/vqa-project"
"Cyanogenoid/pytorch-vqa" -> "tbmoon/basic_vqa"
"Cyanogenoid/pytorch-vqa" -> "jnhwkim/ban-vqa"
"Cyanogenoid/pytorch-vqa" -> "markdtw/vqa-winner-cvprw-2017"
"Cyanogenoid/pytorch-vqa" -> "DenisDsh/VizWiz-VQA-PyTorch"
"Cyanogenoid/pytorch-vqa" -> "Shivanshu-Gupta/Visual-Question-Answering"
"Cyanogenoid/pytorch-vqa" -> "Cadene/block.bootstrap.pytorch"
"Cyanogenoid/pytorch-vqa" -> "KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch"
"Cyanogenoid/pytorch-vqa" -> "jiasenlu/HieCoAttenVQA"
"Cyanogenoid/pytorch-vqa" -> "jokieleung/awesome-visual-question-answering"
"Cyanogenoid/pytorch-vqa" -> "abhshkdz/neural-vqa-attention"
"Cyanogenoid/pytorch-vqa" -> "GT-Vision-Lab/VQA"
"akosiorek/attend_infer_repeat" -> "aakhundov/tf-attend-infer-repeat"
"akosiorek/attend_infer_repeat" -> "Abishekpras/Attend-Infer-Repeat---Pytorch"
"akosiorek/attend_infer_repeat" -> "e2crawfo/auto_yolo"
"aakhundov/tf-attend-infer-repeat" -> "akosiorek/attend_infer_repeat"
"tsenghungchen/show-adapt-and-tell" -> "ruotianluo/DiscCaptioning"
"tsenghungchen/show-adapt-and-tell" -> "chenxinpeng/Optimization_of_image_description_metrics_using_policy_gradient_methods"
"tsenghungchen/show-adapt-and-tell" -> "gujiuxiang/Stack-Captioning"
"tsenghungchen/show-adapt-and-tell" -> "kacky24/stylenet"
"tsenghungchen/show-adapt-and-tell" -> "cesc-park/attend2u"
"tsenghungchen/show-adapt-and-tell" -> "LuoweiZhou/e2e-gLSTM-sc"
"tsenghungchen/show-adapt-and-tell" -> "ruotianluo/Transformer_Captioning"
"tsenghungchen/show-adapt-and-tell" -> "aimagelab/show-control-and-tell"
"tsenghungchen/show-adapt-and-tell" -> "fengyang0317/unsupervised_captioning"
"tsenghungchen/show-adapt-and-tell" -> "yiyang92/vae_captioning"
"ronghanghu/n2nmn" -> "jacobandreas/nmn2"
"ronghanghu/n2nmn" -> "ronghanghu/snmn"
"ronghanghu/n2nmn" -> "HarshTrivedi/nmn-pytorch"
"ronghanghu/n2nmn" -> "ronghanghu/speaker_follower" ["e"=1]
"ronghanghu/n2nmn" -> "stanfordnlp/mac-network"
"ronghanghu/n2nmn" -> "zhoubolei/VQAbaseline"
"ronghanghu/n2nmn" -> "jiasenlu/HieCoAttenVQA"
"ronghanghu/n2nmn" -> "kexinyi/ns-vqa"
"ronghanghu/n2nmn" -> "facebookresearch/clevr-iep" ["e"=1]
"ronghanghu/n2nmn" -> "akirafukui/vqa-mcb"
"ronghanghu/n2nmn" -> "YuJiang01/n2nmn_pytorch"
"ronghanghu/n2nmn" -> "ronghanghu/cmn"
"ronghanghu/n2nmn" -> "aimbrain/vqa-project"
"ronghanghu/n2nmn" -> "davidmascharka/tbd-nets"
"ronghanghu/n2nmn" -> "jnhwkim/ban-vqa"
"paarthneekhara/convolutional-vqa" -> "kapilks/Basic-Kernel"
"GuessWhatGame/guesswhat" -> "ruizhaogit/GuessWhat-TemperedPolicyGradient"
"GuessWhatGame/guesswhat" -> "satwikkottur/clevr-dialog"
"linjieyangsc/densecap" -> "InnerPeace-Wu/densecap-tensorflow"
"jiaqihuang01/DETRIS" -> "liuting20/MaPPER"
"jiaqihuang01/DETRIS" -> "kkakkkka/MambaTalk"
"Cloud-CV/CloudCV" -> "Cloud-CV/Origami"
"Cloud-CV/CloudCV" -> "Cloud-CV/EvalAI-ngx"
"Cloud-CV/CloudCV" -> "Cloud-CV/visual-chatbot"
"Skaldak/MfH" -> "lxa9867/r2bench"
"sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" ["l"="48.608,31.879"]
"ruotianluo/ImageCaptioning.pytorch" ["l"="48.568,31.903"]
"zhjohnchan/awesome-image-captioning" ["l"="48.605,31.919"]
"ruotianluo/self-critical.pytorch" ["l"="48.599,31.937"]
"sgrvinod/Deep-Tutorials-for-PyTorch" ["l"="48.628,31.81"]
"peteanderson80/bottom-up-attention" ["l"="48.645,31.994"]
"tylin/coco-caption" ["l"="48.589,31.901"]
"aimagelab/meshed-memory-transformer" ["l"="48.618,31.937"]
"kelvinxu/arctic-captions" ["l"="48.524,31.903"]
"DeepRNN/image_captioning" ["l"="48.538,31.885"]
"rmokady/CLIP_prefix_caption" ["l"="49.115,30.367"]
"husthuaan/AoANet" ["l"="48.585,31.944"]
"jiasenlu/NeuralBabyTalk" ["l"="48.561,31.926"]
"yunjey/show-attend-and-tell" ["l"="48.541,31.91"]
"microsoft/Oscar" ["l"="48.709,31.976"]
"sgrvinod/a-PyTorch-Tutorial-to-Object-Detection" ["l"="50.824,29.983"]
"dabasajay/Image-Caption-Generator" ["l"="48.514,31.783"]
"anuragmishracse/caption_generator" ["l"="48.526,31.826"]
"MiteshPuthran/Image-Caption-Generator" ["l"="48.48,31.728"]
"yashk2810/Image-Captioning" ["l"="48.517,31.852"]
"neural-nuts/image-caption-generator" ["l"="48.533,31.789"]
"IBM/MAX-Image-Caption-Generator" ["l"="48.513,31.723"]
"Shobhit20/Image-Captioning" ["l"="48.499,31.793"]
"damminhtien/deep-learning-image-caption-generator" ["l"="48.493,31.763"]
"OpenVisualCloud/Smart-City-Sample" ["l"="64.426,-2.563"]
"saahiluppal/catr" ["l"="48.584,31.915"]
"bhushan2311/image_caption_generator" ["l"="48.502,31.743"]
"jnhwkim/ban-vqa" ["l"="48.703,32.111"]
"hengyuan-hu/bottom-up-attention-vqa" ["l"="48.667,32.076"]
"MILVLG/mcan-vqa" ["l"="48.692,32.07"]
"KaihuaTang/VQA2.0-Recent-Approachs-2018.pytorch" ["l"="48.71,32.094"]
"Cadene/vqa.pytorch" ["l"="48.669,32.102"]
"yuzcccc/vqa-mfb" ["l"="48.679,32.117"]
"Cyanogenoid/vqa-counting" ["l"="48.692,32.105"]
"Cadene/block.bootstrap.pytorch" ["l"="48.693,32.092"]
"MILVLG/openvqa" ["l"="48.71,32.078"]
"aimbrain/vqa-project" ["l"="48.704,32.131"]
"Cadene/murel.bootstrap.pytorch" ["l"="48.721,32.119"]
"batra-mlp-lab/visdial-challenge-starter-pytorch" ["l"="48.622,32.248"]
"Cyanogenoid/pytorch-vqa" ["l"="48.692,32.125"]
"linjieli222/VQA_ReGAT" ["l"="48.71,32.057"]
"cvlab-tohoku/Dense-CoAttention-Network" ["l"="48.747,32.101"]
"jokieleung/awesome-visual-question-answering" ["l"="48.73,32.05"]
"ronghanghu/lcgn" ["l"="48.795,32.089"]
"airsplay/lxmert" ["l"="48.7,32.024"]
"facebookresearch/grid-feats-vqa" ["l"="48.672,31.992"]
"yanxinzju/CSS-VQA" ["l"="48.699,32.166"]
"jiasenlu/vilbert_beta" ["l"="48.704,32.008"]
"bckim92/language-evaluation" ["l"="48.183,31.884"]
"ctr4si/MMN" ["l"="48.176,31.866"]
"bckim92/zsh-autoswitch-conda" ["l"="48.164,31.88"]
"xinke-wang/Awesome-Text-VQA" ["l"="48.823,32.101"]
"yuewang-cuhk/awesome-vision-language-pretraining-papers" ["l"="48.733,31.999"]
"forence/Awesome-Visual-Captioning" ["l"="48.645,31.955"]
"TheShadow29/awesome-grounding" ["l"="48.847,31.967"]
"codalab/codalab-competitions" ["l"="48.357,32.371"]
"codalab/codalab-worksheets" ["l"="48.303,32.415"]
"codalab/codabench" ["l"="48.323,32.383"]
"sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution" ["l"="-34.914,21.651"]
"sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling" ["l"="53.634,27.587"]
"sgrvinod/a-PyTorch-Tutorial-to-Transformers" ["l"="48.642,31.751"]
"sgrvinod/a-PyTorch-Tutorial-to-Text-Classification" ["l"="53.321,28.721"]
"bentrevett/pytorch-seq2seq" ["l"="53.128,25.731"]
"ahkarami/Deep-Learning-in-Production" ["l"="48.121,26.175"]
"spro/practical-pytorch" ["l"="53.121,25.666"]
"dsgiitr/d2l-pytorch" ["l"="50.718,28.328"]
"ritchieng/the-incredible-pytorch" ["l"="47.978,26.259"]
"bharathgs/Awesome-pytorch-list" ["l"="50.741,28.402"]
"Atcold/NYU-DLSP20" ["l"="47.601,28.39"]
"facebookresearch/mmf" ["l"="48.638,32.057"]
"pliang279/awesome-multimodal-ml" ["l"="48.592,32.051"]
"facebookresearch/pytext" ["l"="52.976,25.701"]
"zihangdai/xlnet" ["l"="53.051,25.718"]
"facebookresearch/vilbert-multi-task" ["l"="48.687,31.998"]
"facebookresearch/XLM" ["l"="53.036,25.657"]
"facebookresearch/maskrcnn-benchmark" ["l"="50.751,29.917"]
"allenai/allennlp" ["l"="53.002,25.738"]
"facebookresearch/ParlAI" ["l"="53.115,25.785"]
"facebookresearch/multimodal" ["l"="48.947,30.29"]
"ChenRocks/UNITER" ["l"="48.714,31.993"]
"computationalmedia/semstyle" ["l"="48.409,31.931"]
"kacky24/stylenet" ["l"="48.425,31.922"]
"yiyang92/caption-stylenet_tensorflow" ["l"="48.445,31.928"]
"andyweizhao/Multitask_Image_Captioning" ["l"="48.455,31.943"]
"Eurus-Holmes/Awesome-Multimodal-Research" ["l"="48.677,32.034"]
"BradyFU/Awesome-Multimodal-Large-Language-Models" ["l"="47.423,29.9"]
"jason718/awesome-self-supervised-learning" ["l"="52.954,29.587"]
"salesforce/LAVIS" ["l"="49.008,30.194"]
"diff-usion/Awesome-Diffusion-Models" ["l"="45.856,31.577"]
"openai/CLIP" ["l"="50.642,29.547"]
"salesforce/ALBEF" ["l"="48.761,31.992"]
"microsoft/unilm" ["l"="38.785,-0.95"]
"thunlp/PromptPapers" ["l"="50.059,38.052"]
"A2Zadeh/CMU-MultimodalSDK" ["l"="56.618,28.005"]
"mlfoundations/open_clip" ["l"="48.974,30.216"]
"lucidrains/vit-pytorch" ["l"="50.683,29.606"]
"facebookresearch/moco" ["l"="53.011,29.578"]
"Cadene/bootstrap.pytorch" ["l"="48.738,32.143"]
"ThomasRobertFr/gpu-monitor" ["l"="48.779,32.152"]
"emited/gantk2" ["l"="48.792,32.159"]
"cdancette/rubi.bootstrap.pytorch" ["l"="48.697,32.178"]
"cdancette/detect-shortcuts" ["l"="48.726,32.177"]
"google-deepmind/abstract-reasoning-matrices" ["l"="49.022,32.36"]
"WellyZhang/RAVEN" ["l"="48.988,32.351"]
"Fen9/WReN" ["l"="48.998,32.36"]
"WellyZhang/CoPINet" ["l"="49.008,32.371"]
"husheng12345/SRAN" ["l"="49.032,32.342"]
"zkcys001/distracting_feature" ["l"="49.012,32.35"]
"WellyZhang/PrAE" ["l"="49.043,32.354"]
"dhh1995/SCL" ["l"="48.985,32.373"]
"mjedmonds/OpenLock" ["l"="30.814,29.005"]
"facebookresearch/phyre" ["l"="48.951,32.352"]
"HaozhiQi/RPIN" ["l"="48.94,32.375"]
"k-r-allen/tool-games" ["l"="44.373,24.354"]
"facebookresearch/clevr-dataset-gen" ["l"="48.85,32.3"]
"phy-q/benchmark" ["l"="48.965,32.329"]
"lvis-dataset/lvis-api" ["l"="50.893,30.239"]
"facebookresearch/fair_self_supervision_benchmark" ["l"="53.12,29.568"]
"rr-learning/CausalWorld" ["l"="49.08,32.454"]
"wookayin/gpustat-web" ["l"="48.184,31.836"]
"wookayin/dotfiles" ["l"="48.217,31.87"]
"wookayin/gpustat" ["l"="50.961,29.669"]
"fgaim/gpuview" ["l"="48.151,31.805"]
"ethanjperez/film" ["l"="48.781,32.198"]
"facebookresearch/clevr-iep" ["l"="23.499,14.834"]
"caffeinism/FiLM-pytorch" ["l"="48.83,32.22"]
"stanfordnlp/mac-network" ["l"="48.755,32.171"]
"devendrachaplot/DeepRL-Grounding" ["l"="60.153,17.34"]
"davidmascharka/tbd-nets" ["l"="48.774,32.227"]
"fengyang0317/unsupervised_captioning" ["l"="48.55,31.935"]
"JDAI-CV/image-captioning" ["l"="48.601,31.958"]
"aimagelab/show-control-and-tell" ["l"="48.563,31.946"]
"yangxuntu/SGAE" ["l"="48.612,31.953"]
"yahoo/object_relation_transformer" ["l"="48.588,31.956"]
"luo3300612/image-captioning-DLCT" ["l"="48.622,31.947"]
"peteanderson80/Up-Down-Captioner" ["l"="48.57,31.957"]
"GT-Vision-Lab/VQA" ["l"="48.655,32.119"]
"markdtw/vqa-winner-cvprw-2017" ["l"="48.672,32.128"]
"chingyaoc/awesome-vqa" ["l"="48.642,32.108"]
"cshizhe/asg2cap" ["l"="48.609,31.97"]
"ruotianluo/DiscCaptioning" ["l"="48.47,31.945"]
"poojahira/image-captioning-bottom-up-top-down" ["l"="48.575,31.97"]
"fawazsammani/knowing-when-to-look-adaptive-attention" ["l"="48.52,31.944"]
"tsenghungchen/show-adapt-and-tell" ["l"="48.466,31.925"]
"asdf0982/vqa-mfb.pytorch" ["l"="48.686,32.139"]
"google-research-datasets/conceptual-captions" ["l"="48.69,31.939"]
"google-research-datasets/conceptual-12m" ["l"="48.727,31.868"]
"fartashf/vsepp" ["l"="58.307,8.289"]
"igorbrigadir/DownloadConceptualCaptions" ["l"="48.621,32.002"]
"lichengunc/refer" ["l"="48.92,31.938"]
"google-research-datasets/wit" ["l"="48.743,31.945"]
"rowanz/r2c" ["l"="47.563,31.974"]
"ranjaykrishna/visual_genome_python_driver" ["l"="47.555,32.05"]
"foamliu/Image-Captioning-PyTorch" ["l"="48.402,31.826"]
"foamliu/Image-Captioning" ["l"="48.402,31.84"]
"HughChi/Image-Caption" ["l"="48.426,31.829"]
"showkeyjar/chinese_im2text.pytorch" ["l"="48.424,31.844"]
"ruotianluo/Image_Captioning_AI_Challenger" ["l"="48.48,31.863"]
"rosinality/mac-network-pytorch" ["l"="48.816,32.191"]
"kexinyi/ns-vqa" ["l"="48.795,32.245"]
"ceyzaguirre4/NSM" ["l"="48.805,32.214"]
"vacancy/NSCL-PyTorch-Release" ["l"="-1.681,-41.473"]
"ronghanghu/n2nmn" ["l"="48.731,32.195"]
"ashkamath/mdetr" ["l"="48.833,31.941"]
"djiajunustc/TransVG" ["l"="48.975,31.941"]
"MarkMoHR/Awesome-Referring-Image-Segmentation" ["l"="48.925,31.903"]
"lichengunc/MAttNet" ["l"="23.514,14.853"]
"microsoft/GLIP" ["l"="48.866,30.251"]
"facebookresearch/grounded-video-description" ["l"="48.031,32.89"]
"jianzongwu/Awesome-Open-Vocabulary" ["l"="48.681,30.233"]
"vacancy/SceneGraphParser" ["l"="47.549,32.07"]
"iworldtong/Awesome-Temporal-Sentence-Grounding-in-Videos" ["l"="48.092,33.032"]
"zyang-ur/onestage_grounding" ["l"="48.971,31.969"]
"yaohungt/Multimodal-Transformer" ["l"="56.603,28.018"]
"declare-lab/multimodal-deep-learning" ["l"="56.62,28.026"]
"dandelin/ViLT" ["l"="48.751,32.016"]
"soujanyaporia/multimodal-sentiment-analysis" ["l"="56.637,28.009"]
"thuiar/MMSA" ["l"="56.601,28.035"]
"danieljf24/awesome-video-text-retrieval" ["l"="47.938,32.988"]
"jayleicn/ClipBERT" ["l"="48.738,31.975"]
"AIChallenger/AI_Challenger_2017" ["l"="48.492,31.846"]
"AIChallenger/AI_Challenger_2018" ["l"="53.637,27.378"]
"bearpaw/PyraNet" ["l"="31.609,28.222"]
"matteorr/coco-analyze" ["l"="31.659,28.237"]
"TuSimple/mx-maskrcnn" ["l"="51.746,33.537"]
"HouJP/kaggle-quora-question-pairs" ["l"="53.173,27.456"]
"chenyuntc/scene-baseline" ["l"="42.412,23.667"]
"gujiuxiang/chinese_im2text.pytorch" ["l"="48.463,31.81"]
"MVIG-SJTU/RMPE" ["l"="31.587,28.217"]
"salaniz/pycocoevalcap" ["l"="48.622,31.905"]
"ramavedantam/cider" ["l"="48.659,31.838"]
"wangleihitcs/CaptionMetrics" ["l"="48.649,31.863"]
"salesforce/densecap" ["l"="48.014,32.901"]
"krasserm/fairseq-image-captioning" ["l"="48.585,31.931"]
"furkanbiten/GoodNews" ["l"="48.553,32.268"]
"alasdairtran/transform-and-tell" ["l"="48.521,32.198"]
"AndresPMD/StacMR" ["l"="48.595,32.326"]
"AndresPMD/Fine_Grained_Clf" ["l"="48.621,32.322"]
"FuxiaoLiu/VisualNews-Repository" ["l"="52.581,26.749"]
"yz93/LAVT-RIS" ["l"="48.974,31.878"]
"DerrickWang005/CRIS.pytorch" ["l"="48.962,31.89"]
"wjn922/ReferFormer" ["l"="48.95,31.87"]
"amazon-science/polygon-transformer" ["l"="49.006,31.88"]
"kkakkkka/ETRIS" ["l"="48.984,31.839"]
"lxtGH/Awesome-Segmentation-With-Transformer" ["l"="48.757,30.228"]
"henghuiding/Vision-Language-Transformer" ["l"="-52.98,-12.003"]
"Qinying-Liu/Awesome-Open-Vocabulary-Semantic-Segmentation" ["l"="48.667,30.26"]
"dvlab-research/LISA" ["l"="47.454,30.124"]
"henghuiding/ReLA" ["l"="-54.529,-12.651"]
"JerryX1110/awesome-rvos" ["l"="48.961,31.848"]
"Seonghoon-Yu/Zero-shot-RIS" ["l"="48.992,31.869"]
"henghuiding/gRefCOCO" ["l"="-52.878,-11.904"]
"necla-ml/SNLI-VE" ["l"="48.972,31.993"]
"maximek3/e-ViL" ["l"="49.003,32.023"]
"BryanPlummer/flickr30k_entities" ["l"="48.958,31.958"]
"hmorioka/TCL" ["l"="49.314,32.519"]
"siamakz/iVAE" ["l"="49.284,32.511"]
"ilkhem/iVAE" ["l"="49.215,32.494"]
"ilkhem/icebeem" ["l"="49.253,32.51"]
"phlippe/CITRIS" ["l"="49.172,32.479"]
"njchoma/transformer_image_caption" ["l"="48.554,31.99"]
"jiasenlu/AdaptiveAttention" ["l"="48.531,31.928"]
"aditya12agd5/convcap" ["l"="48.49,31.938"]
"gujiuxiang/Stack-Captioning" ["l"="48.504,31.937"]
"ruotianluo/Transformer_Captioning" ["l"="48.479,31.932"]
"doubledaibo/gancaption_iccv2017" ["l"="48.425,31.957"]
"rakshithShetty/captionGAN" ["l"="48.432,31.938"]
"doubledaibo/clcaption_nips2017" ["l"="48.405,31.975"]
"yiyang92/vae_captioning" ["l"="48.449,31.955"]
"zhegan27/Semantic_Compositional_Nets" ["l"="48.438,31.949"]
"chenxinpeng/ARNet" ["l"="48.477,31.92"]
"daqingliu/CAVP" ["l"="48.443,31.913"]
"lukemelas/image-paragraph-captioning" ["l"="48.4,31.898"]
"s-gupta/visual-concepts" ["l"="48.468,31.968"]
"cswhjiang/Recurrent_Fusion_Network" ["l"="48.455,31.933"]
"ChenyunWu/PhraseCutDataset" ["l"="49.057,31.891"]
"spyflying/CMPC-Refseg" ["l"="49.091,31.972"]
"ccvl/iep-ref" ["l"="49.096,31.866"]
"ajamjoom/Image-Captions" ["l"="48.619,31.855"]
"RoyalSkye/Image-Caption" ["l"="48.591,31.858"]
"richardaecn/cvpr18-caption-eval" ["l"="48.417,31.945"]
"fkxssaa/Deliberate-Attention-Networks-for-Image-Captioning" ["l"="48.397,31.945"]
"TimoYoung/im2txt_Chinese" ["l"="48.363,31.823"]
"violetteshev/bottom-up-features" ["l"="48.541,31.981"]
"SinghJasdeep/Attention-on-Attention-for-VQA" ["l"="48.676,32.153"]
"abhshkdz/neural-vqa-attention" ["l"="48.64,32.152"]
"jialinwu17/self_critical_vqa" ["l"="48.683,32.179"]
"tbmoon/basic_vqa" ["l"="48.713,32.204"]
"noagarcia/awesome-vqa-pytorch" ["l"="48.718,32.23"]
"li-xirong/coco-cn" ["l"="48.429,31.811"]
"li-xirong/cross-lingual-cap" ["l"="48.408,31.798"]
"BAAI-WuDao/BriVL" ["l"="49.141,30.497"]
"weiyuk/fluent-cap" ["l"="48.394,31.804"]
"chuangg/CLEVRER" ["l"="48.829,32.273"]
"nerdimite/neuro-symbolic-ai-soc" ["l"="48.83,32.252"]
"kdexd/probnmn-clevr" ["l"="48.852,32.258"]
"vacancy/Jacinle" ["l"="-1.664,-41.452"]
"ronghanghu/snmn" ["l"="48.798,32.192"]
"google-research/clevr_robot_env" ["l"="48.813,32.293"]
"satwikkottur/clevr-dialog" ["l"="48.669,32.282"]
"microsoft/DFOL-VQA" ["l"="48.804,32.272"]
"HobbitLong/shape2prog" ["l"="63.997,0.936"]
"caffeinism/StyleGAN-pytorch" ["l"="45.114,30.472"]
"zyang-ur/ReSC" ["l"="48.991,31.968"]
"svip-lab/LBYLNet" ["l"="48.993,31.98"]
"nku-shengzheliu/Pytorch-TransVG" ["l"="49.005,31.956"]
"ChopinSharp/ref-nms" ["l"="49.003,31.974"]
"zjunlp/HVPNeT" ["l"="54.126,15.211"]
"BigRedT/info-ground" ["l"="49.016,31.981"]
"yangli18/VLTVG" ["l"="48.993,31.946"]
"luogen1996/MCN" ["l"="48.999,31.997"]
"thecharm/Mega" ["l"="54.154,15.209"]
"endernewton/iter-reason" ["l"="53.852,30.572"]
"rsokl/MyGrad" ["l"="48.781,32.316"]
"rsokl/noggin" ["l"="48.778,32.29"]
"e2crawfo/auto_yolo" ["l"="48.967,32.49"]
"stelzner/supair" ["l"="48.995,32.507"]
"Abishekpras/Attend-Infer-Repeat---Pytorch" ["l"="48.951,32.493"]
"aakhundov/tf-attend-infer-repeat" ["l"="48.984,32.523"]
"Yu-Wu/Decoupled-Novel-Object-Captioner" ["l"="48.228,31.951"]
"LisaAnne/DCC" ["l"="48.253,31.95"]
"zhixuan-lin/IODINE" ["l"="48.934,32.452"]
"zhixuan-lin/SPACE" ["l"="48.932,32.466"]
"baudm/MONet-pytorch" ["l"="48.941,32.427"]
"applied-ai-lab/genesis" ["l"="48.951,32.454"]
"stelzner/monet" ["l"="48.944,32.439"]
"zhixuan-lin/G-SWM" ["l"="48.94,32.481"]
"ucfnlp/summarization-sing-pair-mix" ["l"="58.298,29.02"]
"lxtGH/AI_challenger_Chinese_Caption" ["l"="48.442,31.844"]
"rsokl/Learning_Python" ["l"="48.809,32.353"]
"jiasenlu/visDial.pytorch" ["l"="48.602,32.274"]
"batra-mlp-lab/visdial-rl" ["l"="48.614,32.292"]
"yuleiniu/rva" ["l"="48.631,32.272"]
"gicheonkang/dan-visdial" ["l"="48.642,32.23"]
"vmurahari3/visdial-bert" ["l"="48.639,32.246"]
"batra-mlp-lab/visdial" ["l"="48.575,32.292"]
"zilongzheng/visdial-gnn" ["l"="48.639,32.28"]
"taesunwhang/MVAN-VisDial" ["l"="48.653,32.231"]
"hudaAlamri/DSTC7-Audio-Visual-Scene-Aware-Dialog-AVSD-Challenge" ["l"="55.975,29.227"]
"facebookresearch/corefnmn" ["l"="48.621,32.263"]
"shubhamagarwal92/visdial_conv" ["l"="48.646,32.254"]
"simpleshinobu/visdial-principles" ["l"="48.635,32.236"]
"naver/aqm-plus" ["l"="48.616,32.279"]
"idansc/mrr-ndcg" ["l"="48.642,32.26"]
"shijx12/XNM-Net" ["l"="48.809,32.146"]
"vinaybysani/Image-Captioning-System--Deep-learning" ["l"="48.472,31.762"]
"chrisc36/debias" ["l"="48.656,32.206"]
"chrisc36/bottom-up-attention-vqa" ["l"="48.686,32.191"]
"cdancette/vqa-cp-leaderboard" ["l"="48.703,32.192"]
"harpribot/nlp-metrics" ["l"="53.444,27.867"]
"Wind-Ward/Image_Caption_Competition" ["l"="48.445,31.828"]
"chenxinpeng/im2p" ["l"="48.364,31.892"]
"batra-mlp-lab/visdial-amt-chat" ["l"="48.571,32.322"]
"vmurahari3/visdial-diversity" ["l"="48.599,32.299"]
"GuessWhatGame/guesswhat" ["l"="48.649,32.321"]
"bupt-mmai/CNN-Caption" ["l"="48.368,31.88"]
"tgc1997/Awesome-Video-Captioning" ["l"="48.016,32.864"]
"jayleicn/recurrent-transformer" ["l"="47.989,32.893"]
"terry-r123/Awesome-Captioning" ["l"="48.666,31.929"]
"tgc1997/RMN" ["l"="48.02,32.854"]
"yangbang18/Non-Autoregressive-Video-Captioning" ["l"="47.99,32.852"]
"SydCaption/SAAT" ["l"="48.012,32.845"]
"hobincar/SGN" ["l"="48.003,32.854"]
"vsislab/Controllable_XGating" ["l"="48.034,32.863"]
"fawazsammani/show-edit-tell" ["l"="48.558,31.965"]
"YiwuZhong/Sub-GC" ["l"="48.587,31.975"]
"rowanz/neural-motifs" ["l"="47.556,32.035"]
"peteanderson80/SPICE" ["l"="47.525,32.066"]
"yikang-li/MSDN" ["l"="47.517,32.023"]
"Dong-JinKim/DenseRelationalCaptioning" ["l"="48.656,31.887"]
"ezeli/BUTD_model" ["l"="48.46,32.028"]
"ruotianluo/bottom-up-attention-ai-challenger" ["l"="48.455,31.838"]
"cai-lw/image-captioning-chinese" ["l"="48.448,31.802"]
"liqing-ustc/VizWiz_LSTM_CNN_Attention" ["l"="48.751,32.188"]
"DenisDsh/VizWiz-VQA-PyTorch" ["l"="48.733,32.166"]
"ccvl/clevr-refplus-dataset-gen" ["l"="49.121,31.849"]
"davidnvq/visdial" ["l"="48.659,32.251"]
"wh0330/CAG_VisDial" ["l"="48.669,32.202"]
"yufengm/Adaptive" ["l"="48.485,31.907"]
"s1879281/Image-Captioning-with-Adaptive-Attention" ["l"="48.448,31.88"]
"zjuchenlong/sca-cnn.cvpr17" ["l"="48.49,31.921"]
"qingzwang/GHA-ImageCaptioning" ["l"="48.455,31.891"]
"husthuaan/AAT" ["l"="48.482,31.957"]
"hlamba28/Automatic-Image-Captioning" ["l"="48.479,31.825"]
"visinf/cos-cvae" ["l"="48.48,31.996"]
"lichengunc/mask-faster-rcnn" ["l"="49.013,32.058"]
"lichengunc/speaker_listener_reinforcer" ["l"="48.976,32.044"]
"xh-liu/CM-Erase-REG" ["l"="49.018,32.034"]
"mikittt/easy-to-understand-REG" ["l"="48.995,32.064"]
"Cloud-CV/EvalAI-Starters" ["l"="48.415,32.314"]
"Cloud-CV/evalai-cli" ["l"="48.434,32.326"]
"JonghwanMun/TextguidedATT" ["l"="48.411,31.961"]
"ck0123/improved-bertscore-for-image-captioning-evaluation" ["l"="48.326,31.973"]
"tommccoy1/hans" ["l"="48.448,32.242"]
"BIU-NLP/Breaking_NLI" ["l"="48.404,32.248"]
"UKPLab/emnlp2020-debiasing-unknown" ["l"="48.541,32.227"]
"hhexiy/debiased" ["l"="48.421,32.236"]
"decompositional-semantics-initiative/DNC" ["l"="48.421,32.258"]
"daqingliu/NMTree" ["l"="48.908,32.06"]
"youngfly11/LCMCG-PyTorch" ["l"="48.87,32.076"]
"agakshat/visualdialog-pytorch" ["l"="48.631,32.298"]
"google-deepmind/multi_object_datasets" ["l"="48.923,32.434"]
"BCV-Uniandes/DMS" ["l"="49.054,31.996"]
"liruiyu/referseg_rrn" ["l"="49.09,31.991"]
"chenxi116/TF-phrasecut-public" ["l"="49.083,32.006"]
"ronghanghu/text_objseg" ["l"="49.069,32.034"]
"yuanx520/chinese_image_captioning" ["l"="48.432,31.773"]
"InnerPeace-Wu/densecap-tensorflow" ["l"="48.313,31.864"]
"linjieyangsc/densecap" ["l"="48.29,31.849"]
"InnerPeace-Wu/im2p-tensorflow" ["l"="48.331,31.876"]
"bckim92/colloquial-claims" ["l"="48.136,31.886"]
"dfdazac/monet" ["l"="48.961,32.423"]
"ecker-lab/object-centric-representation-benchmark" ["l"="48.915,32.452"]
"vsubhashini/noc" ["l"="48.273,31.947"]
"wenhuchen/Semi-Supervised-Image-Captioning" ["l"="48.307,31.947"]
"bhpfelix/Compositional-Attention-Networks-for-Machine-Reasoning-PyTorch" ["l"="48.853,32.194"]
"ronilp/mac-network-pytorch-gqa" ["l"="48.855,32.208"]
"neural-nuts/Cam2Caption" ["l"="48.531,31.752"]
"ruotianluo/coco-caption" ["l"="48.587,31.992"]
"HaleyPei/Implement-of-SCA-CNN" ["l"="48.429,31.89"]
"stevehuanghe/image_captioning" ["l"="48.443,31.899"]
"lluisgomez/single-shot-str" ["l"="48.585,32.39"]
"AndresPMD/Pytorch-yolo-phoc" ["l"="48.605,32.358"]
"QUVA-Lab/lang-tracker" ["l"="49.138,32.046"]
"fredfung007/snlt" ["l"="49.186,32.056"]
"distkv-project/distkv" ["l"="48.228,32.002"]
"distkv-project/drpc" ["l"="48.255,31.994"]
"Cloud-CV/EvalAI-ngx" ["l"="48.453,32.339"]
"aka-jain/Drango" ["l"="48.451,32.376"]
"ZiliangWang0505/AI-Challenger-Caption-Competition" ["l"="48.428,31.791"]
"ruizhaogit/EnergyBasedPrioritization" ["l"="49.189,32.64"]
"ruizhaogit/mep" ["l"="49.195,32.66"]
"ronghanghu/mmf" ["l"="48.856,32.1"]
"ZephyrZhuQi/ssbaseline" ["l"="48.846,32.094"]
"guanghuixu/AnchorCaptioner" ["l"="48.89,32.104"]
"yashkant/sam-textvqa" ["l"="48.842,32.108"]
"microsoft/TAP" ["l"="48.828,32.083"]
"ChenyuGAO-CS/SMA" ["l"="48.852,32.116"]
"guanghuixu/CRN_tvqa" ["l"="48.87,32.096"]
"tgGuo15/PriorImageCaption" ["l"="48.384,31.934"]
"erobic/negative_analysis_of_grounding" ["l"="48.682,32.204"]
"davidmascharka/MyNN" ["l"="48.769,32.305"]
"jackroos/VL-BERT" ["l"="48.692,31.982"]
"uclanlp/visualbert" ["l"="48.683,32.014"]
"LuoweiZhou/VLP" ["l"="48.676,31.973"]
"airsplay/py-bottom-up-attention" ["l"="48.665,32.003"]
"yikuan8/Transformers-VQA" ["l"="48.645,32.027"]
"tbmoon/kalman_filter" ["l"="48.723,32.292"]
"DonghoonPark12/Book_KalmanFilter" ["l"="48.724,32.326"]
"gyy8426/Computer_Vision_primer" ["l"="48.51,32.029"]
"zchoi/S2-Transformer" ["l"="48.557,32.008"]
"NovaMind-Z/PTSN" ["l"="48.57,31.996"]
"AceCoooool/interview-computer-vision" ["l"="48.462,32.056"]
"zchoi/PKOL" ["l"="48.54,32.018"]
"linjieli222/HERO" ["l"="47.949,32.974"]
"MILVLG/bottom-up-attention.pytorch" ["l"="48.657,31.97"]
"kuanghuei/SCAN" ["l"="58.297,8.273"]
"kdexd/virtex" ["l"="48.72,31.959"]
"mbanani/unsupervisedRR" ["l"="64.366,2.237"]
"pzzhang/VinVL" ["l"="48.721,32.017"]
"researchmm/soho" ["l"="48.769,32.008"]
"clip-vil/CLIP-ViL" ["l"="48.758,31.969"]
"sangminwoo/awesome-vision-and-language" ["l"="48.789,31.976"]
"lucidrains/slot-attention" ["l"="48.903,32.43"]
"evelinehong/slot-attention-pytorch" ["l"="48.89,32.462"]
"imbue-ai/slot_attention" ["l"="51.534,-0.753"]
"tkipf/c-swm" ["l"="48.978,32.44"]
"google-research/slot-attention-video" ["l"="48.883,32.484"]
"amazon-science/object-centric-learning-framework" ["l"="48.893,32.49"]
"singhgautam/slate" ["l"="48.908,32.481"]
"addtt/object-centric-library" ["l"="48.869,32.427"]
"amazon-science/AdaSlot" ["l"="48.872,32.47"]
"ajabri/videowalk" ["l"="47.857,34.564"]
"mila-iqia/atari-representation-learning" ["l"="59.354,17.557"]
"jcoreyes/OP3" ["l"="49.013,32.437"]
"danijar/dreamer" ["l"="59.319,17.63"]
"JindongJiang/SCALOR" ["l"="48.958,32.468"]
"dido1998/Recurrent-Independent-Mechanisms" ["l"="49.018,32.46"]
"dido1998/CausalMBRL" ["l"="49.073,32.473"]
"floodsung/Deep-Reasoning-Papers" ["l"="49.001,32.403"]
"anirudh9119/RIMs" ["l"="49.039,32.466"]
"devjwsong/transformer-translator-pytorch" ["l"="48.648,31.708"]
"BierOne/bottom-up-attention-vqa" ["l"="48.431,32.035"]
"davidnvq/grit" ["l"="48.639,31.933"]
"zhangxuying1004/RSTNet" ["l"="48.629,31.958"]
"codexxxl/GraphVQA" ["l"="48.785,31.921"]
"CrossmodalGroup/GSMN" ["l"="58.324,8.258"]
"KunpengLi1994/VSRN" ["l"="58.311,8.265"]
"catty-project/catty" ["l"="48.286,31.985"]
"phellonchen/awesome-Vision-and-Language-Pre-training" ["l"="48.822,31.987"]
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" ["l"="47.969,34.009"]
"EdisonLeeeee/Awesome-Masked-Autoencoders" ["l"="52.811,29.38"]
"ttengwang/Awesome_Prompting_Papers_in_Computer_Vision" ["l"="50.313,38.23"]
"sfikas/medical-imaging-datasets" ["l"="61.834,36.857"]
"Yangzhangcst/Transformer-in-Computer-Vision" ["l"="50.858,29.642"]
"microsoft/scene_graph_benchmark" ["l"="47.602,32.057"]
"zarzouram/image_captioning_with_transformers" ["l"="48.605,31.839"]
"wtliao/ImageTransformer" ["l"="48.538,31.946"]
"bearcatt/LaBERT" ["l"="48.509,31.982"]
"facebookresearch/mmbt" ["l"="48.546,32.059"]
"johnarevalo/gmu-mmimdb" ["l"="48.494,32.082"]
"WasifurRahman/BERT_multimodal_transformer" ["l"="56.588,28.008"]
"YuanEZhou/Grounded-Image-Captioning" ["l"="48.601,31.993"]
"codalab/codalab-worksheets-old" ["l"="48.275,32.425"]
"codalab/worksheets-examples" ["l"="48.295,32.442"]
"slachapelle/disentanglement_via_mechanism_sparsity" ["l"="49.243,32.527"]
"vislearn/GIN" ["l"="49.261,32.535"]
"HHalva/hmnlica" ["l"="49.271,32.523"]
"HHalva/snica" ["l"="49.268,32.494"]
"JXZe/DualVD" ["l"="48.665,32.238"]
"shilrley6/Faster-R-CNN-with-model-pretrained-on-Visual-Genome" ["l"="48.632,31.975"]
"MILVLG/rosita" ["l"="48.74,31.845"]
"pytorch/hydra-torch" ["l"="48.752,32.413"]
"romesco/hydra-lightning" ["l"="48.743,32.434"]
"mit-ll-responsible-ai/hydra-zen" ["l"="48.769,32.378"]
"eddieantonio/imgcat" ["l"="48.034,31.825"]
"wookayin/python-imgcat" ["l"="48.109,31.844"]
"olivere/iterm2-imagetools" ["l"="47.987,31.811"]
"Gitsamshi/WeakVRD-Captioning" ["l"="48.516,31.997"]
"anirudh9119/neural_production_systems" ["l"="49.05,32.497"]
"zfchenUnique/DCL-Release" ["l"="60.142,16.349"]
"dingmyu/VRDP" ["l"="60.128,16.314"]
"YunzhuLi/PropNet" ["l"="60.05,16.413"]
"hyenal/relate" ["l"="48.984,32.469"]
"martius-lab/cid-in-rl" ["l"="49.107,32.503"]
"wangzizhao/CausalDynamicsLearning" ["l"="49.107,32.49"]
"lwye/CMSA-Net" ["l"="49.122,31.97"]
"e-bug/volta" ["l"="48.768,32.045"]
"anisha2102/docvqa" ["l"="48.972,32.16"]
"mineshmathew/DocVQA" ["l"="48.908,32.134"]
"allanj/LayoutLMv3-DocVQA" ["l"="49.01,32.176"]
"uakarsh/latr" ["l"="48.821,32.131"]
"xiaojino/RUArt" ["l"="48.838,32.125"]
"ZeningLin/ViBERTgrid-PyTorch" ["l"="48.883,32.137"]
"KaihuaTang/Scene-Graph-Benchmark.pytorch" ["l"="47.582,32.063"]
"tejas-gokhale/vqa_mutant" ["l"="48.706,32.222"]
"AndresPMD/GCN_classification" ["l"="48.602,32.344"]
"MCLAB-OCR/KnowledgeMiningWithSceneText" ["l"="48.618,32.34"]
"ezeli/bottom_up_features_extract" ["l"="48.418,32.045"]
"ezeli/Transformer_model" ["l"="48.435,32.05"]
"spitis/mrl" ["l"="49.13,32.534"]
"Stilwell-Git/Hindsight-Goal-Generation" ["l"="49.159,32.576"]
"fenglinliu98/MIA" ["l"="48.51,32.052"]
"hwanheelee1993/ViLBERTScore" ["l"="48.302,31.981"]
"google-deepmind/dsprites-dataset" ["l"="23.028,14.255"]
"qinzzz/Multimodal-Alignment-Framework" ["l"="49.015,31.965"]
"youngfly11/ReIR-WeaklyGrounding.pytorch" ["l"="49.027,31.963"]
"GingL/ARN" ["l"="49.023,32.017"]
"luogen1996/Real-time-Global-Inference-Network" ["l"="49.026,32.002"]
"sibeiyang/sgmn" ["l"="48.955,32.026"]
"fengguang94/BRINet" ["l"="49.067,31.986"]
"abachaa/VQA-Med-2019" ["l"="62.218,37.575"]
"HimariO/HatefulMemesChallenge" ["l"="48.472,32.095"]
"Awenbocc/med-vqa" ["l"="62.204,37.553"]
"aioz-ai/MICCAI21_MMQ" ["l"="62.175,37.548"]
"VirajBagal/MMBERT" ["l"="62.211,37.541"]
"UCSD-AI4H/PathVQA" ["l"="62.226,37.561"]
"gicheonkang/sglkt-visdial" ["l"="48.653,32.264"]
"wangzheng17/awesome-causal-vision" ["l"="48.563,32.242"]
"Wangt-CN/VC-R-CNN" ["l"="-54.476,-14.264"]
"Wangt-CN/Awesome-Causality-in-CV" ["l"="-54.876,-13.524"]
"yangxuntu/lxmertcatt" ["l"="48.623,32.214"]
"daqingliu/awesome-rec" ["l"="48.941,32.08"]
"aravindvarier/Image-Captioning-Pytorch" ["l"="48.584,31.822"]
"senadkurtisi/pytorch-image-captioning" ["l"="48.585,31.807"]
"kapilks/Download-youtube-video" ["l"="48.536,32.14"]
"kapilks/Basic-Kernel" ["l"="48.527,32.129"]
"martius-lab/SMORL" ["l"="48.986,32.486"]
"spyflying/LSCM-Refseg" ["l"="49.12,31.985"]
"LukeForeverYoung/QRNet" ["l"="49.015,31.944"]
"Kien085/SG2Caps" ["l"="48.54,32.002"]
"HKUST-KnowComp/Visual_PCR" ["l"="48.665,32.302"]
"ZihaoW123/UniMM" ["l"="48.675,32.327"]
"mengf1/CHER" ["l"="49.188,32.613"]
"YangRui2015/AWGCSL" ["l"="49.212,32.629"]
"henrycharlesworth/PlanGAN" ["l"="49.21,32.61"]
"UKPLab/acl2020-confidence-regularization" ["l"="48.682,32.252"]
"salesforce/VD-BERT" ["l"="48.653,32.243"]
"jack1yang/image-paragraph-captioning" ["l"="48.35,31.878"]
"GingL/KPRN" ["l"="49.042,32.027"]
"insomnia94/DTWREG" ["l"="49.063,32.021"]
"fawazsammani/look-and-modify" ["l"="48.485,32.018"]
"miriambellver/refvos" ["l"="48.962,31.813"]
"skynbe/Refer-Youtube-VOS" ["l"="48.959,31.828"]
"yuleiniu/vc" ["l"="48.957,32.047"]
"gqa-ood/GQA-OOD" ["l"="48.697,32.207"]
"yuleiniu/cfvqa" ["l"="48.667,32.182"]
"GeraldHan/GGE" ["l"="48.716,32.245"]
"CrossmodalGroup/SSL-VQA" ["l"="58.393,8.268"]
"ronghanghu/vqa-maskrcnn-benchmark-m4c" ["l"="48.87,32.126"]
"visiontao/evar" ["l"="49.093,32.333"]
"visiontao/dcnet" ["l"="49.067,32.338"]
"insomnia94/ISREG" ["l"="49.087,32.024"]
"PluviophileYU/CVC-QA" ["l"="48.692,32.278"]
"HLR/Cross_Modality_Relevance" ["l"="48.684,32.231"]
"salesforce/BLIP" ["l"="48.991,30.249"]
"uta-smile/TCL" ["l"="48.806,32.003"]
"OFA-Sys/OFA" ["l"="49.008,30.282"]
"zdou0830/METER" ["l"="48.784,32.002"]
"KaiyangZhou/CoOp" ["l"="50.344,38.24"]
"zengyan-97/X-VLM" ["l"="48.801,32.022"]
"ArrowLuo/CLIP4Clip" ["l"="47.909,32.98"]
"zhegan27/VILLA" ["l"="48.737,31.803"]
"zhegan27/LXMERT-AdvTrain" ["l"="48.74,31.774"]
"Muennighoff/vilio" ["l"="48.441,32.102"]
"Nithin-Holla/meme_challenge" ["l"="48.446,32.117"]
"rizavelioglu/hateful_memes-hate_detectron" ["l"="48.424,32.114"]
"allenai/mmc4" ["l"="49.046,30.301"]
"FreddeFrallan/Multilingual-CLIP" ["l"="49.042,30.389"]
"kakaobrain/coyo-dataset" ["l"="49.003,30.342"]
"microsoft/UniCL" ["l"="48.798,31.955"]
"rom1504/img2dataset" ["l"="48.99,30.304"]
"Maluuba/nlg-eval" ["l"="58.307,29.128"]
"xiadingZ/video-caption.pytorch" ["l"="48.058,32.858"]
"jazzsaxmafia/show_attend_and_tell.tensorflow" ["l"="48.521,31.967"]
"Element-Research/rnn" ["l"="46.055,27.692"]
"jcjohnson/densecap" ["l"="48.563,31.866"]
"ryankiros/visual-semantic-embedding" ["l"="58.327,8.321"]
"mansimov/text2image" ["l"="45.853,29.279"]
"karpathy/neuraltalk" ["l"="47.92,28.94"]
"jazzsaxmafia/show_and_tell.tensorflow" ["l"="48.496,31.964"]
"nyu-dl/dl4mt-tutorial" ["l"="53.728,24.756"]
"mjhucla/mRNN-CR" ["l"="48.456,31.99"]
"mjhucla/TF-mRNN" ["l"="48.452,31.968"]
"linhuixiao/CLIP-VG" ["l"="49.053,31.935"]
"seanzhuh/SeqTR" ["l"="49.029,31.903"]
"LeapLabTHU/Pseudo-Q" ["l"="49.261,32.958"]
"djiajunustc/H-23D_R-CNN" ["l"="49.033,31.925"]
"ubc-vision/RefTR" ["l"="48.989,31.913"]
"Dmmm1997/SimVG" ["l"="49.074,31.939"]
"ZhanYang-nwpu/RSVG-pytorch" ["l"="41.714,25.127"]
"jiasenlu/HieCoAttenVQA" ["l"="48.632,32.131"]
"GT-Vision-Lab/VQA_LSTM_CNN" ["l"="48.609,32.14"]
"chingyaoc/VQA-tensorflow" ["l"="48.622,32.142"]
"akirafukui/vqa-mcb" ["l"="48.649,32.136"]
"iamaaditya/VQA_Demo" ["l"="48.606,32.126"]
"zhoubolei/VQAbaseline" ["l"="48.616,32.119"]
"jialinwu17/MAVEX" ["l"="48.821,31.829"]
"jlian2/mucko" ["l"="48.768,31.922"]
"facebookresearch/Detic" ["l"="48.797,30.285"]
"alirezazareian/ovr-cnn" ["l"="48.586,30.298"]
"mmaaz60/mvits_for_class_agnostic_od" ["l"="48.479,30.325"]
"guoyang9/UnifER" ["l"="48.83,31.802"]
"alirezasalemi7/DEDR-MM-FiD" ["l"="48.843,31.795"]
"usr922/vgtr" ["l"="49.044,31.959"]
"gokulkarthik/hateclipper" ["l"="48.397,32.124"]
"chapternewscu/image-captioning-with-semantic-attention" ["l"="48.431,31.967"]
"kimiyoung/review_net" ["l"="48.436,31.985"]
"gujiuxiang/MIL.pytorch" ["l"="41.714,27.108"]
"gabeur/mmt" ["l"="47.959,32.963"]
"CryhanFang/CLIP2Video" ["l"="47.918,32.996"]
"m-bain/frozen-in-time" ["l"="47.892,32.958"]
"microsoft/UniVL" ["l"="47.934,32.923"]
"jayleicn/singularity" ["l"="47.846,32.965"]
"tsujuifu/pytorch_violet" ["l"="47.88,32.97"]
"albanie/collaborative-experts" ["l"="47.936,32.964"]
"jayleicn/moment_detr" ["l"="48.026,33.09"]
"antoine77340/MIL-NCE_HowTo100M" ["l"="47.952,32.929"]
"232525/PureT" ["l"="48.652,31.939"]
"luo3300612/Transformer-Captioning" ["l"="48.65,31.929"]
"YicongHong/Recurrent-VLN-BERT" ["l"="60.314,17.623"]
"airsplay/R2R-EnvDrop" ["l"="60.323,17.651"]
"cshizhe/VLN-HAMT" ["l"="60.31,17.636"]
"cshizhe/VLN-DUET" ["l"="60.287,17.615"]
"weituo12321/PREVALENT" ["l"="60.334,17.67"]
"jialuli-luka/EnvEdit" ["l"="60.345,17.661"]
"YicongHong/Discrete-Continuous-VLN" ["l"="60.297,17.635"]
"YicongHong/Fine-Grained-R2R" ["l"="60.343,17.651"]
"arjunmajum/vln-bert" ["l"="60.338,17.639"]
"MarSaKi/VLN-BEVBert" ["l"="60.392,17.632"]
"ronghanghu/speaker_follower" ["l"="60.33,17.657"]
"MILVLG/mt-captioning" ["l"="48.755,31.821"]
"Vision-CAIR/VisualGPT" ["l"="48.646,31.913"]
"Jhhuangkay/DeepOpht-Medical-Report-Generation-for-Retinal-Images-via-Deep-Models-and-Visual-Explanation" ["l"="48.679,31.872"]
"omar-mohamed/GPT2-Chest-X-Ray-Report-Generation" ["l"="62.283,37.459"]
"GT-RIPL/Xmodal-Ctx" ["l"="48.67,31.915"]
"lanfeng4659/STR-TDSL" ["l"="48.609,32.399"]
"mit-ll-responsible-ai/responsible-ai-toolbox" ["l"="48.782,32.412"]
"patrick-kidger/jaxtyping" ["l"="21.793,14.05"]
"caiqi/Cascasde-3D" ["l"="49.069,31.909"]
"China-UK-ZSL/ZS-F-VQA" ["l"="48.788,31.864"]
"GilgameshD/GRADER" ["l"="49.08,32.495"]
"Dantekk/Image-Captioning" ["l"="48.599,31.797"]
"Dantekk/PokeGAN" ["l"="48.597,31.763"]
"visiontao/ncd" ["l"="49.053,32.333"]
"WellyZhang/ALANS" ["l"="49.079,32.36"]
"ThalesGroup/ConceptBERT" ["l"="48.801,31.855"]
"sumedh7/CausalCuriosity" ["l"="49.128,32.492"]
"AndersonStra/MuKEA" ["l"="48.802,31.82"]
"JindongJiang/GNM" ["l"="49.107,32.365"]
"HenryJunW/TAG" ["l"="48.867,32.107"]
"ncs-jss/HTTP_200" ["l"="48.436,32.405"]
"spyshiv/dummytextjs" ["l"="48.446,32.393"]
"mrwu-mac/DIFNet" ["l"="48.614,31.987"]
"xmu-xiaoma666/LSTNet" ["l"="48.663,31.904"]
"google-deepmind/dm_alchemy" ["l"="49.11,32.462"]
"Cloud-CV/CloudCV-Old" ["l"="48.473,32.354"]
"Cloud-CV/origami-lib" ["l"="48.46,32.352"]
"AndresPMD/semantic_adaptive_margin" ["l"="46.189,5.988"]
"YuanEZhou/CBTrans" ["l"="48.687,31.913"]
"SkrighYZ/FGVE" ["l"="49.029,32.041"]
"YangRui2015/GOAT" ["l"="49.23,32.641"]
"wangxiao5791509/TNL2K_evaluation_toolkit" ["l"="49.249,32.066"]
"lizhou-cs/JointNLT" ["l"="49.226,32.063"]
"anirudh9119/shared_workspace" ["l"="49.059,32.519"]
"chojw/genb" ["l"="48.723,32.263"]
"yuleiniu/introd" ["l"="48.71,32.269"]
"AnnikaLindh/Diverse_and_Specific_Image_Captioning" ["l"="48.451,32.014"]
"abhshkdz/neural-vqa" ["l"="48.595,32.113"]
"avisingh599/visual-qa" ["l"="48.571,32.1"]
"paarthneekhara/neural-vqa-tensorflow" ["l"="48.571,32.119"]
"kaishengtai/torch-ntm" ["l"="46.024,27.733"]
"Element-Research/dpnn" ["l"="45.978,27.644"]
"carpedm20/visual-analogy-tensorflow" ["l"="53.439,26.38"]
"ericjang/tdb" ["l"="47.77,28.998"]
"jazzsaxmafia/video_to_sequence" ["l"="48.121,32.801"]
"mosessoh/CNN-LSTM-Caption-Generator" ["l"="48.496,31.898"]
"ry/tensorflow-vgg16" ["l"="50.37,33.222"]
"TensorFlowKR/awesome_tensorflow_implementations" ["l"="-4.844,-23.008"]
"facebookarchive/MIXER" ["l"="46.026,27.713"]
"tsenghungchen/SA-tensorflow" ["l"="48.101,32.833"]
"anantzoid/VQA-Keras-Visual-Question-Answering" ["l"="48.587,32.137"]
"codekansas/keras-language-modeling" ["l"="55.861,28.465"]
"nicolas-ivanov/debug_seq2seq" ["l"="55.829,28.51"]
"siemanko/tf-adversarial" ["l"="45.938,27.531"]
"dblN/stochastic_depth_keras" ["l"="48.494,32.13"]
"vinhkhuc/MemN2N-babi-python" ["l"="46.064,27.865"]
"mila-iqia/summerschool2015" ["l"="44.875,27.654"]
"allenai/deep_qa" ["l"="55.894,28.39"]
"mttr2021/MTTR" ["l"="48.92,31.873"]
"hkchengrex/STCN" ["l"="47.721,34.627"]
"gaomingqi/Awesome-Video-Object-Segmentation" ["l"="47.663,34.581"]
"FoundationVision/VNext" ["l"="50.689,30.608"]
"dzh19990407/LBDT" ["l"="48.927,31.843"]
"seoungwugoh/STM" ["l"="47.756,34.658"]
"yoxu515/aot-benchmark" ["l"="47.732,34.615"]
"wjf5203/SeqFormer" ["l"="50.675,30.596"]
"xmlyqing00/AFB-URR" ["l"="47.754,34.644"]
"jacobandreas/nmn2" ["l"="48.655,32.166"]
"jnhwkim/cbp" ["l"="48.745,32.12"]
"ilyasu123/rlntm" ["l"="45.987,27.795"]
"jacobandreas/psketch" ["l"="48.604,32.206"]
"ruotianluo/neuraltalk2-tensorflow" ["l"="48.428,32.004"]
"woodrush/neural-art-tf" ["l"="45.689,29.454"]
"yukezhu/visual7w-qa-models" ["l"="48.584,32.16"]
"yukezhu/visual7w-toolkit" ["l"="48.564,32.164"]
"HyeonwooNoh/DPPnet" ["l"="48.592,32.181"]
"zcyang/imageqa-san" ["l"="48.614,32.163"]
"pairlab/SlotFormer" ["l"="48.889,32.506"]
"singhgautam/steve" ["l"="48.913,32.499"]
"Wuziyi616/SlotDiffusion" ["l"="48.879,32.513"]
"YuLiu-LY/BO-QSA" ["l"="48.852,32.519"]
"gkakogeorgiou/spot" ["l"="48.867,32.486"]
"junkeun-yi/SAVi-pytorch" ["l"="48.846,32.48"]
"martius-lab/videosaur" ["l"="48.87,32.497"]
"gorkaydemir/SOLV" ["l"="48.855,32.492"]
"karazijal/clevrtex-generation" ["l"="48.915,32.525"]
"gorkaydemir/DINOSAUR" ["l"="48.864,32.504"]
"zichengsaber/LAVT-pytorch" ["l"="48.974,31.857"]
"chongzhou96/MaskCLIP" ["l"="48.735,30.277"]
"toggle1995/RIS-DMMI" ["l"="48.998,31.857"]
"linyq2117/CLIP-ES" ["l"="53.999,31.503"]
"isl-org/lang-seg" ["l"="48.77,30.285"]
"allenai/reclip" ["l"="48.938,31.854"]
"ylingfeng/FGVP" ["l"="48.934,31.81"]
"Show-han/Zeroshot_REC" ["l"="48.921,31.825"]
"zhjohnchan/awesome-vision-and-language-pretraining" ["l"="48.879,31.987"]
"wangxidong06/CS224N" ["l"="48.904,31.982"]
"microsoft/X-Decoder" ["l"="48.821,30.224"]
"facebookresearch/SLIP" ["l"="48.929,30.344"]
"karpathy/neuraltalk2" ["l"="47.958,28.94"]
"yueatsprograms/Stochastic_Depth" ["l"="45.963,27.643"]
"facebookarchive/fb.resnet.torch" ["l"="50.567,33.19"]
"facebookarchive/torchnet" ["l"="46,27.679"]
"jcjohnson/torch-rnn" ["l"="46.17,27.698"]
"DmitryUlyanov/texture_nets" ["l"="45.704,29.409"]
"microsoft/BridgeTower" ["l"="48.839,32.009"]
"ylsung/VL_adapter" ["l"="50.197,38.201"]
"zengyan-97/X2-VLM" ["l"="48.846,32.034"]
"mertyg/vision-language-models-are-bows" ["l"="38.248,-0.115"]
"Paranioar/SGRAF" ["l"="58.314,8.243"]
"MichaelZhouwang/VLUE" ["l"="48.833,32.048"]
"ioanacroi/qb-norm" ["l"="47.892,33.003"]
"zengyan-97/CCLM" ["l"="49.259,30.579"]
"TencentARC/MCQ" ["l"="47.871,32.98"]
"sail-sg/ptp" ["l"="48.874,32.034"]
"adversarial-for-goodness/Co-Attack" ["l"="38.194,-7.133"]
"guilk/KAT" ["l"="48.833,31.776"]
"PhoebusSi/Thinking-while-Observing" ["l"="48.849,31.766"]
"lsa1997/CARIS" ["l"="49.012,31.857"]
"fawnliu/TRIS" ["l"="49.025,31.862"]
"jianzongwu/robust-ref-seg" ["l"="48.999,31.845"]
"SooLab/CGFormer" ["l"="49.015,31.842"]
"yangli18/PDNet" ["l"="49.012,31.931"]
"Qualcomm-AI-research/weakly-supervised-causal-representation-learning" ["l"="49.196,32.474"]
"bo-miao/SgMg" ["l"="49.033,31.813"]
"leonnnop/Locater" ["l"="48.945,31.833"]
"FoundationVision/UniRef" ["l"="48.989,31.891"]
"hustvl/TeViT" ["l"="50.695,30.584"]
"Surrey-UP-Lab/RegionSpot" ["l"="48.485,30.261"]
"luogen1996/SimREC" ["l"="49.12,31.875"]
"mjhucla/Google_Refexp_toolbox" ["l"="48.915,32.004"]
"apple2373/chainer_caption_generation" ["l"="48.818,31.909"]
"varun-nagaraja/referring-expressions" ["l"="48.937,32.032"]
"milkymap/transformer-image-captioning" ["l"="48.612,31.78"]
"jsoft88/cptr-vision-transformer" ["l"="48.612,31.751"]
"aioz-ai/CFR_VQA" ["l"="48.798,31.795"]
"ronghanghu/natural-language-object-retrieval" ["l"="49.111,32.059"]
"andrewliao11/Natural-Language-Object-Retrieval-tensorflow" ["l"="49.104,32.042"]
"hackerchenzhuo/LaKo" ["l"="48.811,31.786"]
"jacobswan1/ViTCAP" ["l"="48.68,31.895"]
"SjokerLily/awesome-image-captioning" ["l"="48.7,31.885"]
"karazijal/probable-motion" ["l"="48.924,32.549"]
"karazijal/guess-what-moves" ["l"="48.925,32.562"]
"furkanbiten/stvqa_amazon_ocr" ["l"="48.791,32.178"]
"lxa9867/R2VOS" ["l"="49.012,31.799"]
"wudongming97/OnlineRefer" ["l"="49.008,31.819"]
"JerryX1110/RPCMVOS" ["l"="47.786,34.629"]
"SCUT-DLVCLab/RFUND" ["l"="46.732,7.45"]
"sciencefictionlab/chargrid-pytorch" ["l"="48.914,32.154"]
"furkanbiten/idl_data" ["l"="48.745,32.224"]
"biswassanket/synth_doc_generation" ["l"="46.2,5.974"]
"mit-ll-responsible-ai/equine" ["l"="48.784,32.433"]
"kingthreestones/RefCLIP" ["l"="49.144,31.87"]
"Disguiser15/RefTeacher" ["l"="49.166,31.857"]
"fengguang94/CEFNet" ["l"="48.981,31.825"]
"gicheonkang/gst-visdial" ["l"="48.656,32.286"]
"microsoft/act" ["l"="48.816,31.881"]
"microsoft/azfuse" ["l"="48.801,31.898"]
"Zhiquan-Wen/D-VQA" ["l"="48.735,32.274"]
"microsoft/GenerativeImage2Text" ["l"="48.772,31.949"]
"microsoft/SwinBERT" ["l"="47.951,32.895"]
"JialianW/GRiT" ["l"="47.45,30.28"]
"TXH-mercury/VALOR" ["l"="47.778,32.932"]
"lucidrains/CoCa-pytorch" ["l"="48.961,30.31"]
"yuweihao/MM-Vet" ["l"="47.314,30.258"]
"chingyaoc/san-torch" ["l"="48.578,32.195"]
"iamaaditya/VQA_Keras" ["l"="48.568,32.147"]
"anujshah1003/VQA-Demo-GUI" ["l"="48.546,32.154"]
"VedantYadav/VQA" ["l"="48.558,32.137"]
"shmsw25/mcb-model-for-vqa" ["l"="48.626,32.173"]
"liuzhi136/Visual-Question-Answering" ["l"="48.571,32.179"]
"jnhwkim/MulLowBiVQA" ["l"="48.699,32.146"]
"cesc-park/attend2u" ["l"="48.4,31.921"]
"wookayin/tensorflow-plot" ["l"="48.268,31.887"]
"wookayin/TensorFlowKR-2017-talk-bestpractice" ["l"="-4.817,-22.982"]
"YunseokJANG/tgif-qa" ["l"="47.925,33.068"]
"krta2/awesome-nonsan" ["l"="-4.063,-21.205"]
"mlpc-ucsd/BoundaryFormer" ["l"="49.046,31.868"]
"Cloud-CV/Fabrik" ["l"="48.496,32.344"]
"Cloud-CV/Origami" ["l"="48.472,32.339"]
"Cloud-CV/visual-chatbot" ["l"="48.504,32.323"]
"Cloud-CV/EvalAI" ["l"="48.463,32.3"]
"Cloud-CV/GSoC-Ideas" ["l"="48.465,32.327"]
"Cloud-CV/CloudCV" ["l"="48.48,32.323"]
"joeddav/devol" ["l"="49.854,26.25"]
"keplr-io/quiver" ["l"="50.406,33.282"]
"reiinakano/xcessiv" ["l"="45.468,26.228"]
"MrNothing/AI-Blocks" ["l"="45.615,29.223"]
"nmhkahn/deep_learning_tutorial" ["l"="-4.857,-23.052"]
"GunhoChoi/PyTorch-FastCampus" ["l"="-4.914,-23.091"]
"MagNet-DL/magnet" ["l"="45.597,25.761"]
"MILVLG/prophet" ["l"="48.798,31.766"]
"microsoft/PICa" ["l"="48.791,31.719"]
"Yushi-Hu/PromptCap" ["l"="48.819,31.739"]
"yuanze-lin/REVIVE" ["l"="48.825,31.721"]
"zhangxi1997/VQACL" ["l"="33.453,31.851"]
"YuJungHeo/kbvqa-public" ["l"="48.785,31.739"]
"singhgautam/sysbinder" ["l"="48.881,32.559"]
"Wuziyi616/nerv" ["l"="48.877,32.536"]
"LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering" ["l"="48.846,31.738"]
"LinWeizheDragon/FLMR" ["l"="48.867,31.692"]
"Go2Heart/EchoSight" ["l"="32.953,30.81"]
"lichengunc/refer-parser2" ["l"="48.945,31.929"]
"redthing1/layoutlm_experiments" ["l"="49.035,32.185"]
"Seth-Park/text_objseg_caffe" ["l"="49.083,32.052"]
"Cloud-CV/diverse-beam-search" ["l"="48.426,32.377"]
"lxa9867/QSD" ["l"="49.036,31.764"]
"JindongJiang/latent-slot-diffusion" ["l"="48.903,32.513"]
"Huntersxsx/RIS-Learning-List" ["l"="49.042,31.848"]
"kdwonn/SaG" ["l"="62.578,36.325"]
"linhuixiao/HiVG" ["l"="49.09,31.938"]
"linhuixiao/Awesome-Visual-Grounding" ["l"="49.096,31.923"]
"chenwei746/EEVG" ["l"="49.077,31.925"]
"MightXiong/FedMIT" ["l"="45.247,29.065"]
"LANMNG/LQVG" ["l"="41.678,25.073"]
"zsdonghao/Image-Captioning" ["l"="48.472,31.878"]
"deepsemantic/image_captioning" ["l"="48.431,31.868"]
"TonyLianLong/RCF-UnsupVideoSeg" ["l"="48.934,32.589"]
"vvvb-github/AVSegFormer" ["l"="49.312,31.941"]
"jinxiang-liu/anno-free-AVS" ["l"="49.328,31.933"]
"yannqi/COMBO-AVS" ["l"="49.338,31.945"]
"GeWu-Lab/Ref-AVS" ["l"="49.273,31.952"]
"OpenSpaceAI/UVLTrack" ["l"="54.393,33.817"]
"zorzi-s/PolyWorldPretrainedNetwork" ["l"="42.604,23.218"]
"paarthneekhara/Weather-From-Map" ["l"="48.534,32.118"]
"paarthneekhara/convolutional-vqa" ["l"="48.52,32.143"]
"adiitya/p2pstream" ["l"="-45.977,-33.81"]
"DeepRNN/visual_question_answering" ["l"="46.004,27.965"]
"paarthneekhara/byteNet-tensorflow" ["l"="53.651,24.801"]
"fgirbal/segment-select-correct" ["l"="49.03,31.838"]
"heshuting555/DsHmp" ["l"="47.633,34.539"]
"bo-miao/RefHuman" ["l"="49.07,31.807"]
"RobertLuo1/NeurIPS2023_SOC" ["l"="49.08,31.79"]
"cilinyan/ReVOS-api" ["l"="33.348,31.728"]
"kkakkkka/MambaTalk" ["l"="48.992,31.81"]
"jiaqihuang01/DETRIS" ["l"="48.985,31.798"]
"liuting20/MaPPER" ["l"="48.998,31.786"]
"kkakkkka/HunyuanPortrait" ["l"="32.227,30.557"]
"zadaianchuk/comus" ["l"="48.893,32.529"]
"GeWu-Lab/Generalizable-Audio-Visual-Segmentation" ["l"="49.317,31.955"]
"RobertLuo1/CoHD" ["l"="49.104,31.778"]
"SooLab/Free-Bloom" ["l"="49.057,31.789"]
"cvlab-kaist/DirecT2V" ["l"="32.261,30.713"]
"SooLab/REP-ERU" ["l"="49.086,31.762"]
"White-Link/gpm" ["l"="48.83,32.169"]
"LuoweiZhou/e2e-gLSTM-sc" ["l"="48.394,31.957"]
"Yeon07/RISCLIP" ["l"="62.567,36.317"]
"edchengg/oven_eval" ["l"="48.9,31.637"]
"edchengg/infoseek_eval" ["l"="48.89,31.657"]
"open-vision-language/oven" ["l"="48.886,31.627"]
"MrZilinXiao/AutoVER" ["l"="48.903,31.617"]
"gy20073/compact_bilinear_pooling" ["l"="-52.888,-16.861"]
"aspirinone/CATR.github.io" ["l"="49.339,31.967"]
"open-vision-language/infoseek" ["l"="48.874,31.655"]
"shvdiwnkozbw/SMTC" ["l"="48.842,32.506"]
"ThomasMrY/VCT" ["l"="48.879,32.596"]
"ChengShiest/Zip-Your-CLIP" ["l"="49.107,31.743"]
"linhuixiao/OneRef" ["l"="49.114,31.929"]
"coala/coala" ["l"="-0.022,-31.832"]
"replicate/keepsake" ["l"="45.305,25.954"]
"adeshpande3/Machine-Learning-Links-And-Lessons-Learned" ["l"="47.618,28.679"]
"CVMI-Lab/CoDet" ["l"="48.536,30.261"]
"zhang-tao-whu/DVIS_Plus" ["l"="50.695,30.708"]
"liuting20/DARA" ["l"="49.009,31.769"]
"liuting20/Sparse-Tuning" ["l"="49.009,31.756"]
"liuting20/SwimVG" ["l"="48.995,31.769"]
"btgraham/Batchwise-Dropout" ["l"="23.572,14.886"]
"GunhoChoi/Deep-Learning-For-Beginners" ["l"="48.561,32.351"]
"6004x/jade" ["l"="23.595,14.871"]
"familyld/Awesome-Causal-RL" ["l"="49.145,32.509"]
"libo-huang/Awesome-Causal-Reinforcement-Learning" ["l"="49.166,32.523"]
"apple2373/chainer-caption" ["l"="48.686,31.845"]
"ThomasMrY/DisDiff" ["l"="48.876,32.65"]
"wuancong/FDAE" ["l"="48.877,32.625"]
"LemonATsu/Keras-Image-Caption" ["l"="48.49,31.807"]
"amaiasalvador/imcap_keras" ["l"="48.48,31.787"]
"lxa9867/r2bench" ["l"="49.053,31.739"]
"oarriaga/neural_image_captioning" ["l"="48.514,31.805"]
"Div99/Image-Captioning" ["l"="48.54,31.811"]
"danieljl/keras-image-captioning" ["l"="48.5,31.819"]
"boluoyu/ImageCaption" ["l"="48.554,31.786"]
"fregu856/CS224n_project" ["l"="48.546,31.766"]
"shaohua0116/Activation-Visualization-Histogram" ["l"="46.096,29.172"]
"GeWu-Lab/Stepping-Stones" ["l"="49.284,31.941"]
"Cloud-CV/VQA" ["l"="48.506,32.372"]
"yannqi/Draw-an-Audio-Code" ["l"="49.368,31.941"]
"appletea233/AL-Ref-SAM2" ["l"="49.193,31.962"]
"Mr-Bigworth/MMCA" ["l"="49.137,31.923"]
"nini0919/SemiRES" ["l"="49.225,31.832"]
"nini0919/DiffPNG" ["l"="49.202,31.842"]
"Aria-Zhangjl/E3-FaceNet" ["l"="49.246,31.822"]
"Wentong-DST/im2p" ["l"="48.333,31.895"]
"zhegan27/SCN_for_video_captioning" ["l"="48.074,32.862"]
"Dmmm1997/DRL" ["l"="59.69,9.719"]
"om-ai-lab/GroundVLP" ["l"="49.125,31.944"]
"andypinxinliu/GestureLSM" ["l"="30.541,28.388"]
"lluisgomez/TextTopicNet" ["l"="48.629,32.369"]
"danielgordon10/thor-iqa-cvpr-2018" ["l"="60.13,17.395"]
"danfeiX/scene-graph-TF-release" ["l"="47.555,32.022"]
"kimhc6028/relational-networks" ["l"="23.489,14.801"]
"shtechair/vqa-sva" ["l"="48.626,32.096"]
"PTNobel/AutoDiff" ["l"="48.779,32.342"]
"Shivanshu-Gupta/Visual-Question-Answering" ["l"="48.762,32.139"]
"akosiorek/attend_infer_repeat" ["l"="48.968,32.516"]
"chenxinpeng/Optimization_of_image_description_metrics_using_policy_gradient_methods" ["l"="48.418,31.904"]
"HarshTrivedi/nmn-pytorch" ["l"="48.757,32.252"]
"YuJiang01/n2nmn_pytorch" ["l"="48.752,32.21"]
"ronghanghu/cmn" ["l"="48.737,32.24"]
"ruizhaogit/GuessWhat-TemperedPolicyGradient" ["l"="48.655,32.345"]
"Skaldak/MfH" ["l"="49.064,31.721"]
}