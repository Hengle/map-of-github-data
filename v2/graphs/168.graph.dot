digraph G {
"facebookresearch/video-nonlocal-net" -> "AlexHex7/Non-local_pytorch" ["e"=1]
"facebookresearch/video-nonlocal-net" -> "google-deepmind/kinetics-i3d"
"facebookresearch/video-nonlocal-net" -> "yjxiong/temporal-segment-networks"
"facebookresearch/video-nonlocal-net" -> "facebookresearch/VMZ"
"facebookresearch/video-nonlocal-net" -> "yjxiong/tsn-pytorch"
"facebookresearch/video-nonlocal-net" -> "mit-han-lab/temporal-shift-module"
"facebookresearch/video-nonlocal-net" -> "activitynet/ActivityNet"
"facebookresearch/video-nonlocal-net" -> "open-mmlab/mmaction"
"facebookresearch/video-nonlocal-net" -> "xvjiarui/GCNet" ["e"=1]
"facebookresearch/video-nonlocal-net" -> "zhoubolei/TRN-pytorch"
"facebookresearch/video-nonlocal-net" -> "yjxiong/action-detection"
"facebookresearch/video-nonlocal-net" -> "msracver/Deformable-ConvNets" ["e"=1]
"facebookresearch/video-nonlocal-net" -> "piergiaj/pytorch-i3d"
"facebookresearch/video-nonlocal-net" -> "jinwchoi/awesome-action-recognition"
"facebookresearch/video-nonlocal-net" -> "facebookarchive/C3D"
"open-mmlab/mmaction" -> "yjxiong/temporal-segment-networks"
"open-mmlab/mmaction" -> "yjxiong/tsn-pytorch"
"open-mmlab/mmaction" -> "mit-han-lab/temporal-shift-module"
"open-mmlab/mmaction" -> "jinwchoi/awesome-action-recognition"
"open-mmlab/mmaction" -> "activitynet/ActivityNet"
"open-mmlab/mmaction" -> "facebookresearch/VMZ"
"open-mmlab/mmaction" -> "decisionforce/TPN"
"open-mmlab/mmaction" -> "facebookresearch/SlowFast"
"open-mmlab/mmaction" -> "gsig/PyVideoResearch"
"open-mmlab/mmaction" -> "open-mmlab/mmskeleton" ["e"=1]
"open-mmlab/mmaction" -> "yjxiong/action-detection"
"open-mmlab/mmaction" -> "open-mmlab/mmaction2"
"open-mmlab/mmaction" -> "facebookresearch/video-nonlocal-net"
"open-mmlab/mmaction" -> "zhoubolei/TRN-pytorch"
"open-mmlab/mmaction" -> "google-deepmind/kinetics-i3d"
"kenshohara/3D-ResNets-PyTorch" -> "kenshohara/video-classification-3d-cnn-pytorch"
"kenshohara/3D-ResNets-PyTorch" -> "jinwchoi/awesome-action-recognition"
"kenshohara/3D-ResNets-PyTorch" -> "google-deepmind/kinetics-i3d"
"kenshohara/3D-ResNets-PyTorch" -> "facebookresearch/SlowFast"
"kenshohara/3D-ResNets-PyTorch" -> "open-mmlab/mmaction"
"kenshohara/3D-ResNets-PyTorch" -> "yjxiong/temporal-segment-networks"
"kenshohara/3D-ResNets-PyTorch" -> "jfzhang95/pytorch-video-recognition"
"kenshohara/3D-ResNets-PyTorch" -> "yjxiong/tsn-pytorch"
"kenshohara/3D-ResNets-PyTorch" -> "jeffreyyihuang/two-stream-action-recognition"
"kenshohara/3D-ResNets-PyTorch" -> "facebookresearch/VMZ"
"kenshohara/3D-ResNets-PyTorch" -> "mit-han-lab/temporal-shift-module"
"kenshohara/3D-ResNets-PyTorch" -> "Cadene/pretrained-models.pytorch" ["e"=1]
"kenshohara/3D-ResNets-PyTorch" -> "piergiaj/pytorch-i3d"
"kenshohara/3D-ResNets-PyTorch" -> "facebookresearch/video-nonlocal-net"
"kenshohara/3D-ResNets-PyTorch" -> "activitynet/ActivityNet"
"NVIDIA/nvvl" -> "dukebw/lintel"
"NVIDIA/nvvl" -> "mitmul/pynvvl"
"NVIDIA/nvvl" -> "ignacio-rocco/detectorch" ["e"=1]
"NVIDIA/nvvl" -> "facebookresearch/VMZ"
"NVIDIA/nvvl" -> "chaoyuaw/pytorch-coviar"
"NVIDIA/nvvl" -> "cvondrick/soundnet" ["e"=1]
"NVIDIA/nvvl" -> "jerryli27/TwinGAN" ["e"=1]
"NVIDIA/nvvl" -> "NVIDIA/DALI" ["e"=1]
"NVIDIA/nvvl" -> "wzmsltw/BSN-boundary-sensitive-network"
"NVIDIA/nvvl" -> "yjxiong/action-detection"
"qijiezhao/pseudo-3d-pytorch" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"qijiezhao/pseudo-3d-pytorch" -> "facebookresearch/VMZ"
"qijiezhao/pseudo-3d-pytorch" -> "zhoubolei/TRN-pytorch"
"qijiezhao/pseudo-3d-pytorch" -> "yjxiong/tsn-pytorch"
"qijiezhao/pseudo-3d-pytorch" -> "irhum/R2Plus1D-PyTorch"
"qijiezhao/pseudo-3d-pytorch" -> "hassony2/kinetics_i3d_pytorch"
"qijiezhao/pseudo-3d-pytorch" -> "hx173149/C3D-tensorflow"
"qijiezhao/pseudo-3d-pytorch" -> "bryanyzhu/two-stream-pytorch"
"qijiezhao/pseudo-3d-pytorch" -> "jfzhang95/pytorch-video-recognition"
"qijiezhao/pseudo-3d-pytorch" -> "mzolfaghari/ECO-efficient-video-understanding"
"qijiezhao/pseudo-3d-pytorch" -> "activitynet/ActivityNet"
"qijiezhao/pseudo-3d-pytorch" -> "yjxiong/temporal-segment-networks"
"qijiezhao/pseudo-3d-pytorch" -> "feichtenhofer/twostreamfusion"
"qijiezhao/pseudo-3d-pytorch" -> "facebookarchive/C3D"
"qijiezhao/pseudo-3d-pytorch" -> "google-deepmind/kinetics-i3d"
"VisionLearningGroup/R-C3D" -> "yjxiong/action-detection"
"VisionLearningGroup/R-C3D" -> "sunnyxiaohu/R-C3D.pytorch"
"VisionLearningGroup/R-C3D" -> "shyamal-b/ss-tad"
"VisionLearningGroup/R-C3D" -> "piergiaj/super-events-cvpr18"
"VisionLearningGroup/R-C3D" -> "escorciav/daps"
"VisionLearningGroup/R-C3D" -> "wzmsltw/BSN-boundary-sensitive-network"
"VisionLearningGroup/R-C3D" -> "jiyanggao/CBR"
"VisionLearningGroup/R-C3D" -> "zhengshou/scnn"
"VisionLearningGroup/R-C3D" -> "wanglimin/UntrimmedNet"
"VisionLearningGroup/R-C3D" -> "shyamal-b/sst"
"VisionLearningGroup/R-C3D" -> "ranjaykrishna/SST"
"VisionLearningGroup/R-C3D" -> "jiyanggao/TURN-TAP"
"VisionLearningGroup/R-C3D" -> "ColumbiaDVMM/CDC"
"VisionLearningGroup/R-C3D" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"VisionLearningGroup/R-C3D" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"TianzhongSong/C3D-keras" -> "axon-research/c3d-keras"
"TianzhongSong/C3D-keras" -> "FingerRec/real_time_video_action_recognition"
"TianzhongSong/C3D-keras" -> "adamcasson/c3d"
"TianzhongSong/C3D-keras" -> "rekon/T3D-keras"
"TianzhongSong/C3D-keras" -> "2012013382/C3D-Tensorflow-slim"
"TianzhongSong/C3D-keras" -> "hx173149/C3D-tensorflow"
"TianzhongSong/C3D-keras" -> "wushidonguc/two-stream-action-recognition-keras"
"TianzhongSong/C3D-keras" -> "dlpbc/keras-kinetics-i3d"
"dlpbc/keras-kinetics-i3d" -> "OanaIgnat/I3D_Keras"
"dlpbc/keras-kinetics-i3d" -> "USTC-Video-Understanding/I3D_Finetune"
"dlpbc/keras-kinetics-i3d" -> "qijiezhao/py-denseflow"
"dlpbc/keras-kinetics-i3d" -> "LossNAN/I3D-Tensorflow"
"dlpbc/keras-kinetics-i3d" -> "JihongJu/keras-resnet3d"
"dlpbc/keras-kinetics-i3d" -> "yoosan/i3d-tensorflow"
"dlpbc/keras-kinetics-i3d" -> "Rhythmblue/i3d_finetune"
"dlpbc/keras-kinetics-i3d" -> "google-deepmind/kinetics-i3d"
"dlpbc/keras-kinetics-i3d" -> "TianzhongSong/C3D-keras"
"dlpbc/keras-kinetics-i3d" -> "axon-research/c3d-keras"
"dlpbc/keras-kinetics-i3d" -> "FrederikSchorr/sign-language" ["e"=1]
"iboing/CliqueNet" -> "visinf/dpp"
"iboing/CliqueNet" -> "shaohua0116/Group-Normalization-Tensorflow"
"iboing/CliqueNet" -> "Betterthinking/CliqueNet-pytorch"
"shaohua0116/Group-Normalization-Tensorflow" -> "taokong/group_normalization"
"FingerRec/real_time_video_action_recognition" -> "danbochman/Real-Time-Action-Recognition"
"FingerRec/real_time_video_action_recognition" -> "TianzhongSong/C3D-keras"
"healthDataScience/deep-learning-HAR" -> "guillaume-chevalier/LSTM-Human-Activity-Recognition"
"healthDataScience/deep-learning-HAR" -> "STRCWearlab/DeepConvLSTM"
"healthDataScience/deep-learning-HAR" -> "bhimmetoglu/time-series-medicine"
"healthDataScience/deep-learning-HAR" -> "RobRomijnders/LSTM_tsc" ["e"=1]
"healthDataScience/deep-learning-HAR" -> "guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs"
"healthDataScience/deep-learning-HAR" -> "bhimmetoglu/talks-and-lectures"
"healthDataScience/deep-learning-HAR" -> "aqibsaeed/Multilabel-timeseries-classification-with-LSTM" ["e"=1]
"healthDataScience/deep-learning-HAR" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"healthDataScience/deep-learning-HAR" -> "hfawaz/bigdata18" ["e"=1]
"healthDataScience/deep-learning-HAR" -> "loliverhennigh/Convolutional-LSTM-in-Tensorflow" ["e"=1]
"healthDataScience/deep-learning-HAR" -> "curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"
"healthDataScience/deep-learning-HAR" -> "RobRomijnders/CNN_tsc" ["e"=1]
"healthDataScience/deep-learning-HAR" -> "Koutoulakis/Deep-Learning-for-Human-Activity-Recognition"
"healthDataScience/deep-learning-HAR" -> "titu1994/LSTM-FCN" ["e"=1]
"healthDataScience/deep-learning-HAR" -> "zjrn/LSTM-CNN_CLASSIFICATION" ["e"=1]
"mit-han-lab/temporal-shift-module" -> "yjxiong/temporal-segment-networks"
"mit-han-lab/temporal-shift-module" -> "open-mmlab/mmaction"
"mit-han-lab/temporal-shift-module" -> "yjxiong/tsn-pytorch"
"mit-han-lab/temporal-shift-module" -> "facebookresearch/SlowFast"
"mit-han-lab/temporal-shift-module" -> "zhoubolei/TRN-pytorch"
"mit-han-lab/temporal-shift-module" -> "facebookresearch/video-nonlocal-net"
"mit-han-lab/temporal-shift-module" -> "facebookresearch/VMZ"
"mit-han-lab/temporal-shift-module" -> "open-mmlab/mmaction2"
"mit-han-lab/temporal-shift-module" -> "activitynet/ActivityNet"
"mit-han-lab/temporal-shift-module" -> "jinwchoi/awesome-action-recognition"
"mit-han-lab/temporal-shift-module" -> "MCG-NJU/TDN"
"mit-han-lab/temporal-shift-module" -> "piergiaj/pytorch-i3d"
"mit-han-lab/temporal-shift-module" -> "google-deepmind/kinetics-i3d"
"mit-han-lab/temporal-shift-module" -> "decisionforce/TPN"
"mit-han-lab/temporal-shift-module" -> "kenshohara/3D-ResNets-PyTorch"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "qijiezhao/pseudo-3d-pytorch"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "wanglimin/ARTNet"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "facebookarchive/C3D"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "VisionLearningGroup/R-C3D"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "facebookresearch/VMZ"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "wanglimin/UntrimmedNet"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "google-deepmind/kinetics-i3d"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "activitynet/ActivityNet"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "rohitgirdhar/ActionVLAD"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "mzolfaghari/ECO-efficient-video-understanding"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "yjxiong/action-detection"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "yjxiong/tsn-pytorch"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "yjxiong/anet2016-cuhk"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "feichtenhofer/twostreamfusion"
"ZhaofanQiu/pseudo-3d-residual-networks" -> "hx173149/C3D-tensorflow"
"yoosan/video-understanding-dataset" -> "zhoubolei/moments_models"
"yoosan/video-understanding-dataset" -> "activitynet/ActivityNet"
"yoosan/video-understanding-dataset" -> "antoine77340/Youtube-8M-WILLOW"
"yoosan/video-understanding-dataset" -> "feichtenhofer/gpu_flow"
"yoosan/video-understanding-dataset" -> "mzolfaghari/ECO-efficient-video-understanding"
"yoosan/video-understanding-dataset" -> "yjxiong/tsn-pytorch"
"yoosan/video-understanding-dataset" -> "yjxiong/action-detection"
"yoosan/video-understanding-dataset" -> "gsig/PyVideoResearch"
"yoosan/video-understanding-dataset" -> "LisaAnne/LocalizingMoments" ["e"=1]
"yoosan/video-understanding-dataset" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"yoosan/video-understanding-dataset" -> "chaoyuaw/pytorch-coviar"
"yoosan/video-understanding-dataset" -> "wanglimin/ARTNet"
"yoosan/video-understanding-dataset" -> "qijiezhao/pseudo-3d-pytorch"
"yoosan/video-understanding-dataset" -> "sujiongming/awesome-video-understanding"
"yoosan/video-understanding-dataset" -> "Alvin-Zeng/PGCN"
"linrongc/youtube-8m" -> "miha-skalic/youtube8mchallenge"
"linrongc/youtube-8m" -> "antoine77340/Youtube-8M-WILLOW"
"linrongc/youtube-8m" -> "antoine77340/LOUPE"
"linrongc/youtube-8m" -> "zhangyaoyuan/NextVLAD-Attention-Model"
"linrongc/youtube-8m" -> "rohitgirdhar/ActionVLAD"
"linrongc/youtube-8m" -> "lyakaap/NetVLAD-pytorch" ["e"=1]
"linrongc/youtube-8m" -> "linrongc/solution_youtube8m_v3"
"linrongc/youtube-8m" -> "zhongzhh8/Video-classification-with-knowledge-distillation"
"linrongc/youtube-8m" -> "wangheda/youtube-8m"
"miha-skalic/youtube8mchallenge" -> "antoine77340/Youtube-8M-WILLOW"
"miha-skalic/youtube8mchallenge" -> "linrongc/youtube-8m"
"miha-skalic/youtube8mchallenge" -> "wangheda/youtube-8m"
"miha-skalic/youtube8mchallenge" -> "antoine77340/LOUPE"
"miha-skalic/youtube8mchallenge" -> "baidu/Youtube-8M"
"miha-skalic/youtube8mchallenge" -> "google/youtube-8m"
"dmlc/decord" -> "facebookresearch/pytorchvideo"
"dmlc/decord" -> "open-mmlab/mmaction"
"dmlc/decord" -> "facebookresearch/SlowFast"
"dmlc/decord" -> "PyAV-Org/PyAV" ["e"=1]
"dmlc/decord" -> "MCG-NJU/VideoMAE"
"dmlc/decord" -> "open-mmlab/mmaction2"
"dmlc/decord" -> "OpenGVLab/InternVideo" ["e"=1]
"dmlc/decord" -> "activitynet/ActivityNet"
"dmlc/decord" -> "facebookresearch/TimeSformer"
"dmlc/decord" -> "NVIDIA/DALI" ["e"=1]
"dmlc/decord" -> "NVIDIA/VideoProcessingFramework" ["e"=1]
"dmlc/decord" -> "mit-han-lab/temporal-shift-module"
"dmlc/decord" -> "Breakthrough/PySceneDetect" ["e"=1]
"dmlc/decord" -> "yjxiong/temporal-segment-networks"
"dmlc/decord" -> "google-deepmind/kinetics-i3d"
"OValery16/Tutorial-about-3D-convolutional-network" -> "kcct-fujimotolab/3DCNN"
"OValery16/Tutorial-about-3D-convolutional-network" -> "ms3001/DeepHandGestureRecognition" ["e"=1]
"OValery16/Tutorial-about-3D-convolutional-network" -> "saetlan/20BN-jester"
"OValery16/Tutorial-about-3D-convolutional-network" -> "udacity/CVND---Gesture-Recognition" ["e"=1]
"OValery16/Tutorial-about-3D-convolutional-network" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"okankop/Efficient-3DCNNs" -> "jfzhang95/pytorch-video-recognition"
"okankop/Efficient-3DCNNs" -> "shijianjian/EfficientNet-PyTorch-3D" ["e"=1]
"okankop/Efficient-3DCNNs" -> "kenshohara/3D-ResNets-PyTorch"
"okankop/Efficient-3DCNNs" -> "wei-tim/YOWO"
"okankop/Efficient-3DCNNs" -> "kenshohara/video-classification-3d-cnn-pytorch"
"okankop/Efficient-3DCNNs" -> "kcct-fujimotolab/3DCNN"
"okankop/Efficient-3DCNNs" -> "mit-han-lab/temporal-shift-module"
"okankop/Efficient-3DCNNs" -> "xmuyzz/3D-CNN-PyTorch"
"okankop/Efficient-3DCNNs" -> "facebookresearch/VMZ"
"okankop/Efficient-3DCNNs" -> "piergiaj/pytorch-i3d"
"okankop/Efficient-3DCNNs" -> "MECLabTUDA/M3d-Cam" ["e"=1]
"okankop/Efficient-3DCNNs" -> "zhoubolei/TRN-pytorch"
"okankop/Efficient-3DCNNs" -> "okankop/vidaug"
"okankop/Efficient-3DCNNs" -> "HHTseng/video-classification"
"okankop/Efficient-3DCNNs" -> "irhum/R2Plus1D-PyTorch"
"jeffdonahue/caffe" -> "LisaAnne/lisa-caffe-public"
"jeffdonahue/caffe" -> "junhyukoh/caffe-lstm"
"jeffdonahue/caffe" -> "vsubhashini/caffe" ["e"=1]
"jeffdonahue/caffe" -> "skaae/recurrent-spatial-transformer-code" ["e"=1]
"jeffdonahue/caffe" -> "Russell91/nlpcaffe"
"HHTseng/video-classification" -> "kenshohara/video-classification-3d-cnn-pytorch"
"HHTseng/video-classification" -> "feichtenhofer/twostreamfusion"
"HHTseng/video-classification" -> "jeffreyyihuang/two-stream-action-recognition"
"HHTseng/video-classification" -> "pranoyr/cnn-lstm"
"HHTseng/video-classification" -> "jfzhang95/pytorch-video-recognition"
"HHTseng/video-classification" -> "harvitronix/five-video-classification-methods"
"HHTseng/video-classification" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"HHTseng/video-classification" -> "woodfrog/ActionRecognition"
"HHTseng/video-classification" -> "sujiongming/UCF-101_video_classification"
"HHTseng/video-classification" -> "google-deepmind/kinetics-i3d"
"HHTseng/video-classification" -> "jinwchoi/awesome-action-recognition"
"HHTseng/video-classification" -> "kenshohara/3D-ResNets-PyTorch"
"HHTseng/video-classification" -> "okankop/Efficient-3DCNNs"
"HHTseng/video-classification" -> "bryanyzhu/two-stream-pytorch"
"HHTseng/video-classification" -> "eriklindernoren/Action-Recognition"
"facebookresearch/VMZ" -> "irhum/R2Plus1D-PyTorch"
"facebookresearch/VMZ" -> "google-deepmind/kinetics-i3d"
"facebookresearch/VMZ" -> "yjxiong/temporal-segment-networks"
"facebookresearch/VMZ" -> "open-mmlab/mmaction"
"facebookresearch/VMZ" -> "facebookresearch/video-nonlocal-net"
"facebookresearch/VMZ" -> "yjxiong/tsn-pytorch"
"facebookresearch/VMZ" -> "activitynet/ActivityNet"
"facebookresearch/VMZ" -> "moabitcoin/ig65m-pytorch" ["e"=1]
"facebookresearch/VMZ" -> "qijiezhao/pseudo-3d-pytorch"
"facebookresearch/VMZ" -> "facebookarchive/C3D"
"facebookresearch/VMZ" -> "zhoubolei/TRN-pytorch"
"facebookresearch/VMZ" -> "gsig/PyVideoResearch"
"facebookresearch/VMZ" -> "jfzhang95/pytorch-video-recognition"
"facebookresearch/VMZ" -> "mit-han-lab/temporal-shift-module"
"facebookresearch/VMZ" -> "yjxiong/action-detection"
"okankop/vidaug" -> "jayChung0302/videomix"
"okankop/vidaug" -> "okankop/Efficient-3DCNNs"
"okankop/vidaug" -> "gsig/PyVideoResearch"
"okankop/vidaug" -> "piergiaj/pytorch-i3d"
"okankop/vidaug" -> "hassony2/torch_videovision"
"okankop/vidaug" -> "holistic-video-understanding/HVU-Dataset"
"okankop/vidaug" -> "craston/MARS"
"okankop/vidaug" -> "okankop/MFF-pytorch" ["e"=1]
"okankop/vidaug" -> "facebookresearch/VMZ"
"okankop/vidaug" -> "Atze00/MoViNet-pytorch"
"okankop/vidaug" -> "wei-tim/YOWO"
"okankop/vidaug" -> "IBM/action-recognition-pytorch"
"okankop/vidaug" -> "decisionforce/TPN"
"okankop/vidaug" -> "chaoyuaw/pytorch-coviar"
"okankop/vidaug" -> "shayanalibhatti/Video-Augmentation-Code"
"cvdfoundation/ava-dataset" -> "activitynet/ActivityNet"
"cvdfoundation/ava-dataset" -> "NVlabs/STEP"
"cvdfoundation/ava-dataset" -> "SmartPorridge/google-AVA-Dataset-downloader"
"cvdfoundation/ava-dataset" -> "vkalogeiton/caffe"
"cvdfoundation/ava-dataset" -> "MVIG-SJTU/AlphAction"
"cvdfoundation/ava-dataset" -> "TaoRuijie/TalkNet-ASD"
"cvdfoundation/ava-dataset" -> "wei-tim/YOWO"
"cvdfoundation/ava-dataset" -> "fuankarion/active-speakers-context"
"cvdfoundation/ava-dataset" -> "kevinlin311tw/ava-dataset-tool"
"cvdfoundation/ava-dataset" -> "Siyu-C/ACAR-Net"
"cvdfoundation/ava-dataset" -> "gurkirt/realtime-action-detection"
"cvdfoundation/ava-dataset" -> "oulutan/ACAM_Demo"
"cvdfoundation/ava-dataset" -> "cvlab-columbia/oops"
"cvdfoundation/ava-dataset" -> "r1c7/SlowFastNetworks"
"cvdfoundation/ava-dataset" -> "gurkirt/corrected-UCF101-Annots"
"oulutan/ACAM_Demo" -> "oulutan/ActorConditionedAttentionMaps"
"oulutan/ACAM_Demo" -> "gurkirt/realtime-action-detection"
"oulutan/ACAM_Demo" -> "NVlabs/STEP"
"oulutan/ACAM_Demo" -> "KevinDuarte/VideoCapsuleNet"
"tomar840/two-stream-fusion-for-action-recognition-in-videos" -> "wushidonguc/two-stream-action-recognition-keras"
"tomar840/two-stream-fusion-for-action-recognition-in-videos" -> "mohammed-elkomy/two-stream-action-recognition"
"tomar840/two-stream-fusion-for-action-recognition-in-videos" -> "bryanyzhu/two-stream-pytorch"
"tomar840/two-stream-fusion-for-action-recognition-in-videos" -> "denizzagli/Spatial-Temporal-CNN"
"tomar840/two-stream-fusion-for-action-recognition-in-videos" -> "jeffreyyihuang/two-stream-action-recognition"
"jfzhang95/pytorch-video-recognition" -> "DavideA/c3d-pytorch"
"jfzhang95/pytorch-video-recognition" -> "facebookresearch/VMZ"
"jfzhang95/pytorch-video-recognition" -> "yjxiong/tsn-pytorch"
"jfzhang95/pytorch-video-recognition" -> "piergiaj/pytorch-i3d"
"jfzhang95/pytorch-video-recognition" -> "jeffreyyihuang/two-stream-action-recognition"
"jfzhang95/pytorch-video-recognition" -> "kenshohara/video-classification-3d-cnn-pytorch"
"jfzhang95/pytorch-video-recognition" -> "r1c7/SlowFastNetworks"
"jfzhang95/pytorch-video-recognition" -> "kenshohara/3D-ResNets-PyTorch"
"jfzhang95/pytorch-video-recognition" -> "bryanyzhu/two-stream-pytorch"
"jfzhang95/pytorch-video-recognition" -> "google-deepmind/kinetics-i3d"
"jfzhang95/pytorch-video-recognition" -> "irhum/R2Plus1D-PyTorch"
"jfzhang95/pytorch-video-recognition" -> "yjxiong/temporal-segment-networks"
"jfzhang95/pytorch-video-recognition" -> "qijiezhao/pseudo-3d-pytorch"
"jfzhang95/pytorch-video-recognition" -> "open-mmlab/mmaction"
"jfzhang95/pytorch-video-recognition" -> "facebookarchive/C3D"
"piergiaj/pytorch-i3d" -> "google-deepmind/kinetics-i3d"
"piergiaj/pytorch-i3d" -> "hassony2/kinetics_i3d_pytorch"
"piergiaj/pytorch-i3d" -> "yjxiong/tsn-pytorch"
"piergiaj/pytorch-i3d" -> "yjxiong/temporal-segment-networks"
"piergiaj/pytorch-i3d" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"piergiaj/pytorch-i3d" -> "jfzhang95/pytorch-video-recognition"
"piergiaj/pytorch-i3d" -> "open-mmlab/mmaction"
"piergiaj/pytorch-i3d" -> "facebookresearch/VMZ"
"piergiaj/pytorch-i3d" -> "activitynet/ActivityNet"
"piergiaj/pytorch-i3d" -> "tomrunia/PyTorchConv3D"
"piergiaj/pytorch-i3d" -> "DavideA/c3d-pytorch"
"piergiaj/pytorch-i3d" -> "v-iashin/video_features" ["e"=1]
"piergiaj/pytorch-i3d" -> "mit-han-lab/temporal-shift-module"
"piergiaj/pytorch-i3d" -> "Finspire13/pytorch-i3d-feature-extraction"
"piergiaj/pytorch-i3d" -> "zhoubolei/TRN-pytorch"
"girishp92/Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones" -> "lingjuanlv/LSTM-CNN-model-for-HAR"
"girishp92/Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones" -> "curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"
"mmalekzadeh/motion-sense" -> "mmalekzadeh/dana"
"mmalekzadeh/motion-sense" -> "arturjordao/WearableSensorData"
"mmalekzadeh/motion-sense" -> "ani8897/Human-Activity-Recognition"
"mmalekzadeh/motion-sense" -> "haoranD/Awesome-Human-Activity-Recognition"
"mmalekzadeh/motion-sense" -> "STRCWearlab/DeepConvLSTM"
"mmalekzadeh/motion-sense" -> "takumiw/Deep-Learning-for-Human-Activity-Recognition"
"mmalekzadeh/motion-sense" -> "guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs"
"mmalekzadeh/motion-sense" -> "iantangc/ContrastiveLearningHAR"
"mmalekzadeh/motion-sense" -> "iantangc/SelfHAR"
"mmalekzadeh/motion-sense" -> "ma-shamshiri/Human-Activity-Recognition"
"mmalekzadeh/motion-sense" -> "curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"
"mmalekzadeh/motion-sense" -> "dapowan/LIMU-BERT-Public" ["e"=1]
"mmalekzadeh/motion-sense" -> "windofshadow/THAT" ["e"=1]
"mmalekzadeh/motion-sense" -> "dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch"
"mmalekzadeh/motion-sense" -> "sztyler/sensordatacollector"
"ni79ls/har-keras-cnn" -> "markub3327/HAR-Transformer"
"ni79ls/har-keras-cnn" -> "meitetsu3/1DCNN"
"kenshohara/video-classification-3d-cnn-pytorch" -> "kenshohara/3D-ResNets-PyTorch"
"kenshohara/video-classification-3d-cnn-pytorch" -> "HHTseng/video-classification"
"kenshohara/video-classification-3d-cnn-pytorch" -> "jfzhang95/pytorch-video-recognition"
"kenshohara/video-classification-3d-cnn-pytorch" -> "google-deepmind/kinetics-i3d"
"kenshohara/video-classification-3d-cnn-pytorch" -> "yjxiong/tsn-pytorch"
"kenshohara/video-classification-3d-cnn-pytorch" -> "yjxiong/temporal-segment-networks"
"kenshohara/video-classification-3d-cnn-pytorch" -> "facebookresearch/VMZ"
"kenshohara/video-classification-3d-cnn-pytorch" -> "piergiaj/pytorch-i3d"
"kenshohara/video-classification-3d-cnn-pytorch" -> "harvitronix/five-video-classification-methods"
"kenshohara/video-classification-3d-cnn-pytorch" -> "bryanyzhu/two-stream-pytorch"
"kenshohara/video-classification-3d-cnn-pytorch" -> "jeffreyyihuang/two-stream-action-recognition"
"kenshohara/video-classification-3d-cnn-pytorch" -> "qijiezhao/pseudo-3d-pytorch"
"kenshohara/video-classification-3d-cnn-pytorch" -> "facebookarchive/C3D"
"kenshohara/video-classification-3d-cnn-pytorch" -> "activitynet/ActivityNet"
"kenshohara/video-classification-3d-cnn-pytorch" -> "DavideA/c3d-pytorch"
"zhoubolei/TRN-pytorch" -> "decisionforce/TPN"
"zhoubolei/TRN-pytorch" -> "yjxiong/tsn-pytorch"
"zhoubolei/TRN-pytorch" -> "yjxiong/temporal-segment-networks"
"zhoubolei/TRN-pytorch" -> "mzolfaghari/ECO-efficient-video-understanding"
"zhoubolei/TRN-pytorch" -> "mzolfaghari/ECO-pytorch"
"zhoubolei/TRN-pytorch" -> "mit-han-lab/temporal-shift-module"
"zhoubolei/TRN-pytorch" -> "yjxiong/action-detection"
"zhoubolei/TRN-pytorch" -> "qijiezhao/pseudo-3d-pytorch"
"zhoubolei/TRN-pytorch" -> "facebookresearch/VMZ"
"zhoubolei/TRN-pytorch" -> "zhoubolei/moments_models"
"zhoubolei/TRN-pytorch" -> "open-mmlab/mmaction"
"zhoubolei/TRN-pytorch" -> "activitynet/ActivityNet"
"zhoubolei/TRN-pytorch" -> "facebookresearch/video-long-term-feature-banks"
"zhoubolei/TRN-pytorch" -> "google-deepmind/kinetics-i3d"
"zhoubolei/TRN-pytorch" -> "facebookresearch/video-nonlocal-net"
"zhoubolei/moments_models" -> "zhoubolei/TRN-pytorch"
"zhoubolei/moments_models" -> "yoosan/video-understanding-dataset"
"zhoubolei/moments_models" -> "gsig/charades-algorithms"
"zhoubolei/moments_models" -> "wanglimin/ARTNet"
"zhoubolei/moments_models" -> "gsig/PyVideoResearch"
"zhoubolei/moments_models" -> "facebookresearch/VMZ"
"zhoubolei/moments_models" -> "wanglimin/UntrimmedNet"
"zhoubolei/moments_models" -> "yjxiong/temporal-segment-networks"
"zhoubolei/moments_models" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"zhoubolei/moments_models" -> "activitynet/ActivityNet"
"zhoubolei/moments_models" -> "qijiezhao/pseudo-3d-pytorch"
"zhoubolei/moments_models" -> "rohitgirdhar/AttentionalPoolingAction"
"zhoubolei/moments_models" -> "antoine77340/Youtube-8M-WILLOW"
"zhoubolei/moments_models" -> "feichtenhofer/gpu_flow"
"zhoubolei/moments_models" -> "hassony2/kinetics_i3d_pytorch"
"hassony2/torch_videovision" -> "yuxumin/CoRe" ["e"=1]
"hassony2/torch_videovision" -> "hassony2/inflated_convnets_pytorch"
"hassony2/torch_videovision" -> "xujinglin/FineDiving" ["e"=1]
"hassony2/torch_videovision" -> "irhum/R2Plus1D-PyTorch"
"hassony2/torch_videovision" -> "baiyang4/aqa_tpt" ["e"=1]
"hassony2/torch_videovision" -> "rimchang/kinetics-i3d-Pytorch"
"hassony2/torch_videovision" -> "nzl-thu/MUSDL" ["e"=1]
"hassony2/torch_videovision" -> "hassony2/kinetics_i3d_pytorch"
"hassony2/torch_videovision" -> "gsig/PyVideoResearch"
"hassony2/torch_videovision" -> "coderSkyChen/Action_Recognition_Zoo"
"hassony2/torch_videovision" -> "vt-vl-lab/SDN"
"chaoyuaw/pytorch-coviar" -> "yjxiong/tsn-pytorch"
"chaoyuaw/pytorch-coviar" -> "piergiaj/representation-flow-cvpr19"
"chaoyuaw/pytorch-coviar" -> "wanglimin/ARTNet"
"chaoyuaw/pytorch-coviar" -> "facebookresearch/dmc-net"
"chaoyuaw/pytorch-coviar" -> "facebookresearch/VMZ"
"chaoyuaw/pytorch-coviar" -> "feichtenhofer/twostreamfusion"
"chaoyuaw/pytorch-coviar" -> "irhum/R2Plus1D-PyTorch"
"chaoyuaw/pytorch-coviar" -> "cypw/PyTorch-MFNet"
"chaoyuaw/pytorch-coviar" -> "jeffreyyihuang/two-stream-action-recognition"
"chaoyuaw/pytorch-coviar" -> "zhoubolei/TRN-pytorch"
"chaoyuaw/pytorch-coviar" -> "mzolfaghari/ECO-efficient-video-understanding"
"chaoyuaw/pytorch-coviar" -> "gsig/PyVideoResearch"
"chaoyuaw/pytorch-coviar" -> "open-mmlab/mmaction"
"chaoyuaw/pytorch-coviar" -> "yjxiong/action-detection"
"chaoyuaw/pytorch-coviar" -> "yjxiong/temporal-segment-networks"
"irhum/R2Plus1D-PyTorch" -> "facebookresearch/VMZ"
"irhum/R2Plus1D-PyTorch" -> "qijiezhao/pseudo-3d-pytorch"
"irhum/R2Plus1D-PyTorch" -> "leftthomas/R2Plus1D-C3D"
"irhum/R2Plus1D-PyTorch" -> "r1c7/SlowFastNetworks"
"irhum/R2Plus1D-PyTorch" -> "jfzhang95/pytorch-video-recognition"
"irhum/R2Plus1D-PyTorch" -> "piergiaj/representation-flow-cvpr19"
"irhum/R2Plus1D-PyTorch" -> "alexandonian/pretorched-x"
"irhum/R2Plus1D-PyTorch" -> "zhoubolei/TRN-pytorch"
"irhum/R2Plus1D-PyTorch" -> "coderSkyChen/Action_Recognition_Zoo"
"irhum/R2Plus1D-PyTorch" -> "chaoyuaw/pytorch-coviar"
"irhum/R2Plus1D-PyTorch" -> "mzolfaghari/ECO-efficient-video-understanding"
"irhum/R2Plus1D-PyTorch" -> "yjxiong/tsn-pytorch"
"irhum/R2Plus1D-PyTorch" -> "MohsenFayyaz89/T3D"
"irhum/R2Plus1D-PyTorch" -> "wanglimin/ARTNet"
"irhum/R2Plus1D-PyTorch" -> "yjxiong/temporal-segment-networks"
"Guocode/SlowFast-Networks" -> "r1c7/SlowFastNetworks"
"Guocode/SlowFast-Networks" -> "ZhaofanQiu/local-and-global-diffusion-networks"
"Guocode/SlowFast-Networks" -> "JJBOY/SlowFast-Network"
"mzolfaghari/ECO-pytorch" -> "mzolfaghari/ECO-efficient-video-understanding"
"mzolfaghari/ECO-pytorch" -> "zhang-can/ECO-pytorch"
"mzolfaghari/ECO-pytorch" -> "zhoubolei/TRN-pytorch"
"mzolfaghari/ECO-pytorch" -> "decisionforce/TPN"
"mzolfaghari/ECO-pytorch" -> "piergiaj/representation-flow-cvpr19"
"mzolfaghari/ECO-pytorch" -> "yjxiong/tsn-pytorch"
"mzolfaghari/ECO-pytorch" -> "coderSkyChen/Action_Recognition_Zoo"
"mzolfaghari/ECO-pytorch" -> "yjxiong/temporal-segment-networks"
"mzolfaghari/ECO-pytorch" -> "craston/MARS"
"mzolfaghari/ECO-pytorch" -> "yjxiong/action-detection"
"mzolfaghari/ECO-pytorch" -> "r1c7/SlowFastNetworks"
"mzolfaghari/ECO-pytorch" -> "open-mmlab/mmaction"
"mzolfaghari/ECO-pytorch" -> "mit-han-lab/temporal-shift-module"
"mzolfaghari/ECO-pytorch" -> "irhum/R2Plus1D-PyTorch"
"mzolfaghari/ECO-pytorch" -> "wzmsltw/BSN-boundary-sensitive-network"
"Rheelt/Materials-Temporal-Action-Detection" -> "sujoyp/wtalc-pytorch"
"Rheelt/Materials-Temporal-Action-Detection" -> "JJBOY/BMN-Boundary-Matching-Network"
"Rheelt/Materials-Temporal-Action-Detection" -> "Finspire13/CMCS-Temporal-Action-Localization"
"Rheelt/Materials-Temporal-Action-Detection" -> "Alvin-Zeng/PGCN"
"Rheelt/Materials-Temporal-Action-Detection" -> "Tencent/ActionDetection-DBG"
"Rheelt/Materials-Temporal-Action-Detection" -> "frostinassiky/gtad"
"Rheelt/Materials-Temporal-Action-Detection" -> "JiaHeeeee/Deep_Learning_Temporal_Action_Detection"
"Rheelt/Materials-Temporal-Action-Detection" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"Rheelt/Materials-Temporal-Action-Detection" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"Rheelt/Materials-Temporal-Action-Detection" -> "Pilhyeon/BaSNet-pytorch"
"Rheelt/Materials-Temporal-Action-Detection" -> "piergiaj/tgm-icml19"
"Rheelt/Materials-Temporal-Action-Detection" -> "zhengshou/AutoLoc"
"Rheelt/Materials-Temporal-Action-Detection" -> "yjxiong/action-detection"
"Rheelt/Materials-Temporal-Action-Detection" -> "wzmsltw/BSN-boundary-sensitive-network"
"Rheelt/Materials-Temporal-Action-Detection" -> "bellos1203/STPN"
"piergiaj/tgm-icml19" -> "Rheelt/Materials-Temporal-Action-Detection"
"piergiaj/tgm-icml19" -> "JunLi-Galios/CDFL"
"piergiaj/tgm-icml19" -> "demianzhang/weakly-action-localization"
"piergiaj/tgm-icml19" -> "JiaHeeeee/Deep_Learning_Temporal_Action_Detection"
"piergiaj/tgm-icml19" -> "jiyanggao/CBR"
"piergiaj/tgm-icml19" -> "Finspire13/CMCS-Temporal-Action-Localization"
"piergiaj/tgm-icml19" -> "sujoyp/wtalc-pytorch"
"piergiaj/tgm-icml19" -> "jiyanggao/CTAP"
"deadskull7/Human-Activity-Recognition-with-Neural-Network-using-Gyroscopic-and-Accelerometer-variables" -> "ani8897/Human-Activity-Recognition"
"deadskull7/Human-Activity-Recognition-with-Neural-Network-using-Gyroscopic-and-Accelerometer-variables" -> "UdiBhaskar/Human-Activity-Recognition--Using-Deep-NN"
"hangzhaomit/HACS-dataset" -> "naraysa/3c-net"
"hangzhaomit/HACS-dataset" -> "Finspire13/CMCS-Temporal-Action-Localization"
"hangzhaomit/HACS-dataset" -> "sujoyp/wtalc-pytorch"
"hangzhaomit/HACS-dataset" -> "Alvin-Zeng/PGCN"
"hangzhaomit/HACS-dataset" -> "wzmsltw/BSN-boundary-sensitive-network"
"hangzhaomit/HACS-dataset" -> "wanglimin/UntrimmedNet"
"hangzhaomit/HACS-dataset" -> "Rheelt/Materials-Temporal-Action-Detection"
"Wizaron/deep-forecast-pytorch" -> "fengjiqiang/LSTM-Wind-Speed-Forecasting"
"Wizaron/deep-forecast-pytorch" -> "amirstar/Deep-Forecast"
"Wizaron/deep-forecast-pytorch" -> "willfleury/wind-forecasting"
"Wizaron/deep-forecast-pytorch" -> "cigroup-ol/windml" ["e"=1]
"hassony2/kinetics_i3d_pytorch" -> "piergiaj/pytorch-i3d"
"hassony2/kinetics_i3d_pytorch" -> "google-deepmind/kinetics-i3d"
"hassony2/kinetics_i3d_pytorch" -> "qijiezhao/pseudo-3d-pytorch"
"hassony2/kinetics_i3d_pytorch" -> "yjxiong/tsn-pytorch"
"hassony2/kinetics_i3d_pytorch" -> "zhoubolei/TRN-pytorch"
"hassony2/kinetics_i3d_pytorch" -> "yjxiong/temporal-segment-networks"
"hassony2/kinetics_i3d_pytorch" -> "open-mmlab/mmaction"
"hassony2/kinetics_i3d_pytorch" -> "yjxiong/action-detection"
"hassony2/kinetics_i3d_pytorch" -> "tomrunia/PyTorchConv3D"
"hassony2/kinetics_i3d_pytorch" -> "DavideA/c3d-pytorch"
"hassony2/kinetics_i3d_pytorch" -> "activitynet/ActivityNet"
"hassony2/kinetics_i3d_pytorch" -> "bryanyzhu/two-stream-pytorch"
"hassony2/kinetics_i3d_pytorch" -> "facebookresearch/VMZ"
"hassony2/kinetics_i3d_pytorch" -> "wzmsltw/BSN-boundary-sensitive-network"
"hassony2/kinetics_i3d_pytorch" -> "facebookresearch/video-nonlocal-net"
"r1c7/SlowFastNetworks" -> "Guocode/SlowFast-Networks"
"r1c7/SlowFastNetworks" -> "irhum/R2Plus1D-PyTorch"
"r1c7/SlowFastNetworks" -> "jfzhang95/pytorch-video-recognition"
"r1c7/SlowFastNetworks" -> "decisionforce/TPN"
"r1c7/SlowFastNetworks" -> "yjxiong/tsn-pytorch"
"r1c7/SlowFastNetworks" -> "JJBOY/SlowFast-Network"
"r1c7/SlowFastNetworks" -> "facebookresearch/VMZ"
"r1c7/SlowFastNetworks" -> "piergiaj/representation-flow-cvpr19"
"r1c7/SlowFastNetworks" -> "cypw/PyTorch-MFNet"
"r1c7/SlowFastNetworks" -> "xuzheyuan624/slowfast-keras"
"r1c7/SlowFastNetworks" -> "zhoubolei/TRN-pytorch"
"r1c7/SlowFastNetworks" -> "yjxiong/temporal-segment-networks"
"r1c7/SlowFastNetworks" -> "mzolfaghari/ECO-pytorch"
"r1c7/SlowFastNetworks" -> "mzolfaghari/ECO-efficient-video-understanding"
"r1c7/SlowFastNetworks" -> "open-mmlab/mmaction"
"alainray/ava_downloader" -> "kevinlin311tw/ava-dataset-tool"
"facebookresearch/video-long-term-feature-banks" -> "gsig/PyVideoResearch"
"facebookresearch/video-long-term-feature-banks" -> "decisionforce/TPN"
"facebookresearch/video-long-term-feature-banks" -> "Alvin-Zeng/PGCN"
"facebookresearch/video-long-term-feature-banks" -> "zhoubolei/TRN-pytorch"
"facebookresearch/video-long-term-feature-banks" -> "xiaolonw/TimeCycle" ["e"=1]
"facebookresearch/video-long-term-feature-banks" -> "noureldien/timeception"
"facebookresearch/video-long-term-feature-banks" -> "Siyu-C/ACAR-Net"
"facebookresearch/video-long-term-feature-banks" -> "cypw/PyTorch-MFNet"
"facebookresearch/video-long-term-feature-banks" -> "MVIG-SJTU/AlphAction"
"facebookresearch/video-long-term-feature-banks" -> "activitynet/ActivityNet"
"facebookresearch/video-long-term-feature-banks" -> "TengdaHan/DPC" ["e"=1]
"facebookresearch/video-long-term-feature-banks" -> "MCG-NJU/MOC-Detector"
"facebookresearch/video-long-term-feature-banks" -> "facebookresearch/VMZ"
"facebookresearch/video-long-term-feature-banks" -> "hangzhaomit/HACS-dataset"
"facebookresearch/video-long-term-feature-banks" -> "NVlabs/STEP"
"pranoyr/cnn-lstm" -> "HHTseng/video-classification"
"pranoyr/cnn-lstm" -> "IDKiro/action-recognition"
"pranoyr/cnn-lstm" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"pranoyr/cnn-lstm" -> "AKASH2907/deepfakes_video_classification"
"pranoyr/cnn-lstm" -> "luoye2333/ResNetLSTM"
"pranoyr/cnn-lstm" -> "siqinli/GestureRecognition-PyTorch"
"pranoyr/cnn-lstm" -> "Yidadaa/Pytorch-Video-Classification"
"pranoyr/cnn-lstm" -> "slaysd/pytorch-sentiment-analysis-classification" ["e"=1]
"pranoyr/cnn-lstm" -> "junyongyou/Attention-boosted-deep-networks-for-video-classification"
"pranoyr/cnn-lstm" -> "doronharitan/human_activity_recognition_LRCN"
"pranoyr/cnn-lstm" -> "kenshohara/video-classification-3d-cnn-pytorch"
"pranoyr/cnn-lstm" -> "eriklindernoren/Action-Recognition"
"pranoyr/cnn-lstm" -> "woodfrog/ActionRecognition"
"pranoyr/cnn-lstm" -> "jfzhang95/pytorch-video-recognition"
"flrngel/TCN-with-attention" -> "haohy/TCAN"
"flrngel/TCN-with-attention" -> "tranducquy/tcn-fx-price-prediction"
"flrngel/TCN-with-attention" -> "lpphd/multivariate-attention-tcn"
"flrngel/TCN-with-attention" -> "sagelywizard/snail" ["e"=1]
"flrngel/TCN-with-attention" -> "815382636/GCN-tffc" ["e"=1]
"flrngel/TCN-with-attention" -> "oneday88/deepTCN" ["e"=1]
"flrngel/TCN-with-attention" -> "sj-li/MS-TCN2"
"MohsenFayyaz89/T3D" -> "MohsenFayyaz89/STFCN"
"DirtyHarryLYL/HAKE" -> "DirtyHarryLYL/HAKE-Action"
"DirtyHarryLYL/HAKE" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"DirtyHarryLYL/HAKE" -> "DirtyHarryLYL/HAKE-Action-Torch"
"DirtyHarryLYL/HAKE" -> "YueLiao/PPDM"
"DirtyHarryLYL/HAKE" -> "DirtyHarryLYL/DJ-RN"
"DirtyHarryLYL/HAKE" -> "DirtyHarryLYL/HOI-Learning-List"
"DirtyHarryLYL/HAKE" -> "BigRedT/no_frills_hoi_det"
"DirtyHarryLYL/HAKE" -> "bobwan1995/PMFNet"
"DirtyHarryLYL/HAKE" -> "DirtyHarryLYL/SymNet"
"DirtyHarryLYL/HAKE" -> "vt-vl-lab/iCAN"
"DirtyHarryLYL/HAKE" -> "s-gupta/v-coco"
"DirtyHarryLYL/HAKE" -> "chinancheng/awesome-human-object-interaction"
"DirtyHarryLYL/HAKE" -> "fredzzhang/spatially-conditioned-graphs"
"DirtyHarryLYL/HAKE" -> "ASMIftekhar/VSGNet"
"DirtyHarryLYL/HAKE" -> "Dong-JinKim/ActionCooccurrencePriors"
"Shahnawax/HAR-CNN-Keras" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"Shahnawax/HAR-CNN-Keras" -> "ani8897/Human-Activity-Recognition"
"Shahnawax/HAR-CNN-Keras" -> "Koutoulakis/Deep-Learning-for-Human-Activity-Recognition"
"Shahnawax/HAR-CNN-Keras" -> "ni79ls/har-keras-cnn"
"Shahnawax/HAR-CNN-Keras" -> "servomac/Human-Activity-Recognition"
"Shahnawax/HAR-CNN-Keras" -> "LiangZai-Embedded/HAR-ON-STM32F401C"
"Shahnawax/HAR-CNN-Keras" -> "STMicroelectronics/stm32ai-wiki"
"hassony2/inflated_convnets_pytorch" -> "hassony2/torch_videovision"
"hassony2/inflated_convnets_pytorch" -> "s9xie/Mini-Kinetics-200"
"hassony2/inflated_convnets_pytorch" -> "rimchang/kinetics-i3d-Pytorch"
"hassony2/inflated_convnets_pytorch" -> "coderSkyChen/Action_Recognition_Zoo"
"dukebw/lintel" -> "piergiaj/representation-flow-cvpr19"
"dukebw/lintel" -> "cypw/PyTorch-MFNet"
"dukebw/lintel" -> "chaoyuaw/pytorch-coviar"
"dukebw/lintel" -> "gsig/charades-algorithms"
"chensun11/dtfv" -> "bo-yang/dtf_fisher"
"chensun11/dtfv" -> "HuNiuC/iDT-FV-for-action-recogniton"
"chensun11/dtfv" -> "anenbergb/CS221_Project"
"jchiang2/Human-Activity-Recognition" -> "sidharthgurbani/HAR-using-PyTorch"
"jchiang2/Human-Activity-Recognition" -> "isukrit/encodingHumanActivity"
"alexandonian/pretorched-x" -> "gsig/PyVideoResearch"
"alexandonian/pretorched-x" -> "irhum/R2Plus1D-PyTorch"
"alexandonian/pretorched-x" -> "qijiezhao/s3d.pytorch"
"alexandonian/pretorched-x" -> "rohitgirdhar/ActionVLAD"
"alexandonian/pretorched-x" -> "facebookresearch/VMZ"
"alexandonian/pretorched-x" -> "ZhaofanQiu/local-and-global-diffusion-networks"
"alexandonian/pretorched-x" -> "qijiezhao/pseudo-3d-pytorch"
"alexandonian/pretorched-x" -> "mzolfaghari/ECO-efficient-video-understanding"
"alexandonian/pretorched-x" -> "facebookresearch/video-long-term-feature-banks"
"alexandonian/pretorched-x" -> "cypw/PyTorch-MFNet"
"alexandonian/pretorched-x" -> "LijieFan/tvnet"
"alexandonian/pretorched-x" -> "Tushar-N/pytorch-resnet3d" ["e"=1]
"alexandonian/pretorched-x" -> "wzmsltw/BSN-boundary-sensitive-network"
"kkanshul/Hide-and-Seek" -> "zhengshou/AutoLoc"
"mzolfaghari/ECO-efficient-video-understanding" -> "mzolfaghari/ECO-pytorch"
"mzolfaghari/ECO-efficient-video-understanding" -> "zhang-can/ECO-pytorch"
"mzolfaghari/ECO-efficient-video-understanding" -> "zhoubolei/TRN-pytorch"
"mzolfaghari/ECO-efficient-video-understanding" -> "wzmsltw/BSN-boundary-sensitive-network"
"mzolfaghari/ECO-efficient-video-understanding" -> "yjxiong/tsn-pytorch"
"mzolfaghari/ECO-efficient-video-understanding" -> "wanglimin/ARTNet"
"mzolfaghari/ECO-efficient-video-understanding" -> "yjxiong/action-detection"
"mzolfaghari/ECO-efficient-video-understanding" -> "qijiezhao/pseudo-3d-pytorch"
"mzolfaghari/ECO-efficient-video-understanding" -> "gurkirt/realtime-action-detection"
"mzolfaghari/ECO-efficient-video-understanding" -> "yjxiong/temporal-segment-networks"
"mzolfaghari/ECO-efficient-video-understanding" -> "cypw/PyTorch-MFNet"
"mzolfaghari/ECO-efficient-video-understanding" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"mzolfaghari/ECO-efficient-video-understanding" -> "irhum/R2Plus1D-PyTorch"
"mzolfaghari/ECO-efficient-video-understanding" -> "facebookresearch/VMZ"
"mzolfaghari/ECO-efficient-video-understanding" -> "VisionLearningGroup/R-C3D"
"zhang-can/ECO-pytorch" -> "mzolfaghari/ECO-pytorch"
"zhang-can/ECO-pytorch" -> "mzolfaghari/ECO-efficient-video-understanding"
"zhang-can/ECO-pytorch" -> "StrangerZhang/pyECO" ["e"=1]
"zhang-can/ECO-pytorch" -> "zhoubolei/TRN-pytorch"
"zhang-can/ECO-pytorch" -> "Rheelt/Materials-Temporal-Action-Detection"
"Showmax/kinetics-downloader" -> "activitynet/ActivityNet"
"Showmax/kinetics-downloader" -> "piaxar/kinetics-downloader"
"Showmax/kinetics-downloader" -> "arunos728/MotionSqueeze"
"Showmax/kinetics-downloader" -> "dancelogue/kinetics-datasets-downloader"
"Showmax/kinetics-downloader" -> "irhum/R2Plus1D-PyTorch"
"Showmax/kinetics-downloader" -> "qijiezhao/py-denseflow"
"Showmax/kinetics-downloader" -> "decisionforce/TPN"
"Showmax/kinetics-downloader" -> "rocksyne/kinetics-dataset-downloader"
"Showmax/kinetics-downloader" -> "r1c7/SlowFastNetworks"
"Showmax/kinetics-downloader" -> "chaoyuaw/pytorch-coviar"
"Showmax/kinetics-downloader" -> "google-deepmind/kinetics-i3d"
"Showmax/kinetics-downloader" -> "gsig/PyVideoResearch"
"Showmax/kinetics-downloader" -> "feichtenhofer/twostreamfusion"
"Showmax/kinetics-downloader" -> "zhoubolei/TRN-pytorch"
"Showmax/kinetics-downloader" -> "vt-vl-lab/SDN"
"MohsenFayyaz89/PyTorch_Video_Dataset" -> "MohsenFayyaz89/SCT"
"MohsenFayyaz89/PyTorch_Video_Dataset" -> "MohsenFayyaz89/STFCN"
"tomrunia/PyTorchConv3D" -> "Tushar-N/pytorch-resnet3d" ["e"=1]
"tomrunia/PyTorchConv3D" -> "piergiaj/pytorch-i3d"
"tomrunia/PyTorchConv3D" -> "feiyunzhang/i3d-non-local-pytorch" ["e"=1]
"tomrunia/PyTorchConv3D" -> "hassony2/kinetics_i3d_pytorch"
"tomrunia/PyTorchConv3D" -> "LossNAN/I3D-Tensorflow"
"tomrunia/PyTorchConv3D" -> "rimchang/kinetics-i3d-Pytorch"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "vt-vl-lab/iCAN"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "DirtyHarryLYL/HAKE"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "bobwan1995/PMFNet"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "BigRedT/no_frills_hoi_det"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "YueLiao/PPDM"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "SiyuanQi-zz/gpnn"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "s-gupta/v-coco"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "DirtyHarryLYL/HAKE-Action"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "DirtyHarryLYL/DJ-RN"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "DirtyHarryLYL/HOI-Learning-List"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "DirtyHarryLYL/HAKE-Action-Torch"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "ASMIftekhar/VSGNet"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "ywchao/ho-rcnn"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "vaesl/IP-Net"
"DirtyHarryLYL/Transferable-Interactiveness-Network" -> "DirtyHarryLYL/SymNet"
"HYPJUDY/Decouple-SSAD" -> "Finspire13/CMCS-Temporal-Action-Localization"
"HYPJUDY/Decouple-SSAD" -> "jiyanggao/CTAP"
"HYPJUDY/Decouple-SSAD" -> "Alvin-Zeng/PGCN"
"HYPJUDY/Decouple-SSAD" -> "bellos1203/STPN"
"HYPJUDY/Decouple-SSAD" -> "jiyanggao/CBR"
"HYPJUDY/Decouple-SSAD" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"HYPJUDY/Decouple-SSAD" -> "zhengshou/AutoLoc"
"HYPJUDY/Decouple-SSAD" -> "demianzhang/weakly-action-localization"
"HYPJUDY/Decouple-SSAD" -> "Rheelt/SSAD_pytorch"
"HYPJUDY/Decouple-SSAD" -> "Rheelt/Materials-Temporal-Action-Detection"
"HYPJUDY/Decouple-SSAD" -> "JJBOY/BMN-Boundary-Matching-Network"
"HYPJUDY/Decouple-SSAD" -> "shyamal-b/ss-tad"
"wjchaoGit/Group-Activity-Recognition" -> "mostafa-saad/deep-activity-rec"
"wjchaoGit/Group-Activity-Recognition" -> "huguyuehuhu/Awesome-Group-Activity-Recognition"
"wjchaoGit/Group-Activity-Recognition" -> "mostafa-saad/hierarchical-relational-network"
"wjchaoGit/Group-Activity-Recognition" -> "ruiyan1995/Group-Activity-Recognition"
"wjchaoGit/Group-Activity-Recognition" -> "cvlab-epfl/social-scene-understanding"
"wjchaoGit/Group-Activity-Recognition" -> "JacobYuan7/DIN-Group-Activity-Recognition-Benchmark"
"wjchaoGit/Group-Activity-Recognition" -> "ruiyan1995/HiGCIN"
"wjchaoGit/Group-Activity-Recognition" -> "mahsaep/Social-human-activity-understanding-and-grouping"
"wjchaoGit/Group-Activity-Recognition" -> "Alvin-Zeng/PGCN"
"wjchaoGit/Group-Activity-Recognition" -> "xueyee/GroupFormer"
"wjchaoGit/Group-Activity-Recognition" -> "hongluzhou/composer"
"cmhungsteve/TA3N" -> "cmhungsteve/SSTDA"
"cmhungsteve/TA3N" -> "jonmun/MM-SADA-code" ["e"=1]
"cmhungsteve/TA3N" -> "Alvin-Zeng/PGCN"
"cmhungsteve/TA3N" -> "deepcs233/TIN"
"cmhungsteve/TA3N" -> "xuyu0010/awesome-video-domain-adaptation"
"cmhungsteve/TA3N" -> "junyuGao/Zero-Shot-Action-Recognition-with-Two-Stream-GCN" ["e"=1]
"jindongwang/Deep-learning-activity-recognition" -> "jindongwang/activityrecognition"
"jindongwang/Deep-learning-activity-recognition" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"jindongwang/Deep-learning-activity-recognition" -> "haoranD/Awesome-Human-Activity-Recognition"
"jindongwang/Deep-learning-activity-recognition" -> "curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"
"jindongwang/Deep-learning-activity-recognition" -> "STRCWearlab/DeepConvLSTM"
"jindongwang/Deep-learning-activity-recognition" -> "takumiw/Deep-Learning-for-Human-Activity-Recognition"
"jindongwang/Deep-learning-activity-recognition" -> "guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs"
"jindongwang/Deep-learning-activity-recognition" -> "iantangc/SelfHAR"
"jindongwang/Deep-learning-activity-recognition" -> "Koutoulakis/Deep-Learning-for-Human-Activity-Recognition"
"jindongwang/Deep-learning-activity-recognition" -> "andreas-bulling/ActRecTut"
"jindongwang/Deep-learning-activity-recognition" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"jindongwang/Deep-learning-activity-recognition" -> "arturjordao/WearableSensorData"
"piergiaj/representation-flow-cvpr19" -> "craston/MARS"
"piergiaj/representation-flow-cvpr19" -> "LijieFan/tvnet"
"piergiaj/representation-flow-cvpr19" -> "mzolfaghari/ECO-pytorch"
"piergiaj/representation-flow-cvpr19" -> "dukebw/lintel"
"piergiaj/representation-flow-cvpr19" -> "decisionforce/TPN"
"piergiaj/representation-flow-cvpr19" -> "cypw/PyTorch-MFNet"
"piergiaj/representation-flow-cvpr19" -> "chaoyuaw/pytorch-coviar"
"piergiaj/representation-flow-cvpr19" -> "yjxiong/tsn-pytorch"
"piergiaj/representation-flow-cvpr19" -> "irhum/R2Plus1D-PyTorch"
"piergiaj/representation-flow-cvpr19" -> "facebookresearch/VMZ"
"piergiaj/representation-flow-cvpr19" -> "qijiezhao/s3d.pytorch"
"piergiaj/representation-flow-cvpr19" -> "coderSkyChen/Action_Recognition_Zoo"
"piergiaj/representation-flow-cvpr19" -> "noureldien/timeception"
"piergiaj/representation-flow-cvpr19" -> "r1c7/SlowFastNetworks"
"piergiaj/representation-flow-cvpr19" -> "Gasoonjia/tvnet_pytorch"
"ahsaniqbal/Kinetics-FeatureExtractor" -> "ChinaYi/asrf_with_asformer"
"ahsaniqbal/Kinetics-FeatureExtractor" -> "yiskw713/asrf"
"yabufarha/ms-tcn" -> "sj-li/MS-TCN2"
"yabufarha/ms-tcn" -> "ChinaYi/ASFormer"
"yabufarha/ms-tcn" -> "yiskw713/asrf"
"yabufarha/ms-tcn" -> "MCG-NJU/BCN"
"yabufarha/ms-tcn" -> "ahsaniqbal/Kinetics-FeatureExtractor"
"yabufarha/ms-tcn" -> "nus-cvml/awesome-temporal-action-segmentation"
"yabufarha/ms-tcn" -> "cmhungsteve/SSTDA"
"yabufarha/ms-tcn" -> "colincsl/TemporalConvolutionalNetworks"
"yabufarha/ms-tcn" -> "ttlmh/Bridge-Prompt"
"yabufarha/ms-tcn" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"yabufarha/ms-tcn" -> "ZheLi2020/TimestampActionSeg"
"yabufarha/ms-tcn" -> "ChinaYi/asrf_with_asformer"
"yabufarha/ms-tcn" -> "Alvin-Zeng/PGCN"
"yabufarha/ms-tcn" -> "alexanderrichard/action-sets"
"yabufarha/ms-tcn" -> "yiskw713/video_feature_extractor"
"Finspire13/CMCS-Temporal-Action-Localization" -> "sujoyp/wtalc-pytorch"
"Finspire13/CMCS-Temporal-Action-Localization" -> "zhengshou/AutoLoc"
"Finspire13/CMCS-Temporal-Action-Localization" -> "bellos1203/STPN"
"Finspire13/CMCS-Temporal-Action-Localization" -> "Finspire13/pytorch-i3d-feature-extraction"
"Finspire13/CMCS-Temporal-Action-Localization" -> "naraysa/3c-net"
"Finspire13/CMCS-Temporal-Action-Localization" -> "HYPJUDY/Decouple-SSAD"
"Finspire13/CMCS-Temporal-Action-Localization" -> "Alvin-Zeng/PGCN"
"Finspire13/CMCS-Temporal-Action-Localization" -> "Pilhyeon/BaSNet-pytorch"
"Finspire13/CMCS-Temporal-Action-Localization" -> "Rheelt/Materials-Temporal-Action-Detection"
"Finspire13/CMCS-Temporal-Action-Localization" -> "asrafulashiq/wsad"
"Finspire13/CMCS-Temporal-Action-Localization" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"Finspire13/CMCS-Temporal-Action-Localization" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"Finspire13/CMCS-Temporal-Action-Localization" -> "MichiganCOG/A2CL-PT"
"Finspire13/CMCS-Temporal-Action-Localization" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"Finspire13/CMCS-Temporal-Action-Localization" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"eriklindernoren/Action-Recognition" -> "IDKiro/action-recognition"
"eriklindernoren/Action-Recognition" -> "woodfrog/ActionRecognition"
"eriklindernoren/Action-Recognition" -> "IBM/action-recognition-pytorch"
"eriklindernoren/Action-Recognition" -> "Keiku/Action-Recognition-CNN-LSTM"
"eriklindernoren/Action-Recognition" -> "peachman05/action-recognition-tutorial"
"eriklindernoren/Action-Recognition" -> "jfzhang95/pytorch-video-recognition"
"gsig/PyVideoResearch" -> "facebookresearch/video-long-term-feature-banks"
"gsig/PyVideoResearch" -> "open-mmlab/mmaction"
"gsig/PyVideoResearch" -> "facebookresearch/VMZ"
"gsig/PyVideoResearch" -> "decisionforce/TPN"
"gsig/PyVideoResearch" -> "activitynet/ActivityNet"
"gsig/PyVideoResearch" -> "alexandonian/pretorched-x"
"gsig/PyVideoResearch" -> "zhoubolei/TRN-pytorch"
"gsig/PyVideoResearch" -> "swathikirans/GSM"
"gsig/PyVideoResearch" -> "coderSkyChen/Action_Recognition_Zoo"
"gsig/PyVideoResearch" -> "yjxiong/action-detection"
"gsig/PyVideoResearch" -> "google-deepmind/kinetics-i3d"
"gsig/PyVideoResearch" -> "noureldien/timeception"
"gsig/PyVideoResearch" -> "Rheelt/Materials-Temporal-Action-Detection"
"gsig/PyVideoResearch" -> "yaohungt/Gated-Spatio-Temporal-Energy-Graph" ["e"=1]
"gsig/PyVideoResearch" -> "jinwchoi/awesome-action-recognition"
"noureldien/timeception" -> "piergiaj/tgm-icml19"
"noureldien/timeception" -> "facebookresearch/video-long-term-feature-banks"
"noureldien/timeception" -> "alexanderrichard/action-sets"
"noureldien/timeception" -> "jiaozizhao/Two-in-One-ActionDetection"
"noureldien/timeception" -> "qinzhi-0110/pytorch-act-detector"
"noureldien/timeception" -> "gsig/PyVideoResearch"
"ghammad/pyActigraphy" -> "wadpac/GGIR"
"ghammad/pyActigraphy" -> "elyiorgos/sleeppy" ["e"=1]
"ghammad/pyActigraphy" -> "OxWearables/actipy"
"ghammad/pyActigraphy" -> "OxWearables/biobankAccelerometerAnalysis"
"ghammad/pyActigraphy" -> "cbrnr/sleepecg" ["e"=1]
"MRzzm/action-recognition-models-pytorch" -> "jfzhang95/pytorch-video-recognition"
"MRzzm/action-recognition-models-pytorch" -> "irhum/R2Plus1D-PyTorch"
"MRzzm/action-recognition-models-pytorch" -> "qijiezhao/pseudo-3d-pytorch"
"MRzzm/action-recognition-models-pytorch" -> "piergiaj/representation-flow-cvpr19"
"MRzzm/action-recognition-models-pytorch" -> "woodfrog/ActionRecognition"
"MRzzm/action-recognition-models-pytorch" -> "V-Sense/ACTION-Net"
"MRzzm/action-recognition-models-pytorch" -> "MCG-NJU/TDN"
"MRzzm/action-recognition-models-pytorch" -> "qijiezhao/s3d.pytorch"
"MRzzm/action-recognition-models-pytorch" -> "bryanyzhu/two-stream-pytorch"
"MRzzm/action-recognition-models-pytorch" -> "coderSkyChen/Action_Recognition_Zoo"
"MRzzm/action-recognition-models-pytorch" -> "yjxiong/tsn-pytorch"
"MRzzm/action-recognition-models-pytorch" -> "decisionforce/TPN"
"MRzzm/action-recognition-models-pytorch" -> "Phoenix1327/tea-action-recognition"
"MRzzm/action-recognition-models-pytorch" -> "jeffreyyihuang/two-stream-action-recognition"
"MRzzm/action-recognition-models-pytorch" -> "qijiezhao/Video-Classification-Action-Recognition"
"Finspire13/pytorch-i3d-feature-extraction" -> "Finspire13/CMCS-Temporal-Action-Localization"
"Finspire13/pytorch-i3d-feature-extraction" -> "happyharrycn/actionformer_release"
"Finspire13/pytorch-i3d-feature-extraction" -> "GowthamGottimukkala/I3D_Feature_Extraction_resnet" ["e"=1]
"Finspire13/pytorch-i3d-feature-extraction" -> "dingfengshi/TriDet"
"Finspire13/pytorch-i3d-feature-extraction" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"Finspire13/pytorch-i3d-feature-extraction" -> "JJBOY/BMN-Boundary-Matching-Network"
"Finspire13/pytorch-i3d-feature-extraction" -> "VividLe/A2Net"
"Finspire13/pytorch-i3d-feature-extraction" -> "sujoyp/wtalc-pytorch"
"Finspire13/pytorch-i3d-feature-extraction" -> "piergiaj/tgm-icml19"
"Finspire13/pytorch-i3d-feature-extraction" -> "xlliu7/TadTR"
"Finspire13/pytorch-i3d-feature-extraction" -> "piergiaj/pytorch-i3d"
"Finspire13/pytorch-i3d-feature-extraction" -> "HumamAlwassel/TSP"
"Finspire13/pytorch-i3d-feature-extraction" -> "TencentYoutuResearch/ActionDetection-AFSD"
"Finspire13/pytorch-i3d-feature-extraction" -> "Alvin-Zeng/PGCN"
"Finspire13/pytorch-i3d-feature-extraction" -> "frostinassiky/gtad"
"sxzrt/Instructions-of-the-PersonX-dataset" -> "sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint"
"sxzrt/Instructions-of-the-PersonX-dataset" -> "zhunzhong07/HHL" ["e"=1]
"sxzrt/Instructions-of-the-PersonX-dataset" -> "sxzrt/Proxy-Set"
"sxzrt/Instructions-of-the-PersonX-dataset" -> "FlyHighest/UnrealPerson"
"wzmsltw/BSN-boundary-sensitive-network" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"wzmsltw/BSN-boundary-sensitive-network" -> "JJBOY/BMN-Boundary-Matching-Network"
"wzmsltw/BSN-boundary-sensitive-network" -> "yjxiong/action-detection"
"wzmsltw/BSN-boundary-sensitive-network" -> "Tencent/ActionDetection-DBG"
"wzmsltw/BSN-boundary-sensitive-network" -> "Alvin-Zeng/PGCN"
"wzmsltw/BSN-boundary-sensitive-network" -> "sujoyp/wtalc-pytorch"
"wzmsltw/BSN-boundary-sensitive-network" -> "Finspire13/CMCS-Temporal-Action-Localization"
"wzmsltw/BSN-boundary-sensitive-network" -> "yjxiong/anet2016-cuhk"
"wzmsltw/BSN-boundary-sensitive-network" -> "activitynet/ActivityNet"
"wzmsltw/BSN-boundary-sensitive-network" -> "Rheelt/Materials-Temporal-Action-Detection"
"wzmsltw/BSN-boundary-sensitive-network" -> "frostinassiky/gtad"
"wzmsltw/BSN-boundary-sensitive-network" -> "jiyanggao/CTAP"
"wzmsltw/BSN-boundary-sensitive-network" -> "yjxiong/temporal-segment-networks"
"wzmsltw/BSN-boundary-sensitive-network" -> "VisionLearningGroup/R-C3D"
"wzmsltw/BSN-boundary-sensitive-network" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "wzmsltw/BSN-boundary-sensitive-network"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "JJBOY/BMN-Boundary-Matching-Network"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "Alvin-Zeng/PGCN"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "Finspire13/CMCS-Temporal-Action-Localization"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "sujoyp/wtalc-pytorch"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "Rheelt/Materials-Temporal-Action-Detection"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "frostinassiky/gtad"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "HYPJUDY/Decouple-SSAD"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "Tencent/ActionDetection-DBG"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "jiyanggao/CBR"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "bellos1203/STPN"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "yjxiong/action-detection"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "sunnyxiaohu/R-C3D.pytorch"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"wzmsltw/BSN-boundary-sensitive-network.pytorch" -> "jiyanggao/CTAP"
"vt-vl-lab/iCAN" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"vt-vl-lab/iCAN" -> "SiyuanQi-zz/gpnn"
"vt-vl-lab/iCAN" -> "s-gupta/v-coco"
"vt-vl-lab/iCAN" -> "ywchao/ho-rcnn"
"vt-vl-lab/iCAN" -> "bobwan1995/PMFNet"
"vt-vl-lab/iCAN" -> "YueLiao/PPDM"
"vt-vl-lab/iCAN" -> "BigRedT/no_frills_hoi_det"
"vt-vl-lab/iCAN" -> "vt-vl-lab/DRG"
"vt-vl-lab/iCAN" -> "vaesl/IP-Net"
"vt-vl-lab/iCAN" -> "ASMIftekhar/VSGNet"
"vt-vl-lab/iCAN" -> "chinancheng/awesome-human-object-interaction"
"vt-vl-lab/iCAN" -> "fredzzhang/spatially-conditioned-graphs"
"vt-vl-lab/iCAN" -> "DirtyHarryLYL/HOI-Learning-List"
"vt-vl-lab/iCAN" -> "tfzhou/C-HOI"
"vt-vl-lab/iCAN" -> "DirtyHarryLYL/DJ-RN"
"PingchuanMa/Temporal-Shift-Module" -> "ZhaofanQiu/local-and-global-diffusion-networks"
"sunnyxiaohu/R-C3D.pytorch" -> "VisionLearningGroup/R-C3D"
"sunnyxiaohu/R-C3D.pytorch" -> "Alvin-Zeng/PGCN"
"sunnyxiaohu/R-C3D.pytorch" -> "Rheelt/Materials-Temporal-Action-Detection"
"sunnyxiaohu/R-C3D.pytorch" -> "yjxiong/action-detection"
"sunnyxiaohu/R-C3D.pytorch" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"sunnyxiaohu/R-C3D.pytorch" -> "sujoyp/wtalc-pytorch"
"sunnyxiaohu/R-C3D.pytorch" -> "DavideA/c3d-pytorch"
"sunnyxiaohu/R-C3D.pytorch" -> "piergiaj/tgm-icml19"
"sunnyxiaohu/R-C3D.pytorch" -> "shyamal-b/ss-tad"
"sunnyxiaohu/R-C3D.pytorch" -> "JJBOY/BMN-Boundary-Matching-Network"
"sunnyxiaohu/R-C3D.pytorch" -> "bellos1203/STPN"
"sunnyxiaohu/R-C3D.pytorch" -> "jfzhang95/pytorch-video-recognition"
"sunnyxiaohu/R-C3D.pytorch" -> "piergiaj/super-events-cvpr18"
"sunnyxiaohu/R-C3D.pytorch" -> "HYPJUDY/Decouple-SSAD"
"sunnyxiaohu/R-C3D.pytorch" -> "wanglimin/UntrimmedNet"
"sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint" -> "sxzrt/Instructions-of-the-PersonX-dataset"
"cypw/PyTorch-MFNet" -> "facebookresearch/video-long-term-feature-banks"
"cypw/PyTorch-MFNet" -> "wanglimin/ARTNet"
"cypw/PyTorch-MFNet" -> "wzmsltw/BSN-boundary-sensitive-network"
"cypw/PyTorch-MFNet" -> "piergiaj/representation-flow-cvpr19"
"cypw/PyTorch-MFNet" -> "mzolfaghari/ECO-efficient-video-understanding"
"cypw/PyTorch-MFNet" -> "yjxiong/tsn-pytorch"
"cypw/PyTorch-MFNet" -> "zhoubolei/TRN-pytorch"
"cypw/PyTorch-MFNet" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"cypw/PyTorch-MFNet" -> "qijiezhao/pseudo-3d-pytorch"
"cypw/PyTorch-MFNet" -> "qijiezhao/s3d.pytorch"
"cypw/PyTorch-MFNet" -> "noureldien/timeception"
"cypw/PyTorch-MFNet" -> "ZhaofanQiu/local-and-global-diffusion-networks"
"cypw/PyTorch-MFNet" -> "facebookresearch/VMZ"
"cypw/PyTorch-MFNet" -> "gsig/PyVideoResearch"
"cypw/PyTorch-MFNet" -> "chaoyuaw/pytorch-coviar"
"dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch" -> "dspanah/Sensor-Based-Human-Activity-Recognition-LSTMsEnsemble-Pytorch"
"dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch" -> "STRCWearlab/DeepConvLSTM_py3"
"dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch" -> "AdelaideAuto-IDLab/Attend-And-Discriminate"
"wanglimin/ARTNet" -> "wanglimin/UntrimmedNet"
"wanglimin/ARTNet" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"wanglimin/ARTNet" -> "qijiezhao/py-denseflow"
"wanglimin/ARTNet" -> "yjxiong/anet2016-cuhk"
"wanglimin/ARTNet" -> "rohitgirdhar/ActionVLAD"
"wanglimin/ARTNet" -> "yjxiong/action-detection"
"wanglimin/ARTNet" -> "feichtenhofer/st-resnet"
"wanglimin/ARTNet" -> "mzolfaghari/ECO-efficient-video-understanding"
"wanglimin/ARTNet" -> "bryanyzhu/Hidden-Two-Stream"
"wanglimin/ARTNet" -> "cypw/PyTorch-MFNet"
"wanglimin/ARTNet" -> "VisionLearningGroup/R-C3D"
"wanglimin/ARTNet" -> "qijiezhao/s3d.pytorch"
"wanglimin/ARTNet" -> "laura-wang/video_repres_mas" ["e"=1]
"wanglimin/ARTNet" -> "feichtenhofer/gpu_flow"
"wanglimin/ARTNet" -> "zhoubolei/TRN-pytorch"
"kenshohara/3D-ResNets" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"Wuie/ST-NBNN-demo" -> "kailiuXD/STWP"
"LijieFan/tvnet" -> "piergiaj/representation-flow-cvpr19"
"LijieFan/tvnet" -> "Gasoonjia/tvnet_pytorch"
"LijieFan/tvnet" -> "wanglimin/ARTNet"
"LijieFan/tvnet" -> "VisionLearningGroup/R-C3D"
"LijieFan/tvnet" -> "zhengshou/scnn"
"LijieFan/tvnet" -> "shyamal-b/sst"
"LijieFan/tvnet" -> "cypw/PyTorch-MFNet"
"LijieFan/tvnet" -> "feichtenhofer/gpu_flow"
"LijieFan/tvnet" -> "yjxiong/action-detection"
"LijieFan/tvnet" -> "s9xie/Mini-Kinetics-200"
"LijieFan/tvnet" -> "rohitgirdhar/ActionVLAD"
"LijieFan/tvnet" -> "coderSkyChen/Action_Recognition_Zoo"
"LijieFan/tvnet" -> "ZhaofanQiu/local-and-global-diffusion-networks"
"LijieFan/tvnet" -> "jiyanggao/TURN-TAP"
"LijieFan/tvnet" -> "hbilen/dynamic-image-nets"
"Yidadaa/Pytorch-Video-Classification" -> "Daisy-Zhang/Video-Classification-Pytorch"
"bartkowiaktomasz/har-wisdm-lstm-rnns" -> "bartkowiaktomasz/har-wisdm-bidirectional-lstm-rnns"
"bartkowiaktomasz/har-wisdm-lstm-rnns" -> "zhangzhao156/CNN-for-HAR-on-WISDM-Dataset"
"rohitgirdhar/AttentionalPoolingAction" -> "rohitgirdhar/ActionVLAD"
"rohitgirdhar/AttentionalPoolingAction" -> "kracwarlock/action-recognition-visual-attention"
"rohitgirdhar/AttentionalPoolingAction" -> "gurkirt/realtime-action-detection"
"rohitgirdhar/AttentionalPoolingAction" -> "qijiezhao/Video-Classification-Action-Recognition"
"rohitgirdhar/AttentionalPoolingAction" -> "gsig/charades-algorithms"
"rohitgirdhar/AttentionalPoolingAction" -> "wanglimin/UntrimmedNet"
"rohitgirdhar/AttentionalPoolingAction" -> "gsig/temporal-fields"
"rohitgirdhar/AttentionalPoolingAction" -> "gsig/actions-for-actions"
"rohitgirdhar/AttentionalPoolingAction" -> "feichtenhofer/st-resnet"
"rohitgirdhar/AttentionalPoolingAction" -> "wanglimin/ARTNet"
"rohitgirdhar/AttentionalPoolingAction" -> "VisionLearningGroup/R-C3D"
"rohitgirdhar/AttentionalPoolingAction" -> "gudongfeng/3d-DenseNet"
"rohitgirdhar/AttentionalPoolingAction" -> "activitynet/ActivityNet"
"rohitgirdhar/AttentionalPoolingAction" -> "jingweio/Action_Recognition_using_Visual_Attention"
"rohitgirdhar/AttentionalPoolingAction" -> "vt-vl-lab/iCAN"
"hjjpku/Action_Detection_DQN" -> "jiyanggao/CBR"
"sujoyp/wtalc-pytorch" -> "Finspire13/CMCS-Temporal-Action-Localization"
"sujoyp/wtalc-pytorch" -> "bellos1203/STPN"
"sujoyp/wtalc-pytorch" -> "zhengshou/AutoLoc"
"sujoyp/wtalc-pytorch" -> "naraysa/3c-net"
"sujoyp/wtalc-pytorch" -> "MichiganCOG/A2CL-PT"
"sujoyp/wtalc-pytorch" -> "Rheelt/Materials-Temporal-Action-Detection"
"sujoyp/wtalc-pytorch" -> "Pilhyeon/BaSNet-pytorch"
"sujoyp/wtalc-pytorch" -> "wanglimin/UntrimmedNet"
"sujoyp/wtalc-pytorch" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"sujoyp/wtalc-pytorch" -> "asrafulashiq/wsad"
"sujoyp/wtalc-pytorch" -> "Alvin-Zeng/PGCN"
"sujoyp/wtalc-pytorch" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"sujoyp/wtalc-pytorch" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"sujoyp/wtalc-pytorch" -> "LeonHLJ/RSKP"
"sujoyp/wtalc-pytorch" -> "wzmsltw/BSN-boundary-sensitive-network"
"JiaHeeeee/Deep_Learning_Temporal_Action_Detection" -> "jiyanggao/CBR"
"JiaHeeeee/Deep_Learning_Temporal_Action_Detection" -> "Rheelt/Materials-Temporal-Action-Detection"
"JiaHeeeee/Deep_Learning_Temporal_Action_Detection" -> "jiyanggao/CTAP"
"JiaHeeeee/Deep_Learning_Temporal_Action_Detection" -> "zhengshou/AutoLoc"
"JiaHeeeee/Deep_Learning_Temporal_Action_Detection" -> "demianzhang/weakly-action-localization"
"JiaHeeeee/Deep_Learning_Temporal_Action_Detection" -> "naraysa/3c-net"
"taokong/group_normalization" -> "kuangliu/pytorch-groupnorm"
"kuangliu/pytorch-groupnorm" -> "taokong/group_normalization"
"USTC-Video-Understanding/I3D_Finetune" -> "LossNAN/I3D-Tensorflow"
"USTC-Video-Understanding/I3D_Finetune" -> "yangwangx/denseFlow_gpu"
"USTC-Video-Understanding/I3D_Finetune" -> "Rhythmblue/i3d_finetune"
"USTC-Video-Understanding/I3D_Finetune" -> "qijiezhao/py-denseflow"
"USTC-Video-Understanding/I3D_Finetune" -> "dlpbc/keras-kinetics-i3d"
"USTC-Video-Understanding/I3D_Finetune" -> "google-deepmind/kinetics-i3d"
"USTC-Video-Understanding/I3D_Finetune" -> "coderSkyChen/Action_Recognition_Zoo"
"USTC-Video-Understanding/I3D_Finetune" -> "feichtenhofer/gpu_flow"
"USTC-Video-Understanding/I3D_Finetune" -> "yoosan/i3d-tensorflow"
"wushidonguc/two-stream-action-recognition-keras" -> "mohammed-elkomy/two-stream-action-recognition"
"wushidonguc/two-stream-action-recognition-keras" -> "tomar840/two-stream-fusion-for-action-recognition-in-videos"
"wushidonguc/two-stream-action-recognition-keras" -> "XiaoCode-er/Two-Stream-CNN" ["e"=1]
"wushidonguc/two-stream-action-recognition-keras" -> "Yorwxue/Two-Stream-Convolutional-Networks"
"wushidonguc/two-stream-action-recognition-keras" -> "jeffreyyihuang/two-stream-action-recognition"
"wushidonguc/two-stream-action-recognition-keras" -> "bryanyzhu/two-stream-pytorch"
"wushidonguc/two-stream-action-recognition-keras" -> "feichtenhofer/twostreamfusion"
"wushidonguc/two-stream-action-recognition-keras" -> "TianzhongSong/C3D-keras"
"wushidonguc/two-stream-action-recognition-keras" -> "woodfrog/ActionRecognition"
"wushidonguc/two-stream-action-recognition-keras" -> "zhuzhuxia1994/CK-TensorFlow"
"wushidonguc/two-stream-action-recognition-keras" -> "axon-research/c3d-keras"
"chen0040/keras-video-classifier" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"chen0040/keras-video-classifier" -> "nim94sha/keras-video-classification"
"Rhythmblue/i3d_finetune" -> "yoosan/i3d-tensorflow"
"Rhythmblue/i3d_finetune" -> "USTC-Video-Understanding/I3D_Finetune"
"craston/MARS" -> "piergiaj/representation-flow-cvpr19"
"craston/MARS" -> "decisionforce/TPN"
"craston/MARS" -> "swathikirans/GSM"
"craston/MARS" -> "zhang-can/PAN-PyTorch"
"craston/MARS" -> "arunos728/MotionSqueeze"
"craston/MARS" -> "mzolfaghari/ECO-pytorch"
"craston/MARS" -> "Phoenix1327/tea-action-recognition"
"craston/MARS" -> "deepcs233/TIN"
"craston/MARS" -> "kylemin/S3D" ["e"=1]
"LossNAN/I3D-Tensorflow" -> "USTC-Video-Understanding/I3D_Finetune"
"LossNAN/I3D-Tensorflow" -> "yangwangx/denseFlow_gpu"
"LossNAN/I3D-Tensorflow" -> "dlpbc/keras-kinetics-i3d"
"LossNAN/I3D-Tensorflow" -> "qijiezhao/py-denseflow"
"LossNAN/I3D-Tensorflow" -> "LossNAN/3D-Resnet-tensorflow"
"jiyanggao/CBR" -> "jiyanggao/TURN-TAP"
"jiyanggao/CBR" -> "vdavid70619/TCN"
"jiyanggao/CBR" -> "jiyanggao/CTAP"
"jiyanggao/CBR" -> "JaywongWang/SST-Tensorflow"
"jiyanggao/CBR" -> "shyamal-b/sst"
"jiyanggao/CBR" -> "shyamal-b/ss-tad"
"jiyanggao/CBR" -> "JiaHeeeee/Deep_Learning_Temporal_Action_Detection"
"bellos1203/STPN" -> "sujoyp/wtalc-pytorch"
"bellos1203/STPN" -> "zhengshou/AutoLoc"
"bellos1203/STPN" -> "Finspire13/CMCS-Temporal-Action-Localization"
"bellos1203/STPN" -> "asrafulashiq/wsad"
"bellos1203/STPN" -> "demianzhang/weakly-action-localization"
"bellos1203/STPN" -> "Pilhyeon/BaSNet-pytorch"
"bellos1203/STPN" -> "naraysa/3c-net"
"bellos1203/STPN" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"bellos1203/STPN" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"bellos1203/STPN" -> "wanglimin/UntrimmedNet"
"bellos1203/STPN" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"wizyoung/Optical-Flow-GPU-Docker" -> "willprice/flowty"
"wizyoung/Optical-Flow-GPU-Docker" -> "qijiezhao/py-denseflow"
"siqinli/GestureRecognition-PyTorch" -> "GuangmingZhu/Conv3D_CLSTM" ["e"=1]
"siqinli/GestureRecognition-PyTorch" -> "IDKiro/action-recognition"
"kevinlin311tw/ava-dataset-tool" -> "alainray/ava_downloader"
"kevinlin311tw/ava-dataset-tool" -> "leaderj1001/Action-Localization"
"ywchao/ho-rcnn" -> "s-gupta/v-coco"
"ywchao/ho-rcnn" -> "ywchao/hico_benchmark"
"ywchao/ho-rcnn" -> "bobwan1995/PMFNet"
"ywchao/ho-rcnn" -> "vt-vl-lab/iCAN"
"ywchao/ho-rcnn" -> "vaesl/IP-Net"
"ywchao/ho-rcnn" -> "fredzzhang/spatially-conditioned-graphs"
"ruiyan1995/Group-Activity-Recognition" -> "ruiyan1995/HiGCIN"
"ruiyan1995/Group-Activity-Recognition" -> "cvlab-epfl/social-scene-understanding"
"ruiyan1995/Group-Activity-Recognition" -> "JacobYuan7/DIN-Group-Activity-Recognition-Benchmark"
"vkalogeiton/caffe" -> "gurkirt/corrected-UCF101-Annots"
"vkalogeiton/caffe" -> "imatge-upc/Action-Tubelet-Detection-in-AVA"
"vkalogeiton/caffe" -> "gurkirt/realtime-action-detection"
"vkalogeiton/caffe" -> "kevinlin311tw/ava-dataset-tool"
"vkalogeiton/caffe" -> "shyamal-b/sst"
"vkalogeiton/caffe" -> "qinzhi-0110/pytorch-act-detector"
"vkalogeiton/caffe" -> "gsig/temporal-fields"
"vkalogeiton/caffe" -> "yjxiong/anet2016-cuhk"
"vkalogeiton/caffe" -> "ranjaykrishna/SST"
"vkalogeiton/caffe" -> "pengxj/action-faster-rcnn"
"imatge-upc/Action-Tubelet-Detection-in-AVA" -> "qinzhi-0110/pytorch-act-detector"
"zhengshou/AutoLoc" -> "sujoyp/wtalc-pytorch"
"zhengshou/AutoLoc" -> "Finspire13/CMCS-Temporal-Action-Localization"
"zhengshou/AutoLoc" -> "bellos1203/STPN"
"zhengshou/AutoLoc" -> "demianzhang/weakly-action-localization"
"zhengshou/AutoLoc" -> "naraysa/3c-net"
"zhengshou/AutoLoc" -> "asrafulashiq/wsad"
"zhengshou/AutoLoc" -> "wanglimin/UntrimmedNet"
"zhengshou/AutoLoc" -> "MichiganCOG/A2CL-PT"
"zhengshou/AutoLoc" -> "vdavid70619/TCN"
"zhengshou/AutoLoc" -> "jiyanggao/CBR"
"THLfi/read.gt3x" -> "paulhibbing/AGread"
"mitmul/pynvvl" -> "MTCloudVision/MTSVRC"
"visinf/dpp" -> "sebgao/LIP"
"coderSkyChen/Action_Recognition_Zoo" -> "gurkirt/realtime-action-detection"
"coderSkyChen/Action_Recognition_Zoo" -> "qijiezhao/s3d.pytorch"
"coderSkyChen/Action_Recognition_Zoo" -> "qijiezhao/Video-Classification-Action-Recognition"
"coderSkyChen/Action_Recognition_Zoo" -> "kevin-ssy/Optical-Flow-Guided-Feature"
"coderSkyChen/Action_Recognition_Zoo" -> "qijiezhao/py-denseflow"
"coderSkyChen/Action_Recognition_Zoo" -> "gsig/PyVideoResearch"
"coderSkyChen/Action_Recognition_Zoo" -> "mzolfaghari/ECO-pytorch"
"coderSkyChen/Action_Recognition_Zoo" -> "piergiaj/representation-flow-cvpr19"
"coderSkyChen/Action_Recognition_Zoo" -> "irhum/R2Plus1D-PyTorch"
"coderSkyChen/Action_Recognition_Zoo" -> "USTC-Video-Understanding/I3D_Finetune"
"coderSkyChen/Action_Recognition_Zoo" -> "FingerRec/real_time_video_action_recognition"
"coderSkyChen/Action_Recognition_Zoo" -> "woodfrog/ActionRecognition"
"coderSkyChen/Action_Recognition_Zoo" -> "tomar840/two-stream-fusion-for-action-recognition-in-videos"
"coderSkyChen/Action_Recognition_Zoo" -> "gudongfeng/3d-DenseNet"
"coderSkyChen/Action_Recognition_Zoo" -> "zhoubolei/TRN-pytorch"
"jingweio/Action_Recognition_using_Visual_Attention" -> "kracwarlock/action-recognition-visual-attention"
"vdavid70619/TCN" -> "jiyanggao/CBR"
"vdavid70619/TCN" -> "jiyanggao/TURN-TAP"
"yoosan/i3d-tensorflow" -> "Rhythmblue/i3d_finetune"
"JaywongWang/SST-Tensorflow" -> "shyamal-b/sst"
"JaywongWang/SST-Tensorflow" -> "escorciav/daps"
"JaywongWang/SST-Tensorflow" -> "jiyanggao/CBR"
"JaywongWang/SST-Tensorflow" -> "escorciav/deep-action-proposals"
"JaywongWang/SST-Tensorflow" -> "demianzhang/weakly-action-localization"
"huajh/action_recognition" -> "hueihan/Action_Recognition"
"ZhaofanQiu/local-and-global-diffusion-networks" -> "KevinDuarte/VideoCapsuleNet"
"ZhaofanQiu/local-and-global-diffusion-networks" -> "Guocode/SlowFast-Networks"
"jishnujayakumar/MV-Tractus" -> "vadimkantorov/mpegflow"
"jishnujayakumar/MV-Tractus" -> "LukasBommes/mv-extractor"
"mohammed-elkomy/two-stream-action-recognition" -> "wushidonguc/two-stream-action-recognition-keras"
"mohammed-elkomy/two-stream-action-recognition" -> "tomar840/two-stream-fusion-for-action-recognition-in-videos"
"SiyuanQi-zz/gpnn" -> "vt-vl-lab/iCAN"
"SiyuanQi-zz/gpnn" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"SiyuanQi-zz/gpnn" -> "bobwan1995/PMFNet"
"SiyuanQi-zz/gpnn" -> "s-gupta/v-coco"
"SiyuanQi-zz/gpnn" -> "ywchao/ho-rcnn"
"SiyuanQi-zz/gpnn" -> "chinancheng/awesome-human-object-interaction"
"SiyuanQi-zz/gpnn" -> "BigRedT/no_frills_hoi_det"
"SiyuanQi-zz/gpnn" -> "yaohungt/Gated-Spatio-Temporal-Energy-Graph" ["e"=1]
"SiyuanQi-zz/gpnn" -> "YueLiao/PPDM"
"SiyuanQi-zz/gpnn" -> "Prof-Lu-Cewu/Visual-Relationship-Detection" ["e"=1]
"SiyuanQi-zz/gpnn" -> "ywchao/hico_benchmark"
"paulhibbing/AGread" -> "THLfi/read.gt3x"
"paulhibbing/AGread" -> "wadpac/oss-dev-webinar-series-pb-field"
"piergiaj/super-events-cvpr18" -> "jiyanggao/CBR"
"piergiaj/super-events-cvpr18" -> "shyamal-b/ss-tad"
"piergiaj/super-events-cvpr18" -> "VisionLearningGroup/R-C3D"
"piergiaj/super-events-cvpr18" -> "demianzhang/weakly-action-localization"
"piergiaj/super-events-cvpr18" -> "vdavid70619/TCN"
"piergiaj/super-events-cvpr18" -> "bellos1203/STPN"
"piergiaj/super-events-cvpr18" -> "jiyanggao/CTAP"
"piergiaj/super-events-cvpr18" -> "piergiaj/tgm-icml19"
"piergiaj/super-events-cvpr18" -> "wzmsltw/BSN-boundary-sensitive-network"
"IDKiro/Xenoblade2Voice" -> "IDKiro/ssmgr-install"
"kevin-ssy/Optical-Flow-Guided-Feature" -> "coderSkyChen/Action_Recognition_Zoo"
"kevin-ssy/Optical-Flow-Guided-Feature" -> "ZhaofanQiu/local-and-global-diffusion-networks"
"kevin-ssy/Optical-Flow-Guided-Feature" -> "wanglimin/UntrimmedNet"
"kevin-ssy/Optical-Flow-Guided-Feature" -> "MohsenFayyaz89/T3D"
"kevin-ssy/Optical-Flow-Guided-Feature" -> "wanglimin/ARTNet"
"zhujiagang/DTPP" -> "jiyanggao/CBR"
"jiaozizhao/Two-in-One-ActionDetection" -> "qinzhi-0110/pytorch-act-detector"
"jiaozizhao/Two-in-One-ActionDetection" -> "KevinDuarte/VideoCapsuleNet"
"arturjordao/WearableSensorData" -> "saif-mahmud/self-attention-HAR"
"arturjordao/WearableSensorData" -> "AdelaideAuto-IDLab/Attend-And-Discriminate"
"arturjordao/WearableSensorData" -> "isukrit/encodingHumanActivity"
"arturjordao/WearableSensorData" -> "saif-mahmud/hierarchical-attention-HAR"
"arturjordao/WearableSensorData" -> "ani8897/Human-Activity-Recognition"
"HumamAlwassel/DETAD" -> "xlliu7/MUSES"
"mostafa-saad/hierarchical-relational-network" -> "mostafa-saad/deep-activity-rec"
"mostafa-saad/hierarchical-relational-network" -> "wjchaoGit/Group-Activity-Recognition"
"KevinDuarte/VideoCapsuleNet" -> "jiaozizhao/Two-in-One-ActionDetection"
"KevinDuarte/VideoCapsuleNet" -> "KevinDuarte/CapsuleVOS"
"KevinDuarte/VideoCapsuleNet" -> "ZhaofanQiu/local-and-global-diffusion-networks"
"leaderj1001/Action-Localization" -> "oulutan/ActorConditionedAttentionMaps"
"qijiezhao/s3d.pytorch" -> "kylemin/S3D" ["e"=1]
"qijiezhao/s3d.pytorch" -> "qijiezhao/py-denseflow"
"qijiezhao/s3d.pytorch" -> "wanglimin/ARTNet"
"qijiezhao/s3d.pytorch" -> "coderSkyChen/Action_Recognition_Zoo"
"qijiezhao/s3d.pytorch" -> "MohsenFayyaz89/T3D"
"leftthomas/R2Plus1D-C3D" -> "irhum/R2Plus1D-PyTorch"
"salaniz/pytorch-gve-lrcn" -> "JaggerYoung/LRCN-for-Activity-Recognition"
"salaniz/pytorch-gve-lrcn" -> "garythung/torch-lrcn"
"SmartPorridge/google-AVA-Dataset-downloader" -> "xuzheyuan624/slowfast-keras"
"JunLi-Galios/CDFL" -> "alexanderrichard/NeuralNetwork-Viterbi"
"JunLi-Galios/CDFL" -> "alexanderrichard/action-sets"
"IDKiro/action-recognition" -> "siqinli/GestureRecognition-PyTorch"
"IDKiro/action-recognition" -> "bpeck81/CNN_RNN_Human_Action_Recognition"
"IDKiro/action-recognition" -> "IDKiro/ssmgr-install"
"IDKiro/action-recognition" -> "eriklindernoren/Action-Recognition"
"yabufarha/anticipating-activities" -> "ld-ing/tcfpn-isba"
"yabufarha/anticipating-activities" -> "alexanderrichard/NeuralNetwork-Viterbi"
"alexanderrichard/NeuralNetwork-Viterbi" -> "JunLi-Galios/CDFL"
"alexanderrichard/NeuralNetwork-Viterbi" -> "ld-ing/tcfpn-isba"
"alexanderrichard/NeuralNetwork-Viterbi" -> "alexanderrichard/squirrel"
"alexanderrichard/NeuralNetwork-Viterbi" -> "alexanderrichard/action-sets"
"qinzhi-0110/pytorch-act-detector" -> "jiaozizhao/Two-in-One-ActionDetection"
"qinzhi-0110/pytorch-act-detector" -> "imatge-upc/Action-Tubelet-Detection-in-AVA"
"bryanyzhu/awesome-action-recognition" -> "bryanyzhu/GuidedNet"
"blacknwhite5/pytorch-two-stream-CNN" -> "nsbb/SpatiotemporalNet"
"blacknwhite5/pytorch-two-stream-CNN" -> "Yorwxue/Two-Stream-Convolutional-Networks"
"FingerRec/FingerveinRecognitionModel3" -> "MohsenFayyaz89/FingerVein-SelfTaughtLearning"
"BigRedT/no_frills_hoi_det" -> "bobwan1995/PMFNet"
"BigRedT/no_frills_hoi_det" -> "vt-vl-lab/DRG"
"BigRedT/no_frills_hoi_det" -> "Dong-JinKim/ActionCooccurrencePriors"
"BigRedT/no_frills_hoi_det" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"BigRedT/no_frills_hoi_det" -> "chinancheng/awesome-human-object-interaction"
"BigRedT/no_frills_hoi_det" -> "vaesl/IP-Net"
"BigRedT/no_frills_hoi_det" -> "scwangdyd/zero_shot_hoi"
"BigRedT/no_frills_hoi_det" -> "fredzzhang/spatially-conditioned-graphs"
"BigRedT/no_frills_hoi_det" -> "ASMIftekhar/VSGNet"
"BigRedT/no_frills_hoi_det" -> "vt-vl-lab/iCAN"
"BigRedT/no_frills_hoi_det" -> "YueLiao/PIC_HOIW"
"BigRedT/no_frills_hoi_det" -> "s-gupta/v-coco"
"IBM/BigLittleNet" -> "IBM/bLVNet-TAM"
"MichiganCOG/M-PACT" -> "gurkirt/2D-kinectics"
"dazhang-cv/S3D" -> "demianzhang/weakly-action-localization"
"drbinliang/HMM-Action-Recognition" -> "kailiuXD/STWP"
"drbinliang/HMM-Action-Recognition" -> "huajh/action_recognition"
"demianzhang/weakly-action-localization" -> "dazhang-cv/S3D"
"demianzhang/weakly-action-localization" -> "zhengshou/AutoLoc"
"demianzhang/weakly-action-localization" -> "bellos1203/STPN"
"demianzhang/weakly-action-localization" -> "jiyanggao/CTAP"
"dspanah/Sensor-Based-Human-Activity-Recognition-LSTMsEnsemble-Pytorch" -> "tploetz/LSTMEnsemble4HAR"
"dspanah/Sensor-Based-Human-Activity-Recognition-LSTMsEnsemble-Pytorch" -> "dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch"
"ld-ing/tcfpn-isba" -> "alexanderrichard/NeuralNetwork-Viterbi"
"ld-ing/tcfpn-isba" -> "alexanderrichard/action-sets"
"ld-ing/tcfpn-isba" -> "yabufarha/anticipating-activities"
"holistic-video-understanding/Mini-HVU" -> "yassersouri/fandak"
"danbochman/Real-Time-Action-Recognition" -> "FingerRec/real_time_video_action_recognition"
"bo-yang/dtf_fisher" -> "bo-yang/stip_fisher"
"bo-yang/dtf_fisher" -> "chensun11/dtfv"
"alexanderrichard/action-sets" -> "ld-ing/tcfpn-isba"
"alexanderrichard/action-sets" -> "alexanderrichard/NeuralNetwork-Viterbi"
"alexanderrichard/action-sets" -> "demianzhang/weakly-action-localization"
"jiyanggao/CTAP" -> "jiyanggao/CBR"
"jiyanggao/CTAP" -> "demianzhang/weakly-action-localization"
"jiyanggao/CTAP" -> "asrafulashiq/wsad"
"jiyanggao/CTAP" -> "jiyanggao/TURN-TAP"
"jiyanggao/CTAP" -> "HYPJUDY/Decouple-SSAD"
"nsbb/SpatiotemporalNet" -> "blacknwhite5/pytorch-two-stream-CNN"
"masoudpz/AVID-Adversarial-Visual-Irregularity-Detection" -> "MohsenFayyaz89/FingerVein-SelfTaughtLearning"
"NVlabs/STEP" -> "aimagelab/STAGE_action_detection"
"NVlabs/STEP" -> "MVIG-SJTU/AlphAction"
"NVlabs/STEP" -> "MCG-NJU/MOC-Detector"
"NVlabs/STEP" -> "gurkirt/realtime-action-detection"
"NVlabs/STEP" -> "cvdfoundation/ava-dataset"
"NVlabs/STEP" -> "oulutan/ACAM_Demo"
"NVlabs/STEP" -> "Rheelt/Materials-Temporal-Action-Detection"
"NVlabs/STEP" -> "Siyu-C/ACAR-Net"
"NVlabs/STEP" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"NVlabs/STEP" -> "JJBOY/BMN-Boundary-Matching-Network"
"NVlabs/STEP" -> "leaderj1001/Action-Localization"
"NVlabs/STEP" -> "imatge-upc/Action-Tubelet-Detection-in-AVA"
"NVlabs/STEP" -> "facebookresearch/video-long-term-feature-banks"
"NVlabs/STEP" -> "qinzhi-0110/pytorch-act-detector"
"NVlabs/STEP" -> "jiaozizhao/Two-in-One-ActionDetection"
"open-mmlab/mmaction2" -> "facebookresearch/SlowFast"
"open-mmlab/mmaction2" -> "kennymckormick/pyskl" ["e"=1]
"open-mmlab/mmaction2" -> "open-mmlab/mmaction"
"open-mmlab/mmaction2" -> "SwinTransformer/Video-Swin-Transformer"
"open-mmlab/mmaction2" -> "mit-han-lab/temporal-shift-module"
"open-mmlab/mmaction2" -> "facebookresearch/pytorchvideo"
"open-mmlab/mmaction2" -> "open-mmlab/mmpose" ["e"=1]
"open-mmlab/mmaction2" -> "jinwchoi/awesome-action-recognition"
"open-mmlab/mmaction2" -> "open-mmlab/mmskeleton" ["e"=1]
"open-mmlab/mmaction2" -> "facebookresearch/TimeSformer"
"open-mmlab/mmaction2" -> "PaddlePaddle/PaddleVideo" ["e"=1]
"open-mmlab/mmaction2" -> "MCG-NJU/VideoMAE"
"open-mmlab/mmaction2" -> "yysijie/st-gcn" ["e"=1]
"open-mmlab/mmaction2" -> "open-mmlab/mmpretrain" ["e"=1]
"open-mmlab/mmaction2" -> "open-mmlab/mmcv" ["e"=1]
"facebookresearch/SlowFast" -> "open-mmlab/mmaction2"
"facebookresearch/SlowFast" -> "facebookresearch/pytorchvideo"
"facebookresearch/SlowFast" -> "mit-han-lab/temporal-shift-module"
"facebookresearch/SlowFast" -> "open-mmlab/mmaction"
"facebookresearch/SlowFast" -> "jinwchoi/awesome-action-recognition"
"facebookresearch/SlowFast" -> "kenshohara/3D-ResNets-PyTorch"
"facebookresearch/SlowFast" -> "facebookresearch/TimeSformer"
"facebookresearch/SlowFast" -> "facebookresearch/detr" ["e"=1]
"facebookresearch/SlowFast" -> "yjxiong/temporal-segment-networks"
"facebookresearch/SlowFast" -> "google-deepmind/kinetics-i3d"
"facebookresearch/SlowFast" -> "open-mmlab/mmskeleton" ["e"=1]
"facebookresearch/SlowFast" -> "facebookresearch/moco" ["e"=1]
"facebookresearch/SlowFast" -> "xingyizhou/CenterNet" ["e"=1]
"facebookresearch/SlowFast" -> "facebookresearch/VMZ"
"facebookresearch/SlowFast" -> "yjxiong/tsn-pytorch"
"IBM/action-recognition-pytorch" -> "Phoenix1327/tea-action-recognition"
"IBM/action-recognition-pytorch" -> "MCG-NJU/TDN"
"IBM/action-recognition-pytorch" -> "swathikirans/GSM"
"IBM/action-recognition-pytorch" -> "decisionforce/TPN"
"IBM/action-recognition-pytorch" -> "kkahatapitiya/X3D-Multigrid"
"IBM/action-recognition-pytorch" -> "zhang-can/PAN-PyTorch"
"IBM/action-recognition-pytorch" -> "joaanna/something_else"
"IBM/action-recognition-pytorch" -> "arunos728/MotionSqueeze"
"IBM/action-recognition-pytorch" -> "liu-zhy/temporal-adaptive-module"
"IBM/action-recognition-pytorch" -> "StanfordVL/RubiksNet"
"IBM/action-recognition-pytorch" -> "Chuhanxx/Temporal_Query_Networks" ["e"=1]
"IBM/action-recognition-pytorch" -> "mengyuest/AR-Net"
"IBM/action-recognition-pytorch" -> "eriklindernoren/Action-Recognition"
"mengyuest/AR-Net" -> "mengyuest/AdaFuse"
"mengyuest/AR-Net" -> "blackfeather-wang/AdaFocus" ["e"=1]
"junhyukoh/caffe-lstm" -> "jeffdonahue/caffe"
"junhyukoh/caffe-lstm" -> "LisaAnne/lisa-caffe-public"
"junhyukoh/caffe-lstm" -> "dophist/kaldi-lstm" ["e"=1]
"junhyukoh/caffe-lstm" -> "Russell91/nlpcaffe"
"junhyukoh/caffe-lstm" -> "yjxiong/caffe"
"junhyukoh/caffe-lstm" -> "tmbdev/clstm" ["e"=1]
"junhyukoh/caffe-lstm" -> "purine/purine2" ["e"=1]
"junhyukoh/caffe-lstm" -> "Russell91/apollocaffe" ["e"=1]
"junhyukoh/caffe-lstm" -> "JonathanRaiman/theano_lstm" ["e"=1]
"junhyukoh/caffe-lstm" -> "dmlc/cxxnet" ["e"=1]
"junhyukoh/caffe-lstm" -> "xingwangsfu/caffe-yolo" ["e"=1]
"junhyukoh/caffe-lstm" -> "daerduoCarey/SpatialTransformerLayer" ["e"=1]
"junhyukoh/caffe-lstm" -> "christopher5106/last_caffe_with_stn" ["e"=1]
"junhyukoh/caffe-lstm" -> "torrvision/crfasrnn" ["e"=1]
"junhyukoh/caffe-lstm" -> "xiaolonw/caffe-video_triplet" ["e"=1]
"sebgao/LIP" -> "alexandrosstergiou/SoftPool"
"sebgao/LIP" -> "visinf/dpp"
"sebgao/LIP" -> "MCG-NJU/BCN"
"sebgao/LIP" -> "MCG-NJU/FCOT" ["e"=1]
"sebgao/LIP" -> "MCG-NJU/AdaMixer" ["e"=1]
"sebgao/LIP" -> "MCG-NJU/CPD-Video"
"sebgao/LIP" -> "MCG-NJU/MMN" ["e"=1]
"sebgao/LIP" -> "MCG-NJU/CRCNN-Action"
"DirtyHarryLYL/HAKE-Action-Torch" -> "DirtyHarryLYL/HAKE-Action"
"DirtyHarryLYL/HAKE-Action-Torch" -> "DirtyHarryLYL/HAKE"
"DirtyHarryLYL/HAKE-Action-Torch" -> "DirtyHarryLYL/DJ-RN"
"DirtyHarryLYL/HAKE-Action-Torch" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"DirtyHarryLYL/HAKE-Action-Torch" -> "vt-vl-lab/DRG"
"DirtyHarryLYL/HAKE-Action-Torch" -> "DirtyHarryLYL/HOI-Learning-List"
"DirtyHarryLYL/HAKE-Action-Torch" -> "hitachi-rd-cv/qpic"
"DirtyHarryLYL/HAKE-Action-Torch" -> "fredzzhang/spatially-conditioned-graphs"
"DirtyHarryLYL/HAKE-Action-Torch" -> "DirtyHarryLYL/SymNet"
"DirtyHarryLYL/HAKE-Action-Torch" -> "vaesl/IP-Net"
"DirtyHarryLYL/HAKE-Action-Torch" -> "BigRedT/no_frills_hoi_det"
"DirtyHarryLYL/HAKE-Action-Torch" -> "s-gupta/v-coco"
"DirtyHarryLYL/HAKE-Action-Torch" -> "chinancheng/awesome-human-object-interaction"
"DirtyHarryLYL/HAKE-Action-Torch" -> "Dong-JinKim/ActionCooccurrencePriors"
"DirtyHarryLYL/HAKE-Action-Torch" -> "YueLiao/PPDM"
"yjxiong/caffe" -> "yjxiong/temporal-segment-networks"
"yjxiong/caffe" -> "yjxiong/anet2016-cuhk"
"yjxiong/caffe" -> "feichtenhofer/twostreamfusion"
"yjxiong/caffe" -> "wanglimin/dense_flow"
"yjxiong/caffe" -> "soeaver/caffe-model" ["e"=1]
"yjxiong/caffe" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"yjxiong/caffe" -> "wanglimin/ARTNet"
"yjxiong/caffe" -> "LisaAnne/lisa-caffe-public"
"yjxiong/caffe" -> "wanglimin/MRCNN-Scene-Recognition"
"yjxiong/caffe" -> "yjxiong/dense_flow"
"yjxiong/caffe" -> "facebookarchive/C3D"
"yjxiong/caffe" -> "sanghoon/pva-faster-rcnn" ["e"=1]
"yjxiong/caffe" -> "kracwarlock/action-recognition-visual-attention"
"yjxiong/caffe" -> "junhyukoh/caffe-lstm"
"yjxiong/caffe" -> "wanglimin/TDD"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "sujoyp/wtalc-pytorch"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "MichiganCOG/A2CL-PT"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "Finspire13/CMCS-Temporal-Action-Localization"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "bellos1203/STPN"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "Pilhyeon/BaSNet-pytorch"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "asrafulashiq/wsad"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "Flowerfan/SF-Net"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "naraysa/3c-net"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "Alvin-Zeng/PGCN"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "Rheelt/Materials-Temporal-Action-Detection"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "zhengshou/AutoLoc"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "HumamAlwassel/TSP"
"bfshi/DGAM-Weakly-Supervised-Action-Localization" -> "frostinassiky/gtad"
"STRCWearlab/DeepConvLSTM_py3" -> "dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "happyharrycn/actionformer_release"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "TencentYoutuResearch/ActionDetection-AFSD"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "frostinassiky/gtad"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "Rheelt/Materials-Temporal-Action-Detection"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "dingfengshi/TriDet"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "Alvin-Zeng/PGCN"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "xlliu7/TadTR"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "JJBOY/BMN-Boundary-Matching-Network"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "sujoyp/wtalc-pytorch"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "Finspire13/CMCS-Temporal-Action-Localization"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "HumamAlwassel/TSP"
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" -> "Finspire13/pytorch-i3d-feature-extraction"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "Alvin-Zeng/PGCN"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "JJBOY/BMN-Boundary-Matching-Network"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "Rheelt/Materials-Temporal-Action-Detection"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "frostinassiky/gtad"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "TencentYoutuResearch/ActionDetection-AFSD"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "happyharrycn/actionformer_release"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "wzmsltw/BSN-boundary-sensitive-network"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "Finspire13/pytorch-i3d-feature-extraction"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "piergiaj/pytorch-i3d"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "Tencent/ActionDetection-DBG"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "Finspire13/CMCS-Temporal-Action-Localization"
"Alvin-Zeng/Awesome-Temporal-Action-Localization" -> "Pilhyeon/BaSNet-pytorch"
"MVIG-SJTU/AlphAction" -> "Siyu-C/ACAR-Net"
"MVIG-SJTU/AlphAction" -> "NVlabs/STEP"
"MVIG-SJTU/AlphAction" -> "wei-tim/YOWO"
"MVIG-SJTU/AlphAction" -> "MCG-NJU/MOC-Detector"
"MVIG-SJTU/AlphAction" -> "Alpha-Video/AlphaVideo"
"MVIG-SJTU/AlphAction" -> "facebookresearch/video-long-term-feature-banks"
"MVIG-SJTU/AlphAction" -> "DirtyHarryLYL/HAKE-Action-Torch"
"MVIG-SJTU/AlphAction" -> "amazon-science/tubelet-transformer"
"MVIG-SJTU/AlphAction" -> "joslefaure/HIT"
"MVIG-SJTU/AlphAction" -> "decisionforce/TPN"
"MVIG-SJTU/AlphAction" -> "gurkirt/realtime-action-detection"
"MVIG-SJTU/AlphAction" -> "DirtyHarryLYL/HAKE"
"MVIG-SJTU/AlphAction" -> "cvdfoundation/ava-dataset"
"MVIG-SJTU/AlphAction" -> "MCG-NJU/TDN"
"MVIG-SJTU/AlphAction" -> "DirtyHarryLYL/HAKE-Action"
"Siyu-C/ACAR-Net" -> "MVIG-SJTU/AlphAction"
"Siyu-C/ACAR-Net" -> "MCG-NJU/MOC-Detector"
"Siyu-C/ACAR-Net" -> "amazon-science/tubelet-transformer"
"Siyu-C/ACAR-Net" -> "facebookresearch/video-long-term-feature-banks"
"Siyu-C/ACAR-Net" -> "joaanna/something_else"
"Siyu-C/ACAR-Net" -> "NVlabs/STEP"
"Siyu-C/ACAR-Net" -> "ShoufaChen/WOO"
"Siyu-C/ACAR-Net" -> "wei-tim/YOWO"
"Siyu-C/ACAR-Net" -> "decisionforce/TPN"
"Siyu-C/ACAR-Net" -> "MCG-NJU/MultiSports"
"Siyu-C/ACAR-Net" -> "Alvin-Zeng/PGCN"
"Siyu-C/ACAR-Net" -> "MCG-NJU/TDN"
"Siyu-C/ACAR-Net" -> "aimagelab/STAGE_action_detection"
"Siyu-C/ACAR-Net" -> "joslefaure/HIT"
"haoranD/Awesome-Human-Activity-Recognition" -> "yolish/har-with-imu-transformer"
"haoranD/Awesome-Human-Activity-Recognition" -> "saif-mahmud/self-attention-HAR"
"haoranD/Awesome-Human-Activity-Recognition" -> "Chaolei98/Baseline-with-HAR-datasets"
"haoranD/Awesome-Human-Activity-Recognition" -> "xushige/HAR-Dataset-Preprocess"
"haoranD/Awesome-Human-Activity-Recognition" -> "takumiw/Deep-Learning-for-Human-Activity-Recognition"
"haoranD/Awesome-Human-Activity-Recognition" -> "crocodilegogogo/IF-ConvTransformer-UbiComp2022"
"haoranD/Awesome-Human-Activity-Recognition" -> "markub3327/HAR-Transformer"
"haoranD/Awesome-Human-Activity-Recognition" -> "OxWearables/ssl-wearables"
"haoranD/Awesome-Human-Activity-Recognition" -> "getalp/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices"
"haoranD/Awesome-Human-Activity-Recognition" -> "dapowan/LIMU-BERT-Public" ["e"=1]
"haoranD/Awesome-Human-Activity-Recognition" -> "ma-shamshiri/Human-Activity-Recognition"
"haoranD/Awesome-Human-Activity-Recognition" -> "FIGLAB/IMUPoser" ["e"=1]
"haoranD/Awesome-Human-Activity-Recognition" -> "Hangwei12358/cross-person-HAR"
"haoranD/Awesome-Human-Activity-Recognition" -> "mmalekzadeh/dana"
"haoranD/Awesome-Human-Activity-Recognition" -> "mmalekzadeh/motion-sense"
"wei-tim/YOWO" -> "MCG-NJU/MOC-Detector"
"wei-tim/YOWO" -> "MVIG-SJTU/AlphAction"
"wei-tim/YOWO" -> "yjh0410/YOWOv2"
"wei-tim/YOWO" -> "yjh0410/PyTorch_YOWO"
"wei-tim/YOWO" -> "gurkirt/realtime-action-detection"
"wei-tim/YOWO" -> "Siyu-C/ACAR-Net"
"wei-tim/YOWO" -> "open-mmlab/mmaction"
"wei-tim/YOWO" -> "NVlabs/STEP"
"wei-tim/YOWO" -> "Tencent/ActionDetection-DBG"
"wei-tim/YOWO" -> "decisionforce/TPN"
"wei-tim/YOWO" -> "mit-han-lab/temporal-shift-module"
"wei-tim/YOWO" -> "cvdfoundation/ava-dataset"
"wei-tim/YOWO" -> "okankop/Efficient-3DCNNs"
"wei-tim/YOWO" -> "facebookresearch/video-long-term-feature-banks"
"wei-tim/YOWO" -> "Alvin-Zeng/PGCN"
"haohy/TCAN" -> "flrngel/TCN-with-attention"
"sj-li/MS-TCN2" -> "yabufarha/ms-tcn"
"sj-li/MS-TCN2" -> "ChinaYi/ASFormer"
"sj-li/MS-TCN2" -> "yiskw713/asrf"
"sj-li/MS-TCN2" -> "ttlmh/Bridge-Prompt"
"sj-li/MS-TCN2" -> "nus-cvml/awesome-temporal-action-segmentation"
"sj-li/MS-TCN2" -> "boschresearch/UVAST"
"sj-li/MS-TCN2" -> "ChinaYi/asrf_with_asformer"
"sj-li/MS-TCN2" -> "ahsaniqbal/Kinetics-FeatureExtractor"
"sj-li/MS-TCN2" -> "yiskw713/video_feature_extractor"
"yiskw713/asrf" -> "ChinaYi/asrf_with_asformer"
"yiskw713/asrf" -> "yiskw713/video_feature_extractor"
"yiskw713/asrf" -> "ChinaYi/ASFormer"
"yiskw713/asrf" -> "MCG-NJU/BCN"
"yiskw713/asrf" -> "cotton-ahn/HASR_iccv2021"
"yiskw713/asrf" -> "sj-li/MS-TCN2"
"JJBOY/BMN-Boundary-Matching-Network" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"JJBOY/BMN-Boundary-Matching-Network" -> "wzmsltw/BSN-boundary-sensitive-network"
"JJBOY/BMN-Boundary-Matching-Network" -> "frostinassiky/gtad"
"JJBOY/BMN-Boundary-Matching-Network" -> "Tencent/ActionDetection-DBG"
"JJBOY/BMN-Boundary-Matching-Network" -> "Alvin-Zeng/PGCN"
"JJBOY/BMN-Boundary-Matching-Network" -> "Rheelt/Materials-Temporal-Action-Detection"
"JJBOY/BMN-Boundary-Matching-Network" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"JJBOY/BMN-Boundary-Matching-Network" -> "Finspire13/CMCS-Temporal-Action-Localization"
"JJBOY/BMN-Boundary-Matching-Network" -> "yjxiong/action-detection"
"JJBOY/BMN-Boundary-Matching-Network" -> "HYPJUDY/Decouple-SSAD"
"JJBOY/BMN-Boundary-Matching-Network" -> "Pilhyeon/BaSNet-pytorch"
"JJBOY/BMN-Boundary-Matching-Network" -> "sujoyp/wtalc-pytorch"
"JJBOY/BMN-Boundary-Matching-Network" -> "TencentYoutuResearch/ActionDetection-AFSD"
"JJBOY/BMN-Boundary-Matching-Network" -> "MCG-NJU/RTD-Action"
"JJBOY/BMN-Boundary-Matching-Network" -> "Finspire13/pytorch-i3d-feature-extraction"
"frostinassiky/gtad" -> "Alvin-Zeng/PGCN"
"frostinassiky/gtad" -> "JJBOY/BMN-Boundary-Matching-Network"
"frostinassiky/gtad" -> "Rheelt/Materials-Temporal-Action-Detection"
"frostinassiky/gtad" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"frostinassiky/gtad" -> "HumamAlwassel/TSP"
"frostinassiky/gtad" -> "TencentYoutuResearch/ActionDetection-AFSD"
"frostinassiky/gtad" -> "Tencent/ActionDetection-DBG"
"frostinassiky/gtad" -> "MCG-NJU/RTD-Action"
"frostinassiky/gtad" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"frostinassiky/gtad" -> "Finspire13/CMCS-Temporal-Action-Localization"
"frostinassiky/gtad" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"frostinassiky/gtad" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"frostinassiky/gtad" -> "xlliu7/MUSES"
"frostinassiky/gtad" -> "wzmsltw/BSN-boundary-sensitive-network"
"frostinassiky/gtad" -> "Pilhyeon/BaSNet-pytorch"
"swathikirans/GSM" -> "deepcs233/TIN"
"swathikirans/GSM" -> "chenxuluo/GST-video"
"swathikirans/GSM" -> "decisionforce/TPN"
"swathikirans/GSM" -> "zhang-can/PAN-PyTorch"
"swathikirans/GSM" -> "Phoenix1327/tea-action-recognition"
"swathikirans/GSM" -> "arunos728/MotionSqueeze"
"swathikirans/GSM" -> "xhl-video/SmallBigNet"
"swathikirans/GSM" -> "MCG-NJU/TDN"
"swathikirans/GSM" -> "StanfordVL/RubiksNet"
"swathikirans/GSM" -> "liu-zhy/temporal-adaptive-module"
"swathikirans/GSM" -> "IBM/bLVNet-TAM"
"cmhungsteve/SSTDA" -> "MCG-NJU/BCN"
"cmhungsteve/SSTDA" -> "cmhungsteve/TA3N"
"cmhungsteve/SSTDA" -> "ZheLi2020/TimestampActionSeg"
"cmhungsteve/SSTDA" -> "alexanderrichard/NeuralNetwork-Viterbi"
"cmhungsteve/SSTDA" -> "yiskw713/asrf"
"cmhungsteve/SSTDA" -> "yabufarha/ms-tcn"
"cmhungsteve/SSTDA" -> "JunLi-Galios/CDFL"
"decisionforce/TPN" -> "Phoenix1327/tea-action-recognition"
"decisionforce/TPN" -> "zhoubolei/TRN-pytorch"
"decisionforce/TPN" -> "swathikirans/GSM"
"decisionforce/TPN" -> "MCG-NJU/TDN"
"decisionforce/TPN" -> "open-mmlab/mmaction"
"decisionforce/TPN" -> "arunos728/MotionSqueeze"
"decisionforce/TPN" -> "facebookresearch/video-long-term-feature-banks"
"decisionforce/TPN" -> "yjxiong/tsn-pytorch"
"decisionforce/TPN" -> "Alvin-Zeng/PGCN"
"decisionforce/TPN" -> "mzolfaghari/ECO-pytorch"
"decisionforce/TPN" -> "Rheelt/Materials-Temporal-Action-Detection"
"decisionforce/TPN" -> "Sense-X/X-Temporal" ["e"=1]
"decisionforce/TPN" -> "deepcs233/TIN"
"decisionforce/TPN" -> "r1c7/SlowFastNetworks"
"decisionforce/TPN" -> "Tencent/ActionDetection-DBG"
"PeisenZhao/Bottom-Up-TAL-with-MR" -> "VividLe/A2Net"
"PeisenZhao/Bottom-Up-TAL-with-MR" -> "MichiganCOG/A2CL-PT"
"PeisenZhao/Bottom-Up-TAL-with-MR" -> "LeonHLJ/FAC-Net"
"StanfordVL/RubiksNet" -> "arunos728/MotionSqueeze"
"StanfordVL/RubiksNet" -> "deepcs233/TIN"
"StanfordVL/RubiksNet" -> "msight-tech/research-v4d"
"StanfordVL/RubiksNet" -> "Phoenix1327/tea-action-recognition"
"mmalekzadeh/dana" -> "AdelaideAuto-IDLab/Attend-And-Discriminate"
"mmalekzadeh/dana" -> "flowerinheart/AttnSense"
"Tencent/ActionDetection-DBG" -> "JJBOY/BMN-Boundary-Matching-Network"
"Tencent/ActionDetection-DBG" -> "Rheelt/Materials-Temporal-Action-Detection"
"Tencent/ActionDetection-DBG" -> "Alvin-Zeng/PGCN"
"Tencent/ActionDetection-DBG" -> "frostinassiky/gtad"
"Tencent/ActionDetection-DBG" -> "wzmsltw/BSN-boundary-sensitive-network"
"Tencent/ActionDetection-DBG" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"Tencent/ActionDetection-DBG" -> "HYPJUDY/Decouple-SSAD"
"Tencent/ActionDetection-DBG" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"Tencent/ActionDetection-DBG" -> "Finspire13/CMCS-Temporal-Action-Localization"
"Tencent/ActionDetection-DBG" -> "zhengshou/scnn"
"Tencent/ActionDetection-DBG" -> "sujoyp/wtalc-pytorch"
"Tencent/ActionDetection-DBG" -> "decisionforce/TPN"
"Tencent/ActionDetection-DBG" -> "yjxiong/action-detection"
"Tencent/ActionDetection-DBG" -> "Pilhyeon/BaSNet-pytorch"
"Tencent/ActionDetection-DBG" -> "HumamAlwassel/TSP"
"DirtyHarryLYL/HOI-Learning-List" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"DirtyHarryLYL/HOI-Learning-List" -> "YueLiao/PPDM"
"DirtyHarryLYL/HOI-Learning-List" -> "fredzzhang/upt"
"DirtyHarryLYL/HOI-Learning-List" -> "hitachi-rd-cv/qpic"
"DirtyHarryLYL/HOI-Learning-List" -> "DirtyHarryLYL/HAKE-Action-Torch"
"DirtyHarryLYL/HOI-Learning-List" -> "DirtyHarryLYL/HAKE"
"DirtyHarryLYL/HOI-Learning-List" -> "YueLiao/gen-vlkt"
"DirtyHarryLYL/HOI-Learning-List" -> "s-gupta/v-coco"
"DirtyHarryLYL/HOI-Learning-List" -> "vt-vl-lab/iCAN"
"DirtyHarryLYL/HOI-Learning-List" -> "BigRedT/no_frills_hoi_det"
"DirtyHarryLYL/HOI-Learning-List" -> "zhihou7/HOI-CL"
"DirtyHarryLYL/HOI-Learning-List" -> "DirtyHarryLYL/HAKE-Action"
"DirtyHarryLYL/HOI-Learning-List" -> "YueLiao/CDN"
"DirtyHarryLYL/HOI-Learning-List" -> "kakaobrain/hotr"
"DirtyHarryLYL/HOI-Learning-List" -> "vt-vl-lab/DRG"
"Alvin-Zeng/PGCN" -> "frostinassiky/gtad"
"Alvin-Zeng/PGCN" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"Alvin-Zeng/PGCN" -> "JJBOY/BMN-Boundary-Matching-Network"
"Alvin-Zeng/PGCN" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"Alvin-Zeng/PGCN" -> "Finspire13/CMCS-Temporal-Action-Localization"
"Alvin-Zeng/PGCN" -> "Rheelt/Materials-Temporal-Action-Detection"
"Alvin-Zeng/PGCN" -> "Tencent/ActionDetection-DBG"
"Alvin-Zeng/PGCN" -> "sujoyp/wtalc-pytorch"
"Alvin-Zeng/PGCN" -> "wzmsltw/BSN-boundary-sensitive-network"
"Alvin-Zeng/PGCN" -> "HYPJUDY/Decouple-SSAD"
"Alvin-Zeng/PGCN" -> "TencentYoutuResearch/ActionDetection-AFSD"
"Alvin-Zeng/PGCN" -> "yjxiong/action-detection"
"Alvin-Zeng/PGCN" -> "sunnyxiaohu/R-C3D.pytorch"
"Alvin-Zeng/PGCN" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"Alvin-Zeng/PGCN" -> "Pilhyeon/BaSNet-pytorch"
"vaesl/IP-Net" -> "bobwan1995/PMFNet"
"vaesl/IP-Net" -> "tfzhou/C-HOI"
"vaesl/IP-Net" -> "YueLiao/PPDM"
"vaesl/IP-Net" -> "SherlockHolmes221/GGNet"
"vaesl/IP-Net" -> "fredzzhang/spatially-conditioned-graphs"
"vaesl/IP-Net" -> "vt-vl-lab/DRG"
"vaesl/IP-Net" -> "chinancheng/awesome-human-object-interaction"
"vaesl/IP-Net" -> "BigRedT/no_frills_hoi_det"
"vaesl/IP-Net" -> "yoyomimi/AS-Net"
"vaesl/IP-Net" -> "ASMIftekhar/VSGNet"
"vaesl/IP-Net" -> "zhihou7/VCL"
"vaesl/IP-Net" -> "ywchao/ho-rcnn"
"vaesl/IP-Net" -> "vt-vl-lab/iCAN"
"vaesl/IP-Net" -> "Dong-JinKim/ActionCooccurrencePriors"
"open-mmlab/denseflow" -> "innerlee/setup" ["e"=1]
"open-mmlab/denseflow" -> "yjxiong/dense_flow"
"open-mmlab/denseflow" -> "Finspire13/pytorch-i3d-feature-extraction"
"open-mmlab/denseflow" -> "HumamAlwassel/TSP"
"open-mmlab/denseflow" -> "frostinassiky/gtad"
"open-mmlab/denseflow" -> "TencentYoutuResearch/ActionDetection-AFSD"
"open-mmlab/denseflow" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"open-mmlab/denseflow" -> "MCG-NJU/RTD-Action"
"open-mmlab/denseflow" -> "amazon-science/long-short-term-transformer" ["e"=1]
"open-mmlab/denseflow" -> "open-mmlab/pre-commit-hooks" ["e"=1]
"open-mmlab/denseflow" -> "MCG-NJU/TDN"
"open-mmlab/denseflow" -> "wizyoung/Optical-Flow-GPU-Docker"
"open-mmlab/denseflow" -> "qijiezhao/py-denseflow"
"open-mmlab/denseflow" -> "JJBOY/BMN-Boundary-Matching-Network"
"open-mmlab/denseflow" -> "demianzhang/thumos14-i3d"
"DirtyHarryLYL/HAKE-Action" -> "DirtyHarryLYL/HAKE"
"DirtyHarryLYL/HAKE-Action" -> "DirtyHarryLYL/HAKE-Action-Torch"
"DirtyHarryLYL/HAKE-Action" -> "DirtyHarryLYL/SymNet"
"DirtyHarryLYL/HAKE-Action" -> "DirtyHarryLYL/DJ-RN"
"DirtyHarryLYL/HAKE-Action" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"DirtyHarryLYL/HAKE-Action" -> "bobwan1995/PMFNet"
"LukasBommes/mv-extractor" -> "jishnujayakumar/MV-Tractus"
"LukasBommes/mv-extractor" -> "vadimkantorov/mpegflow"
"LukasBommes/mv-extractor" -> "acherstyx/Compressed-Video-Reader"
"LukasBommes/mv-extractor" -> "chaoyuaw/pytorch-coviar"
"LukasBommes/mv-extractor" -> "LukasBommes/rtsp-streamsync"
"OxWearables/biobankAccelerometerAnalysis" -> "wadpac/GGIR"
"OxWearables/biobankAccelerometerAnalysis" -> "OxWearables/actipy"
"OxWearables/biobankAccelerometerAnalysis" -> "openmovementproject/openmovement"
"OxWearables/biobankAccelerometerAnalysis" -> "OxWearables/ssl-wearables"
"OxWearables/biobankAccelerometerAnalysis" -> "OxWearables/stepcount"
"OxWearables/biobankAccelerometerAnalysis" -> "OxWearables/capture24"
"OxWearables/biobankAccelerometerAnalysis" -> "ghammad/pyActigraphy"
"OxWearables/biobankAccelerometerAnalysis" -> "aschalkamp/UKBBprodromalPD"
"peachman05/action-recognition-tutorial" -> "ahmedgamaleldin14/online-action-recognition"
"joaanna/something_else" -> "gorjanradevski/revisiting-spatial-temporal-layouts"
"joaanna/something_else" -> "JingweiJ/ActionGenome" ["e"=1]
"joaanna/something_else" -> "Phoenix1327/tea-action-recognition"
"joaanna/something_else" -> "eladb3/ORViT"
"joaanna/something_else" -> "decisionforce/TPN"
"joaanna/something_else" -> "coldmanck/VidHOI" ["e"=1]
"joaanna/something_else" -> "pengzhansun/Counterfactual-Debiasing-Network"
"liu-zhy/temporal-adaptive-module" -> "Phoenix1327/tea-action-recognition"
"liu-zhy/temporal-adaptive-module" -> "MCG-NJU/TDN"
"liu-zhy/temporal-adaptive-module" -> "V-Sense/ACTION-Net"
"liu-zhy/temporal-adaptive-module" -> "arunos728/MotionSqueeze"
"liu-zhy/temporal-adaptive-module" -> "deepcs233/TIN"
"liu-zhy/temporal-adaptive-module" -> "decisionforce/TPN"
"liu-zhy/temporal-adaptive-module" -> "swathikirans/GSM"
"liu-zhy/temporal-adaptive-module" -> "alibaba-mmai-research/TAdaConv"
"liu-zhy/temporal-adaptive-module" -> "MCG-NJU/CPD-Video"
"liu-zhy/temporal-adaptive-module" -> "zhang-can/PAN-PyTorch"
"liu-zhy/temporal-adaptive-module" -> "tobyperrett/trx"
"liu-zhy/temporal-adaptive-module" -> "joaanna/something_else"
"liu-zhy/temporal-adaptive-module" -> "Anirudh257/strm"
"takumiw/Deep-Learning-for-Human-Activity-Recognition" -> "DigitalBiomarkerDiscoveryPipeline/Human-Activity-Recognition"
"takumiw/Deep-Learning-for-Human-Activity-Recognition" -> "AdelaideAuto-IDLab/Attend-And-Discriminate"
"doronharitan/human_activity_recognition_LRCN" -> "BizhuWu/LRCN_PyTorch"
"doronharitan/human_activity_recognition_LRCN" -> "DJAlexJ/LRCN-for-Video-Regression"
"doronharitan/human_activity_recognition_LRCN" -> "JaggerYoung/LRCN-for-Activity-Recognition"
"artest08/LateTemporalModeling3DCNN" -> "arunos728/MotionSqueeze"
"artest08/LateTemporalModeling3DCNN" -> "zhang-can/PAN-PyTorch"
"artest08/LateTemporalModeling3DCNN" -> "xhl-video/SmallBigNet"
"artest08/LateTemporalModeling3DCNN" -> "swathikirans/GSM"
"artest08/LateTemporalModeling3DCNN" -> "V-Sense/ACTION-Net"
"chinancheng/awesome-human-object-interaction" -> "BigRedT/no_frills_hoi_det"
"chinancheng/awesome-human-object-interaction" -> "vaesl/IP-Net"
"chinancheng/awesome-human-object-interaction" -> "yoyomimi/AS-Net"
"chinancheng/awesome-human-object-interaction" -> "MVIG-SJTU/DIRV"
"chinancheng/awesome-human-object-interaction" -> "SHI-Labs/Human-Object-Interaction-Detection"
"chinancheng/awesome-human-object-interaction" -> "bobwan1995/PMFNet"
"chinancheng/awesome-human-object-interaction" -> "vt-vl-lab/DRG"
"aimagelab/STAGE_action_detection" -> "NVlabs/STEP"
"wanglimin/dense_flow" -> "yjxiong/dense_flow"
"wanglimin/dense_flow" -> "feichtenhofer/gpu_flow"
"wanglimin/dense_flow" -> "qijiezhao/py-denseflow"
"wanglimin/dense_flow" -> "wanglimin/TDD"
"wanglimin/dense_flow" -> "yjxiong/caffe"
"wanglimin/dense_flow" -> "yangwangx/denseFlow_gpu"
"wanglimin/dense_flow" -> "feichtenhofer/twostreamfusion"
"wanglimin/dense_flow" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"wanglimin/dense_flow" -> "yjxiong/temporal-segment-networks"
"wanglimin/dense_flow" -> "USTC-Video-Understanding/I3D_Finetune"
"wanglimin/dense_flow" -> "wanglimin/improved_trajectory"
"wanglimin/dense_flow" -> "yjxiong/tsn-pytorch"
"wanglimin/dense_flow" -> "wanglimin/ARTNet"
"wanglimin/dense_flow" -> "wanglimin/UntrimmedNet"
"yiskw713/video_feature_extractor" -> "ChinaYi/asrf_with_asformer"
"yiskw713/video_feature_extractor" -> "yiskw713/asrf"
"ChiYaoLa/TimeSeriesPredict" -> "tranducquy/tcn-fx-price-prediction"
"isukrit/encodingHumanActivity" -> "flowerinheart/AttnSense"
"wangxiang1230/OHEM" -> "alibaba-mmai-research/HyRSMPlusPlus"
"YueLiao/PPDM" -> "vaesl/IP-Net"
"YueLiao/PPDM" -> "bobwan1995/PMFNet"
"YueLiao/PPDM" -> "YueLiao/gen-vlkt"
"YueLiao/PPDM" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"YueLiao/PPDM" -> "YueLiao/CDN"
"YueLiao/PPDM" -> "hitachi-rd-cv/qpic"
"YueLiao/PPDM" -> "yoyomimi/AS-Net"
"YueLiao/PPDM" -> "vt-vl-lab/iCAN"
"YueLiao/PPDM" -> "vt-vl-lab/DRG"
"YueLiao/PPDM" -> "tfzhou/C-HOI"
"YueLiao/PPDM" -> "kakaobrain/hotr"
"YueLiao/PPDM" -> "s-gupta/v-coco"
"YueLiao/PPDM" -> "DirtyHarryLYL/HOI-Learning-List"
"YueLiao/PPDM" -> "fredzzhang/upt"
"YueLiao/PPDM" -> "ASMIftekhar/VSGNet"
"Pilhyeon/BaSNet-pytorch" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"Pilhyeon/BaSNet-pytorch" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"Pilhyeon/BaSNet-pytorch" -> "Pilhyeon/Learning-Action-Completeness-from-Points"
"Pilhyeon/BaSNet-pytorch" -> "sujoyp/wtalc-pytorch"
"Pilhyeon/BaSNet-pytorch" -> "Finspire13/CMCS-Temporal-Action-Localization"
"Pilhyeon/BaSNet-pytorch" -> "bellos1203/STPN"
"Pilhyeon/BaSNet-pytorch" -> "MichiganCOG/A2CL-PT"
"Pilhyeon/BaSNet-pytorch" -> "naraysa/3c-net"
"Pilhyeon/BaSNet-pytorch" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"Pilhyeon/BaSNet-pytorch" -> "asrafulashiq/wsad"
"Pilhyeon/BaSNet-pytorch" -> "Rheelt/Materials-Temporal-Action-Detection"
"Pilhyeon/BaSNet-pytorch" -> "ispc-lab/ACM-Net"
"Pilhyeon/BaSNet-pytorch" -> "Alvin-Zeng/PGCN"
"Pilhyeon/BaSNet-pytorch" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"Pilhyeon/BaSNet-pytorch" -> "JJBOY/BMN-Boundary-Matching-Network"
"Dong-JinKim/ActionCooccurrencePriors" -> "UCSB-VRL/GTNet"
"Dong-JinKim/ActionCooccurrencePriors" -> "BigRedT/no_frills_hoi_det"
"lpphd/multivariate-attention-tcn" -> "flaviagiammarino/tcan-tensorflow"
"kkahatapitiya/X3D-Multigrid" -> "kkahatapitiya/Coarse-Fine-Networks"
"fuankarion/active-speakers-context" -> "okankop/ASDNet"
"fuankarion/active-speakers-context" -> "Overcautious/ADENet"
"fuankarion/active-speakers-context" -> "tuanchien/asd"
"fuankarion/active-speakers-context" -> "SRA2/SPELL"
"MCG-NJU/MOC-Detector" -> "wei-tim/YOWO"
"MCG-NJU/MOC-Detector" -> "Siyu-C/ACAR-Net"
"MCG-NJU/MOC-Detector" -> "MCG-NJU/MultiSports"
"MCG-NJU/MOC-Detector" -> "NVlabs/STEP"
"MCG-NJU/MOC-Detector" -> "MCG-NJU/CRCNN-Action"
"MCG-NJU/MOC-Detector" -> "amazon-science/tubelet-transformer"
"MCG-NJU/MOC-Detector" -> "MVIG-SJTU/AlphAction"
"MCG-NJU/MOC-Detector" -> "gurkirt/realtime-action-detection"
"MCG-NJU/MOC-Detector" -> "Rheelt/Materials-Temporal-Action-Detection"
"MCG-NJU/MOC-Detector" -> "JJBOY/BMN-Boundary-Matching-Network"
"MCG-NJU/MOC-Detector" -> "MCG-NJU/FCOT" ["e"=1]
"MCG-NJU/MOC-Detector" -> "MCG-NJU/TDN"
"MCG-NJU/MOC-Detector" -> "facebookresearch/video-long-term-feature-banks"
"MCG-NJU/MOC-Detector" -> "qinzhi-0110/pytorch-act-detector"
"MCG-NJU/MOC-Detector" -> "Phoenix1327/tea-action-recognition"
"DirtyHarryLYL/DJ-RN" -> "DirtyHarryLYL/SymNet"
"DirtyHarryLYL/DJ-RN" -> "DirtyHarryLYL/HAKE-Action"
"DirtyHarryLYL/DJ-RN" -> "DirtyHarryLYL/HAKE-Action-Torch"
"DirtyHarryLYL/DJ-RN" -> "scwangdyd/zero_shot_hoi"
"DirtyHarryLYL/DJ-RN" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"MichiganCOG/A2CL-PT" -> "naraysa/3c-net"
"MichiganCOG/A2CL-PT" -> "VividLe/A2Net"
"MichiganCOG/A2CL-PT" -> "asrafulashiq/wsad"
"MichiganCOG/A2CL-PT" -> "asrafulashiq/hamnet"
"MichiganCOG/A2CL-PT" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"MichiganCOG/A2CL-PT" -> "sujoyp/wtalc-pytorch"
"MichiganCOG/A2CL-PT" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"vt-vl-lab/SDN" -> "gurkirt/2D-kinectics"
"vt-vl-lab/SDN" -> "chenxuluo/GST-video"
"DigitalBiomarkerDiscoveryPipeline/Human-Activity-Recognition" -> "saif-mahmud/self-attention-HAR"
"mxbi/youtube8m-2019" -> "linrongc/solution_youtube8m_v3"
"openmovementproject/openmovement" -> "wadpac/GGIR"
"openmovementproject/openmovement" -> "OxWearables/biobankAccelerometerAnalysis"
"openmovementproject/openmovement" -> "jbrond/ActigraphCounts"
"openmovementproject/openmovement" -> "OxWearables/actipy"
"openmovementproject/openmovement" -> "OxWearables/ssl-wearables"
"hou-yz/MVDet" -> "hou-yz/MultiviewX"
"hou-yz/MVDet" -> "hou-yz/MVDeTr"
"hou-yz/MVDet" -> "tteepe/EarlyBird"
"hou-yz/MVDet" -> "xjtlu-cvlab/3DROM"
"hou-yz/MVDet" -> "sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint"
"scwangdyd/zero_shot_hoi" -> "yeliudev/ConsNet"
"wadpac/oss-dev-webinar-series-pb-field" -> "paulhibbing/AGread"
"Phoenix1327/tea-action-recognition" -> "MCG-NJU/TDN"
"Phoenix1327/tea-action-recognition" -> "V-Sense/ACTION-Net"
"Phoenix1327/tea-action-recognition" -> "decisionforce/TPN"
"Phoenix1327/tea-action-recognition" -> "arunos728/MotionSqueeze"
"Phoenix1327/tea-action-recognition" -> "liu-zhy/temporal-adaptive-module"
"Phoenix1327/tea-action-recognition" -> "zhang-can/PAN-PyTorch"
"Phoenix1327/tea-action-recognition" -> "swathikirans/GSM"
"Phoenix1327/tea-action-recognition" -> "deepcs233/TIN"
"Phoenix1327/tea-action-recognition" -> "IBM/action-recognition-pytorch"
"Phoenix1327/tea-action-recognition" -> "joaanna/something_else"
"Phoenix1327/tea-action-recognition" -> "StanfordVL/RubiksNet"
"Phoenix1327/tea-action-recognition" -> "MCG-NJU/MOC-Detector"
"Phoenix1327/tea-action-recognition" -> "Alibaba-MIIL/STAM"
"srvCodes/continual-learning-benchmark" -> "saif-mahmud/self-attention-HAR"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "Pilhyeon/BaSNet-pytorch"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "Pilhyeon/Learning-Action-Completeness-from-Points"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "MichiganCOG/A2CL-PT"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "zhang-can/CoLA"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "Finspire13/CMCS-Temporal-Action-Localization"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "sujoyp/wtalc-pytorch"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "asrafulashiq/wsad"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "ispc-lab/ACM-Net"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "bellos1203/STPN"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "asrafulashiq/hamnet"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "naraysa/3c-net"
"Pilhyeon/WTAL-Uncertainty-Modeling" -> "harlanhong/MM2021-CO2-Net"
"hueihan/Action_Recognition" -> "huajh/action_recognition"
"deepcs233/TIN" -> "xhl-video/SmallBigNet"
"deepcs233/TIN" -> "swathikirans/GSM"
"deepcs233/TIN" -> "chenxuluo/GST-video"
"deepcs233/TIN" -> "zhang-can/PAN-PyTorch"
"deepcs233/TIN" -> "arunos728/MotionSqueeze"
"deepcs233/TIN" -> "StanfordVL/RubiksNet"
"deepcs233/TIN" -> "Phoenix1327/tea-action-recognition"
"MCG-NJU/CPD-Video" -> "MCG-NJU/SSD-LT"
"MCG-NJU/CPD-Video" -> "mcg2019/MOC-Detector"
"anenbergb/CS221_Project" -> "kaiqiangh/IDT_Fisher_Vector"
"anenbergb/CS221_Project" -> "chensun11/dtfv"
"anenbergb/CS221_Project" -> "jvgemert/apt"
"holistic-video-understanding/HVU-Dataset" -> "holistic-video-understanding/Mini-HVU"
"holistic-video-understanding/HVU-Dataset" -> "yassersouri/fandak"
"holistic-video-understanding/HVU-Dataset" -> "MohsenFayyaz89/SCT"
"Alpha-Video/AlphaVideo" -> "BoPang1996/TubeTK" ["e"=1]
"Alpha-Video/AlphaVideo" -> "MVIG-SJTU/AlphAction"
"Alpha-Video/AlphaVideo" -> "BoPang1996/Semi-Coupled-Structure-for-visual-sequental-tasks"
"Alpha-Video/AlphaVideo" -> "DirtyHarryLYL/HAKE-Action-Torch"
"ASMIftekhar/VSGNet" -> "zhihou7/VCL"
"ASMIftekhar/VSGNet" -> "bobwan1995/PMFNet"
"ASMIftekhar/VSGNet" -> "vt-vl-lab/DRG"
"ASMIftekhar/VSGNet" -> "vaesl/IP-Net"
"ASMIftekhar/VSGNet" -> "BigRedT/no_frills_hoi_det"
"ASMIftekhar/VSGNet" -> "Dong-JinKim/ActionCooccurrencePriors"
"ASMIftekhar/VSGNet" -> "YueLiao/PPDM"
"ASMIftekhar/VSGNet" -> "tfzhou/C-HOI"
"ASMIftekhar/VSGNet" -> "bbepoch/HoiTransformer"
"ASMIftekhar/VSGNet" -> "fredzzhang/spatially-conditioned-graphs"
"ASMIftekhar/VSGNet" -> "YueLiao/gen-vlkt"
"ASMIftekhar/VSGNet" -> "vt-vl-lab/iCAN"
"ASMIftekhar/VSGNet" -> "YueLiao/CDN"
"zhihou7/VCL" -> "scwangdyd/large_vocabulary_hoi_detection"
"zhihou7/VCL" -> "vt-vl-lab/DRG"
"zhihou7/VCL" -> "ASMIftekhar/VSGNet"
"vadimkantorov/fastvideofeat" -> "zbwglory/MV-release"
"vadimkantorov/fastvideofeat" -> "huajh/action_recognition"
"Flowerfan/SF-Net" -> "asrafulashiq/hamnet"
"Flowerfan/SF-Net" -> "ispc-lab/ACM-Net"
"Flowerfan/SF-Net" -> "Pilhyeon/Learning-Action-Completeness-from-Points"
"Flowerfan/SF-Net" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"Flowerfan/SF-Net" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"hou-yz/MultiviewX" -> "hou-yz/MVDeTr"
"hou-yz/MultiviewX" -> "hou-yz/MVDet"
"tfzhou/C-HOI" -> "vaesl/IP-Net"
"tfzhou/C-HOI" -> "bobwan1995/PMFNet"
"tfzhou/C-HOI" -> "yoyomimi/AS-Net"
"tfzhou/C-HOI" -> "fredzzhang/spatially-conditioned-graphs"
"tfzhou/C-HOI" -> "YueLiao/PPDM"
"tfzhou/C-HOI" -> "vt-vl-lab/DRG"
"tfzhou/C-HOI" -> "ASMIftekhar/VSGNet"
"tfzhou/C-HOI" -> "ywchao/ho-rcnn"
"tfzhou/C-HOI" -> "YueLiao/PIC_HOIW"
"KevinDuarte/CapsuleVOS" -> "KevinDuarte/VideoCapsuleNet"
"gautamkumarjaiswal/videoClassification" -> "rekon/T3D-keras"
"naraysa/3c-net" -> "MichiganCOG/A2CL-PT"
"naraysa/3c-net" -> "sujoyp/wtalc-pytorch"
"naraysa/3c-net" -> "asrafulashiq/wsad"
"naraysa/3c-net" -> "zhengshou/AutoLoc"
"naraysa/3c-net" -> "Finspire13/CMCS-Temporal-Action-Localization"
"naraysa/3c-net" -> "demianzhang/weakly-action-localization"
"naraysa/3c-net" -> "bellos1203/STPN"
"naraysa/3c-net" -> "Pilhyeon/BaSNet-pytorch"
"bobwan1995/PMFNet" -> "BigRedT/no_frills_hoi_det"
"bobwan1995/PMFNet" -> "vaesl/IP-Net"
"bobwan1995/PMFNet" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"bobwan1995/PMFNet" -> "ASMIftekhar/VSGNet"
"bobwan1995/PMFNet" -> "YueLiao/PPDM"
"bobwan1995/PMFNet" -> "Dong-JinKim/ActionCooccurrencePriors"
"bobwan1995/PMFNet" -> "ywchao/ho-rcnn"
"bobwan1995/PMFNet" -> "MVIG-SJTU/DIRV"
"bobwan1995/PMFNet" -> "yoyomimi/AS-Net"
"bobwan1995/PMFNet" -> "tfzhou/C-HOI"
"bobwan1995/PMFNet" -> "birlrobotics/PMN"
"bobwan1995/PMFNet" -> "fredzzhang/spatially-conditioned-graphs"
"bobwan1995/PMFNet" -> "vt-vl-lab/iCAN"
"bobwan1995/PMFNet" -> "zhihou7/VCL"
"bobwan1995/PMFNet" -> "YueLiao/PIC_HOIW"
"MCG-NJU/BCN" -> "yiskw713/asrf"
"MCG-NJU/BCN" -> "ZheLi2020/TimestampActionSeg"
"MCG-NJU/BCN" -> "cotton-ahn/HASR_iccv2021"
"MCG-NJU/BCN" -> "ChinaYi/ASFormer"
"IBM/bLVNet-TAM" -> "IBM/BigLittleNet"
"huguyuehuhu/Awesome-Group-Activity-Recognition" -> "ruiyan1995/HiGCIN"
"huguyuehuhu/Awesome-Group-Activity-Recognition" -> "wjchaoGit/Group-Activity-Recognition"
"huguyuehuhu/Awesome-Group-Activity-Recognition" -> "ruiyan1995/Group-Activity-Recognition"
"vt-vl-lab/DRG" -> "fredzzhang/spatially-conditioned-graphs"
"vt-vl-lab/DRG" -> "zhihou7/VCL"
"vt-vl-lab/DRG" -> "BigRedT/no_frills_hoi_det"
"vt-vl-lab/DRG" -> "vaesl/IP-Net"
"vt-vl-lab/DRG" -> "YueLiao/CDN"
"vt-vl-lab/DRG" -> "hitachi-rd-cv/qpic"
"vt-vl-lab/DRG" -> "ASMIftekhar/VSGNet"
"vt-vl-lab/DRG" -> "yeliudev/ConsNet"
"vt-vl-lab/DRG" -> "vt-vl-lab/iCAN"
"vt-vl-lab/DRG" -> "Dong-JinKim/ActionCooccurrencePriors"
"VividLe/A2Net" -> "MichiganCOG/A2CL-PT"
"VividLe/A2Net" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"arunos728/MotionSqueeze" -> "Phoenix1327/tea-action-recognition"
"arunos728/MotionSqueeze" -> "StanfordVL/RubiksNet"
"arunos728/MotionSqueeze" -> "zhang-can/PAN-PyTorch"
"arunos728/MotionSqueeze" -> "deepcs233/TIN"
"arunos728/MotionSqueeze" -> "Andy1621/CT-Net"
"arunos728/MotionSqueeze" -> "KimManjin/RSA" ["e"=1]
"arunos728/MotionSqueeze" -> "arunos728/SELFY"
"arunos728/MotionSqueeze" -> "xhl-video/SmallBigNet"
"arunos728/MotionSqueeze" -> "swathikirans/GSM"
"arunos728/MotionSqueeze" -> "decisionforce/TPN"
"arunos728/MotionSqueeze" -> "MCG-NJU/TDN"
"arunos728/MotionSqueeze" -> "chenxuluo/GST-video"
"arunos728/MotionSqueeze" -> "artest08/LateTemporalModeling3DCNN"
"arunos728/MotionSqueeze" -> "liu-zhy/temporal-adaptive-module"
"arunos728/MotionSqueeze" -> "mengyuest/AR-Net"
"chenxuluo/GST-video" -> "deepcs233/TIN"
"xhl-video/SmallBigNet" -> "deepcs233/TIN"
"xhl-video/SmallBigNet" -> "arunos728/SELFY"
"yeliudev/ConsNet" -> "scwangdyd/large_vocabulary_hoi_detection"
"zhang-can/PAN-PyTorch" -> "Phoenix1327/tea-action-recognition"
"zhang-can/PAN-PyTorch" -> "arunos728/MotionSqueeze"
"zhang-can/PAN-PyTorch" -> "deepcs233/TIN"
"zhang-can/PAN-PyTorch" -> "swathikirans/GSM"
"zhang-can/PAN-PyTorch" -> "chenxuluo/GST-video"
"zhang-can/PAN-PyTorch" -> "tianyuan168326/EAN-Pytorch"
"zhang-can/PAN-PyTorch" -> "MCG-NJU/MGSampler"
"zhang-can/PAN-PyTorch" -> "MCG-NJU/TDN"
"zhang-can/PAN-PyTorch" -> "xhl-video/SmallBigNet"
"asrafulashiq/wsad" -> "MichiganCOG/A2CL-PT"
"asrafulashiq/wsad" -> "naraysa/3c-net"
"asrafulashiq/wsad" -> "bellos1203/STPN"
"knmac/LCDC_release" -> "ChinaYi/asrf_with_asformer"
"MohsenFayyaz89/SCT" -> "yassersouri/fandak"
"MohsenFayyaz89/SCT" -> "MohsenFayyaz89/FingerVein-SelfTaughtLearning"
"khoiucd/LearningToCompare-Tensorflow" -> "nguyentthong/CrossSummOptimalTransport"
"khoiucd/LearningToCompare-Tensorflow" -> "lehduong/poodle"
"linrongc/solution_youtube8m_v3" -> "mxbi/youtube8m-2019"
"DirtyHarryLYL/SymNet" -> "DirtyHarryLYL/DJ-RN"
"DirtyHarryLYL/SymNet" -> "DirtyHarryLYL/HAKE-Action"
"DirtyHarryLYL/SymNet" -> "silicx/GoldFromOres-BiLP" ["e"=1]
"ceshine/yt8m-2019" -> "mxbi/youtube8m-2019"
"ceshine/yt8m-2019" -> "zhangyaoyuan/NextVLAD-Attention-Model"
"lehduong/Knowledge-Distillation-by-Replacing-Cheap-Conv" -> "lehduong/poodle"
"rishikksh20/ViViT-pytorch" -> "mx-mark/VideoTransformer-pytorch"
"rishikksh20/ViViT-pytorch" -> "drv-agwl/ViViT-pytorch"
"rishikksh20/ViViT-pytorch" -> "lucidrains/STAM-pytorch"
"rishikksh20/ViViT-pytorch" -> "SwinTransformer/Video-Swin-Transformer"
"rishikksh20/ViViT-pytorch" -> "facebookresearch/TimeSformer"
"rishikksh20/ViViT-pytorch" -> "lucidrains/TimeSformer-pytorch"
"rishikksh20/ViViT-pytorch" -> "google-research/scenic"
"rishikksh20/ViViT-pytorch" -> "haofanwang/video-swin-transformer-pytorch"
"rishikksh20/ViViT-pytorch" -> "noureldien/vivit_pytorch"
"rishikksh20/ViViT-pytorch" -> "cvdfoundation/kinetics-dataset"
"rishikksh20/ViViT-pytorch" -> "bomri/SlowFast"
"rishikksh20/ViViT-pytorch" -> "sallymmx/ActionCLIP"
"rishikksh20/ViViT-pytorch" -> "antoine77340/MIL-NCE_HowTo100M" ["e"=1]
"rishikksh20/ViViT-pytorch" -> "v-iashin/video_features" ["e"=1]
"rishikksh20/ViViT-pytorch" -> "MCG-NJU/VideoMAE"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "YuxinZhaozyx/pytorch-VideoDataset"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "xiaobai1217/Awesome-Video-Datasets"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "IBM/action-recognition-pytorch"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "rishikksh20/ViViT-pytorch"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "okankop/vidaug"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "hassony2/torch_videovision"
"RaivoKoot/Video-Dataset-Loading-Pytorch" -> "dmlc/decord"
"drv-agwl/ViViT-pytorch" -> "noureldien/vivit_pytorch"
"drv-agwl/ViViT-pytorch" -> "rishikksh20/ViViT-pytorch"
"noureldien/vivit_pytorch" -> "drv-agwl/ViViT-pytorch"
"google-research/scenic" -> "SwinTransformer/Video-Swin-Transformer"
"google-research/scenic" -> "microsoft/GLIP" ["e"=1]
"google-research/scenic" -> "google-research/big_vision" ["e"=1]
"google-research/scenic" -> "rishikksh20/ViViT-pytorch"
"google-research/scenic" -> "facebookresearch/TimeSformer"
"google-research/scenic" -> "MCG-NJU/VideoMAE"
"google-research/scenic" -> "OpenGVLab/InternVideo" ["e"=1]
"google-research/scenic" -> "open-mmlab/mmaction2"
"google-research/scenic" -> "salesforce/LAVIS" ["e"=1]
"google-research/scenic" -> "facebookresearch/SlowFast"
"google-research/scenic" -> "facebookresearch/pytorchvideo"
"google-research/scenic" -> "google-research/vision_transformer" ["e"=1]
"google-research/scenic" -> "google/flax" ["e"=1]
"google-research/scenic" -> "mlfoundations/open_clip" ["e"=1]
"google-research/scenic" -> "facebookresearch/deit" ["e"=1]
"TaoRuijie/TalkNet-ASD" -> "Junhua-Liao/Light-ASD"
"TaoRuijie/TalkNet-ASD" -> "okankop/ASDNet"
"TaoRuijie/TalkNet-ASD" -> "SRA2/SPELL"
"TaoRuijie/TalkNet-ASD" -> "Jiang-Yidi/TS-TalkNet"
"TaoRuijie/TalkNet-ASD" -> "fuankarion/active-speakers-context"
"TaoRuijie/TalkNet-ASD" -> "X-LANCE/MSDWILD"
"TaoRuijie/TalkNet-ASD" -> "facebookresearch/VisualVoice" ["e"=1]
"TaoRuijie/TalkNet-ASD" -> "danmic/av-se" ["e"=1]
"TaoRuijie/TalkNet-ASD" -> "joonson/syncnet_python" ["e"=1]
"TaoRuijie/TalkNet-ASD" -> "EGO4D/audio-visual"
"TaoRuijie/TalkNet-ASD" -> "afourast/avobjects" ["e"=1]
"TaoRuijie/TalkNet-ASD" -> "cvdfoundation/ava-dataset"
"TaoRuijie/TalkNet-ASD" -> "zcxu-eric/Ego4d_TalkNet_ASD"
"TaoRuijie/TalkNet-ASD" -> "clovaai/lookwhostalking"
"TaoRuijie/TalkNet-ASD" -> "TaoRuijie/Speaker-Recognition-Demo"
"facebookarchive/C3D" -> "hx173149/C3D-tensorflow"
"facebookarchive/C3D" -> "yjxiong/temporal-segment-networks"
"facebookarchive/C3D" -> "google-deepmind/kinetics-i3d"
"facebookarchive/C3D" -> "feichtenhofer/twostreamfusion"
"facebookarchive/C3D" -> "facebookresearch/VMZ"
"facebookarchive/C3D" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"facebookarchive/C3D" -> "activitynet/ActivityNet"
"facebookarchive/C3D" -> "yjxiong/tsn-pytorch"
"facebookarchive/C3D" -> "chuckcho/video-caffe"
"facebookarchive/C3D" -> "jfzhang95/pytorch-video-recognition"
"facebookarchive/C3D" -> "qijiezhao/pseudo-3d-pytorch"
"facebookarchive/C3D" -> "DavideA/c3d-pytorch"
"facebookarchive/C3D" -> "jeffreyyihuang/two-stream-action-recognition"
"facebookarchive/C3D" -> "feichtenhofer/gpu_flow"
"facebookarchive/C3D" -> "yjxiong/caffe"
"bbepoch/HoiTransformer" -> "kakaobrain/hotr"
"bbepoch/HoiTransformer" -> "hitachi-rd-cv/qpic"
"bbepoch/HoiTransformer" -> "yoyomimi/AS-Net"
"bbepoch/HoiTransformer" -> "YueLiao/CDN"
"bbepoch/HoiTransformer" -> "cjw2021/QAHOI"
"bbepoch/HoiTransformer" -> "zyong812/STIP"
"bbepoch/HoiTransformer" -> "vaesl/IP-Net"
"bbepoch/HoiTransformer" -> "ASMIftekhar/VSGNet"
"bbepoch/HoiTransformer" -> "YueLiao/PPDM"
"bbepoch/HoiTransformer" -> "fredzzhang/spatially-conditioned-graphs"
"bbepoch/HoiTransformer" -> "SHI-Labs/Human-Object-Interaction-Detection"
"bbepoch/HoiTransformer" -> "vt-vl-lab/DRG"
"bbepoch/HoiTransformer" -> "zhihou7/HOI-CL"
"bbepoch/HoiTransformer" -> "bobwan1995/PMFNet"
"bbepoch/HoiTransformer" -> "MuchHair/HQM"
"xiaobai1217/Awesome-Video-Datasets" -> "yunlong10/Awesome-LLMs-for-Video-Understanding" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "OpenGVLab/InternVideo" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "mbzuai-oryx/Video-ChatGPT" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "showlab/Awesome-Video-Diffusion" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "ChenHsing/Awesome-Video-Diffusion-Models" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "microsoft/XPretrain" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "MCG-NJU/VideoMAE"
"xiaobai1217/Awesome-Video-Datasets" -> "facebookresearch/pytorchvideo"
"xiaobai1217/Awesome-Video-Datasets" -> "iejMac/video2dataset" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "Vchitect/VBench" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "krantiparida/awesome-audio-visual" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "google-research/scenic"
"xiaobai1217/Awesome-Video-Datasets" -> "DAMO-NLP-SG/Video-LLaMA" ["e"=1]
"xiaobai1217/Awesome-Video-Datasets" -> "cvdfoundation/kinetics-dataset"
"xiaobai1217/Awesome-Video-Datasets" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"facebookresearch/pytorchvideo" -> "facebookresearch/SlowFast"
"facebookresearch/pytorchvideo" -> "open-mmlab/mmaction2"
"facebookresearch/pytorchvideo" -> "facebookresearch/TimeSformer"
"facebookresearch/pytorchvideo" -> "MCG-NJU/VideoMAE"
"facebookresearch/pytorchvideo" -> "dmlc/decord"
"facebookresearch/pytorchvideo" -> "mit-han-lab/temporal-shift-module"
"facebookresearch/pytorchvideo" -> "SwinTransformer/Video-Swin-Transformer"
"facebookresearch/pytorchvideo" -> "facebookresearch/vissl" ["e"=1]
"facebookresearch/pytorchvideo" -> "kenshohara/3D-ResNets-PyTorch"
"facebookresearch/pytorchvideo" -> "cvdfoundation/kinetics-dataset"
"facebookresearch/pytorchvideo" -> "piergiaj/pytorch-i3d"
"facebookresearch/pytorchvideo" -> "google-research/scenic"
"facebookresearch/pytorchvideo" -> "open-mmlab/mmaction"
"facebookresearch/pytorchvideo" -> "google-deepmind/kinetics-i3d"
"facebookresearch/pytorchvideo" -> "activitynet/ActivityNet"
"MCG-NJU/MultiSports" -> "amazon-science/tubelet-transformer"
"MCG-NJU/MultiSports" -> "MCG-NJU/STMixer"
"MCG-NJU/MultiSports" -> "MCG-NJU/MOC-Detector"
"MCG-NJU/MultiSports" -> "MCG-NJU/SportsMOT" ["e"=1]
"MCG-NJU/MultiSports" -> "MCG-NJU/BasicTAD"
"MCG-NJU/MultiSports" -> "MCG-NJU/RTD-Action"
"MCG-NJU/MultiSports" -> "joslefaure/HIT"
"MCG-NJU/MultiSports" -> "ShoufaChen/WOO"
"Atze00/MoViNet-pytorch" -> "mengyuest/AR-Net"
"Atze00/MoViNet-pytorch" -> "arunos728/MotionSqueeze"
"SwinTransformer/Video-Swin-Transformer" -> "facebookresearch/TimeSformer"
"SwinTransformer/Video-Swin-Transformer" -> "haofanwang/video-swin-transformer-pytorch"
"SwinTransformer/Video-Swin-Transformer" -> "open-mmlab/mmaction2"
"SwinTransformer/Video-Swin-Transformer" -> "MCG-NJU/VideoMAE"
"SwinTransformer/Video-Swin-Transformer" -> "rishikksh20/ViViT-pytorch"
"SwinTransformer/Video-Swin-Transformer" -> "lucidrains/TimeSformer-pytorch"
"SwinTransformer/Video-Swin-Transformer" -> "cvdfoundation/kinetics-dataset"
"SwinTransformer/Video-Swin-Transformer" -> "mit-han-lab/temporal-shift-module"
"SwinTransformer/Video-Swin-Transformer" -> "piergiaj/pytorch-i3d"
"SwinTransformer/Video-Swin-Transformer" -> "google-research/scenic"
"SwinTransformer/Video-Swin-Transformer" -> "facebookresearch/SlowFast"
"SwinTransformer/Video-Swin-Transformer" -> "Sense-X/UniFormer"
"SwinTransformer/Video-Swin-Transformer" -> "sallymmx/ActionCLIP"
"SwinTransformer/Video-Swin-Transformer" -> "OpenGVLab/InternVideo" ["e"=1]
"SwinTransformer/Video-Swin-Transformer" -> "facebookresearch/pytorchvideo"
"facebookresearch/TimeSformer" -> "lucidrains/TimeSformer-pytorch"
"facebookresearch/TimeSformer" -> "SwinTransformer/Video-Swin-Transformer"
"facebookresearch/TimeSformer" -> "MCG-NJU/VideoMAE"
"facebookresearch/TimeSformer" -> "rishikksh20/ViViT-pytorch"
"facebookresearch/TimeSformer" -> "facebookresearch/SlowFast"
"facebookresearch/TimeSformer" -> "open-mmlab/mmaction2"
"facebookresearch/TimeSformer" -> "cvdfoundation/kinetics-dataset"
"facebookresearch/TimeSformer" -> "facebookresearch/pytorchvideo"
"facebookresearch/TimeSformer" -> "mit-han-lab/temporal-shift-module"
"facebookresearch/TimeSformer" -> "piergiaj/pytorch-i3d"
"facebookresearch/TimeSformer" -> "google-research/scenic"
"facebookresearch/TimeSformer" -> "Sense-X/UniFormer"
"facebookresearch/TimeSformer" -> "sallymmx/ActionCLIP"
"facebookresearch/TimeSformer" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"facebookresearch/TimeSformer" -> "yjxiong/temporal-segment-networks"
"Martlgap/face-alignment-mtcnn" -> "Blueblue4/IoU-AwareCalibration"
"Andy1621/CT-Net" -> "tianyuan168326/EAN-Pytorch"
"Andy1621/CT-Net" -> "arunos728/SELFY"
"MCG-NJU/TDN" -> "Phoenix1327/tea-action-recognition"
"MCG-NJU/TDN" -> "decisionforce/TPN"
"MCG-NJU/TDN" -> "liu-zhy/temporal-adaptive-module"
"MCG-NJU/TDN" -> "V-Sense/ACTION-Net"
"MCG-NJU/TDN" -> "zhang-can/PAN-PyTorch"
"MCG-NJU/TDN" -> "arunos728/MotionSqueeze"
"MCG-NJU/TDN" -> "mit-han-lab/temporal-shift-module"
"MCG-NJU/TDN" -> "swathikirans/GSM"
"MCG-NJU/TDN" -> "MCG-NJU/MOC-Detector"
"MCG-NJU/TDN" -> "IBM/action-recognition-pytorch"
"MCG-NJU/TDN" -> "MCG-NJU/CPD-Video"
"MCG-NJU/TDN" -> "yjxiong/tsn-pytorch"
"MCG-NJU/TDN" -> "deepcs233/TIN"
"MCG-NJU/TDN" -> "JJBOY/BMN-Boundary-Matching-Network"
"MCG-NJU/TDN" -> "Siyu-C/ACAR-Net"
"tobyperrett/trx" -> "tobyperrett/few-shot-action-recognition"
"tobyperrett/trx" -> "Anirudh257/strm"
"tobyperrett/trx" -> "alibaba-mmai-research/HyRSM"
"tobyperrett/trx" -> "ffmpbgrnn/CMN"
"tobyperrett/trx" -> "lovelyqian/AMeFu-Net"
"okankop/ASDNet" -> "fuankarion/active-speakers-context"
"okankop/ASDNet" -> "SRA2/SPELL"
"okankop/ASDNet" -> "Blueblue4/IoU-AwareCalibration"
"okankop/ASDNet" -> "Blueblue4/Object-Detection-Confidence-Bias"
"okankop/ASDNet" -> "TaoRuijie/TalkNet-ASD"
"vadimkantorov/mpegflow" -> "jishnujayakumar/MV-Tractus"
"vadimkantorov/mpegflow" -> "LukasBommes/mv-extractor"
"vadimkantorov/mpegflow" -> "zbwglory/MV-release"
"vadimkantorov/mpegflow" -> "vadimkantorov/fastvideofeat"
"cvdfoundation/kinetics-dataset" -> "facebookresearch/TimeSformer"
"cvdfoundation/kinetics-dataset" -> "SwinTransformer/Video-Swin-Transformer"
"cvdfoundation/kinetics-dataset" -> "MCG-NJU/VideoMAE"
"cvdfoundation/kinetics-dataset" -> "activitynet/ActivityNet"
"cvdfoundation/kinetics-dataset" -> "open-mmlab/mmaction2"
"cvdfoundation/kinetics-dataset" -> "facebookresearch/pytorchvideo"
"cvdfoundation/kinetics-dataset" -> "sallymmx/ActionCLIP"
"cvdfoundation/kinetics-dataset" -> "OpenGVLab/InternVideo" ["e"=1]
"cvdfoundation/kinetics-dataset" -> "piergiaj/pytorch-i3d"
"cvdfoundation/kinetics-dataset" -> "microsoft/VideoX" ["e"=1]
"cvdfoundation/kinetics-dataset" -> "OpenGVLab/VideoMAEv2"
"cvdfoundation/kinetics-dataset" -> "mit-han-lab/temporal-shift-module"
"cvdfoundation/kinetics-dataset" -> "yjxiong/temporal-segment-networks"
"cvdfoundation/kinetics-dataset" -> "rishikksh20/ViViT-pytorch"
"cvdfoundation/kinetics-dataset" -> "facebookresearch/SlowFast"
"ma-shamshiri/Human-Activity-Recognition" -> "ani8897/Human-Activity-Recognition"
"V-Sense/ACTION-Net" -> "Phoenix1327/tea-action-recognition"
"V-Sense/ACTION-Net" -> "liu-zhy/temporal-adaptive-module"
"V-Sense/ACTION-Net" -> "MCG-NJU/TDN"
"V-Sense/ACTION-Net" -> "decisionforce/TPN"
"V-Sense/ACTION-Net" -> "arunos728/MotionSqueeze"
"V-Sense/ACTION-Net" -> "zhang-can/PAN-PyTorch"
"V-Sense/ACTION-Net" -> "tianyuan168326/EAN-Pytorch"
"V-Sense/ACTION-Net" -> "swathikirans/GSM"
"V-Sense/ACTION-Net" -> "arunos728/SELFY"
"V-Sense/ACTION-Net" -> "artest08/LateTemporalModeling3DCNN"
"LisaAnne/lisa-caffe-public" -> "jeffdonahue/caffe"
"LisaAnne/lisa-caffe-public" -> "junhyukoh/caffe-lstm"
"LisaAnne/lisa-caffe-public" -> "yjxiong/caffe"
"LisaAnne/lisa-caffe-public" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"LisaAnne/lisa-caffe-public" -> "garythung/torch-lrcn"
"LisaAnne/lisa-caffe-public" -> "kracwarlock/action-recognition-visual-attention"
"LisaAnne/lisa-caffe-public" -> "facebookarchive/C3D"
"LisaAnne/lisa-caffe-public" -> "chuckcho/video-caffe"
"LisaAnne/lisa-caffe-public" -> "vsubhashini/caffe" ["e"=1]
"LisaAnne/lisa-caffe-public" -> "wanglimin/dense_flow"
"LisaAnne/lisa-caffe-public" -> "JaggerYoung/LRCN-for-Activity-Recognition"
"BizhuWu/LRCN_PyTorch" -> "doronharitan/human_activity_recognition_LRCN"
"gtoderici/sports-1m-dataset" -> "facebookarchive/C3D"
"hitachi-rd-cv/qpic" -> "YueLiao/CDN"
"hitachi-rd-cv/qpic" -> "YueLiao/gen-vlkt"
"hitachi-rd-cv/qpic" -> "yoyomimi/AS-Net"
"hitachi-rd-cv/qpic" -> "cjw2021/QAHOI"
"hitachi-rd-cv/qpic" -> "bbepoch/HoiTransformer"
"hitachi-rd-cv/qpic" -> "fredzzhang/upt"
"hitachi-rd-cv/qpic" -> "kakaobrain/hotr"
"hitachi-rd-cv/qpic" -> "zyong812/STIP"
"hitachi-rd-cv/qpic" -> "fredzzhang/spatially-conditioned-graphs"
"hitachi-rd-cv/qpic" -> "YueLiao/PPDM"
"hitachi-rd-cv/qpic" -> "vt-vl-lab/DRG"
"hitachi-rd-cv/qpic" -> "fredzzhang/hicodet"
"hitachi-rd-cv/qpic" -> "zhihou7/HOI-CL"
"hitachi-rd-cv/qpic" -> "SherlockHolmes221/DOQ"
"hitachi-rd-cv/qpic" -> "MuchHair/HQM"
"MCG-NJU/RTD-Action" -> "HumamAlwassel/TSP"
"MCG-NJU/RTD-Action" -> "frostinassiky/gtad"
"MCG-NJU/RTD-Action" -> "TencentYoutuResearch/ActionDetection-AFSD"
"MCG-NJU/RTD-Action" -> "MCG-NJU/BasicTAD"
"MCG-NJU/RTD-Action" -> "Nmegha2601/activitygraph_transformer"
"lucidrains/TimeSformer-pytorch" -> "facebookresearch/TimeSformer"
"lucidrains/TimeSformer-pytorch" -> "SwinTransformer/Video-Swin-Transformer"
"lucidrains/TimeSformer-pytorch" -> "Alibaba-MIIL/STAM"
"lucidrains/TimeSformer-pytorch" -> "rishikksh20/ViViT-pytorch"
"lucidrains/TimeSformer-pytorch" -> "davide-coccomini/TimeSformer-Video-Classification"
"lucidrains/TimeSformer-pytorch" -> "MCG-NJU/TDN"
"lucidrains/TimeSformer-pytorch" -> "lucidrains/STAM-pytorch"
"lucidrains/TimeSformer-pytorch" -> "decisionforce/TPN"
"lucidrains/TimeSformer-pytorch" -> "sallymmx/ActionCLIP"
"lucidrains/TimeSformer-pytorch" -> "mit-han-lab/temporal-shift-module"
"lucidrains/TimeSformer-pytorch" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"lucidrains/TimeSformer-pytorch" -> "facebookresearch/pytorchvideo"
"lucidrains/TimeSformer-pytorch" -> "V-Sense/ACTION-Net"
"lucidrains/TimeSformer-pytorch" -> "The-AI-Summer/self-attention-cv" ["e"=1]
"lucidrains/TimeSformer-pytorch" -> "antoine77340/S3D_HowTo100M" ["e"=1]
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "Pilhyeon/Learning-Action-Completeness-from-Points"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "Pilhyeon/BaSNet-pytorch"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "ispc-lab/ACM-Net"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "LeonHLJ/RSKP"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "zhang-can/CoLA"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "harlanhong/MM2021-CO2-Net"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "boheumd/ASM-Loc"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "MengyuanChen21/CVPR2022-FTCL"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "MengyuanChen21/ECCV2022-DELU"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "sujoyp/wtalc-pytorch"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "zhou745/GauFuse_WSTAL"
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" -> "Finspire13/CMCS-Temporal-Action-Localization"
"saif-mahmud/self-attention-HAR" -> "AdelaideAuto-IDLab/Attend-And-Discriminate"
"Alibaba-MIIL/STAM" -> "lucidrains/STAM-pytorch"
"Alibaba-MIIL/STAM" -> "lucidrains/TimeSformer-pytorch"
"Alibaba-MIIL/STAM" -> "airsplay/vimpac" ["e"=1]
"Alibaba-MIIL/STAM" -> "Phoenix1327/tea-action-recognition"
"Alibaba-MIIL/STAM" -> "joaanna/something_else"
"Alibaba-MIIL/STAM" -> "zhang-can/PAN-PyTorch"
"haofanwang/video-swin-transformer-pytorch" -> "SwinTransformer/Video-Swin-Transformer"
"haofanwang/video-swin-transformer-pytorch" -> "mx-mark/VideoTransformer-pytorch"
"haofanwang/video-swin-transformer-pytorch" -> "rishikksh20/ViViT-pytorch"
"haofanwang/video-swin-transformer-pytorch" -> "microsoft/SwinBERT" ["e"=1]
"haofanwang/video-swin-transformer-pytorch" -> "xyzforever/BEVT"
"alexandrosstergiou/SoftPool" -> "sebgao/LIP"
"alexandrosstergiou/SoftPool" -> "alexandrosstergiou/adaPool"
"alexandrosstergiou/SoftPool" -> "rentainhe/pytorch-pooling"
"alexandrosstergiou/SoftPool" -> "wofmanaf/SA-Net" ["e"=1]
"alexandrosstergiou/SoftPool" -> "implus/GFocalV2" ["e"=1]
"alexandrosstergiou/SoftPool" -> "yifan123/IC-Conv" ["e"=1]
"alexandrosstergiou/SoftPool" -> "d-li14/PSConv" ["e"=1]
"whwu95/MVFNet" -> "whwu95/DSANet"
"whwu95/MVFNet" -> "whwu95/Text4Vis"
"alibaba-mmai-research/TAdaConv" -> "alibaba-mmai-research/DiST"
"alibaba-mmai-research/TAdaConv" -> "liu-zhy/temporal-adaptive-module"
"alibaba-mmai-research/TAdaConv" -> "daniel-code/TubeViT"
"alibaba-mmai-research/TAdaConv" -> "whwu95/BIKE"
"alibaba-mmai-research/TAdaConv" -> "whwu95/ATM"
"alibaba-mmai-research/TAdaConv" -> "vision4robotics/TCTrack" ["e"=1]
"alibaba-mmai-research/TAdaConv" -> "MCG-NJU/MGSampler"
"alibaba-mmai-research/TAdaConv" -> "Andy1621/CT-Net"
"ptirupat/MLAD" -> "dairui01/PDAN"
"qinzhi-0110/Temporal-Context-Aggregation-Network-Pytorch" -> "TencentYoutuResearch/ActionDetection-AFSD"
"qinzhi-0110/Temporal-Context-Aggregation-Network-Pytorch" -> "sming256/ETAD"
"wanglimin/TDD" -> "wanglimin/improved_trajectory"
"wanglimin/TDD" -> "yjxiong/anet2016-cuhk"
"wanglimin/TDD" -> "wanglimin/UntrimmedNet"
"wanglimin/TDD" -> "wanglimin/dense_flow"
"iantangc/ContrastiveLearningHAR" -> "iantangc/SelfHAR"
"iantangc/ContrastiveLearningHAR" -> "Tian0426/CL-HAR"
"iantangc/ContrastiveLearningHAR" -> "bulatkh/csshar_tfa"
"iantangc/ContrastiveLearningHAR" -> "harkash/contrastive-predictive-coding-for-har"
"Hangwei12358/cross-person-HAR" -> "Rxannro/SWL-Adapt"
"Hangwei12358/cross-person-HAR" -> "Tian0426/CL-HAR"
"Hangwei12358/cross-person-HAR" -> "harkash/contrastive-predictive-coding-for-har"
"Hangwei12358/cross-person-HAR" -> "Rxannro/HMGAN"
"wangxiang1230/SSTAP" -> "qinzhi-0110/Temporal-Context-Aggregation-Network-Pytorch"
"wangxiang1230/SSTAP" -> "LeonHLJ/RSKP"
"wangxiang1230/SSTAP" -> "alibaba-mmai-research/HyRSM"
"wangxiang1230/SSTAP" -> "zhang-can/UP-TAL"
"ispc-lab/ACM-Net" -> "boheumd/ASM-Loc"
"ispc-lab/ACM-Net" -> "harlanhong/MM2021-CO2-Net"
"ispc-lab/ACM-Net" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"ispc-lab/ACM-Net" -> "zhang-can/CoLA"
"ispc-lab/ACM-Net" -> "zhou745/GauFuse_WSTAL"
"ispc-lab/ACM-Net" -> "Pilhyeon/Learning-Action-Completeness-from-Points"
"ispc-lab/ACM-Net" -> "MengyuanChen21/ECCV2022-DELU"
"ispc-lab/ACM-Net" -> "Flowerfan/SF-Net"
"ispc-lab/ACM-Net" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"JacobYuan7/DIN-Group-Activity-Recognition-Benchmark" -> "xueyee/GroupFormer"
"JacobYuan7/DIN-Group-Activity-Recognition-Benchmark" -> "ruiyan1995/HiGCIN"
"JacobYuan7/DIN-Group-Activity-Recognition-Benchmark" -> "ruiyan1995/Group-Activity-Recognition"
"JacobYuan7/DIN-Group-Activity-Recognition-Benchmark" -> "dk-kim/DFWSGAR"
"JacobYuan7/DIN-Group-Activity-Recognition-Benchmark" -> "hongluzhou/composer"
"Russell91/nlpcaffe" -> "Russell91/apollocaffe" ["e"=1]
"Russell91/nlpcaffe" -> "junhyukoh/caffe-lstm"
"Russell91/nlpcaffe" -> "jeffdonahue/caffe"
"Russell91/nlpcaffe" -> "mila-iqia/blocks-examples" ["e"=1]
"Simon4Yan/Meta-set" -> "Simon4Yan/Accuracy-estimation-with-self-supervision"
"Simon4Yan/Meta-set" -> "sxzrt/Proxy-Set"
"yolish/har-with-imu-transformer" -> "getalp/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices"
"yolish/har-with-imu-transformer" -> "markub3327/HAR-Transformer"
"yolish/har-with-imu-transformer" -> "jidhu-mohan/Activity-Detection-using-IMU-sensor"
"zhang-can/CoLA" -> "LeonHLJ/RSKP"
"zhang-can/CoLA" -> "ispc-lab/ACM-Net"
"zhang-can/CoLA" -> "zhang-can/UP-TAL"
"zhang-can/CoLA" -> "harlanhong/MM2021-CO2-Net"
"zhang-can/CoLA" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"zhang-can/CoLA" -> "MengyuanChen21/CVPR2022-FTCL"
"zhang-can/CoLA" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"zhang-can/CoLA" -> "zhou745/GauFuse_WSTAL"
"zhang-can/CoLA" -> "LeonHLJ/FAC-Net"
"auduno/deepdraw" -> "kylemcdonald/deepdream"
"auduno/deepdraw" -> "yjxiong/dense_flow"
"auduno/deepdraw" -> "yjxiong/temporal-segment-networks"
"auduno/deepdraw" -> "MohsenFayyaz89/T3D"
"auduno/deepdraw" -> "yjxiong/anet2016-cuhk"
"auduno/deepdraw" -> "yjxiong/caffe"
"auduno/deepdraw" -> "wanglimin/UntrimmedNet"
"auduno/deepdraw" -> "chuckcho/video-caffe"
"auduno/deepdraw" -> "qijiezhao/pseudo-3d-pytorch"
"auduno/deepdraw" -> "yjxiong/action-detection"
"auduno/deepdraw" -> "vkalogeiton/caffe"
"auduno/deepdraw" -> "activitynet/ActivityNet"
"auduno/deepdraw" -> "facebookarchive/C3D"
"auduno/deepdraw" -> "feichtenhofer/twostreamfusion"
"auduno/deepdraw" -> "zhoubolei/TRN-pytorch"
"xlliu7/TadTR" -> "xlliu7/E2E-TAD"
"xlliu7/TadTR" -> "xlliu7/MUSES"
"xlliu7/TadTR" -> "TencentYoutuResearch/ActionDetection-AFSD"
"xlliu7/TadTR" -> "MCG-NJU/BasicTAD"
"xlliu7/TadTR" -> "dingfengshi/ReAct"
"xlliu7/TadTR" -> "happyharrycn/actionformer_release"
"xlliu7/TadTR" -> "dingfengshi/TriDet"
"xlliu7/TadTR" -> "harlanhong/MM2021-CO2-Net"
"xlliu7/TadTR" -> "MCG-NJU/PointTAD"
"xlliu7/TadTR" -> "LeonHLJ/RSKP"
"xlliu7/TadTR" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"xlliu7/TadTR" -> "zhang-can/UP-TAL"
"xlliu7/TadTR" -> "klauscc/TALLFormer"
"LeonHLJ/FAC-Net" -> "LeonHLJ/RSKP"
"LeonHLJ/FAC-Net" -> "HumamAlwassel/RefineLoc"
"LeonHLJ/FAC-Net" -> "harlanhong/MM2021-CO2-Net"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "Pilhyeon/WTAL-Uncertainty-Modeling"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "Pilhyeon/BaSNet-pytorch"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "harlanhong/MM2021-CO2-Net"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "ispc-lab/ACM-Net"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "LeonHLJ/RSKP"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "Flowerfan/SF-Net"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "zhang-can/UP-TAL"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "boheumd/ASM-Loc"
"Pilhyeon/Learning-Action-Completeness-from-Points" -> "LeonHLJ/FAC-Net"
"mengyuest/AdaFuse" -> "mengyuest/AR-Net"
"TencentYoutuResearch/ActionDetection-AFSD" -> "qinzhi-0110/Temporal-Context-Aggregation-Network-Pytorch"
"TencentYoutuResearch/ActionDetection-AFSD" -> "xlliu7/E2E-TAD"
"TencentYoutuResearch/ActionDetection-AFSD" -> "frostinassiky/gtad"
"TencentYoutuResearch/ActionDetection-AFSD" -> "MCG-NJU/RTD-Action"
"TencentYoutuResearch/ActionDetection-AFSD" -> "xlliu7/TadTR"
"TencentYoutuResearch/ActionDetection-AFSD" -> "Alvin-Zeng/PGCN"
"TencentYoutuResearch/ActionDetection-AFSD" -> "HumamAlwassel/TSP"
"TencentYoutuResearch/ActionDetection-AFSD" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"TencentYoutuResearch/ActionDetection-AFSD" -> "Finspire13/CMCS-Temporal-Action-Localization"
"TencentYoutuResearch/ActionDetection-AFSD" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"TencentYoutuResearch/ActionDetection-AFSD" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"TencentYoutuResearch/ActionDetection-AFSD" -> "Flowerfan/SF-Net"
"TencentYoutuResearch/ActionDetection-AFSD" -> "happyharrycn/actionformer_release"
"TencentYoutuResearch/ActionDetection-AFSD" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"TencentYoutuResearch/ActionDetection-AFSD" -> "dingfengshi/TriDet"
"Nmegha2601/activitygraph_transformer" -> "VividLe/A2Net"
"Nmegha2601/activitygraph_transformer" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"Nmegha2601/activitygraph_transformer" -> "MCG-NJU/RTD-Action"
"Nmegha2601/activitygraph_transformer" -> "Flowerfan/SF-Net"
"kakaobrain/hotr" -> "bbepoch/HoiTransformer"
"kakaobrain/hotr" -> "hitachi-rd-cv/qpic"
"kakaobrain/hotr" -> "YueLiao/CDN"
"kakaobrain/hotr" -> "YueLiao/gen-vlkt"
"kakaobrain/hotr" -> "cjw2021/QAHOI"
"kakaobrain/hotr" -> "zyong812/STIP"
"kakaobrain/hotr" -> "yoyomimi/AS-Net"
"kakaobrain/hotr" -> "YueLiao/PPDM"
"kakaobrain/hotr" -> "SherlockHolmes221/GGNet"
"kakaobrain/hotr" -> "zhihou7/HOI-CL"
"kakaobrain/hotr" -> "fredzzhang/spatially-conditioned-graphs"
"kakaobrain/hotr" -> "fredzzhang/upt"
"kakaobrain/hotr" -> "MVIG-SJTU/DIRV"
"kakaobrain/hotr" -> "JacobYuan7/RLIP"
"cjw2021/QAHOI" -> "yoyomimi/AS-Net"
"cjw2021/QAHOI" -> "zyong812/STIP"
"cjw2021/QAHOI" -> "hitachi-rd-cv/qpic"
"cjw2021/QAHOI" -> "YueLiao/gen-vlkt"
"cjw2021/QAHOI" -> "SherlockHolmes221/DOQ"
"cjw2021/QAHOI" -> "SherlockHolmes221/GGNet"
"cjw2021/QAHOI" -> "fredzzhang/upt"
"cjw2021/QAHOI" -> "YueLiao/CDN"
"cjw2021/QAHOI" -> "fredzzhang/spatially-conditioned-graphs"
"iantangc/SelfHAR" -> "Jie-su/Awesome_Human_Activity_Recognition"
"iantangc/SelfHAR" -> "iantangc/ContrastiveLearningHAR"
"Jie-su/Awesome_Human_Activity_Recognition" -> "iantangc/SelfHAR"
"Jie-su/Awesome_Human_Activity_Recognition" -> "Jie-su/BPD"
"lovelyqian/AMeFu-Net" -> "lovelyqian/Embodied-One-Shot-Video-Recognition"
"elb3k/vtn" -> "bomri/SlowFast"
"HumamAlwassel/TSP" -> "frostinassiky/gtad"
"HumamAlwassel/TSP" -> "MCG-NJU/RTD-Action"
"HumamAlwassel/TSP" -> "zhang-can/UP-TAL"
"HumamAlwassel/TSP" -> "TencentYoutuResearch/ActionDetection-AFSD"
"HumamAlwassel/TSP" -> "xlliu7/MUSES"
"HumamAlwassel/TSP" -> "qinzhi-0110/Temporal-Context-Aggregation-Network-Pytorch"
"HumamAlwassel/TSP" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"HumamAlwassel/TSP" -> "bfshi/DGAM-Weakly-Supervised-Action-Localization"
"HumamAlwassel/TSP" -> "dingfengshi/ReAct"
"HumamAlwassel/TSP" -> "happyharrycn/actionformer_release"
"bomri/SlowFast" -> "elb3k/vtn"
"mariusbock/dl-for-har" -> "AdelaideAuto-IDLab/Attend-And-Discriminate"
"leftthomas/ACRNet" -> "lizhilin-ustc/AAAI2023-AICL"
"HumamAlwassel/RefineLoc" -> "LeonHLJ/FAC-Net"
"alexandrosstergiou/adaPool" -> "alexandrosstergiou/SoftPool"
"layer6ai-labs/ASL" -> "LeonHLJ/FAC-Net"
"xlliu7/MUSES" -> "xlliu7/TadTR"
"xlliu7/MUSES" -> "VividLe/A2Net"
"xlliu7/MUSES" -> "skelemoa/tal-hmo"
"xlliu7/MUSES" -> "PeisenZhao/Bottom-Up-TAL-with-MR"
"xlliu7/MUSES" -> "xlliu7/E2E-TAD"
"fredzzhang/hicodet" -> "fredzzhang/spatially-conditioned-graphs"
"fredzzhang/hicodet" -> "hitachi-rd-cv/qpic"
"fredzzhang/hicodet" -> "zhihou7/HOI-CL"
"1jsingh/semantic-guidance" -> "sxzrt/Proxy-Set"
"kkahatapitiya/Coarse-Fine-Networks" -> "dairui01/PDAN"
"kkahatapitiya/Coarse-Fine-Networks" -> "dairui01/MS-TCT"
"tianyuan168326/EAN-Pytorch" -> "arunos728/SELFY"
"FlyHighest/UnrealPerson" -> "tagperson/tagperson-blender"
"fredzzhang/spatially-conditioned-graphs" -> "yoyomimi/AS-Net"
"fredzzhang/spatially-conditioned-graphs" -> "vt-vl-lab/DRG"
"fredzzhang/spatially-conditioned-graphs" -> "cjw2021/QAHOI"
"fredzzhang/spatially-conditioned-graphs" -> "fredzzhang/hicodet"
"fredzzhang/spatially-conditioned-graphs" -> "UCSB-VRL/GTNet"
"fredzzhang/spatially-conditioned-graphs" -> "hitachi-rd-cv/qpic"
"fredzzhang/spatially-conditioned-graphs" -> "vaesl/IP-Net"
"fredzzhang/spatially-conditioned-graphs" -> "zhihou7/HOI-CL"
"fredzzhang/spatially-conditioned-graphs" -> "yeliudev/ConsNet"
"fredzzhang/spatially-conditioned-graphs" -> "fredzzhang/upt"
"fredzzhang/spatially-conditioned-graphs" -> "bobwan1995/PMFNet"
"Martlgap/FaceIDLight" -> "Blueblue4/IoU-AwareCalibration"
"Martlgap/FaceIDLight" -> "Blueblue4/Object-Detection-Confidence-Bias"
"AdelaideAuto-IDLab/Attend-And-Discriminate" -> "mmalekzadeh/dana"
"AdelaideAuto-IDLab/Attend-And-Discriminate" -> "flowerinheart/AttnSense"
"lucidrains/STAM-pytorch" -> "Alibaba-MIIL/STAM"
"lucidrains/STAM-pytorch" -> "rishikksh20/ViViT-pytorch"
"hou-yz/MVDeTr" -> "hou-yz/MultiviewX"
"hou-yz/MVDeTr" -> "xjtlu-cvlab/3DROM"
"hou-yz/MVDeTr" -> "hou-yz/MVDet"
"zhihou7/HOI-CL" -> "yoyomimi/AS-Net"
"zhihou7/HOI-CL" -> "fredzzhang/upt"
"zhihou7/HOI-CL" -> "fredzzhang/spatially-conditioned-graphs"
"zhihou7/HOI-CL" -> "hitachi-rd-cv/qpic"
"zhihou7/HOI-CL" -> "cjw2021/QAHOI"
"zhihou7/HOI-CL" -> "fredzzhang/hicodet"
"zhihou7/HOI-CL" -> "YueLiao/CDN"
"zhihou7/HOI-CL" -> "YueLiao/gen-vlkt"
"zhihou7/HOI-CL" -> "ltttpku/ADA-CM"
"zhihou7/HOI-CL" -> "SherlockHolmes221/DOQ"
"ruiyan1995/HiGCIN" -> "ruiyan1995/Group-Activity-Recognition"
"ruiyan1995/HiGCIN" -> "JacobYuan7/DIN-Group-Activity-Recognition-Benchmark"
"ruiyan1995/HiGCIN" -> "huguyuehuhu/Awesome-Group-Activity-Recognition"
"MVIG-SJTU/DIRV" -> "SHI-Labs/Human-Object-Interaction-Detection"
"MVIG-SJTU/DIRV" -> "yoyomimi/AS-Net"
"skelemoa/tal-hmo" -> "xlliu7/MUSES"
"tobyperrett/few-shot-action-recognition" -> "tobyperrett/trx"
"tobyperrett/few-shot-action-recognition" -> "MCG-NJU/FSL-Video"
"tobyperrett/few-shot-action-recognition" -> "HongguangZhang/arn-eccv20-master"
"tobyperrett/few-shot-action-recognition" -> "alibaba-mmai-research/HyRSM"
"asrafulashiq/hamnet" -> "MichiganCOG/A2CL-PT"
"asrafulashiq/hamnet" -> "HumamAlwassel/RefineLoc"
"asrafulashiq/hamnet" -> "LeonHLJ/FAC-Net"
"asrafulashiq/hamnet" -> "Flowerfan/SF-Net"
"SherlockHolmes221/GGNet" -> "cjw2021/QAHOI"
"BoPang1996/PGT" -> "BoPang1996/Semi-Coupled-Structure-for-visual-sequental-tasks"
"BoPang1996/PGT" -> "DirtyHarryLYL/sampling-argmax"
"yoyomimi/AS-Net" -> "cjw2021/QAHOI"
"yoyomimi/AS-Net" -> "fredzzhang/spatially-conditioned-graphs"
"yoyomimi/AS-Net" -> "hitachi-rd-cv/qpic"
"yoyomimi/AS-Net" -> "YueLiao/CDN"
"yoyomimi/AS-Net" -> "MVIG-SJTU/DIRV"
"yoyomimi/AS-Net" -> "zhihou7/HOI-CL"
"yoyomimi/AS-Net" -> "SherlockHolmes221/GGNet"
"yoyomimi/AS-Net" -> "bobwan1995/PMFNet"
"papermsucode/mutual-modality-learning" -> "arunos728/SELFY"
"STRCWearlab/DeepConvLSTM" -> "guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs"
"STRCWearlab/DeepConvLSTM" -> "STRCWearlab/DeepConvLSTM_py3"
"STRCWearlab/DeepConvLSTM" -> "dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch"
"STRCWearlab/DeepConvLSTM" -> "arturjordao/WearableSensorData"
"STRCWearlab/DeepConvLSTM" -> "nhammerla/deepHAR"
"STRCWearlab/DeepConvLSTM" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"STRCWearlab/DeepConvLSTM" -> "yscacaca/DeepSense"
"STRCWearlab/DeepConvLSTM" -> "drewanye/har-joint-model"
"STRCWearlab/DeepConvLSTM" -> "jindongwang/activityrecognition"
"STRCWearlab/DeepConvLSTM" -> "Koutoulakis/Deep-Learning-for-Human-Activity-Recognition"
"STRCWearlab/DeepConvLSTM" -> "saif-mahmud/self-attention-HAR"
"STRCWearlab/DeepConvLSTM" -> "wisdal/Deep-Learning-for-Sensor-based-Human-Activity-Recognition"
"STRCWearlab/DeepConvLSTM" -> "ani8897/Human-Activity-Recognition"
"STRCWearlab/DeepConvLSTM" -> "isukrit/encodingHumanActivity"
"STRCWearlab/DeepConvLSTM" -> "youngminoh7/DeepConvLSTM_Python3"
"MCG-NJU/VideoMAE" -> "OpenGVLab/VideoMAEv2"
"MCG-NJU/VideoMAE" -> "facebookresearch/TimeSformer"
"MCG-NJU/VideoMAE" -> "OpenGVLab/InternVideo" ["e"=1]
"MCG-NJU/VideoMAE" -> "SwinTransformer/Video-Swin-Transformer"
"MCG-NJU/VideoMAE" -> "facebookresearch/mae_st"
"MCG-NJU/VideoMAE" -> "open-mmlab/mmaction2"
"MCG-NJU/VideoMAE" -> "Sense-X/UniFormer"
"MCG-NJU/VideoMAE" -> "facebookresearch/pytorchvideo"
"MCG-NJU/VideoMAE" -> "facebookresearch/mae" ["e"=1]
"MCG-NJU/VideoMAE" -> "cvdfoundation/kinetics-dataset"
"MCG-NJU/VideoMAE" -> "facebookresearch/SlowFast"
"MCG-NJU/VideoMAE" -> "sallymmx/ActionCLIP"
"MCG-NJU/VideoMAE" -> "happyharrycn/actionformer_release"
"MCG-NJU/VideoMAE" -> "OpenGVLab/VideoMamba" ["e"=1]
"MCG-NJU/VideoMAE" -> "google-research/scenic"
"mx-mark/VideoTransformer-pytorch" -> "rishikksh20/ViViT-pytorch"
"mx-mark/VideoTransformer-pytorch" -> "haofanwang/video-swin-transformer-pytorch"
"mx-mark/VideoTransformer-pytorch" -> "lucidrains/STAM-pytorch"
"mx-mark/VideoTransformer-pytorch" -> "drv-agwl/ViViT-pytorch"
"mx-mark/VideoTransformer-pytorch" -> "facebookresearch/TimeSformer"
"mx-mark/VideoTransformer-pytorch" -> "SwinTransformer/Video-Swin-Transformer"
"mx-mark/VideoTransformer-pytorch" -> "fcakyon/video-transformers"
"mx-mark/VideoTransformer-pytorch" -> "MCG-NJU/VideoMAE"
"mx-mark/VideoTransformer-pytorch" -> "SforAiDl/vformer" ["e"=1]
"mx-mark/VideoTransformer-pytorch" -> "bomri/SlowFast"
"facebookresearch/mvit" -> "karttikeya/minREV"
"facebookresearch/mvit" -> "facebookresearch/hiera"
"facebookresearch/mvit" -> "microsoft/CSWin-Transformer" ["e"=1]
"facebookresearch/mvit" -> "OpenGVLab/efficient-video-recognition"
"facebookresearch/mvit" -> "OliverRensu/Shunted-Transformer" ["e"=1]
"facebookresearch/mvit" -> "Sense-X/UniFormer"
"facebookresearch/mvit" -> "rishikksh20/ViViT-pytorch"
"facebookresearch/mvit" -> "Chenglin-Yang/LVT" ["e"=1]
"facebookresearch/mvit" -> "raoyongming/HorNet" ["e"=1]
"facebookresearch/mvit" -> "SwinTransformer/Video-Swin-Transformer"
"facebookresearch/mvit" -> "amazon-science/long-short-term-transformer" ["e"=1]
"facebookresearch/mvit" -> "facebookresearch/TimeSformer"
"facebookresearch/mvit" -> "IBM/CrossViT" ["e"=1]
"facebookresearch/mvit" -> "facebookresearch/Motionformer"
"facebookresearch/mvit" -> "Atze00/MoViNet-pytorch"
"Tian0426/CL-HAR" -> "Hangwei12358/cross-person-HAR"
"Tian0426/CL-HAR" -> "iantangc/ContrastiveLearningHAR"
"Tian0426/CL-HAR" -> "Intelligent-Computing-Lab-Yale/SNN_HAR"
"xmuyzz/3D-CNN-PyTorch" -> "okankop/Efficient-3DCNNs"
"happyharrycn/actionformer_release" -> "dingfengshi/TriDet"
"happyharrycn/actionformer_release" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"happyharrycn/actionformer_release" -> "Finspire13/pytorch-i3d-feature-extraction"
"happyharrycn/actionformer_release" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"happyharrycn/actionformer_release" -> "xlliu7/TadTR"
"happyharrycn/actionformer_release" -> "TencentYoutuResearch/ActionDetection-AFSD"
"happyharrycn/actionformer_release" -> "sming256/OpenTAD"
"happyharrycn/actionformer_release" -> "HumamAlwassel/TSP"
"happyharrycn/actionformer_release" -> "OpenGVLab/VideoMAEv2"
"happyharrycn/actionformer_release" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"happyharrycn/actionformer_release" -> "sauradip/STALE"
"happyharrycn/actionformer_release" -> "MCG-NJU/BasicTAD"
"happyharrycn/actionformer_release" -> "v-iashin/video_features" ["e"=1]
"happyharrycn/actionformer_release" -> "ju-chen/Efficient-Prompt"
"happyharrycn/actionformer_release" -> "klauscc/TALLFormer"
"boheumd/ASM-Loc" -> "ispc-lab/ACM-Net"
"boheumd/ASM-Loc" -> "LeonHLJ/RSKP"
"boheumd/ASM-Loc" -> "lizhi1104/HAAN"
"boheumd/ASM-Loc" -> "zhou745/GauFuse_WSTAL"
"fshp971/mcmc-unlearning" -> "tmllab/2022_NeurIPS_PICMM"
"fshp971/mcmc-unlearning" -> "xianruizhong/SpHAM"
"fshp971/mcmc-unlearning" -> "fshp971/robust-unlearnable-examples"
"fshp971/robust-unlearnable-examples" -> "fshp971/mcmc-unlearning"
"fshp971/robust-unlearnable-examples" -> "tmllab/2022_NeurIPS_PICMM"
"fshp971/robust-unlearnable-examples" -> "xianruizhong/SpHAM"
"fshp971/robust-unlearnable-examples" -> "Raiden-Zhu/Generalization-of-DSGD"
"fshp971/robust-unlearnable-examples" -> "dingfengshi/ReAct"
"EGO4D/audio-visual" -> "zcxu-eric/Ego4d_TalkNet_ASD"
"Sense-X/UniFormer" -> "OpenGVLab/UniFormerV2"
"Sense-X/UniFormer" -> "MCG-NJU/VideoMAE"
"Sense-X/UniFormer" -> "SwinTransformer/Video-Swin-Transformer"
"Sense-X/UniFormer" -> "sail-sg/poolformer" ["e"=1]
"Sense-X/UniFormer" -> "facebookresearch/TimeSformer"
"Sense-X/UniFormer" -> "OpenGVLab/VideoMAEv2"
"Sense-X/UniFormer" -> "MCG-NJU/TDN"
"Sense-X/UniFormer" -> "facebookresearch/omnivore" ["e"=1]
"Sense-X/UniFormer" -> "Meituan-AutoML/Twins" ["e"=1]
"Sense-X/UniFormer" -> "czczup/ViT-Adapter" ["e"=1]
"Sense-X/UniFormer" -> "SHI-Labs/Neighborhood-Attention-Transformer" ["e"=1]
"Sense-X/UniFormer" -> "facebookresearch/mvit"
"Sense-X/UniFormer" -> "whai362/PVT" ["e"=1]
"Sense-X/UniFormer" -> "sallymmx/ActionCLIP"
"Sense-X/UniFormer" -> "dingmyu/davit" ["e"=1]
"fredzzhang/upt" -> "YueLiao/gen-vlkt"
"fredzzhang/upt" -> "hitachi-rd-cv/qpic"
"fredzzhang/upt" -> "cjw2021/QAHOI"
"fredzzhang/upt" -> "zhihou7/HOI-CL"
"fredzzhang/upt" -> "YueLiao/CDN"
"fredzzhang/upt" -> "zyong812/STIP"
"fredzzhang/upt" -> "fredzzhang/spatially-conditioned-graphs"
"fredzzhang/upt" -> "fredzzhang/pvic"
"fredzzhang/upt" -> "vt-vl-lab/DRG"
"fredzzhang/upt" -> "fredzzhang/hicodet"
"fredzzhang/upt" -> "JacobYuan7/RLIP"
"fredzzhang/upt" -> "SherlockHolmes221/DOQ"
"fredzzhang/upt" -> "yoyomimi/AS-Net"
"fredzzhang/upt" -> "YueLiao/PPDM"
"fredzzhang/upt" -> "DirtyHarryLYL/HOI-Learning-List"
"alibaba-mmai-research/HyRSM" -> "alibaba-mmai-research/HyRSMPlusPlus"
"alibaba-mmai-research/HyRSM" -> "R00Kie-Liu/TA2N"
"NVlabs/RelViT" -> "NVlabs/Bongard-HOI"
"ShoufaChen/WOO" -> "4paradigm-CV/SE-STAD"
"ShoufaChen/WOO" -> "joslefaure/HIT"
"activitynet/ActivityNet" -> "yjxiong/action-detection"
"activitynet/ActivityNet" -> "yjxiong/tsn-pytorch"
"activitynet/ActivityNet" -> "google-deepmind/kinetics-i3d"
"activitynet/ActivityNet" -> "wzmsltw/BSN-boundary-sensitive-network"
"activitynet/ActivityNet" -> "yjxiong/temporal-segment-networks"
"activitynet/ActivityNet" -> "open-mmlab/mmaction"
"activitynet/ActivityNet" -> "facebookresearch/VMZ"
"activitynet/ActivityNet" -> "feichtenhofer/twostreamfusion"
"activitynet/ActivityNet" -> "facebookresearch/video-nonlocal-net"
"activitynet/ActivityNet" -> "zhoubolei/TRN-pytorch"
"activitynet/ActivityNet" -> "Alvin-Zeng/PGCN"
"activitynet/ActivityNet" -> "jeffreyyihuang/two-stream-action-recognition"
"activitynet/ActivityNet" -> "mit-han-lab/temporal-shift-module"
"activitynet/ActivityNet" -> "piergiaj/pytorch-i3d"
"activitynet/ActivityNet" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"ChinaYi/ASFormer" -> "ttlmh/Bridge-Prompt"
"ChinaYi/ASFormer" -> "yiskw713/asrf"
"ChinaYi/ASFormer" -> "ChinaYi/asrf_with_asformer"
"ChinaYi/ASFormer" -> "boschresearch/UVAST"
"ChinaYi/ASFormer" -> "sj-li/MS-TCN2"
"ChinaYi/ASFormer" -> "Finspire13/DiffAct"
"ChinaYi/ASFormer" -> "nus-cvml/awesome-temporal-action-segmentation"
"ChinaYi/ASFormer" -> "yiskw713/video_feature_extractor"
"ChinaYi/ASFormer" -> "yabufarha/ms-tcn"
"ChinaYi/ASFormer" -> "LTContext/LTContext"
"ChinaYi/ASFormer" -> "ZijiaLewisLu/CVPR2024-FACT"
"ChinaYi/ASFormer" -> "MCG-NJU/BCN"
"ChinaYi/ASFormer" -> "Thinksky5124/SVTAS"
"markub3327/HAR-Transformer" -> "getalp/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices"
"markub3327/HAR-Transformer" -> "yolish/har-with-imu-transformer"
"markub3327/HAR-Transformer" -> "saif-mahmud/self-attention-HAR"
"sallymmx/ActionCLIP" -> "ju-chen/Efficient-Prompt"
"sallymmx/ActionCLIP" -> "ArrowLuo/CLIP4Clip" ["e"=1]
"sallymmx/ActionCLIP" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"sallymmx/ActionCLIP" -> "OpenGVLab/efficient-video-recognition"
"sallymmx/ActionCLIP" -> "microsoft/VideoX" ["e"=1]
"sallymmx/ActionCLIP" -> "ttlmh/Bridge-Prompt"
"sallymmx/ActionCLIP" -> "whwu95/BIKE"
"sallymmx/ActionCLIP" -> "SwinTransformer/Video-Swin-Transformer"
"sallymmx/ActionCLIP" -> "happyharrycn/actionformer_release"
"sallymmx/ActionCLIP" -> "MartinXM/GAP" ["e"=1]
"sallymmx/ActionCLIP" -> "Alvin-Zeng/Awesome-Temporal-Action-Localization"
"sallymmx/ActionCLIP" -> "CryhanFang/CLIP2Video" ["e"=1]
"sallymmx/ActionCLIP" -> "sauradip/STALE"
"sallymmx/ActionCLIP" -> "MCG-NJU/VideoMAE"
"sallymmx/ActionCLIP" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"MengyuanChen21/CVPR2022-FTCL" -> "MengyuanChen21/ECCV2022-DELU"
"MengyuanChen21/CVPR2022-FTCL" -> "MengyuanChen21/CVPR2023-CMPAE"
"MengyuanChen21/CVPR2022-FTCL" -> "LeonHLJ/RSKP"
"MengyuanChen21/CVPR2022-FTCL" -> "MengyuanChen21/CVPR2023-OWTAL"
"MengyuanChen21/CVPR2022-FTCL" -> "MengyuanChen21/ICLR2024-REDL"
"MengyuanChen21/CVPR2022-FTCL" -> "harlanhong/MM2021-CO2-Net"
"MengyuanChen21/CVPR2022-FTCL" -> "zhang-can/CoLA"
"sibosutd/cnn-timeseries" -> "jianboyang/CNNHAR"
"xueyee/GroupFormer" -> "JacobYuan7/DIN-Group-Activity-Recognition-Benchmark"
"xueyee/GroupFormer" -> "hongluzhou/composer"
"xueyee/GroupFormer" -> "dk-kim/DFWSGAR"
"xueyee/GroupFormer" -> "huguyuehuhu/Awesome-Group-Activity-Recognition"
"yjxiong/dense_flow" -> "feichtenhofer/gpu_flow"
"yjxiong/dense_flow" -> "wanglimin/dense_flow"
"yjxiong/dense_flow" -> "qijiezhao/py-denseflow"
"yjxiong/dense_flow" -> "yjxiong/tsn-pytorch"
"yjxiong/dense_flow" -> "bryanyzhu/two-stream-pytorch"
"yjxiong/dense_flow" -> "yjxiong/temporal-segment-networks"
"yjxiong/dense_flow" -> "yjxiong/action-detection"
"yjxiong/dense_flow" -> "feichtenhofer/twostreamfusion"
"yjxiong/dense_flow" -> "jeffreyyihuang/two-stream-action-recognition"
"yjxiong/dense_flow" -> "wzmsltw/BSN-boundary-sensitive-network"
"yjxiong/dense_flow" -> "wanglimin/ARTNet"
"yjxiong/dense_flow" -> "sunnyxiaohu/R-C3D.pytorch"
"yjxiong/dense_flow" -> "yjxiong/caffe"
"yjxiong/dense_flow" -> "open-mmlab/denseflow"
"yjxiong/dense_flow" -> "activitynet/ActivityNet"
"OxWearables/ssl-wearables" -> "OxWearables/capture24"
"OxWearables/ssl-wearables" -> "iantangc/SelfHAR"
"OxWearables/ssl-wearables" -> "OxWearables/biobankAccelerometerAnalysis"
"xushige/HAR-Dataset-Preprocess" -> "Chaolei98/Baseline-with-HAR-datasets"
"getalp/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices" -> "yolish/har-with-imu-transformer"
"getalp/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices" -> "markub3327/HAR-Transformer"
"Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset" -> "Whiffe/yolov5-slowfast-deepsort-PytorchVideo"
"Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset" -> "yjh0410/YOWOv2"
"Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset" -> "wufan-tb/yolo_slowfast" ["e"=1]
"Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset" -> "amazon-science/tubelet-transformer"
"Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset" -> "yjh0410/AVA_Dataset"
"Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset" -> "Huterox/Real-time-online-multi-target-behavior-detection-project-for-face-recognition"
"SherlockHolmes221/DOQ" -> "MuchHair/HQM"
"Anirudh257/strm" -> "tobyperrett/trx"
"Anirudh257/strm" -> "alibaba-mmai-research/HyRSM"
"Anirudh257/strm" -> "alibaba-mmai-research/CLIP-FSAR"
"Anirudh257/strm" -> "tobyperrett/few-shot-action-recognition"
"zyong812/STIP" -> "cjw2021/QAHOI"
"zyong812/STIP" -> "YueLiao/gen-vlkt"
"zyong812/STIP" -> "OreoChocolate/MUREN"
"zyong812/STIP" -> "YueLiao/CDN"
"zyong812/STIP" -> "JacobYuan7/RLIP"
"zyong812/STIP" -> "SherlockHolmes221/DOQ"
"zyong812/STIP" -> "MuchHair/HQM"
"zyong812/STIP" -> "hitachi-rd-cv/qpic"
"Cogito2012/OpenTAL" -> "LeonHLJ/RSKP"
"harlanhong/MM2021-CO2-Net" -> "zhou745/GauFuse_WSTAL"
"harlanhong/MM2021-CO2-Net" -> "LeonHLJ/RSKP"
"harlanhong/MM2021-CO2-Net" -> "MengyuanChen21/ECCV2022-DELU"
"harlanhong/MM2021-CO2-Net" -> "LeonHLJ/FAC-Net"
"harlanhong/MM2021-CO2-Net" -> "ispc-lab/ACM-Net"
"harlanhong/MM2021-CO2-Net" -> "RenHuan1999/CVPR2023_P-MIL"
"kracwarlock/action-recognition-visual-attention" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"kracwarlock/action-recognition-visual-attention" -> "feichtenhofer/twostreamfusion"
"kracwarlock/action-recognition-visual-attention" -> "hbilen/dynamic-image-nets"
"kracwarlock/action-recognition-visual-attention" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"kracwarlock/action-recognition-visual-attention" -> "oswaldoludwig/Human-Action-Recognition-with-Keras"
"kracwarlock/action-recognition-visual-attention" -> "imatge-upc/activitynet-2016-cvprw"
"kracwarlock/action-recognition-visual-attention" -> "qijiezhao/Video-Classification-Action-Recognition"
"kracwarlock/action-recognition-visual-attention" -> "TaeSoo-Kim/TCNActionRecognition" ["e"=1]
"kracwarlock/action-recognition-visual-attention" -> "jingweio/Action_Recognition_using_Visual_Attention"
"kracwarlock/action-recognition-visual-attention" -> "rohitgirdhar/AttentionalPoolingAction"
"kracwarlock/action-recognition-visual-attention" -> "LisaAnne/lisa-caffe-public"
"kracwarlock/action-recognition-visual-attention" -> "yjxiong/caffe"
"kracwarlock/action-recognition-visual-attention" -> "gudongfeng/3d-DenseNet"
"kracwarlock/action-recognition-visual-attention" -> "gurkirt/realtime-action-detection"
"kracwarlock/action-recognition-visual-attention" -> "rohitgirdhar/ActionVLAD"
"eladb3/ORViT" -> "gorjanradevski/revisiting-spatial-temporal-layouts"
"ju-chen/Efficient-Prompt" -> "sallymmx/ActionCLIP"
"ju-chen/Efficient-Prompt" -> "OpenGVLab/efficient-video-recognition"
"ju-chen/Efficient-Prompt" -> "sauradip/STALE"
"ju-chen/Efficient-Prompt" -> "TalalWasim/Vita-CLIP"
"ju-chen/Efficient-Prompt" -> "ttlmh/Bridge-Prompt"
"ju-chen/Efficient-Prompt" -> "MediaBrain-SJTU/BCL" ["e"=1]
"ju-chen/Efficient-Prompt" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"ju-chen/Efficient-Prompt" -> "happyharrycn/actionformer_release"
"ju-chen/Efficient-Prompt" -> "whwu95/BIKE"
"ju-chen/Efficient-Prompt" -> "TengdaHan/TemporalAlignNet" ["e"=1]
"xlliu7/E2E-TAD" -> "xlliu7/TadTR"
"xlliu7/E2E-TAD" -> "TencentYoutuResearch/ActionDetection-AFSD"
"xlliu7/E2E-TAD" -> "xlliu7/MUSES"
"xlliu7/E2E-TAD" -> "MCG-NJU/BasicTAD"
"klauscc/TALLFormer" -> "buxiangzhiren/ContextLoc"
"YueLiao/CDN" -> "YueLiao/gen-vlkt"
"YueLiao/CDN" -> "hitachi-rd-cv/qpic"
"YueLiao/CDN" -> "yoyomimi/AS-Net"
"YueLiao/CDN" -> "zyong812/STIP"
"YueLiao/CDN" -> "OreoChocolate/MUREN"
"YueLiao/CDN" -> "cjw2021/QAHOI"
"YueLiao/CDN" -> "kakaobrain/hotr"
"YueLiao/CDN" -> "fredzzhang/upt"
"YueLiao/CDN" -> "YueLiao/PPDM"
"YueLiao/CDN" -> "JacobYuan7/RLIP"
"YueLiao/CDN" -> "mrwu-mac/EoID"
"YueLiao/CDN" -> "vt-vl-lab/DRG"
"YueLiao/CDN" -> "SherlockHolmes221/DOQ"
"YueLiao/CDN" -> "scwangdyd/large_vocabulary_hoi_detection"
"LeonHLJ/RSKP" -> "zhang-can/UP-TAL"
"LeonHLJ/RSKP" -> "harlanhong/MM2021-CO2-Net"
"LeonHLJ/RSKP" -> "LeonHLJ/FAC-Net"
"LeonHLJ/RSKP" -> "zhang-can/CoLA"
"LeonHLJ/RSKP" -> "MengyuanChen21/CVPR2022-FTCL"
"LeonHLJ/RSKP" -> "zhou745/GauFuse_WSTAL"
"LeonHLJ/RSKP" -> "boheumd/ASM-Loc"
"LeonHLJ/RSKP" -> "MengyuanChen21/ECCV2022-DELU"
"YueLiao/gen-vlkt" -> "YueLiao/CDN"
"YueLiao/gen-vlkt" -> "hitachi-rd-cv/qpic"
"YueLiao/gen-vlkt" -> "zyong812/STIP"
"YueLiao/gen-vlkt" -> "cjw2021/QAHOI"
"YueLiao/gen-vlkt" -> "fredzzhang/upt"
"YueLiao/gen-vlkt" -> "SherlockHolmes221/DOQ"
"YueLiao/gen-vlkt" -> "OreoChocolate/MUREN"
"YueLiao/gen-vlkt" -> "YueLiao/PPDM"
"YueLiao/gen-vlkt" -> "kakaobrain/hotr"
"YueLiao/gen-vlkt" -> "enlighten0707/Body-Part-Map-for-Interactiveness"
"YueLiao/gen-vlkt" -> "mrwu-mac/EoID"
"YueLiao/gen-vlkt" -> "scwangdyd/large_vocabulary_hoi_detection"
"YueLiao/gen-vlkt" -> "yoyomimi/AS-Net"
"YueLiao/gen-vlkt" -> "zhihou7/VCL"
"YueLiao/gen-vlkt" -> "zhihou7/HOI-CL"
"ChinaYi/asrf_with_asformer" -> "yiskw713/video_feature_extractor"
"ChinaYi/asrf_with_asformer" -> "yiskw713/asrf"
"ttlmh/Bridge-Prompt" -> "ChinaYi/ASFormer"
"ttlmh/Bridge-Prompt" -> "ChinaYi/asrf_with_asformer"
"ttlmh/Bridge-Prompt" -> "yiskw713/video_feature_extractor"
"ttlmh/Bridge-Prompt" -> "boschresearch/UVAST"
"ttlmh/Bridge-Prompt" -> "yiskw713/asrf"
"ttlmh/Bridge-Prompt" -> "Finspire13/DiffAct"
"ttlmh/Bridge-Prompt" -> "sj-li/MS-TCN2"
"ttlmh/Bridge-Prompt" -> "nus-cvml/awesome-temporal-action-segmentation"
"harkash/contrastive-predictive-coding-for-har" -> "Hangwei12358/cross-person-HAR"
"yorkeyao/Automated-Retail-Checkout" -> "sxzrt/Proxy-Set"
"Rxannro/SWL-Adapt" -> "Rxannro/HMGAN"
"s-gupta/v-coco" -> "ywchao/ho-rcnn"
"s-gupta/v-coco" -> "vt-vl-lab/iCAN"
"s-gupta/v-coco" -> "DirtyHarryLYL/Transferable-Interactiveness-Network"
"s-gupta/v-coco" -> "BigRedT/no_frills_hoi_det"
"s-gupta/v-coco" -> "fredzzhang/hicodet"
"s-gupta/v-coco" -> "YueLiao/PPDM"
"s-gupta/v-coco" -> "bobwan1995/PMFNet"
"s-gupta/v-coco" -> "SiyuanQi-zz/gpnn"
"s-gupta/v-coco" -> "YueLiao/gen-vlkt"
"s-gupta/v-coco" -> "chinancheng/awesome-human-object-interaction"
"s-gupta/v-coco" -> "fredzzhang/spatially-conditioned-graphs"
"s-gupta/v-coco" -> "MVIG-SJTU/DIRV"
"s-gupta/v-coco" -> "vt-vl-lab/DRG"
"s-gupta/v-coco" -> "hitachi-rd-cv/qpic"
"s-gupta/v-coco" -> "cjw2021/QAHOI"
"Chaolei98/Baseline-with-HAR-datasets" -> "xushige/HAR-Dataset-Preprocess"
"xyzforever/BEVT" -> "ruiwang2021/mvd"
"ywchao/hico_benchmark" -> "ywchao/ho-rcnn"
"dairui01/MS-TCT" -> "dairui01/PDAN"
"xianruizhong/SpHAM" -> "fshp971/mcmc-unlearning"
"xianruizhong/SpHAM" -> "tmllab/2022_NeurIPS_PICMM"
"amazon-science/tubelet-transformer" -> "joslefaure/HIT"
"amazon-science/tubelet-transformer" -> "MCG-NJU/MultiSports"
"amazon-science/tubelet-transformer" -> "webber2933/iCLIP"
"amazon-science/tubelet-transformer" -> "MCG-NJU/STMixer"
"escorciav/deep-action-proposals" -> "escorciav/daps"
"nguyentthong/CLNTM" -> "nguyentthong/CrossSummOptimalTransport"
"nguyentthong/CLNTM" -> "lehduong/poodle"
"ShiYaya/emscore" -> "lzp870/RSFD"
"zcxu-eric/Ego4d_TalkNet_ASD" -> "EGO4D/audio-visual"
"NVlabs/Bongard-HOI" -> "NVlabs/RelViT"
"NVlabs/Bongard-HOI" -> "zyong812/STIP"
"NVlabs/Bongard-HOI" -> "cjw2021/QAHOI"
"NVlabs/Bongard-HOI" -> "yeliudev/ConsNet"
"NVlabs/Bongard-HOI" -> "neu-vi/Diag-HOI"
"lehduong/NPTM" -> "nguyentthong/CrossSummOptimalTransport"
"nguyentthong/CrossSummOptimalTransport" -> "nguyentthong/CLNTM"
"nguyentthong/CrossSummOptimalTransport" -> "lehduong/poodle"
"nguyentthong/CrossSummOptimalTransport" -> "lehduong/NPTM"
"dk-kim/DFWSGAR" -> "JacobYuan7/DIN-Group-Activity-Recognition-Benchmark"
"X-LANCE/MSDWILD" -> "Jiang-Yidi/TS-TalkNet"
"X-LANCE/MSDWILD" -> "xiaoxiaomiao323/MSA"
"MCG-NJU/FSL-Video" -> "VinAIResearch/fsvc-ata"
"Simon4Yan/Accuracy-estimation-with-self-supervision" -> "Simon4Yan/Meta-set"
"zhang-can/UP-TAL" -> "LeonHLJ/RSKP"
"Blueblue4/Object-Detection-Confidence-Bias" -> "Blueblue4/IoU-AwareCalibration"
"Blueblue4/Object-Detection-Confidence-Bias" -> "tteepe/EarlyBird"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "terryum/TensorFlow_Exercises" ["e"=1]
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "sjchoi86/Tensorflow-101" ["e"=1]
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "jindongwang/activityrecognition"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "jinwchoi/awesome-action-recognition"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "guillaume-chevalier/Awesome-Deep-Learning-Resources" ["e"=1]
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "healthDataScience/deep-learning-HAR"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "stuarteiffert/RNN-for-Human-Activity-Recognition-using-2D-Pose-Input" ["e"=1]
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "nicodjimenez/lstm" ["e"=1]
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "STRCWearlab/DeepConvLSTM"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "ZheC/Realtime_Multi-Person_Pose_Estimation" ["e"=1]
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"
"guillaume-chevalier/LSTM-Human-Activity-Recognition" -> "guillaume-chevalier/seq2seq-signal-prediction" ["e"=1]
"NLeSC/mcfly" -> "NLeSC/mcfly-tutorial"
"NLeSC/mcfly" -> "STRCWearlab/DeepConvLSTM"
"NLeSC/mcfly" -> "FrancisArgnR/Time-series---deep-learning---state-of-the-art"
"NLeSC/mcfly" -> "dmbee/seglearn" ["e"=1]
"feichtenhofer/twostreamfusion" -> "jeffreyyihuang/two-stream-action-recognition"
"feichtenhofer/twostreamfusion" -> "feichtenhofer/gpu_flow"
"feichtenhofer/twostreamfusion" -> "bryanyzhu/two-stream-pytorch"
"feichtenhofer/twostreamfusion" -> "yjxiong/temporal-segment-networks"
"feichtenhofer/twostreamfusion" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"feichtenhofer/twostreamfusion" -> "yjxiong/tsn-pytorch"
"feichtenhofer/twostreamfusion" -> "facebookarchive/C3D"
"feichtenhofer/twostreamfusion" -> "activitynet/ActivityNet"
"feichtenhofer/twostreamfusion" -> "google-deepmind/kinetics-i3d"
"feichtenhofer/twostreamfusion" -> "hx173149/C3D-tensorflow"
"feichtenhofer/twostreamfusion" -> "yjxiong/dense_flow"
"feichtenhofer/twostreamfusion" -> "facebookresearch/VMZ"
"feichtenhofer/twostreamfusion" -> "feichtenhofer/st-resnet"
"feichtenhofer/twostreamfusion" -> "HHTseng/video-classification"
"feichtenhofer/twostreamfusion" -> "yjxiong/caffe"
"MengyuanChen21/ECCV2022-DELU" -> "MengyuanChen21/CVPR2022-FTCL"
"MengyuanChen21/ECCV2022-DELU" -> "MengyuanChen21/CVPR2023-CMPAE"
"MengyuanChen21/ECCV2022-DELU" -> "MengyuanChen21/CVPR2023-OWTAL"
"MengyuanChen21/ECCV2022-DELU" -> "harlanhong/MM2021-CO2-Net"
"MengyuanChen21/ECCV2022-DELU" -> "MengyuanChen21/ICLR2024-REDL"
"MengyuanChen21/ECCV2022-DELU" -> "LeonHLJ/RSKP"
"MengyuanChen21/ECCV2022-DELU" -> "zhou745/GauFuse_WSTAL"
"sauradip/STALE" -> "sauradip/MUPPET"
"sauradip/STALE" -> "benedettaliberatori/T3AL" ["e"=1]
"sauradip/STALE" -> "ju-chen/Efficient-Prompt"
"Raiden-Zhu/Generalization-of-DSGD" -> "fshp971/mcmc-unlearning"
"Raiden-Zhu/Generalization-of-DSGD" -> "xianruizhong/SpHAM"
"Raiden-Zhu/Generalization-of-DSGD" -> "tmllab/2022_NeurIPS_PICMM"
"Raiden-Zhu/Generalization-of-DSGD" -> "fshp971/robust-unlearnable-examples"
"dingfengshi/ReAct" -> "fshp971/mcmc-unlearning"
"dingfengshi/ReAct" -> "tmllab/2022_NeurIPS_PICMM"
"dingfengshi/ReAct" -> "fshp971/robust-unlearnable-examples"
"dingfengshi/ReAct" -> "xianruizhong/SpHAM"
"dingfengshi/ReAct" -> "Raiden-Zhu/Generalization-of-DSGD"
"tmllab/2022_NeurIPS_PICMM" -> "fshp971/mcmc-unlearning"
"tmllab/2022_NeurIPS_PICMM" -> "xianruizhong/SpHAM"
"MuchHair/HQM" -> "SherlockHolmes221/DOQ"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "kracwarlock/action-recognition-visual-attention"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "imatge-upc/activitynet-2016-cvprw"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "qijiezhao/Video-Classification-Action-Recognition"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "feichtenhofer/twostreamfusion"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "LisaAnne/lisa-caffe-public"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "rohitgirdhar/ActionVLAD"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "yjxiong/temporal-segment-networks"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "zhengshou/scnn"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "oswaldoludwig/Human-Action-Recognition-with-Keras"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "feichtenhofer/st-resnet"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "hx173149/C3D-tensorflow"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "jeffreyyihuang/two-stream-action-recognition"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "feichtenhofer/gpu_flow"
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" -> "gurkirt/realtime-action-detection"
"R00Kie-Liu/TA2N" -> "R00Kie-Liu/Sampler"
"joslefaure/HIT" -> "webber2933/iCLIP"
"joslefaure/HIT" -> "amazon-science/tubelet-transformer"
"joslefaure/HIT" -> "MCG-NJU/EVAD"
"joslefaure/HIT" -> "ShoufaChen/WOO"
"joslefaure/HIT" -> "4paradigm-CV/SE-STAD"
"joslefaure/HIT" -> "joslefaure/HIT_ava"
"yjh0410/YOWOv2" -> "yjh0410/PyTorch_YOWO"
"yjh0410/YOWOv2" -> "wei-tim/YOWO"
"yjh0410/YOWOv2" -> "Hope1337/YOWOv3"
"yjh0410/YOWOv2" -> "Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset"
"yjh0410/YOWOv2" -> "yjh0410/AVA_Dataset"
"yjh0410/YOWOv2" -> "yjh0410/YOWOF"
"yjh0410/YOWOv2" -> "amazon-science/tubelet-transformer"
"yjh0410/YOWOv2" -> "joslefaure/HIT"
"yjh0410/YOWOv2" -> "ShoufaChen/WOO"
"yjh0410/YOWOv2" -> "joslefaure/HIT_ava"
"ZiweiXU/DTL-action-segmentation" -> "derkbreeze/AwesomeActionSegmentation"
"hbilen/dynamic-image-nets" -> "kracwarlock/action-recognition-visual-attention"
"hbilen/dynamic-image-nets" -> "bryanyzhu/Hidden-Two-Stream"
"hbilen/dynamic-image-nets" -> "feichtenhofer/st-resnet"
"hbilen/dynamic-image-nets" -> "jrbtaylor/ActivityNet"
"hbilen/dynamic-image-nets" -> "tcvrick/dynamic-images-for-action-recognition"
"hbilen/dynamic-image-nets" -> "gurkirt/realtime-action-detection"
"hbilen/dynamic-image-nets" -> "yifita/action.sr_cnn"
"hbilen/dynamic-image-nets" -> "wanglimin/TDD"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "feichtenhofer/twostreamfusion"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "sujiongming/UCF-101_video_classification"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "bryanyzhu/two-stream-pytorch"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "feichtenhofer/gpu_flow"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "qijiezhao/Video-Classification-Action-Recognition"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "harvitronix/five-video-classification-methods"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "rohitgirdhar/ActionVLAD"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "kracwarlock/action-recognition-visual-attention"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "jeffreyyihuang/two-stream-action-recognition"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "yjxiong/caffe"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "imatge-upc/activitynet-2016-cvprw"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "hx173149/C3D-tensorflow"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "facebookarchive/C3D"
"wadhwasahil/Video-Classification-2-Stream-CNN" -> "wanglimin/dense_flow"
"yjh0410/PyTorch_YOWO" -> "yjh0410/YOWOv2"
"yjh0410/PyTorch_YOWO" -> "yjh0410/YOWOF"
"yjh0410/PyTorch_YOWO" -> "amazon-science/tubelet-transformer"
"yjh0410/PyTorch_YOWO" -> "wei-tim/YOWO"
"yjh0410/PyTorch_YOWO" -> "MCG-NJU/EVAD"
"VinAIResearch/tise-toolbox" -> "lehduong/poodle"
"VinAIResearch/tise-toolbox" -> "nguyentthong/CrossSummOptimalTransport"
"VinAIResearch/tise-toolbox" -> "VinAIResearch/fsvc-ata"
"linziyi96/st-adapter" -> "alibaba-mmai-research/DiST"
"linziyi96/st-adapter" -> "park-jungin/DualPath"
"facebookresearch/mae_st" -> "MCG-NJU/VideoMAE"
"facebookresearch/mae_st" -> "OpenGVLab/VideoMAEv2"
"facebookresearch/mae_st" -> "EdisonLeeeee/Awesome-Masked-Autoencoders" ["e"=1]
"facebookresearch/mae_st" -> "ruiwang2021/mvd"
"facebookresearch/mae_st" -> "facebookresearch/hiera"
"facebookresearch/mae_st" -> "Alpha-VL/ConvMAE" ["e"=1]
"facebookresearch/mae_st" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"facebookresearch/mae_st" -> "facebookresearch/flip"
"dingfengshi/TriDet" -> "happyharrycn/actionformer_release"
"dingfengshi/TriDet" -> "dingfengshi/ReAct"
"dingfengshi/TriDet" -> "sming256/OpenTAD"
"dingfengshi/TriDet" -> "dingfengshi/tridetplus"
"dingfengshi/TriDet" -> "xlliu7/TadTR"
"dingfengshi/TriDet" -> "TencentYoutuResearch/ActionDetection-AFSD"
"dingfengshi/TriDet" -> "Finspire13/pytorch-i3d-feature-extraction"
"dingfengshi/TriDet" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"dingfengshi/TriDet" -> "yingsen1/UniMD"
"dingfengshi/TriDet" -> "MCG-NJU/BasicTAD"
"dingfengshi/TriDet" -> "MCG-NJU/RTD-Action"
"dingfengshi/TriDet" -> "HumamAlwassel/TSP"
"dingfengshi/TriDet" -> "Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization"
"dingfengshi/TriDet" -> "TuanTNG/TemporalMaxer"
"dingfengshi/TriDet" -> "zhou745/GauFuse_WSTAL"
"mostafa-saad/deep-activity-rec" -> "wjchaoGit/Group-Activity-Recognition"
"mostafa-saad/deep-activity-rec" -> "cvlab-epfl/social-scene-understanding"
"mostafa-saad/deep-activity-rec" -> "mostafa-saad/hierarchical-relational-network"
"mostafa-saad/deep-activity-rec" -> "huguyuehuhu/Awesome-Group-Activity-Recognition"
"mostafa-saad/deep-activity-rec" -> "xueyee/GroupFormer"
"mostafa-saad/deep-activity-rec" -> "ruiyan1995/Group-Activity-Recognition"
"mostafa-saad/deep-activity-rec" -> "gurkirt/corrected-UCF101-Annots"
"mostafa-saad/deep-activity-rec" -> "JacobYuan7/DIN-Group-Activity-Recognition-Benchmark"
"mostafa-saad/deep-activity-rec" -> "ruiyan1995/HiGCIN"
"mostafa-saad/deep-activity-rec" -> "dk-kim/DFWSGAR"
"whwu95/BIKE" -> "whwu95/Text4Vis"
"whwu95/BIKE" -> "whwu95/ATM"
"whwu95/BIKE" -> "whwu95/GPT4Vis"
"whwu95/BIKE" -> "OpenGVLab/efficient-video-recognition"
"whwu95/BIKE" -> "whwu95/Cap4Video" ["e"=1]
"whwu95/BIKE" -> "daniel-code/TubeViT"
"whwu95/BIKE" -> "TalalWasim/Vita-CLIP"
"whwu95/BIKE" -> "ruiwang2021/mvd"
"whwu95/BIKE" -> "park-jungin/DualPath"
"whwu95/BIKE" -> "alibaba-mmai-research/Masked-Action-Recognition"
"whwu95/Text4Vis" -> "whwu95/BIKE"
"whwu95/Text4Vis" -> "whwu95/GPT4Vis"
"whwu95/Text4Vis" -> "whwu95/Cap4Video" ["e"=1]
"whwu95/Text4Vis" -> "OpenGVLab/efficient-video-recognition"
"whwu95/Text4Vis" -> "whwu95/MVFNet"
"whwu95/Text4Vis" -> "alibaba-mmai-research/CLIP-FSAR"
"whwu95/Text4Vis" -> "wengzejia1/Open-VCLIP"
"OpenGVLab/efficient-video-recognition" -> "alibaba-mmai-research/DiST"
"OpenGVLab/efficient-video-recognition" -> "park-jungin/DualPath"
"OpenGVLab/efficient-video-recognition" -> "ju-chen/Efficient-Prompt"
"OpenGVLab/efficient-video-recognition" -> "whwu95/BIKE"
"OpenGVLab/efficient-video-recognition" -> "linziyi96/st-adapter"
"OpenGVLab/efficient-video-recognition" -> "farewellthree/STAN"
"OpenGVLab/efficient-video-recognition" -> "whwu95/Text4Vis"
"OpenGVLab/efficient-video-recognition" -> "sallymmx/ActionCLIP"
"OpenGVLab/efficient-video-recognition" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"park-jungin/DualPath" -> "alibaba-mmai-research/DiST"
"imatge-upc/activitynet-2016-cvprw" -> "escorciav/daps"
"imatge-upc/activitynet-2016-cvprw" -> "shyamal-b/sst"
"imatge-upc/activitynet-2016-cvprw" -> "yjxiong/anet2016-cuhk"
"imatge-upc/activitynet-2016-cvprw" -> "shyamal-b/ss-tad"
"imatge-upc/activitynet-2016-cvprw" -> "zhengshou/scnn"
"imatge-upc/activitynet-2016-cvprw" -> "kracwarlock/action-recognition-visual-attention"
"imatge-upc/activitynet-2016-cvprw" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"imatge-upc/activitynet-2016-cvprw" -> "syyeung/frameglimpses"
"imatge-upc/activitynet-2016-cvprw" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"imatge-upc/activitynet-2016-cvprw" -> "wanglimin/UntrimmedNet"
"imatge-upc/activitynet-2016-cvprw" -> "vdavid70619/TCN"
"imatge-upc/activitynet-2016-cvprw" -> "rohitgirdhar/ActionVLAD"
"imatge-upc/activitynet-2016-cvprw" -> "cabaf/sparseprop"
"imatge-upc/activitynet-2016-cvprw" -> "gurkirt/actNet-inAct"
"imatge-upc/activitynet-2016-cvprw" -> "JaywongWang/SST-Tensorflow"
"chuckcho/video-caffe" -> "facebookarchive/C3D"
"chuckcho/video-caffe" -> "ColumbiaDVMM/CDC"
"chuckcho/video-caffe" -> "wanglimin/TDD"
"chuckcho/video-caffe" -> "LisaAnne/lisa-caffe-public"
"chuckcho/video-caffe" -> "yjxiong/anet2016-cuhk"
"nus-cvml/awesome-temporal-action-segmentation" -> "ZijiaLewisLu/CVPR2024-FACT"
"nus-cvml/awesome-temporal-action-segmentation" -> "ChinaYi/ASFormer"
"nus-cvml/awesome-temporal-action-segmentation" -> "Finspire13/DiffAct"
"nus-cvml/awesome-temporal-action-segmentation" -> "ttlmh/Bridge-Prompt"
"nus-cvml/awesome-temporal-action-segmentation" -> "yabufarha/ms-tcn"
"nus-cvml/awesome-temporal-action-segmentation" -> "LTContext/LTContext"
"nus-cvml/awesome-temporal-action-segmentation" -> "yiskw713/asrf"
"nus-cvml/awesome-temporal-action-segmentation" -> "sj-li/MS-TCN2"
"nus-cvml/awesome-temporal-action-segmentation" -> "derkbreeze/AwesomeActionSegmentation"
"nus-cvml/awesome-temporal-action-segmentation" -> "Yuhan-Shen/ProTAS"
"nus-cvml/awesome-temporal-action-segmentation" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"nus-cvml/awesome-temporal-action-segmentation" -> "mingu6/action_seg_ot"
"nus-cvml/awesome-temporal-action-segmentation" -> "boschresearch/UVAST"
"nus-cvml/awesome-temporal-action-segmentation" -> "walker1126/Latent_Action_Composition"
"nus-cvml/awesome-temporal-action-segmentation" -> "yiskw713/video_feature_extractor"
"taoyang1122/adapt-image-models" -> "whwu95/BIKE"
"taoyang1122/adapt-image-models" -> "linziyi96/st-adapter"
"taoyang1122/adapt-image-models" -> "TalalWasim/Vita-CLIP"
"taoyang1122/adapt-image-models" -> "whwu95/Text4Vis"
"taoyang1122/adapt-image-models" -> "alibaba-mmai-research/DiST"
"taoyang1122/adapt-image-models" -> "OpenGVLab/efficient-video-recognition"
"taoyang1122/adapt-image-models" -> "ju-chen/Efficient-Prompt"
"taoyang1122/adapt-image-models" -> "wengzejia1/Open-VCLIP"
"taoyang1122/adapt-image-models" -> "sallymmx/ActionCLIP"
"taoyang1122/adapt-image-models" -> "alibaba-mmai-research/CLIP-FSAR"
"taoyang1122/adapt-image-models" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"taoyang1122/adapt-image-models" -> "JieShibo/PETL-ViT" ["e"=1]
"taoyang1122/adapt-image-models" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"taoyang1122/adapt-image-models" -> "OpenGVLab/VideoMAEv2"
"taoyang1122/adapt-image-models" -> "whwu95/ATM"
"OpenGVLab/UniFormerV2" -> "Sense-X/UniFormer"
"OpenGVLab/UniFormerV2" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"OpenGVLab/UniFormerV2" -> "OpenGVLab/VideoMAEv2"
"OpenGVLab/UniFormerV2" -> "daniel-code/TubeViT"
"OpenGVLab/UniFormerV2" -> "OpenGVLab/efficient-video-recognition"
"OpenGVLab/UniFormerV2" -> "OpenGVLab/InternVideo" ["e"=1]
"OpenGVLab/UniFormerV2" -> "MCG-NJU/VideoMAE"
"OpenGVLab/UniFormerV2" -> "whwu95/BIKE"
"alibaba-mmai-research/CLIP-FSAR" -> "alibaba-mmai-research/MoLo"
"alibaba-mmai-research/CLIP-FSAR" -> "alibaba-mmai-research/HyRSM"
"zhengshou/scnn" -> "ColumbiaDVMM/CDC"
"zhengshou/scnn" -> "shyamal-b/sst"
"zhengshou/scnn" -> "jiyanggao/TURN-TAP"
"zhengshou/scnn" -> "wanglimin/UntrimmedNet"
"zhengshou/scnn" -> "yjxiong/action-detection"
"zhengshou/scnn" -> "yjxiong/anet2016-cuhk"
"zhengshou/scnn" -> "jiyanggao/CBR"
"zhengshou/scnn" -> "escorciav/daps"
"zhengshou/scnn" -> "VisionLearningGroup/R-C3D"
"zhengshou/scnn" -> "Alvin-Zeng/PGCN"
"zhengshou/scnn" -> "wzmsltw/BSN-boundary-sensitive-network"
"zhengshou/scnn" -> "Tencent/ActionDetection-DBG"
"zhengshou/scnn" -> "Rheelt/Materials-Temporal-Action-Detection"
"zhengshou/scnn" -> "jiyanggao/CTAP"
"zhengshou/scnn" -> "JaywongWang/SST-Tensorflow"
"SRA2/SPELL" -> "okankop/ASDNet"
"SRA2/SPELL" -> "IntelLabs/GraVi-T"
"SRA2/SPELL" -> "fuankarion/active-speakers-context"
"alibaba-mmai-research/HyRSMPlusPlus" -> "alibaba-mmai-research/HyRSM"
"Whiffe/yolov5-slowfast-deepsort-PytorchVideo" -> "Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset"
"Whiffe/yolov5-slowfast-deepsort-PytorchVideo" -> "wangkang12/yolo_slowfast-master"
"garythung/torch-lrcn" -> "salaniz/pytorch-gve-lrcn"
"jianboyang/CNNHAR" -> "sibosutd/cnn-timeseries"
"LiangZai-Embedded/ThermalGesture_ESP32" -> "LiangZai-Embedded/HAR-ON-STM32F401C"
"fredzzhang/pvic" -> "OreoChocolate/MUREN"
"fredzzhang/pvic" -> "Artanic30/HOICLIP"
"fredzzhang/pvic" -> "ltttpku/ADA-CM"
"fredzzhang/pvic" -> "Jeeseung-Park/ViPLO"
"fredzzhang/pvic" -> "JacobYuan7/RLIP"
"fubel/synthehicle" -> "Blueblue4/IoU-AwareCalibration"
"fubel/synthehicle" -> "Blueblue4/Object-Detection-Confidence-Bias"
"xiaomabufei/FGAHOI" -> "enlighten0707/Body-Part-Map-for-Interactiveness"
"daniel-code/TubeViT" -> "ruiwang2021/mvd"
"daniel-code/TubeViT" -> "whwu95/BIKE"
"daniel-code/TubeViT" -> "whwu95/ATM"
"ruiwang2021/mvd" -> "whwu95/ATM"
"ruiwang2021/mvd" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"ruiwang2021/mvd" -> "XinyuSun/MME" ["e"=1]
"ruiwang2021/mvd" -> "daniel-code/TubeViT"
"ruiwang2021/mvd" -> "whwu95/BIKE"
"ruiwang2021/mvd" -> "xyzforever/BEVT"
"ruiwang2021/mvd" -> "MCG-NJU/VideoMAE-Action-Detection"
"ruiwang2021/mvd" -> "MCG-NJU/AMD"
"ruiwang2021/mvd" -> "OpenGVLab/VideoMAEv2"
"xuyu0010/awesome-video-domain-adaptation" -> "xuyu0010/ATCoN"
"VinAIResearch/fsvc-ata" -> "MCG-NJU/FSL-Video"
"4paradigm-CV/SE-STAD" -> "joslefaure/HIT_ava"
"JacobYuan7/RLIP" -> "OreoChocolate/MUREN"
"JacobYuan7/RLIP" -> "JacobYuan7/RLIPv2"
"JacobYuan7/RLIP" -> "enlighten0707/Body-Part-Map-for-Interactiveness"
"JacobYuan7/RLIP" -> "zyong812/STIP"
"JacobYuan7/RLIP" -> "YueLiao/CDN"
"JacobYuan7/RLIP" -> "cjw2021/QAHOI"
"JacobYuan7/RLIP" -> "Artanic30/HOICLIP"
"JacobYuan7/RLIP" -> "fredzzhang/pvic"
"boschresearch/UVAST" -> "ChinaYi/ASFormer"
"derkbreeze/LPT" -> "FedeSpu/HVQ"
"Yanyi-Zhang/Awesome-Compositional-Zero-Shot" -> "soberguo/HOIGen"
"enlighten0707/Body-Part-Map-for-Interactiveness" -> "xiaomabufei/FGAHOI"
"enlighten0707/Body-Part-Map-for-Interactiveness" -> "Jeeseung-Park/ViPLO"
"enlighten0707/Body-Part-Map-for-Interactiveness" -> "JacobYuan7/RLIP"
"wdkhuans/DynamicWHAR" -> "Rxannro/HMGAN"
"hongluzhou/composer" -> "xueyee/GroupFormer"
"hongluzhou/composer" -> "JacobYuan7/DIN-Group-Activity-Recognition-Benchmark"
"farewellthree/STAN" -> "alibaba-mmai-research/DiST"
"alexanderrichard/squirrel" -> "alexanderrichard/NeuralNetwork-Viterbi"
"alexanderrichard/squirrel" -> "hildekuehne/HTK_actionRecognition"
"alexanderrichard/squirrel" -> "alexanderrichard/action-sets"
"TaoRuijie/AVCleanse" -> "Jiang-Yidi/TS-TalkNet"
"lehduong/poodle" -> "nguyentthong/CrossSummOptimalTransport"
"lehduong/poodle" -> "lehduong/Knowledge-Distillation-by-Replacing-Cheap-Conv"
"lehduong/poodle" -> "nguyentthong/CLNTM"
"R00Kie-Liu/Sampler" -> "R00Kie-Liu/TA2N"
"R00Kie-Liu/Sampler" -> "alibaba-mmai-research/HyRSMPlusPlus"
"derkbreeze/AwesomeActionSegmentation" -> "olga-zats/GTDA"
"derkbreeze/AwesomeActionSegmentation" -> "ZiweiXU/DTL-action-segmentation"
"jinwchoi/awesome-action-recognition" -> "kenshohara/3D-ResNets-PyTorch"
"jinwchoi/awesome-action-recognition" -> "open-mmlab/mmaction"
"jinwchoi/awesome-action-recognition" -> "open-mmlab/mmskeleton" ["e"=1]
"jinwchoi/awesome-action-recognition" -> "yjxiong/temporal-segment-networks"
"jinwchoi/awesome-action-recognition" -> "jeffreyyihuang/two-stream-action-recognition"
"jinwchoi/awesome-action-recognition" -> "facebookresearch/SlowFast"
"jinwchoi/awesome-action-recognition" -> "google-deepmind/kinetics-i3d"
"jinwchoi/awesome-action-recognition" -> "yjxiong/tsn-pytorch"
"jinwchoi/awesome-action-recognition" -> "facebookresearch/VMZ"
"jinwchoi/awesome-action-recognition" -> "mit-han-lab/temporal-shift-module"
"jinwchoi/awesome-action-recognition" -> "activitynet/ActivityNet"
"jinwchoi/awesome-action-recognition" -> "niais/Awesome-Skeleton-based-Action-Recognition" ["e"=1]
"jinwchoi/awesome-action-recognition" -> "open-mmlab/mmaction2"
"jinwchoi/awesome-action-recognition" -> "yjxiong/action-detection"
"jinwchoi/awesome-action-recognition" -> "jfzhang95/pytorch-video-recognition"
"yjxiong/anet2016-cuhk" -> "wzmsltw/BSN-boundary-sensitive-network"
"yjxiong/anet2016-cuhk" -> "shyamal-b/sst"
"yjxiong/anet2016-cuhk" -> "yjxiong/action-detection"
"yjxiong/anet2016-cuhk" -> "wanglimin/UntrimmedNet"
"yjxiong/anet2016-cuhk" -> "yjxiong/temporal-segment-networks"
"yjxiong/anet2016-cuhk" -> "zhengshou/scnn"
"yjxiong/anet2016-cuhk" -> "shyamal-b/ss-tad"
"yjxiong/anet2016-cuhk" -> "imatge-upc/activitynet-2016-cvprw"
"yjxiong/anet2016-cuhk" -> "wanglimin/ARTNet"
"yjxiong/anet2016-cuhk" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"yjxiong/anet2016-cuhk" -> "yjxiong/caffe"
"yjxiong/anet2016-cuhk" -> "wanglimin/TDD"
"yjxiong/anet2016-cuhk" -> "zbwglory/MV-release"
"yjxiong/anet2016-cuhk" -> "ranjaykrishna/SST"
"yjxiong/anet2016-cuhk" -> "sujoyp/wtalc-pytorch"
"zbwglory/MV-release" -> "vadimkantorov/fastvideofeat"
"zbwglory/MV-release" -> "yjxiong/anet2016-cuhk"
"hx173149/C3D-tensorflow" -> "facebookarchive/C3D"
"hx173149/C3D-tensorflow" -> "axon-research/c3d-keras"
"hx173149/C3D-tensorflow" -> "qijiezhao/pseudo-3d-pytorch"
"hx173149/C3D-tensorflow" -> "feichtenhofer/twostreamfusion"
"hx173149/C3D-tensorflow" -> "yjxiong/temporal-segment-networks"
"hx173149/C3D-tensorflow" -> "DavideA/c3d-pytorch"
"hx173149/C3D-tensorflow" -> "google-deepmind/kinetics-i3d"
"hx173149/C3D-tensorflow" -> "VisionLearningGroup/R-C3D"
"hx173149/C3D-tensorflow" -> "2012013382/C3D-Tensorflow-slim"
"hx173149/C3D-tensorflow" -> "TianzhongSong/C3D-keras"
"hx173149/C3D-tensorflow" -> "facebookresearch/VMZ"
"hx173149/C3D-tensorflow" -> "jeffreyyihuang/two-stream-action-recognition"
"hx173149/C3D-tensorflow" -> "yjxiong/tsn-pytorch"
"hx173149/C3D-tensorflow" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"hx173149/C3D-tensorflow" -> "rohitgirdhar/ActionVLAD"
"OpenGVLab/VideoMAEv2" -> "MCG-NJU/VideoMAE"
"OpenGVLab/VideoMAEv2" -> "OpenGVLab/InternVideo" ["e"=1]
"OpenGVLab/VideoMAEv2" -> "happyharrycn/actionformer_release"
"OpenGVLab/VideoMAEv2" -> "OpenGVLab/unmasked_teacher" ["e"=1]
"OpenGVLab/VideoMAEv2" -> "ruiwang2021/mvd"
"OpenGVLab/VideoMAEv2" -> "OpenGVLab/UniFormerV2"
"OpenGVLab/VideoMAEv2" -> "OpenGVLab/VideoMamba" ["e"=1]
"OpenGVLab/VideoMAEv2" -> "facebookresearch/mae_st"
"OpenGVLab/VideoMAEv2" -> "sallymmx/ActionCLIP"
"OpenGVLab/VideoMAEv2" -> "SwinTransformer/Video-Swin-Transformer"
"OpenGVLab/VideoMAEv2" -> "sming256/OpenTAD"
"OpenGVLab/VideoMAEv2" -> "dingfengshi/TriDet"
"OpenGVLab/VideoMAEv2" -> "taoyang1122/adapt-image-models"
"OpenGVLab/VideoMAEv2" -> "MCG-NJU/VideoMAE-Action-Detection"
"OpenGVLab/VideoMAEv2" -> "cvdfoundation/kinetics-dataset"
"Junhua-Liao/Light-ASD" -> "Jiang-Yidi/TS-TalkNet"
"Junhua-Liao/Light-ASD" -> "TaoRuijie/TalkNet-ASD"
"Junhua-Liao/Light-ASD" -> "zcxu-eric/AVA-AVD"
"Junhua-Liao/Light-ASD" -> "X-LANCE/MSDWILD"
"Junhua-Liao/Light-ASD" -> "Tiago-Roxo/WASD"
"Junhua-Liao/Light-ASD" -> "Junhua-Liao/LR-ASD"
"Junhua-Liao/Light-ASD" -> "okankop/ASDNet"
"Junhua-Liao/Light-ASD" -> "SRA2/SPELL"
"Junhua-Liao/Light-ASD" -> "TaoRuijie/MFV-KSD"
"facebookresearch/hiera" -> "NVlabs/FasterViT" ["e"=1]
"facebookresearch/hiera" -> "OpenGVLab/VideoMAEv2"
"facebookresearch/hiera" -> "facebookresearch/mvit"
"facebookresearch/hiera" -> "facebookresearch/mae_st"
"facebookresearch/hiera" -> "facebookresearch/ConvNeXt-V2" ["e"=1]
"facebookresearch/hiera" -> "facebookresearch/ToMe" ["e"=1]
"facebookresearch/hiera" -> "NVlabs/RADIO" ["e"=1]
"facebookresearch/hiera" -> "facebookresearch/MetaCLIP" ["e"=1]
"facebookresearch/hiera" -> "MCG-NJU/VideoMAE"
"facebookresearch/hiera" -> "czczup/ViT-Adapter" ["e"=1]
"facebookresearch/hiera" -> "brjathu/LART" ["e"=1]
"facebookresearch/hiera" -> "apple/ml-aim" ["e"=1]
"facebookresearch/hiera" -> "mit-han-lab/efficientvit" ["e"=1]
"facebookresearch/hiera" -> "Lupin1998/Awesome-MIM" ["e"=1]
"facebookresearch/hiera" -> "apple/ml-fastvit" ["e"=1]
"lgzlIlIlI/Boosting-WTAL" -> "RenHuan1999/CVPR2023_P-MIL"
"lgzlIlIlI/Boosting-WTAL" -> "zhou745/GauFuse_WSTAL"
"lgzlIlIlI/Boosting-WTAL" -> "harlanhong/MM2021-CO2-Net"
"RenHuan1999/CVPR2023_P-MIL" -> "lgzlIlIlI/Boosting-WTAL"
"RenHuan1999/CVPR2023_P-MIL" -> "zhou745/GauFuse_WSTAL"
"RenHuan1999/CVPR2023_P-MIL" -> "leftthomas/ACRNet"
"RenHuan1999/CVPR2023_P-MIL" -> "harlanhong/MM2021-CO2-Net"
"RenHuan1999/CVPR2023_P-MIL" -> "lizhilin-ustc/AAAI2023-AICL"
"haonanwang0522/SRPose" -> "haonanwang0522/GTPT"
"MengyuanChen21/CVPR2023-CMPAE" -> "MengyuanChen21/ECCV2022-DELU"
"MengyuanChen21/CVPR2023-CMPAE" -> "MengyuanChen21/ICLR2024-REDL"
"MengyuanChen21/CVPR2023-CMPAE" -> "MengyuanChen21/CVPR2023-OWTAL"
"MengyuanChen21/CVPR2023-CMPAE" -> "MengyuanChen21/CVPR2022-FTCL"
"yjxiong/temporal-segment-networks" -> "yjxiong/tsn-pytorch"
"yjxiong/temporal-segment-networks" -> "open-mmlab/mmaction"
"yjxiong/temporal-segment-networks" -> "mit-han-lab/temporal-shift-module"
"yjxiong/temporal-segment-networks" -> "google-deepmind/kinetics-i3d"
"yjxiong/temporal-segment-networks" -> "feichtenhofer/twostreamfusion"
"yjxiong/temporal-segment-networks" -> "yjxiong/action-detection"
"yjxiong/temporal-segment-networks" -> "zhoubolei/TRN-pytorch"
"yjxiong/temporal-segment-networks" -> "jeffreyyihuang/two-stream-action-recognition"
"yjxiong/temporal-segment-networks" -> "facebookarchive/C3D"
"yjxiong/temporal-segment-networks" -> "activitynet/ActivityNet"
"yjxiong/temporal-segment-networks" -> "facebookresearch/VMZ"
"yjxiong/temporal-segment-networks" -> "jinwchoi/awesome-action-recognition"
"yjxiong/temporal-segment-networks" -> "yjxiong/anet2016-cuhk"
"yjxiong/temporal-segment-networks" -> "bryanyzhu/two-stream-pytorch"
"yjxiong/temporal-segment-networks" -> "facebookresearch/video-nonlocal-net"
"alibaba-mmai-research/MoLo" -> "alibaba-mmai-research/CLIP-FSAR"
"alibaba-mmai-research/MoLo" -> "alibaba-mmai-research/HyRSM"
"alibaba-mmai-research/MoLo" -> "alibaba-mmai-research/HyRSMPlusPlus"
"IDEA-Research/DiffHOI" -> "Artanic30/HOICLIP"
"IDEA-Research/DiffHOI" -> "ltttpku/CMMP"
"IDEA-Research/DiffHOI" -> "scwangdyd/large_vocabulary_hoi_detection"
"IDEA-Research/DiffHOI" -> "ChelsieLei/EZ-HOI"
"jindongwang/activityrecognition" -> "jindongwang/MachineLearning" ["e"=1]
"jindongwang/activityrecognition" -> "jindongwang/Deep-learning-activity-recognition"
"jindongwang/activityrecognition" -> "STRCWearlab/DeepConvLSTM"
"jindongwang/activityrecognition" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"jindongwang/activityrecognition" -> "jinwchoi/awesome-action-recognition"
"jindongwang/activityrecognition" -> "guillaume-chevalier/LSTM-Human-Activity-Recognition"
"jindongwang/activityrecognition" -> "jeffreyyihuang/two-stream-action-recognition"
"jindongwang/activityrecognition" -> "yjxiong/tsn-pytorch"
"jindongwang/activityrecognition" -> "yjxiong/temporal-segment-networks"
"jindongwang/activityrecognition" -> "facebookarchive/C3D"
"jindongwang/activityrecognition" -> "feichtenhofer/twostreamfusion"
"jindongwang/activityrecognition" -> "TianzhongSong/Real-Time-Action-Recognition" ["e"=1]
"jindongwang/activityrecognition" -> "bryanyzhu/two-stream-pytorch"
"jindongwang/activityrecognition" -> "qijiezhao/Video-Classification-Action-Recognition"
"jindongwang/activityrecognition" -> "jfzhang95/pytorch-video-recognition"
"TalalWasim/Vita-CLIP" -> "muzairkhattak/ViFi-CLIP" ["e"=1]
"feichtenhofer/gpu_flow" -> "feichtenhofer/twostreamfusion"
"feichtenhofer/gpu_flow" -> "yjxiong/dense_flow"
"feichtenhofer/gpu_flow" -> "jeffreyyihuang/two-stream-action-recognition"
"feichtenhofer/gpu_flow" -> "qijiezhao/py-denseflow"
"feichtenhofer/gpu_flow" -> "wanglimin/dense_flow"
"feichtenhofer/gpu_flow" -> "bryanyzhu/two-stream-pytorch"
"feichtenhofer/gpu_flow" -> "lmb-freiburg/flownet2-docker"
"feichtenhofer/gpu_flow" -> "yjxiong/tsn-pytorch"
"feichtenhofer/gpu_flow" -> "yjxiong/temporal-segment-networks"
"feichtenhofer/gpu_flow" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"feichtenhofer/gpu_flow" -> "wanglimin/ARTNet"
"feichtenhofer/gpu_flow" -> "google-deepmind/kinetics-i3d"
"feichtenhofer/gpu_flow" -> "wzmsltw/BSN-boundary-sensitive-network"
"feichtenhofer/gpu_flow" -> "activitynet/ActivityNet"
"feichtenhofer/gpu_flow" -> "feichtenhofer/st-resnet"
"crocodilegogogo/IF-ConvTransformer-UbiComp2022" -> "Rxannro/HMGAN"
"Charles-Xie/CQL" -> "OreoChocolate/MUREN"
"dingfengshi/tridetplus" -> "sming256/AdaTAD"
"syyeung/frameglimpses" -> "shyamal-b/sst"
"syyeung/frameglimpses" -> "jiyanggao/TURN-TAP"
"syyeung/frameglimpses" -> "escorciav/daps"
"wanglimin/MRCNN-Scene-Recognition" -> "wangzheallen/vsad"
"wanglimin/MRCNN-Scene-Recognition" -> "wanglimin/Places205-VGGNet"
"wanglimin/MRCNN-Scene-Recognition" -> "yifita/action.sr_cnn"
"wanglimin/MRCNN-Scene-Recognition" -> "yjxiong/caffe"
"wanglimin/MRCNN-Scene-Recognition" -> "wanglimin/TDD"
"jrbtaylor/ActivityNet" -> "jvgemert/apt"
"Artanic30/HOICLIP" -> "fredzzhang/pvic"
"Artanic30/HOICLIP" -> "ltttpku/CMD-SE-release"
"Artanic30/HOICLIP" -> "JacobYuan7/RLIP"
"Artanic30/HOICLIP" -> "IDEA-Research/DiffHOI"
"Artanic30/HOICLIP" -> "scwangdyd/promting_hoi"
"Artanic30/HOICLIP" -> "JacobYuan7/RLIPv2"
"Artanic30/HOICLIP" -> "soberguo/HOIGen"
"Artanic30/HOICLIP" -> "YueLiao/gen-vlkt"
"cabaf/sparseprop" -> "shyamal-b/sst"
"zhou745/GauFuse_WSTAL" -> "harlanhong/MM2021-CO2-Net"
"JacobYuan7/RLIPv2" -> "JacobYuan7/RLIP"
"JacobYuan7/RLIPv2" -> "Artanic30/HOICLIP"
"JacobYuan7/RLIPv2" -> "YueLiao/gen-vlkt"
"JacobYuan7/RLIPv2" -> "OreoChocolate/MUREN"
"JacobYuan7/RLIPv2" -> "hitachi-rd-cv/qpic"
"JacobYuan7/RLIPv2" -> "fredzzhang/upt"
"JacobYuan7/RLIPv2" -> "YueLiao/CDN"
"acherstyx/Compressed-Video-Reader" -> "acherstyx/CoCap"
"oswaldoludwig/Human-Action-Recognition-with-Keras" -> "kracwarlock/action-recognition-visual-attention"
"oswaldoludwig/Human-Action-Recognition-with-Keras" -> "fomorians/distracted-drivers-keras" ["e"=1]
"oswaldoludwig/Human-Action-Recognition-with-Keras" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"oswaldoludwig/Human-Action-Recognition-with-Keras" -> "qijiezhao/Video-Classification-Action-Recognition"
"oswaldoludwig/Human-Action-Recognition-with-Keras" -> "Yorwxue/Two-Stream-Convolutional-Networks"
"OreoChocolate/MUREN" -> "JacobYuan7/RLIP"
"OreoChocolate/MUREN" -> "zyong812/STIP"
"OreoChocolate/MUREN" -> "YueLiao/CDN"
"OreoChocolate/MUREN" -> "fredzzhang/pvic"
"OreoChocolate/MUREN" -> "Jeeseung-Park/ViPLO"
"OreoChocolate/MUREN" -> "YueLiao/gen-vlkt"
"OreoChocolate/MUREN" -> "Charles-Xie/CQL"
"Rxannro/HMGAN" -> "Rxannro/SWL-Adapt"
"joslefaure/HIT_ava" -> "4paradigm-CV/SE-STAD"
"whwu95/ATM" -> "whwu95/BIKE"
"wengzejia1/Open-VCLIP" -> "Visual-AI/FROSTER"
"pipixin321/HR-Pro" -> "RenHuan1999/CVPR2023_P-MIL"
"acherstyx/CoCap" -> "zohrehghaderi/VASTA"
"acherstyx/CoCap" -> "lzp870/RSFD"
"acherstyx/CoCap" -> "acherstyx/Compressed-Video-Reader"
"alibaba-mmai-research/DiST" -> "park-jungin/DualPath"
"Finspire13/DiffAct" -> "ZijiaLewisLu/CVPR2024-FACT"
"Finspire13/DiffAct" -> "ChinaYi/ASFormer"
"Finspire13/DiffAct" -> "LTContext/LTContext"
"Finspire13/DiffAct" -> "nus-cvml/awesome-temporal-action-segmentation"
"Finspire13/DiffAct" -> "boschresearch/UVAST"
"Finspire13/DiffAct" -> "Yuhan-Shen/ProTAS"
"Jeeseung-Park/ViPLO" -> "enlighten0707/Body-Part-Map-for-Interactiveness"
"Jeeseung-Park/ViPLO" -> "OreoChocolate/MUREN"
"Jeeseung-Park/ViPLO" -> "zyong812/STIP"
"Jeeseung-Park/ViPLO" -> "fredzzhang/pvic"
"Blueblue4/IoU-AwareCalibration" -> "Blueblue4/Object-Detection-Confidence-Bias"
"yangwangx/denseFlow_gpu" -> "LossNAN/I3D-Tensorflow"
"neu-vi/Diag-HOI" -> "neu-vi/FleVRS"
"bryanyzhu/deepOF" -> "bryanyzhu/GuidedNet"
"wangzheallen/vsad" -> "wanglimin/MRCNN-Scene-Recognition"
"MohsenFayyaz89/STFCN" -> "MohsenFayyaz89/FingerVein-SelfTaughtLearning"
"BonnBytes/PyTorch-FWD" -> "FedeSpu/HVQ"
"Jiang-Yidi/TS-TalkNet" -> "Junhua-Liao/Light-ASD"
"Jiang-Yidi/TS-TalkNet" -> "X-LANCE/MSDWILD"
"Jiang-Yidi/TS-TalkNet" -> "TaoRuijie/AVCleanse"
"Jiang-Yidi/TS-TalkNet" -> "TaoRuijie/MFV-KSD"
"Jiang-Yidi/TS-TalkNet" -> "zcxu-eric/AVA-AVD"
"IntelLabs/GraVi-T" -> "SRA2/SPELL"
"Jiang-Yidi/FlatTrajectoryDistillation_FTD" -> "xiaoxiaomiao323/MSA"
"colincsl/TemporalConvolutionalNetworks" -> "yabufarha/ms-tcn"
"colincsl/TemporalConvolutionalNetworks" -> "MCG-NJU/BCN"
"colincsl/TemporalConvolutionalNetworks" -> "ZheLi2020/TimestampActionSeg"
"colincsl/TemporalConvolutionalNetworks" -> "shyamal-b/sst"
"colincsl/TemporalConvolutionalNetworks" -> "yiskw713/asrf"
"colincsl/TemporalConvolutionalNetworks" -> "ahsaniqbal/Kinetics-FeatureExtractor"
"colincsl/TemporalConvolutionalNetworks" -> "ld-ing/tcfpn-isba"
"colincsl/TemporalConvolutionalNetworks" -> "zhengshou/scnn"
"colincsl/TemporalConvolutionalNetworks" -> "TaeSoo-Kim/TCNActionRecognition" ["e"=1]
"colincsl/TemporalConvolutionalNetworks" -> "ChinaYi/ASFormer"
"colincsl/TemporalConvolutionalNetworks" -> "JJBOY/BMN-Boundary-Matching-Network"
"colincsl/TemporalConvolutionalNetworks" -> "vdavid70619/TCN"
"colincsl/TemporalConvolutionalNetworks" -> "yjxiong/action-detection"
"colincsl/TemporalConvolutionalNetworks" -> "cmhungsteve/SSTDA"
"colincsl/TemporalConvolutionalNetworks" -> "nus-cvml/awesome-temporal-action-segmentation"
"gulvarol/ltc" -> "jrbtaylor/ActivityNet"
"gulvarol/ltc" -> "wanglimin/UntrimmedNet"
"axon-research/c3d-keras" -> "TianzhongSong/C3D-keras"
"axon-research/c3d-keras" -> "hx173149/C3D-tensorflow"
"axon-research/c3d-keras" -> "facebookarchive/C3D"
"axon-research/c3d-keras" -> "wushidonguc/two-stream-action-recognition-keras"
"axon-research/c3d-keras" -> "imatge-upc/activitynet-2016-cvprw"
"axon-research/c3d-keras" -> "dlpbc/keras-kinetics-i3d"
"axon-research/c3d-keras" -> "2012013382/C3D-Tensorflow-slim"
"axon-research/c3d-keras" -> "gudongfeng/C3D-tensorflow"
"axon-research/c3d-keras" -> "yjxiong/anet2016-cuhk"
"axon-research/c3d-keras" -> "chuckcho/video-caffe"
"axon-research/c3d-keras" -> "DavideA/c3d-pytorch"
"axon-research/c3d-keras" -> "kracwarlock/action-recognition-visual-attention"
"google/youtube-8m" -> "antoine77340/Youtube-8M-WILLOW"
"google/youtube-8m" -> "miha-skalic/youtube8mchallenge"
"google/youtube-8m" -> "antoine77340/LOUPE"
"google/youtube-8m" -> "wangheda/youtube-8m"
"google/youtube-8m" -> "linrongc/youtube-8m"
"google/youtube-8m" -> "facebookresearch/video-nonlocal-net"
"google/youtube-8m" -> "activitynet/ActivityNet"
"google/youtube-8m" -> "google-deepmind/kinetics-i3d"
"google/youtube-8m" -> "harvitronix/five-video-classification-methods"
"google/youtube-8m" -> "facebookarchive/C3D"
"google/youtube-8m" -> "yjxiong/temporal-segment-networks"
"google/youtube-8m" -> "yoosan/video-understanding-dataset"
"google/youtube-8m" -> "openimages/dataset" ["e"=1]
"google/youtube-8m" -> "mit-han-lab/temporal-shift-module"
"google/youtube-8m" -> "ethereon/caffe-tensorflow" ["e"=1]
"rh20624/Awesome-IMU-Sensing" -> "kingdomrush2/CrossHAR"
"rh20624/Awesome-IMU-Sensing" -> "OxWearables/capture24"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "STRCWearlab/DeepConvLSTM"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "guillaume-chevalier/LSTM-Human-Activity-Recognition"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "ani8897/Human-Activity-Recognition"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "haoranD/Awesome-Human-Activity-Recognition"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "Koutoulakis/Deep-Learning-for-Human-Activity-Recognition"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "servomac/Human-Activity-Recognition"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "takumiw/Deep-Learning-for-Human-Activity-Recognition"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "mmalekzadeh/motion-sense"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "deadskull7/Human-Activity-Recognition-with-Neural-Network-using-Gyroscopic-and-Accelerometer-variables"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "UdiBhaskar/Human-Activity-Recognition--Using-Deep-NN"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "arturjordao/WearableSensorData"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "ma-shamshiri/Human-Activity-Recognition"
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" -> "girishp92/Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones"
"OpenGVLab/video-mamba-suite" -> "sming256/OpenTAD"
"OpenGVLab/video-mamba-suite" -> "OpenGVLab/VideoMamba" ["e"=1]
"OpenGVLab/video-mamba-suite" -> "hotfinda/VideoMambaPro"
"OpenGVLab/video-mamba-suite" -> "happyharrycn/actionformer_release"
"OpenGVLab/video-mamba-suite" -> "OpenGVLab/VideoChat-Flash" ["e"=1]
"OpenGVLab/video-mamba-suite" -> "NVlabs/LITA" ["e"=1]
"OpenGVLab/video-mamba-suite" -> "sudo-Boris/mr-Blip" ["e"=1]
"sming256/OpenTAD" -> "OpenGVLab/video-mamba-suite"
"sming256/OpenTAD" -> "dingfengshi/TriDet"
"sming256/OpenTAD" -> "happyharrycn/actionformer_release"
"sming256/OpenTAD" -> "sming256/AdaTAD"
"sming256/OpenTAD" -> "zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation"
"sming256/OpenTAD" -> "sauradip/STALE"
"sming256/OpenTAD" -> "yingsen1/UniMD"
"sming256/OpenTAD" -> "sauradip/DiffusionTAD"
"sming256/OpenTAD" -> "NVlabs/LITA" ["e"=1]
"sming256/OpenTAD" -> "MCG-NJU/BasicTAD"
"sming256/OpenTAD" -> "OpenGVLab/VideoMAEv2"
"sming256/OpenTAD" -> "xlliu7/TadTR"
"sming256/OpenTAD" -> "benedettaliberatori/T3AL" ["e"=1]
"sming256/OpenTAD" -> "yangle15/DyFADet-pytorch"
"sming256/OpenTAD" -> "OpenGVLab/VideoChat-Flash" ["e"=1]
"MengyuanChen21/ICLR2024-REDL" -> "MengyuanChen21/CVPR2023-CMPAE"
"MengyuanChen21/ICLR2024-REDL" -> "MengyuanChen21/Awesome-Visual-Dialog"
"MengyuanChen21/ICLR2024-REDL" -> "MengyuanChen21/ECCV2022-DELU"
"MengyuanChen21/ICLR2024-REDL" -> "MengyuanChen21/CVPR2023-OWTAL"
"MengyuanChen21/ICLR2024-REDL" -> "MengyuanChen21/CVPR2022-FTCL"
"ZijiaLewisLu/CVPR2024-FACT" -> "Yuhan-Shen/ProTAS"
"ZijiaLewisLu/CVPR2024-FACT" -> "Finspire13/DiffAct"
"ZijiaLewisLu/CVPR2024-FACT" -> "nus-cvml/awesome-temporal-action-segmentation"
"ZijiaLewisLu/CVPR2024-FACT" -> "ChinaYi/ASFormer"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "guillaume-chevalier/LSTM-Human-Activity-Recognition"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "STRCWearlab/DeepConvLSTM"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "jindongwang/Deep-learning-activity-recognition"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "jindongwang/activityrecognition"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "Shahnawax/HAR-CNN-Keras"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "ani8897/Human-Activity-Recognition"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "andreas-bulling/ActRecTut"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "yscacaca/DeepSense"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "wisdal/Deep-Learning-for-Sensor-based-Human-Activity-Recognition"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "jianboyang/CNNHAR"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "girishp92/Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "haoranD/Awesome-Human-Activity-Recognition"
"aqibsaeed/Human-Activity-Recognition-using-CNN" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"feichtenhofer/st-resnet" -> "wanglimin/UntrimmedNet"
"feichtenhofer/st-resnet" -> "wanglimin/ARTNet"
"feichtenhofer/st-resnet" -> "bryanyzhu/Hidden-Two-Stream"
"feichtenhofer/st-resnet" -> "feichtenhofer/twostreamfusion"
"feichtenhofer/st-resnet" -> "rohitgirdhar/ActionVLAD"
"feichtenhofer/st-resnet" -> "feichtenhofer/gpu_flow"
"feichtenhofer/st-resnet" -> "hbilen/dynamic-image-nets"
"feichtenhofer/st-resnet" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"feichtenhofer/st-resnet" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"jiuntian/interactdiffusion" -> "IDEA-Research/DiffHOI"
"jiuntian/interactdiffusion" -> "kamwoh/partcraft"
"tteepe/TrackTacular" -> "tteepe/EarlyBird"
"tteepe/TrackTacular" -> "cvlab-epfl/MVFlow"
"harvitronix/continuous-online-video-classification-blog" -> "harvitronix/five-video-classification-methods"
"harvitronix/continuous-online-video-classification-blog" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"harvitronix/continuous-online-video-classification-blog" -> "chen0040/keras-video-classifier"
"gurkirt/corrected-UCF101-Annots" -> "vkalogeiton/caffe"
"gurkirt/corrected-UCF101-Annots" -> "gurkirt/realtime-action-detection"
"gurkirt/corrected-UCF101-Annots" -> "imatge-upc/Action-Tubelet-Detection-in-AVA"
"sming256/AdaTAD" -> "dingfengshi/tridetplus"
"Visual-AI/FROSTER" -> "wengzejia1/Open-VCLIP"
"Visual-AI/FROSTER" -> "alibaba-mmai-research/DiST"
"escorciav/daps" -> "shyamal-b/sst"
"escorciav/daps" -> "escorciav/deep-action-proposals"
"escorciav/daps" -> "jiyanggao/TURN-TAP"
"escorciav/daps" -> "JaywongWang/SST-Tensorflow"
"escorciav/daps" -> "shyamal-b/ss-tad"
"escorciav/daps" -> "ranjaykrishna/SST"
"escorciav/daps" -> "jiyanggao/CBR"
"escorciav/daps" -> "cabaf/sparseprop"
"Ectsang/3D-CNN-Keras" -> "melmikaty/3D_CNN"
"whwu95/GPT4Vis" -> "whwu95/Text4Vis"
"whwu95/GPT4Vis" -> "whwu95/BIKE"
"MCG-NJU/AMD" -> "haonanwang0522/GTPT"
"Hope1337/YOWOv3" -> "yjh0410/YOWOv2"
"Hope1337/YOWOv3" -> "joslefaure/HIT_ava"
"tteepe/EarlyBird" -> "tteepe/TrackTacular"
"tteepe/EarlyBird" -> "Blueblue4/Object-Detection-Confidence-Bias"
"tteepe/EarlyBird" -> "Blueblue4/IoU-AwareCalibration"
"gsig/temporal-fields" -> "gsig/actions-for-actions"
"gsig/temporal-fields" -> "gsig/charades-algorithms"
"harvitronix/five-video-classification-methods" -> "sujiongming/UCF-101_video_classification"
"harvitronix/five-video-classification-methods" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"harvitronix/five-video-classification-methods" -> "harvitronix/continuous-online-video-classification-blog"
"harvitronix/five-video-classification-methods" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"harvitronix/five-video-classification-methods" -> "kenshohara/video-classification-3d-cnn-pytorch"
"harvitronix/five-video-classification-methods" -> "HHTseng/video-classification"
"harvitronix/five-video-classification-methods" -> "hx173149/C3D-tensorflow"
"harvitronix/five-video-classification-methods" -> "jeffreyyihuang/two-stream-action-recognition"
"harvitronix/five-video-classification-methods" -> "google-deepmind/kinetics-i3d"
"harvitronix/five-video-classification-methods" -> "facebookresearch/video-nonlocal-net"
"harvitronix/five-video-classification-methods" -> "jinwchoi/awesome-action-recognition"
"harvitronix/five-video-classification-methods" -> "imatge-upc/activitynet-2016-cvprw"
"harvitronix/five-video-classification-methods" -> "feichtenhofer/twostreamfusion"
"harvitronix/five-video-classification-methods" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"harvitronix/five-video-classification-methods" -> "bryanyzhu/two-stream-pytorch"
"JihongJu/keras-resnet3d" -> "TianzhongSong/3D-ConvNets-for-Action-Recognition"
"JihongJu/keras-resnet3d" -> "pantheon5100/3D-CNN-resnet-keras"
"JihongJu/keras-resnet3d" -> "dlpbc/keras-kinetics-i3d"
"JihongJu/keras-resnet3d" -> "dhuy228/augmented-volumetric-image-generator"
"FrancisArgnR/Time-series---deep-learning---state-of-the-art" -> "amirstar/Deep-Forecast"
"FrancisArgnR/Time-series---deep-learning---state-of-the-art" -> "pipidog/DeepTimeSeries"
"FrancisArgnR/Time-series---deep-learning---state-of-the-art" -> "NLeSC/mcfly"
"kcct-fujimotolab/3DCNN" -> "OValery16/Tutorial-about-3D-convolutional-network"
"kcct-fujimotolab/3DCNN" -> "okankop/Efficient-3DCNNs"
"kcct-fujimotolab/3DCNN" -> "JihongJu/keras-resnet3d"
"kcct-fujimotolab/3DCNN" -> "gudongfeng/3d-DenseNet"
"kcct-fujimotolab/3DCNN" -> "sujiongming/UCF-101_video_classification"
"kcct-fujimotolab/3DCNN" -> "jibikbam/CNN-3D-images-Tensorflow" ["e"=1]
"kcct-fujimotolab/3DCNN" -> "feichtenhofer/twostreamfusion"
"kcct-fujimotolab/3DCNN" -> "kenshohara/video-classification-3d-cnn-pytorch"
"kcct-fujimotolab/3DCNN" -> "dipakkr/3d-cnn-action-recognition"
"kcct-fujimotolab/3DCNN" -> "bityangke/3DCNN"
"kcct-fujimotolab/3DCNN" -> "Ectsang/3D-CNN-Keras"
"kcct-fujimotolab/3DCNN" -> "axon-research/c3d-keras"
"kcct-fujimotolab/3DCNN" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"kcct-fujimotolab/3DCNN" -> "paolo626/3DCNN-with-keras"
"kcct-fujimotolab/3DCNN" -> "qijiezhao/pseudo-3d-pytorch"
"haonanwang0522/GTPT" -> "MCG-NJU/AMD"
"haonanwang0522/GTPT" -> "haonanwang0522/SRPose"
"yjxiong/action-detection" -> "wzmsltw/BSN-boundary-sensitive-network"
"yjxiong/action-detection" -> "VisionLearningGroup/R-C3D"
"yjxiong/action-detection" -> "yjxiong/temporal-segment-networks"
"yjxiong/action-detection" -> "activitynet/ActivityNet"
"yjxiong/action-detection" -> "Alvin-Zeng/PGCN"
"yjxiong/action-detection" -> "Rheelt/Materials-Temporal-Action-Detection"
"yjxiong/action-detection" -> "yjxiong/tsn-pytorch"
"yjxiong/action-detection" -> "JJBOY/BMN-Boundary-Matching-Network"
"yjxiong/action-detection" -> "yjxiong/anet2016-cuhk"
"yjxiong/action-detection" -> "gurkirt/realtime-action-detection"
"yjxiong/action-detection" -> "wanglimin/UntrimmedNet"
"yjxiong/action-detection" -> "wzmsltw/BSN-boundary-sensitive-network.pytorch"
"yjxiong/action-detection" -> "zhengshou/scnn"
"yjxiong/action-detection" -> "sujoyp/wtalc-pytorch"
"yjxiong/action-detection" -> "sunnyxiaohu/R-C3D.pytorch"
"MCG-NJU/AWT" -> "MCG-NJU/ProVP"
"MCG-NJU/AWT" -> "MCG-NJU/AMD"
"MCG-NJU/AWT" -> "haonanwang0522/GTPT"
"MCG-NJU/AWT" -> "MCG-NJU/MoG-VFI"
"wanglimin/UntrimmedNet" -> "sujoyp/wtalc-pytorch"
"wanglimin/UntrimmedNet" -> "zhengshou/AutoLoc"
"wanglimin/UntrimmedNet" -> "bellos1203/STPN"
"wanglimin/UntrimmedNet" -> "wanglimin/ARTNet"
"wanglimin/UntrimmedNet" -> "yjxiong/anet2016-cuhk"
"wanglimin/UntrimmedNet" -> "Finspire13/CMCS-Temporal-Action-Localization"
"wanglimin/UntrimmedNet" -> "zhengshou/scnn"
"wanglimin/UntrimmedNet" -> "yjxiong/action-detection"
"wanglimin/UntrimmedNet" -> "ColumbiaDVMM/CDC"
"wanglimin/UntrimmedNet" -> "escorciav/daps"
"wanglimin/UntrimmedNet" -> "demianzhang/weakly-action-localization"
"wanglimin/UntrimmedNet" -> "jiyanggao/CBR"
"wanglimin/UntrimmedNet" -> "Rheelt/Materials-Temporal-Action-Detection"
"wanglimin/UntrimmedNet" -> "VisionLearningGroup/R-C3D"
"wanglimin/UntrimmedNet" -> "shyamal-b/sst"
"tploetz/LSTMEnsemble4HAR" -> "dspanah/Sensor-Based-Human-Activity-Recognition-LSTMsEnsemble-Pytorch"
"ani8897/Human-Activity-Recognition" -> "servomac/Human-Activity-Recognition"
"ani8897/Human-Activity-Recognition" -> "deadskull7/Human-Activity-Recognition-with-Neural-Network-using-Gyroscopic-and-Accelerometer-variables"
"woodfrog/ActionRecognition" -> "qijiezhao/Video-Classification-Action-Recognition"
"woodfrog/ActionRecognition" -> "jeffreyyihuang/two-stream-action-recognition"
"woodfrog/ActionRecognition" -> "bryanyzhu/two-stream-pytorch"
"woodfrog/ActionRecognition" -> "feichtenhofer/twostreamfusion"
"woodfrog/ActionRecognition" -> "wushidonguc/two-stream-action-recognition-keras"
"woodfrog/ActionRecognition" -> "coderSkyChen/Action_Recognition_Zoo"
"woodfrog/ActionRecognition" -> "sujiongming/UCF-101_video_classification"
"woodfrog/ActionRecognition" -> "IDKiro/action-recognition"
"woodfrog/ActionRecognition" -> "eriklindernoren/Action-Recognition"
"woodfrog/ActionRecognition" -> "XiaoCode-er/Two-Stream-CNN" ["e"=1]
"woodfrog/ActionRecognition" -> "kracwarlock/action-recognition-visual-attention"
"woodfrog/ActionRecognition" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"woodfrog/ActionRecognition" -> "TianzhongSong/Real-Time-Action-Recognition" ["e"=1]
"woodfrog/ActionRecognition" -> "MRzzm/action-recognition-models-pytorch"
"woodfrog/ActionRecognition" -> "bryanyzhu/Hidden-Two-Stream"
"NLeSC/mcfly-tutorial" -> "NLeSC/mcfly"
"rohitgirdhar/ActionVLAD" -> "qijiezhao/Video-Classification-Action-Recognition"
"rohitgirdhar/ActionVLAD" -> "rohitgirdhar/AttentionalPoolingAction"
"rohitgirdhar/ActionVLAD" -> "wanglimin/ARTNet"
"rohitgirdhar/ActionVLAD" -> "bryanyzhu/Hidden-Two-Stream"
"rohitgirdhar/ActionVLAD" -> "gsig/actions-for-actions"
"rohitgirdhar/ActionVLAD" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"rohitgirdhar/ActionVLAD" -> "shyamal-b/sst"
"rohitgirdhar/ActionVLAD" -> "feichtenhofer/st-resnet"
"rohitgirdhar/ActionVLAD" -> "gsig/temporal-fields"
"rohitgirdhar/ActionVLAD" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"rohitgirdhar/ActionVLAD" -> "qijiezhao/py-denseflow"
"rohitgirdhar/ActionVLAD" -> "gsig/charades-algorithms"
"rohitgirdhar/ActionVLAD" -> "antoine77340/LOUPE"
"rohitgirdhar/ActionVLAD" -> "bryanyzhu/two-stream-pytorch"
"rohitgirdhar/ActionVLAD" -> "hx173149/C3D-tensorflow"
"wangheda/youtube-8m" -> "antoine77340/Youtube-8M-WILLOW"
"wangheda/youtube-8m" -> "baidu/Youtube-8M"
"wangheda/youtube-8m" -> "antoine77340/LOUPE"
"wangheda/youtube-8m" -> "miha-skalic/youtube8mchallenge"
"wangheda/youtube-8m" -> "forwchen/yt8m"
"gudongfeng/3d-DenseNet" -> "TianzhongSong/3D-ConvNets-for-Action-Recognition"
"gudongfeng/3d-DenseNet" -> "qijiezhao/Video-Classification-Action-Recognition"
"ColumbiaDVMM/CDC" -> "jiyanggao/CBR"
"ColumbiaDVMM/CDC" -> "naraysa/3c-net"
"ColumbiaDVMM/CDC" -> "zhengshou/scnn"
"lmb-freiburg/flownet2-docker" -> "feichtenhofer/gpu_flow"
"lmb-freiburg/flownet2-docker" -> "feichtenhofer/twostreamfusion"
"lmb-freiburg/flownet2-docker" -> "lmb-freiburg/flownet2" ["e"=1]
"lmb-freiburg/flownet2-docker" -> "jeffreyyihuang/two-stream-action-recognition"
"lmb-freiburg/flownet2-docker" -> "yjxiong/dense_flow"
"lmb-freiburg/flownet2-docker" -> "sampepose/flownet2-tf" ["e"=1]
"lmb-freiburg/flownet2-docker" -> "qijiezhao/py-denseflow"
"MCG-NJU/ProVP" -> "MCG-NJU/ZeroI2V"
"MengyuanChen21/Awesome-Evidential-Deep-Learning" -> "MengyuanChen21/NeurIPS2024-CSP"
"MengyuanChen21/Awesome-Evidential-Deep-Learning" -> "MengyuanChen21/ICLR2024-REDL"
"farewellthree/PPLLaVA" -> "farewellthree/STAN"
"MengyuanChen21/CVPR2023-OWTAL" -> "MengyuanChen21/CVPR2023-CMPAE"
"MengyuanChen21/CVPR2023-OWTAL" -> "MengyuanChen21/ECCV2022-DELU"
"MengyuanChen21/NeurIPS2024-CSP" -> "MengyuanChen21/Awesome-Evidential-Deep-Learning"
"MengyuanChen21/NeurIPS2024-CSP" -> "MengyuanChen21/CVPR2023-OWTAL"
"MengyuanChen21/NeurIPS2024-CSP" -> "MengyuanChen21/ICLR2024-REDL"
"gsig/charades-algorithms" -> "gsig/temporal-fields"
"gsig/charades-algorithms" -> "gsig/actions-for-actions"
"gsig/charades-algorithms" -> "piergiaj/super-events-cvpr18"
"gsig/charades-algorithms" -> "rohitgirdhar/ActionVLAD"
"gsig/charades-algorithms" -> "zhoubolei/moments_models"
"gsig/charades-algorithms" -> "noureldien/timeception"
"gsig/charades-algorithms" -> "wanglimin/ARTNet"
"ChelsieLei/EZ-HOI" -> "ltttpku/CMMP"
"ChelsieLei/EZ-HOI" -> "ltttpku/ADA-CM"
"pengxj/action-faster-rcnn" -> "gurkirt/realtime-action-detection"
"wadpac/GGIR" -> "OxWearables/biobankAccelerometerAnalysis"
"wadpac/GGIR" -> "OxWearables/actipy"
"wadpac/GGIR" -> "openmovementproject/openmovement"
"wadpac/GGIR" -> "ghammad/pyActigraphy"
"wadpac/GGIR" -> "THLfi/read.gt3x"
"ranjaykrishna/SST" -> "shyamal-b/sst"
"ranjaykrishna/SST" -> "shyamal-b/ss-tad"
"ranjaykrishna/SST" -> "escorciav/daps"
"ranjaykrishna/SST" -> "escorciav/deep-action-proposals"
"ranjaykrishna/SST" -> "JaywongWang/SST-Tensorflow"
"shyamal-b/sst" -> "ranjaykrishna/SST"
"shyamal-b/sst" -> "shyamal-b/ss-tad"
"shyamal-b/sst" -> "escorciav/daps"
"shyamal-b/sst" -> "JaywongWang/SST-Tensorflow"
"shyamal-b/sst" -> "vdavid70619/TCN"
"shyamal-b/sst" -> "jiyanggao/CBR"
"shyamal-b/sst" -> "jiyanggao/TURN-TAP"
"shyamal-b/sst" -> "cabaf/sparseprop"
"olga-zats/GTDA" -> "FedeSpu/HVQ"
"cvlab-epfl/social-scene-understanding" -> "ruiyan1995/Group-Activity-Recognition"
"cvlab-epfl/social-scene-understanding" -> "ruiyan1995/HiGCIN"
"cvlab-epfl/social-scene-understanding" -> "mostafa-saad/deep-activity-rec"
"cvlab-epfl/social-scene-understanding" -> "wjchaoGit/Group-Activity-Recognition"
"TaoRuijie/MFV-KSD" -> "xiaoxiaomiao323/MSA"
"Yuhan-Shen/ProTAS" -> "robert80203/EgoPER_official" ["e"=1]
"MCG-NJU/ZeroI2V" -> "MCG-NJU/ProVP"
"bryanyzhu/two-stream-pytorch" -> "jeffreyyihuang/two-stream-action-recognition"
"bryanyzhu/two-stream-pytorch" -> "feichtenhofer/twostreamfusion"
"bryanyzhu/two-stream-pytorch" -> "bryanyzhu/Hidden-Two-Stream"
"bryanyzhu/two-stream-pytorch" -> "yjxiong/tsn-pytorch"
"bryanyzhu/two-stream-pytorch" -> "yjxiong/temporal-segment-networks"
"bryanyzhu/two-stream-pytorch" -> "feichtenhofer/gpu_flow"
"bryanyzhu/two-stream-pytorch" -> "yjxiong/dense_flow"
"bryanyzhu/two-stream-pytorch" -> "tomar840/two-stream-fusion-for-action-recognition-in-videos"
"bryanyzhu/two-stream-pytorch" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"bryanyzhu/two-stream-pytorch" -> "qijiezhao/pseudo-3d-pytorch"
"bryanyzhu/two-stream-pytorch" -> "jfzhang95/pytorch-video-recognition"
"bryanyzhu/two-stream-pytorch" -> "qijiezhao/Video-Classification-Action-Recognition"
"bryanyzhu/two-stream-pytorch" -> "google-deepmind/kinetics-i3d"
"bryanyzhu/two-stream-pytorch" -> "yjxiong/action-detection"
"bryanyzhu/two-stream-pytorch" -> "activitynet/ActivityNet"
"bryanyzhu/Hidden-Two-Stream" -> "bryanyzhu/two-stream-pytorch"
"bryanyzhu/Hidden-Two-Stream" -> "bryanyzhu/GuidedNet"
"bryanyzhu/Hidden-Two-Stream" -> "rohitgirdhar/ActionVLAD"
"bryanyzhu/Hidden-Two-Stream" -> "feichtenhofer/st-resnet"
"bryanyzhu/Hidden-Two-Stream" -> "wanglimin/ARTNet"
"bryanyzhu/Hidden-Two-Stream" -> "wanglimin/UntrimmedNet"
"bryanyzhu/Hidden-Two-Stream" -> "hbilen/dynamic-image-nets"
"bryanyzhu/Hidden-Two-Stream" -> "feichtenhofer/gpu_flow"
"bryanyzhu/Hidden-Two-Stream" -> "qijiezhao/Video-Classification-Action-Recognition"
"bryanyzhu/Hidden-Two-Stream" -> "feichtenhofer/twostreamfusion"
"bryanyzhu/Hidden-Two-Stream" -> "yjxiong/temporal-segment-networks"
"bryanyzhu/Hidden-Two-Stream" -> "gurkirt/corrected-UCF101-Annots"
"bryanyzhu/Hidden-Two-Stream" -> "gurkirt/realtime-action-detection"
"bryanyzhu/Hidden-Two-Stream" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"bryanyzhu/Hidden-Two-Stream" -> "bryanyzhu/awesome-action-recognition"
"gurkirt/realtime-action-detection" -> "gurkirt/corrected-UCF101-Annots"
"gurkirt/realtime-action-detection" -> "vkalogeiton/caffe"
"gurkirt/realtime-action-detection" -> "pengxj/action-faster-rcnn"
"gurkirt/realtime-action-detection" -> "yjxiong/action-detection"
"gurkirt/realtime-action-detection" -> "Feynman27/realtime-action-detection"
"gurkirt/realtime-action-detection" -> "NVlabs/STEP"
"gurkirt/realtime-action-detection" -> "MCG-NJU/MOC-Detector"
"gurkirt/realtime-action-detection" -> "VisionLearningGroup/R-C3D"
"gurkirt/realtime-action-detection" -> "jiaozizhao/Two-in-One-ActionDetection"
"gurkirt/realtime-action-detection" -> "oulutan/ACAM_Demo"
"gurkirt/realtime-action-detection" -> "shyamal-b/ss-tad"
"gurkirt/realtime-action-detection" -> "zhoubolei/TRN-pytorch"
"gurkirt/realtime-action-detection" -> "activitynet/ActivityNet"
"gurkirt/realtime-action-detection" -> "coderSkyChen/Action_Recognition_Zoo"
"gurkirt/realtime-action-detection" -> "wzmsltw/BSN-boundary-sensitive-network"
"antoine77340/LOUPE" -> "antoine77340/Youtube-8M-WILLOW"
"antoine77340/LOUPE" -> "wangheda/youtube-8m"
"antoine77340/LOUPE" -> "miha-skalic/youtube8mchallenge"
"antoine77340/LOUPE" -> "linrongc/youtube-8m"
"antoine77340/LOUPE" -> "baidu/Youtube-8M"
"antoine77340/LOUPE" -> "rohitgirdhar/ActionVLAD"
"antoine77340/LOUPE" -> "antoine77340/Mixture-of-Embedding-Experts" ["e"=1]
"antoine77340/LOUPE" -> "wanglimin/ARTNet"
"antoine77340/LOUPE" -> "Relja/netvlad" ["e"=1]
"antoine77340/LOUPE" -> "shamangary/LOUPE_Keras"
"antoine77340/LOUPE" -> "google/youtube-8m"
"antoine77340/LOUPE" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"google-deepmind/kinetics-i3d" -> "piergiaj/pytorch-i3d"
"google-deepmind/kinetics-i3d" -> "hassony2/kinetics_i3d_pytorch"
"google-deepmind/kinetics-i3d" -> "yjxiong/temporal-segment-networks"
"google-deepmind/kinetics-i3d" -> "activitynet/ActivityNet"
"google-deepmind/kinetics-i3d" -> "yjxiong/tsn-pytorch"
"google-deepmind/kinetics-i3d" -> "facebookresearch/VMZ"
"google-deepmind/kinetics-i3d" -> "facebookresearch/video-nonlocal-net"
"google-deepmind/kinetics-i3d" -> "facebookarchive/C3D"
"google-deepmind/kinetics-i3d" -> "jinwchoi/awesome-action-recognition"
"google-deepmind/kinetics-i3d" -> "open-mmlab/mmaction"
"google-deepmind/kinetics-i3d" -> "feichtenhofer/twostreamfusion"
"google-deepmind/kinetics-i3d" -> "jeffreyyihuang/two-stream-action-recognition"
"google-deepmind/kinetics-i3d" -> "kenshohara/3D-ResNets-PyTorch"
"google-deepmind/kinetics-i3d" -> "yjxiong/action-detection"
"google-deepmind/kinetics-i3d" -> "kenshohara/video-classification-3d-cnn-pytorch"
"antoine77340/Youtube-8M-WILLOW" -> "antoine77340/LOUPE"
"antoine77340/Youtube-8M-WILLOW" -> "wangheda/youtube-8m"
"antoine77340/Youtube-8M-WILLOW" -> "miha-skalic/youtube8mchallenge"
"antoine77340/Youtube-8M-WILLOW" -> "baidu/Youtube-8M"
"antoine77340/Youtube-8M-WILLOW" -> "linrongc/youtube-8m"
"antoine77340/Youtube-8M-WILLOW" -> "google/youtube-8m"
"antoine77340/Youtube-8M-WILLOW" -> "yoosan/video-understanding-dataset"
"antoine77340/Youtube-8M-WILLOW" -> "ZhaofanQiu/pseudo-3d-residual-networks"
"antoine77340/Youtube-8M-WILLOW" -> "zhoubolei/moments_models"
"antoine77340/Youtube-8M-WILLOW" -> "facebookresearch/video-nonlocal-net"
"antoine77340/Youtube-8M-WILLOW" -> "wanglimin/ARTNet"
"antoine77340/Youtube-8M-WILLOW" -> "rohitgirdhar/ActionVLAD"
"antoine77340/Youtube-8M-WILLOW" -> "forwchen/yt8m"
"antoine77340/Youtube-8M-WILLOW" -> "mzolfaghari/ECO-efficient-video-understanding"
"antoine77340/Youtube-8M-WILLOW" -> "wzmsltw/BSN-boundary-sensitive-network"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "gsig/PyVideoResearch"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "antoine77340/video_feature_extractor" ["e"=1]
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "HHTseng/video-classification"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "krantiparida/awesome-audio-visual" ["e"=1]
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "open-mmlab/mmaction"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "kenshohara/video-classification-3d-cnn-pytorch"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "Rheelt/Materials-Temporal-Action-Detection"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "danieljf24/awesome-video-text-retrieval" ["e"=1]
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "xiaobai1217/Awesome-Video-Datasets"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "jinwchoi/awesome-action-recognition"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "decisionforce/TPN"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "facebookresearch/pytorchvideo"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "Sense-X/X-Temporal" ["e"=1]
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "Alvin-Zeng/PGCN"
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" -> "facebookresearch/ClassyVision" ["e"=1]
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "harvitronix/five-video-classification-methods"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "sujiongming/UCF-101_video_classification"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "chen0040/keras-video-classifier"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "SBoyNumber1/LSTM-video-classification"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "HHTseng/video-classification"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "harvitronix/continuous-online-video-classification-blog"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "woodfrog/ActionRecognition"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "alxcnwy/Deep-Neural-Networks-for-Video-Classification"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "talhasaruhan/video-action-classification"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "vighneshvnkt/keras-deep-learning"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "kenshohara/video-classification-3d-cnn-pytorch"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "pranoyr/cnn-lstm"
"sagarvegad/Video-Classification-CNN-and-LSTM-" -> "hx173149/C3D-tensorflow"
"jeffreyyihuang/two-stream-action-recognition" -> "bryanyzhu/two-stream-pytorch"
"jeffreyyihuang/two-stream-action-recognition" -> "feichtenhofer/twostreamfusion"
"jeffreyyihuang/two-stream-action-recognition" -> "yjxiong/tsn-pytorch"
"jeffreyyihuang/two-stream-action-recognition" -> "yjxiong/temporal-segment-networks"
"jeffreyyihuang/two-stream-action-recognition" -> "feichtenhofer/gpu_flow"
"jeffreyyihuang/two-stream-action-recognition" -> "jinwchoi/awesome-action-recognition"
"jeffreyyihuang/two-stream-action-recognition" -> "jfzhang95/pytorch-video-recognition"
"jeffreyyihuang/two-stream-action-recognition" -> "google-deepmind/kinetics-i3d"
"jeffreyyihuang/two-stream-action-recognition" -> "qijiezhao/Video-Classification-Action-Recognition"
"jeffreyyihuang/two-stream-action-recognition" -> "activitynet/ActivityNet"
"jeffreyyihuang/two-stream-action-recognition" -> "facebookresearch/VMZ"
"jeffreyyihuang/two-stream-action-recognition" -> "wushidonguc/two-stream-action-recognition-keras"
"jeffreyyihuang/two-stream-action-recognition" -> "yjxiong/dense_flow"
"jeffreyyihuang/two-stream-action-recognition" -> "kenshohara/3D-ResNets-PyTorch"
"jeffreyyihuang/two-stream-action-recognition" -> "HHTseng/video-classification"
"yjxiong/tsn-pytorch" -> "yjxiong/temporal-segment-networks"
"yjxiong/tsn-pytorch" -> "open-mmlab/mmaction"
"yjxiong/tsn-pytorch" -> "jeffreyyihuang/two-stream-action-recognition"
"yjxiong/tsn-pytorch" -> "zhoubolei/TRN-pytorch"
"yjxiong/tsn-pytorch" -> "mit-han-lab/temporal-shift-module"
"yjxiong/tsn-pytorch" -> "bryanyzhu/two-stream-pytorch"
"yjxiong/tsn-pytorch" -> "activitynet/ActivityNet"
"yjxiong/tsn-pytorch" -> "yjxiong/action-detection"
"yjxiong/tsn-pytorch" -> "google-deepmind/kinetics-i3d"
"yjxiong/tsn-pytorch" -> "feichtenhofer/twostreamfusion"
"yjxiong/tsn-pytorch" -> "facebookresearch/VMZ"
"yjxiong/tsn-pytorch" -> "yjxiong/dense_flow"
"yjxiong/tsn-pytorch" -> "facebookresearch/video-nonlocal-net"
"yjxiong/tsn-pytorch" -> "piergiaj/pytorch-i3d"
"yjxiong/tsn-pytorch" -> "qijiezhao/pseudo-3d-pytorch"
"yscacaca/DeepSense" -> "yscacaca/HHAR-Data-Process"
"yscacaca/DeepSense" -> "STRCWearlab/DeepConvLSTM"
"yscacaca/DeepSense" -> "AdelaideAuto-IDLab/Attend-And-Discriminate"
"yscacaca/DeepSense" -> "arturjordao/WearableSensorData"
"yscacaca/DeepSense" -> "yscacaca/DeepIoT"
"amirstar/Deep-Forecast" -> "Wizaron/deep-forecast-pytorch"
"servomac/Human-Activity-Recognition" -> "ani8897/Human-Activity-Recognition"
"servomac/Human-Activity-Recognition" -> "bartkowiaktomasz/har-wisdm-lstm-rnns"
"curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" -> "aqibsaeed/Human-Activity-Recognition-using-CNN"
"curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" -> "ani8897/Human-Activity-Recognition"
"curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" -> "girishp92/Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones"
"curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" -> "guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs"
"curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" -> "wisdal/Deep-Learning-for-Sensor-based-Human-Activity-Recognition"
"curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" -> "deadskull7/Human-Activity-Recognition-with-Neural-Network-using-Gyroscopic-and-Accelerometer-variables"
"qijiezhao/py-denseflow" -> "yjxiong/dense_flow"
"qijiezhao/py-denseflow" -> "feichtenhofer/gpu_flow"
"qijiezhao/py-denseflow" -> "wizyoung/Optical-Flow-GPU-Docker"
"qijiezhao/py-denseflow" -> "qijiezhao/s3d.pytorch"
"qijiezhao/py-denseflow" -> "wanglimin/ARTNet"
"qijiezhao/py-denseflow" -> "USTC-Video-Understanding/I3D_Finetune"
"qijiezhao/py-denseflow" -> "qijiezhao/Video-Classification-Action-Recognition"
"qijiezhao/py-denseflow" -> "wanglimin/dense_flow"
"qijiezhao/py-denseflow" -> "yangwangx/denseFlow_gpu"
"qijiezhao/py-denseflow" -> "dlpbc/keras-kinetics-i3d"
"qijiezhao/py-denseflow" -> "rohitgirdhar/ActionVLAD"
"qijiezhao/py-denseflow" -> "feichtenhofer/twostreamfusion"
"qijiezhao/py-denseflow" -> "LossNAN/I3D-Tensorflow"
"qijiezhao/py-denseflow" -> "coderSkyChen/Action_Recognition_Zoo"
"qijiezhao/py-denseflow" -> "rimchang/kinetics-i3d-Pytorch"
"LDenninger/CamContextI2V" -> "FedeSpu/HVQ"
"STAIR-Lab-CIT/STAIR-actions" -> "gulvarol/ltc"
"baidu/Youtube-8M" -> "wangheda/youtube-8m"
"baidu/Youtube-8M" -> "antoine77340/Youtube-8M-WILLOW"
"baidu/Youtube-8M" -> "forwchen/yt8m"
"baidu/Youtube-8M" -> "miha-skalic/youtube8mchallenge"
"baidu/Youtube-8M" -> "antoine77340/LOUPE"
"shyamal-b/ss-tad" -> "shyamal-b/sst"
"shyamal-b/ss-tad" -> "ranjaykrishna/SST"
"shyamal-b/ss-tad" -> "escorciav/daps"
"shyamal-b/ss-tad" -> "vdavid70619/TCN"
"shyamal-b/ss-tad" -> "jiyanggao/CBR"
"shyamal-b/ss-tad" -> "JaywongWang/SST-Tensorflow"
"shyamal-b/ss-tad" -> "jiyanggao/TURN-TAP"
"shyamal-b/ss-tad" -> "cabaf/sparseprop"
"shyamal-b/ss-tad" -> "ColumbiaDVMM/CDC"
"shyamal-b/ss-tad" -> "VisionLearningGroup/R-C3D"
"shyamal-b/ss-tad" -> "arpane4c5/ActivityNet"
"shyamal-b/ss-tad" -> "HYPJUDY/Decouple-SSAD"
"jiyanggao/TURN-TAP" -> "jiyanggao/CBR"
"jiyanggao/TURN-TAP" -> "escorciav/daps"
"jiyanggao/TURN-TAP" -> "vdavid70619/TCN"
"jiyanggao/TURN-TAP" -> "jiyanggao/CTAP"
"jiyanggao/TURN-TAP" -> "shyamal-b/sst"
"jiyanggao/TURN-TAP" -> "shyamal-b/ss-tad"
"jiyanggao/TURN-TAP" -> "JaywongWang/SST-Tensorflow"
"jiyanggao/TURN-TAP" -> "syyeung/frameglimpses"
"jiyanggao/TURN-TAP" -> "zhengshou/scnn"
"DavideA/c3d-pytorch" -> "yyuanad/Pytorch_C3D_Feature_Extractor" ["e"=1]
"DavideA/c3d-pytorch" -> "jfzhang95/pytorch-video-recognition"
"DavideA/c3d-pytorch" -> "sunnyxiaohu/R-C3D.pytorch"
"DavideA/c3d-pytorch" -> "piergiaj/pytorch-i3d"
"DavideA/c3d-pytorch" -> "hx173149/C3D-tensorflow"
"DavideA/c3d-pytorch" -> "hassony2/kinetics_i3d_pytorch"
"DavideA/c3d-pytorch" -> "facebookarchive/C3D"
"DavideA/c3d-pytorch" -> "kenshohara/video-classification-3d-cnn-pytorch"
"DavideA/c3d-pytorch" -> "yjxiong/tsn-pytorch"
"DavideA/c3d-pytorch" -> "ekosman/AnomalyDetectionCVPR2018-Pytorch" ["e"=1]
"DavideA/c3d-pytorch" -> "bryanyzhu/two-stream-pytorch"
"DavideA/c3d-pytorch" -> "qijiezhao/pseudo-3d-pytorch"
"DavideA/c3d-pytorch" -> "jx-zhong-for-academic-purpose/GCN-Anomaly-Detection" ["e"=1]
"DavideA/c3d-pytorch" -> "google-deepmind/kinetics-i3d"
"DavideA/c3d-pytorch" -> "yjxiong/action-detection"
"TwentyBN/GulpIO" -> "willprice/GulpIO2"
"TwentyBN/GulpIO" -> "mitmul/pynvvl"
"qijiezhao/Video-Classification-Action-Recognition" -> "rohitgirdhar/ActionVLAD"
"qijiezhao/Video-Classification-Action-Recognition" -> "gudongfeng/3d-DenseNet"
"qijiezhao/Video-Classification-Action-Recognition" -> "jeffreyyihuang/two-stream-action-recognition"
"qijiezhao/Video-Classification-Action-Recognition" -> "qijiezhao/py-denseflow"
"qijiezhao/Video-Classification-Action-Recognition" -> "woodfrog/ActionRecognition"
"qijiezhao/Video-Classification-Action-Recognition" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"qijiezhao/Video-Classification-Action-Recognition" -> "sujiongming/UCF-101_video_classification"
"qijiezhao/Video-Classification-Action-Recognition" -> "bryanyzhu/two-stream-pytorch"
"qijiezhao/Video-Classification-Action-Recognition" -> "coderSkyChen/Action_Recognition_Zoo"
"qijiezhao/Video-Classification-Action-Recognition" -> "TaeSoo-Kim/TCNActionRecognition" ["e"=1]
"qijiezhao/Video-Classification-Action-Recognition" -> "chihyaoma/Activity-Recognition-with-CNN-and-RNN"
"qijiezhao/Video-Classification-Action-Recognition" -> "kracwarlock/action-recognition-visual-attention"
"qijiezhao/Video-Classification-Action-Recognition" -> "wanglimin/ARTNet"
"qijiezhao/Video-Classification-Action-Recognition" -> "gsig/temporal-fields"
"qijiezhao/Video-Classification-Action-Recognition" -> "FingerRec/real_time_video_action_recognition"
"sujiongming/UCF-101_video_classification" -> "wadhwasahil/Video-Classification-2-Stream-CNN"
"sujiongming/UCF-101_video_classification" -> "harvitronix/five-video-classification-methods"
"sujiongming/UCF-101_video_classification" -> "qijiezhao/Video-Classification-Action-Recognition"
"sujiongming/UCF-101_video_classification" -> "sagarvegad/Video-Classification-CNN-and-LSTM-"
"sujiongming/UCF-101_video_classification" -> "woodfrog/ActionRecognition"
"sujiongming/UCF-101_video_classification" -> "hx173149/C3D-tensorflow"
"sujiongming/UCF-101_video_classification" -> "rohitgirdhar/ActionVLAD"
"sujiongming/UCF-101_video_classification" -> "jeffreyyihuang/two-stream-action-recognition"
"sujiongming/UCF-101_video_classification" -> "feichtenhofer/twostreamfusion"
"sujiongming/UCF-101_video_classification" -> "FingerRec/real_time_video_action_recognition"
"sujiongming/UCF-101_video_classification" -> "bryanyzhu/two-stream-pytorch"
"sujiongming/UCF-101_video_classification" -> "anenbergb/CS221_Project"
"sujiongming/UCF-101_video_classification" -> "zhuzhuxia1994/CK-TensorFlow"
"sujiongming/UCF-101_video_classification" -> "wushidonguc/two-stream-action-recognition-keras"
"sujiongming/UCF-101_video_classification" -> "jingweio/Action_Recognition_using_Visual_Attention"
"Koutoulakis/Deep-Learning-for-Human-Activity-Recognition" -> "nhammerla/deepHAR"
"gsig/actions-for-actions" -> "gsig/temporal-fields"
"gsig/actions-for-actions" -> "gsig/charades-algorithms"
"gsig/actions-for-actions" -> "jrbtaylor/ActivityNet"
"bryanyzhu/GuidedNet" -> "bryanyzhu/deepOF"
"bryanyzhu/GuidedNet" -> "bryanyzhu/awesome-action-recognition"
"FedeSpu/HVQ" -> "olga-zats/GTDA"
"FedeSpu/HVQ" -> "BonnBytes/PyTorch-FWD"
"FedeSpu/HVQ" -> "derkbreeze/LPT"
"Yorwxue/Two-Stream-Convolutional-Networks" -> "nsbb/SpatiotemporalNet"
"Yorwxue/Two-Stream-Convolutional-Networks" -> "blacknwhite5/pytorch-two-stream-CNN"
"neu-vi/FleVRS" -> "neu-vi/Diag-HOI"
"achalddave/predictive-corrective" -> "arpane4c5/ActivityNet"
"facebookresearch/video-nonlocal-net" ["l"="47.861,33.881"]
"AlexHex7/Non-local_pytorch" ["l"="53.479,31.065"]
"google-deepmind/kinetics-i3d" ["l"="47.901,33.899"]
"yjxiong/temporal-segment-networks" ["l"="47.882,33.925"]
"facebookresearch/VMZ" ["l"="47.854,33.9"]
"yjxiong/tsn-pytorch" ["l"="47.872,33.911"]
"mit-han-lab/temporal-shift-module" ["l"="47.901,33.854"]
"activitynet/ActivityNet" ["l"="47.862,33.92"]
"open-mmlab/mmaction" ["l"="47.884,33.873"]
"xvjiarui/GCNet" ["l"="53.475,31.045"]
"zhoubolei/TRN-pytorch" ["l"="47.834,33.906"]
"yjxiong/action-detection" ["l"="47.85,33.968"]
"msracver/Deformable-ConvNets" ["l"="50.906,30.047"]
"piergiaj/pytorch-i3d" ["l"="47.919,33.891"]
"jinwchoi/awesome-action-recognition" ["l"="47.956,33.887"]
"facebookarchive/C3D" ["l"="47.864,33.944"]
"decisionforce/TPN" ["l"="47.83,33.884"]
"facebookresearch/SlowFast" ["l"="47.949,33.829"]
"gsig/PyVideoResearch" ["l"="47.828,33.919"]
"open-mmlab/mmskeleton" ["l"="46.989,34.677"]
"open-mmlab/mmaction2" ["l"="47.969,33.787"]
"kenshohara/3D-ResNets-PyTorch" ["l"="47.936,33.865"]
"kenshohara/video-classification-3d-cnn-pytorch" ["l"="47.926,33.912"]
"jfzhang95/pytorch-video-recognition" ["l"="47.882,33.897"]
"jeffreyyihuang/two-stream-action-recognition" ["l"="47.916,33.934"]
"Cadene/pretrained-models.pytorch" ["l"="50.832,29.881"]
"NVIDIA/nvvl" ["l"="47.725,33.947"]
"dukebw/lintel" ["l"="47.748,33.948"]
"mitmul/pynvvl" ["l"="47.582,33.975"]
"ignacio-rocco/detectorch" ["l"="51.033,30.107"]
"chaoyuaw/pytorch-coviar" ["l"="47.791,33.918"]
"cvondrick/soundnet" ["l"="39.701,5.56"]
"jerryli27/TwinGAN" ["l"="-34.907,20.445"]
"NVIDIA/DALI" ["l"="50.737,29.833"]
"wzmsltw/BSN-boundary-sensitive-network" ["l"="47.858,33.992"]
"qijiezhao/pseudo-3d-pytorch" ["l"="47.852,33.931"]
"ZhaofanQiu/pseudo-3d-residual-networks" ["l"="47.831,33.952"]
"irhum/R2Plus1D-PyTorch" ["l"="47.811,33.906"]
"hassony2/kinetics_i3d_pytorch" ["l"="47.892,33.913"]
"hx173149/C3D-tensorflow" ["l"="47.909,33.959"]
"bryanyzhu/two-stream-pytorch" ["l"="47.901,33.947"]
"mzolfaghari/ECO-efficient-video-understanding" ["l"="47.814,33.934"]
"feichtenhofer/twostreamfusion" ["l"="47.884,33.951"]
"VisionLearningGroup/R-C3D" ["l"="47.831,34.014"]
"sunnyxiaohu/R-C3D.pytorch" ["l"="47.865,34.011"]
"shyamal-b/ss-tad" ["l"="47.825,34.047"]
"piergiaj/super-events-cvpr18" ["l"="47.85,34.046"]
"escorciav/daps" ["l"="47.813,34.059"]
"jiyanggao/CBR" ["l"="47.842,34.062"]
"zhengshou/scnn" ["l"="47.842,34.034"]
"wanglimin/UntrimmedNet" ["l"="47.842,34.021"]
"shyamal-b/sst" ["l"="47.813,34.046"]
"ranjaykrishna/SST" ["l"="47.794,34.049"]
"jiyanggao/TURN-TAP" ["l"="47.828,34.059"]
"ColumbiaDVMM/CDC" ["l"="47.838,34.052"]
"wzmsltw/BSN-boundary-sensitive-network.pytorch" ["l"="47.879,34.021"]
"TianzhongSong/C3D-keras" ["l"="47.977,33.98"]
"axon-research/c3d-keras" ["l"="47.925,33.984"]
"FingerRec/real_time_video_action_recognition" ["l"="47.956,33.98"]
"adamcasson/c3d" ["l"="48.045,33.984"]
"rekon/T3D-keras" ["l"="48.049,34.031"]
"2012013382/C3D-Tensorflow-slim" ["l"="47.969,33.991"]
"wushidonguc/two-stream-action-recognition-keras" ["l"="47.949,33.968"]
"dlpbc/keras-kinetics-i3d" ["l"="47.971,33.962"]
"OanaIgnat/I3D_Keras" ["l"="48.052,33.96"]
"USTC-Video-Understanding/I3D_Finetune" ["l"="47.933,33.954"]
"qijiezhao/py-denseflow" ["l"="47.894,33.964"]
"LossNAN/I3D-Tensorflow" ["l"="47.966,33.951"]
"JihongJu/keras-resnet3d" ["l"="48.035,33.962"]
"yoosan/i3d-tensorflow" ["l"="48,33.953"]
"Rhythmblue/i3d_finetune" ["l"="47.984,33.951"]
"FrederikSchorr/sign-language" ["l"="30.291,30.11"]
"iboing/CliqueNet" ["l"="47.644,34.32"]
"visinf/dpp" ["l"="47.691,34.23"]
"shaohua0116/Group-Normalization-Tensorflow" ["l"="47.614,34.375"]
"Betterthinking/CliqueNet-pytorch" ["l"="47.634,34.341"]
"taokong/group_normalization" ["l"="47.594,34.412"]
"danbochman/Real-Time-Action-Recognition" ["l"="48,33.999"]
"healthDataScience/deep-learning-HAR" ["l"="48.224,34.049"]
"guillaume-chevalier/LSTM-Human-Activity-Recognition" ["l"="48.152,34.039"]
"STRCWearlab/DeepConvLSTM" ["l"="48.207,34.102"]
"bhimmetoglu/time-series-medicine" ["l"="48.261,34.029"]
"RobRomijnders/LSTM_tsc" ["l"="-9.351,12.564"]
"guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs" ["l"="48.239,34.085"]
"bhimmetoglu/talks-and-lectures" ["l"="48.278,34.025"]
"aqibsaeed/Multilabel-timeseries-classification-with-LSTM" ["l"="-9.338,12.546"]
"aqibsaeed/Human-Activity-Recognition-using-CNN" ["l"="48.193,34.062"]
"hfawaz/bigdata18" ["l"="43.622,26.82"]
"loliverhennigh/Convolutional-LSTM-in-Tensorflow" ["l"="41.507,25.75"]
"curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" ["l"="48.224,34.072"]
"RobRomijnders/CNN_tsc" ["l"="-9.4,12.534"]
"Koutoulakis/Deep-Learning-for-Human-Activity-Recognition" ["l"="48.224,34.086"]
"titu1994/LSTM-FCN" ["l"="43.617,26.797"]
"zjrn/LSTM-CNN_CLASSIFICATION" ["l"="50.177,22.308"]
"MCG-NJU/TDN" ["l"="47.837,33.868"]
"wanglimin/ARTNet" ["l"="47.821,33.965"]
"rohitgirdhar/ActionVLAD" ["l"="47.834,33.971"]
"yjxiong/anet2016-cuhk" ["l"="47.82,34.013"]
"yoosan/video-understanding-dataset" ["l"="47.808,33.92"]
"zhoubolei/moments_models" ["l"="47.827,33.939"]
"antoine77340/Youtube-8M-WILLOW" ["l"="47.77,33.899"]
"feichtenhofer/gpu_flow" ["l"="47.867,33.959"]
"LisaAnne/LocalizingMoments" ["l"="48.08,33.013"]
"sujiongming/awesome-video-understanding" ["l"="47.684,33.871"]
"Alvin-Zeng/PGCN" ["l"="47.875,34"]
"linrongc/youtube-8m" ["l"="47.738,33.855"]
"miha-skalic/youtube8mchallenge" ["l"="47.743,33.868"]
"antoine77340/LOUPE" ["l"="47.753,33.891"]
"zhangyaoyuan/NextVLAD-Attention-Model" ["l"="47.684,33.804"]
"lyakaap/NetVLAD-pytorch" ["l"="59.353,9.439"]
"linrongc/solution_youtube8m_v3" ["l"="47.699,33.804"]
"zhongzhh8/Video-classification-with-knowledge-distillation" ["l"="47.704,33.821"]
"wangheda/youtube-8m" ["l"="47.725,33.868"]
"baidu/Youtube-8M" ["l"="47.725,33.882"]
"google/youtube-8m" ["l"="47.786,33.867"]
"dmlc/decord" ["l"="47.928,33.807"]
"facebookresearch/pytorchvideo" ["l"="47.977,33.813"]
"PyAV-Org/PyAV" ["l"="44.229,20.116"]
"MCG-NJU/VideoMAE" ["l"="48.045,33.812"]
"OpenGVLab/InternVideo" ["l"="47.549,30.045"]
"facebookresearch/TimeSformer" ["l"="48.006,33.813"]
"NVIDIA/VideoProcessingFramework" ["l"="53.274,32.622"]
"Breakthrough/PySceneDetect" ["l"="44.147,20.072"]
"OValery16/Tutorial-about-3D-convolutional-network" ["l"="48.042,33.917"]
"kcct-fujimotolab/3DCNN" ["l"="47.981,33.931"]
"ms3001/DeepHandGestureRecognition" ["l"="30.562,29.365"]
"saetlan/20BN-jester" ["l"="48.082,33.904"]
"udacity/CVND---Gesture-Recognition" ["l"="49.1,27.779"]
"sagarvegad/Video-Classification-CNN-and-LSTM-" ["l"="47.967,33.92"]
"okankop/Efficient-3DCNNs" ["l"="47.868,33.858"]
"shijianjian/EfficientNet-PyTorch-3D" ["l"="45.872,25.37"]
"wei-tim/YOWO" ["l"="47.752,33.915"]
"xmuyzz/3D-CNN-PyTorch" ["l"="47.86,33.797"]
"MECLabTUDA/M3d-Cam" ["l"="61.829,36.72"]
"okankop/vidaug" ["l"="47.81,33.837"]
"HHTseng/video-classification" ["l"="47.936,33.898"]
"jeffdonahue/caffe" ["l"="47.735,34.076"]
"LisaAnne/lisa-caffe-public" ["l"="47.793,34.019"]
"junhyukoh/caffe-lstm" ["l"="47.75,34.06"]
"vsubhashini/caffe" ["l"="48.105,32.817"]
"skaae/recurrent-spatial-transformer-code" ["l"="45.832,27.683"]
"Russell91/nlpcaffe" ["l"="47.715,34.1"]
"pranoyr/cnn-lstm" ["l"="47.898,33.828"]
"harvitronix/five-video-classification-methods" ["l"="47.94,33.937"]
"woodfrog/ActionRecognition" ["l"="47.903,33.922"]
"sujiongming/UCF-101_video_classification" ["l"="47.929,33.966"]
"eriklindernoren/Action-Recognition" ["l"="47.875,33.821"]
"moabitcoin/ig65m-pytorch" ["l"="47.902,32.901"]
"jayChung0302/videomix" ["l"="47.76,33.799"]
"hassony2/torch_videovision" ["l"="47.85,33.86"]
"holistic-video-understanding/HVU-Dataset" ["l"="47.709,33.753"]
"craston/MARS" ["l"="47.807,33.855"]
"okankop/MFF-pytorch" ["l"="30.538,29.358"]
"Atze00/MoViNet-pytorch" ["l"="47.894,33.761"]
"IBM/action-recognition-pytorch" ["l"="47.838,33.805"]
"shayanalibhatti/Video-Augmentation-Code" ["l"="47.776,33.801"]
"cvdfoundation/ava-dataset" ["l"="47.675,33.97"]
"NVlabs/STEP" ["l"="47.739,33.97"]
"SmartPorridge/google-AVA-Dataset-downloader" ["l"="47.631,33.947"]
"vkalogeiton/caffe" ["l"="47.743,34.005"]
"MVIG-SJTU/AlphAction" ["l"="47.681,33.914"]
"TaoRuijie/TalkNet-ASD" ["l"="47.49,34.035"]
"fuankarion/active-speakers-context" ["l"="47.507,34.048"]
"kevinlin311tw/ava-dataset-tool" ["l"="47.662,34.014"]
"Siyu-C/ACAR-Net" ["l"="47.74,33.928"]
"gurkirt/realtime-action-detection" ["l"="47.772,33.977"]
"oulutan/ACAM_Demo" ["l"="47.699,33.989"]
"cvlab-columbia/oops" ["l"="47.609,33.989"]
"r1c7/SlowFastNetworks" ["l"="47.793,33.899"]
"gurkirt/corrected-UCF101-Annots" ["l"="47.715,33.978"]
"oulutan/ActorConditionedAttentionMaps" ["l"="47.655,34.001"]
"KevinDuarte/VideoCapsuleNet" ["l"="47.677,33.991"]
"tomar840/two-stream-fusion-for-action-recognition-in-videos" ["l"="47.952,33.954"]
"mohammed-elkomy/two-stream-action-recognition" ["l"="47.987,33.965"]
"denizzagli/Spatial-Temporal-CNN" ["l"="48.006,33.964"]
"DavideA/c3d-pytorch" ["l"="47.896,33.935"]
"Alvin-Zeng/Awesome-Temporal-Action-Localization" ["l"="47.939,33.994"]
"tomrunia/PyTorchConv3D" ["l"="47.969,33.906"]
"v-iashin/video_features" ["l"="47.966,32.886"]
"Finspire13/pytorch-i3d-feature-extraction" ["l"="47.95,34.009"]
"girishp92/Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones" ["l"="48.246,34.06"]
"lingjuanlv/LSTM-CNN-model-for-HAR" ["l"="48.281,34.044"]
"mmalekzadeh/motion-sense" ["l"="48.273,34.123"]
"mmalekzadeh/dana" ["l"="48.288,34.144"]
"arturjordao/WearableSensorData" ["l"="48.244,34.116"]
"ani8897/Human-Activity-Recognition" ["l"="48.261,34.089"]
"haoranD/Awesome-Human-Activity-Recognition" ["l"="48.313,34.12"]
"takumiw/Deep-Learning-for-Human-Activity-Recognition" ["l"="48.262,34.107"]
"iantangc/ContrastiveLearningHAR" ["l"="48.339,34.167"]
"iantangc/SelfHAR" ["l"="48.313,34.153"]
"ma-shamshiri/Human-Activity-Recognition" ["l"="48.289,34.1"]
"dapowan/LIMU-BERT-Public" ["l"="57.336,10.986"]
"windofshadow/THAT" ["l"="64.107,35.613"]
"dspanah/Sensor-Based-Human-Activity-Recognition-DeepConvLSTM-Pytorch" ["l"="48.248,34.144"]
"sztyler/sensordatacollector" ["l"="48.295,34.161"]
"ni79ls/har-keras-cnn" ["l"="48.367,34.075"]
"markub3327/HAR-Transformer" ["l"="48.347,34.105"]
"meitetsu3/1DCNN" ["l"="48.412,34.073"]
"mzolfaghari/ECO-pytorch" ["l"="47.821,33.899"]
"facebookresearch/video-long-term-feature-banks" ["l"="47.78,33.94"]
"gsig/charades-algorithms" ["l"="47.791,33.994"]
"rohitgirdhar/AttentionalPoolingAction" ["l"="47.794,33.977"]
"yuxumin/CoRe" ["l"="47.944,30.225"]
"hassony2/inflated_convnets_pytorch" ["l"="47.909,33.912"]
"xujinglin/FineDiving" ["l"="47.96,30.225"]
"baiyang4/aqa_tpt" ["l"="47.95,30.218"]
"rimchang/kinetics-i3d-Pytorch" ["l"="47.945,33.917"]
"nzl-thu/MUSDL" ["l"="47.978,30.238"]
"coderSkyChen/Action_Recognition_Zoo" ["l"="47.847,33.944"]
"vt-vl-lab/SDN" ["l"="47.793,33.789"]
"piergiaj/representation-flow-cvpr19" ["l"="47.797,33.93"]
"facebookresearch/dmc-net" ["l"="47.705,33.893"]
"cypw/PyTorch-MFNet" ["l"="47.803,33.947"]
"leftthomas/R2Plus1D-C3D" ["l"="47.762,33.874"]
"alexandonian/pretorched-x" ["l"="47.791,33.952"]
"MohsenFayyaz89/T3D" ["l"="47.738,33.904"]
"Guocode/SlowFast-Networks" ["l"="47.722,33.919"]
"ZhaofanQiu/local-and-global-diffusion-networks" ["l"="47.722,33.965"]
"JJBOY/SlowFast-Network" ["l"="47.722,33.898"]
"zhang-can/ECO-pytorch" ["l"="47.779,33.926"]
"Rheelt/Materials-Temporal-Action-Detection" ["l"="47.878,34.009"]
"sujoyp/wtalc-pytorch" ["l"="47.899,34.036"]
"JJBOY/BMN-Boundary-Matching-Network" ["l"="47.896,34.013"]
"Finspire13/CMCS-Temporal-Action-Localization" ["l"="47.912,34.038"]
"Tencent/ActionDetection-DBG" ["l"="47.891,34"]
"frostinassiky/gtad" ["l"="47.926,34.024"]
"JiaHeeeee/Deep_Learning_Temporal_Action_Detection" ["l"="47.872,34.065"]
"Pilhyeon/BaSNet-pytorch" ["l"="47.928,34.044"]
"piergiaj/tgm-icml19" ["l"="47.869,34.051"]
"zhengshou/AutoLoc" ["l"="47.885,34.061"]
"bellos1203/STPN" ["l"="47.896,34.052"]
"JunLi-Galios/CDFL" ["l"="47.834,34.134"]
"demianzhang/weakly-action-localization" ["l"="47.864,34.073"]
"jiyanggao/CTAP" ["l"="47.86,34.056"]
"deadskull7/Human-Activity-Recognition-with-Neural-Network-using-Gyroscopic-and-Accelerometer-variables" ["l"="48.265,34.074"]
"UdiBhaskar/Human-Activity-Recognition--Using-Deep-NN" ["l"="48.271,34.06"]
"hangzhaomit/HACS-dataset" ["l"="47.869,34.033"]
"naraysa/3c-net" ["l"="47.902,34.061"]
"Wizaron/deep-forecast-pytorch" ["l"="48.41,34.376"]
"fengjiqiang/LSTM-Wind-Speed-Forecasting" ["l"="48.421,34.402"]
"amirstar/Deep-Forecast" ["l"="48.388,34.35"]
"willfleury/wind-forecasting" ["l"="48.435,34.385"]
"cigroup-ol/windml" ["l"="-11.253,17.588"]
"xuzheyuan624/slowfast-keras" ["l"="47.682,33.898"]
"alainray/ava_downloader" ["l"="47.631,34.028"]
"xiaolonw/TimeCycle" ["l"="47.9,34.57"]
"noureldien/timeception" ["l"="47.779,33.987"]
"TengdaHan/DPC" ["l"="47.904,34.526"]
"MCG-NJU/MOC-Detector" ["l"="47.762,33.94"]
"IDKiro/action-recognition" ["l"="47.875,33.785"]
"AKASH2907/deepfakes_video_classification" ["l"="47.881,33.745"]
"luoye2333/ResNetLSTM" ["l"="47.913,33.749"]
"siqinli/GestureRecognition-PyTorch" ["l"="47.871,33.765"]
"Yidadaa/Pytorch-Video-Classification" ["l"="47.896,33.728"]
"slaysd/pytorch-sentiment-analysis-classification" ["l"="53.245,28.724"]
"junyongyou/Attention-boosted-deep-networks-for-video-classification" ["l"="47.892,33.775"]
"doronharitan/human_activity_recognition_LRCN" ["l"="47.718,33.846"]
"flrngel/TCN-with-attention" ["l"="47.94,34.304"]
"haohy/TCAN" ["l"="47.952,34.327"]
"tranducquy/tcn-fx-price-prediction" ["l"="47.962,34.353"]
"lpphd/multivariate-attention-tcn" ["l"="47.929,34.352"]
"sagelywizard/snail" ["l"="57.606,19.419"]
"815382636/GCN-tffc" ["l"="52.278,17.377"]
"oneday88/deepTCN" ["l"="45.027,24.229"]
"sj-li/MS-TCN2" ["l"="47.913,34.171"]
"MohsenFayyaz89/STFCN" ["l"="47.658,33.799"]
"DirtyHarryLYL/HAKE" ["l"="47.45,33.842"]
"DirtyHarryLYL/HAKE-Action" ["l"="47.48,33.845"]
"DirtyHarryLYL/Transferable-Interactiveness-Network" ["l"="47.433,33.835"]
"DirtyHarryLYL/HAKE-Action-Torch" ["l"="47.458,33.832"]
"YueLiao/PPDM" ["l"="47.395,33.818"]
"DirtyHarryLYL/DJ-RN" ["l"="47.468,33.822"]
"DirtyHarryLYL/HOI-Learning-List" ["l"="47.396,33.834"]
"BigRedT/no_frills_hoi_det" ["l"="47.431,33.818"]
"bobwan1995/PMFNet" ["l"="47.42,33.824"]
"DirtyHarryLYL/SymNet" ["l"="47.465,33.85"]
"vt-vl-lab/iCAN" ["l"="47.443,33.827"]
"s-gupta/v-coco" ["l"="47.41,33.819"]
"chinancheng/awesome-human-object-interaction" ["l"="47.433,33.806"]
"fredzzhang/spatially-conditioned-graphs" ["l"="47.399,33.811"]
"ASMIftekhar/VSGNet" ["l"="47.417,33.802"]
"Dong-JinKim/ActionCooccurrencePriors" ["l"="47.444,33.807"]
"Shahnawax/HAR-CNN-Keras" ["l"="48.289,34.062"]
"servomac/Human-Activity-Recognition" ["l"="48.296,34.076"]
"LiangZai-Embedded/HAR-ON-STM32F401C" ["l"="48.366,34.048"]
"STMicroelectronics/stm32ai-wiki" ["l"="48.323,34.048"]
"s9xie/Mini-Kinetics-200" ["l"="47.874,33.982"]
"chensun11/dtfv" ["l"="47.977,34.187"]
"bo-yang/dtf_fisher" ["l"="47.986,34.216"]
"HuNiuC/iDT-FV-for-action-recogniton" ["l"="47.998,34.209"]
"anenbergb/CS221_Project" ["l"="47.946,34.132"]
"jchiang2/Human-Activity-Recognition" ["l"="48.242,34.202"]
"sidharthgurbani/HAR-using-PyTorch" ["l"="48.248,34.225"]
"isukrit/encodingHumanActivity" ["l"="48.238,34.154"]
"qijiezhao/s3d.pytorch" ["l"="47.816,33.95"]
"LijieFan/tvnet" ["l"="47.806,33.987"]
"Tushar-N/pytorch-resnet3d" ["l"="53.095,14.623"]
"kkanshul/Hide-and-Seek" ["l"="47.905,34.112"]
"StrangerZhang/pyECO" ["l"="54.691,33.697"]
"Showmax/kinetics-downloader" ["l"="47.813,33.874"]
"piaxar/kinetics-downloader" ["l"="47.76,33.832"]
"arunos728/MotionSqueeze" ["l"="47.835,33.82"]
"dancelogue/kinetics-datasets-downloader" ["l"="47.774,33.837"]
"rocksyne/kinetics-dataset-downloader" ["l"="47.769,33.821"]
"MohsenFayyaz89/PyTorch_Video_Dataset" ["l"="47.626,33.756"]
"MohsenFayyaz89/SCT" ["l"="47.66,33.74"]
"feiyunzhang/i3d-non-local-pytorch" ["l"="54.296,31.511"]
"SiyuanQi-zz/gpnn" ["l"="47.42,33.85"]
"ywchao/ho-rcnn" ["l"="47.416,33.838"]
"vaesl/IP-Net" ["l"="47.419,33.812"]
"HYPJUDY/Decouple-SSAD" ["l"="47.878,34.042"]
"Rheelt/SSAD_pytorch" ["l"="47.888,34.087"]
"wjchaoGit/Group-Activity-Recognition" ["l"="47.619,33.875"]
"mostafa-saad/deep-activity-rec" ["l"="47.599,33.886"]
"huguyuehuhu/Awesome-Group-Activity-Recognition" ["l"="47.603,33.858"]
"mostafa-saad/hierarchical-relational-network" ["l"="47.582,33.882"]
"ruiyan1995/Group-Activity-Recognition" ["l"="47.58,33.871"]
"cvlab-epfl/social-scene-understanding" ["l"="47.596,33.871"]
"JacobYuan7/DIN-Group-Activity-Recognition-Benchmark" ["l"="47.573,33.859"]
"ruiyan1995/HiGCIN" ["l"="47.589,33.862"]
"mahsaep/Social-human-activity-understanding-and-grouping" ["l"="47.606,33.842"]
"xueyee/GroupFormer" ["l"="47.586,33.852"]
"hongluzhou/composer" ["l"="47.575,33.842"]
"cmhungsteve/TA3N" ["l"="47.833,34.088"]
"cmhungsteve/SSTDA" ["l"="47.855,34.137"]
"jonmun/MM-SADA-code" ["l"="47.483,32.881"]
"deepcs233/TIN" ["l"="47.824,33.846"]
"xuyu0010/awesome-video-domain-adaptation" ["l"="47.789,34.164"]
"junyuGao/Zero-Shot-Action-Recognition-with-Two-Stream-GCN" ["l"="58.036,19.304"]
"jindongwang/Deep-learning-activity-recognition" ["l"="48.178,34.078"]
"jindongwang/activityrecognition" ["l"="48.044,34.001"]
"andreas-bulling/ActRecTut" ["l"="48.2,34.046"]
"chihyaoma/Activity-Recognition-with-CNN-and-RNN" ["l"="47.913,33.999"]
"Gasoonjia/tvnet_pytorch" ["l"="47.752,33.98"]
"ahsaniqbal/Kinetics-FeatureExtractor" ["l"="47.882,34.162"]
"ChinaYi/asrf_with_asformer" ["l"="47.901,34.163"]
"yiskw713/asrf" ["l"="47.892,34.153"]
"yabufarha/ms-tcn" ["l"="47.891,34.133"]
"ChinaYi/ASFormer" ["l"="47.911,34.157"]
"MCG-NJU/BCN" ["l"="47.86,34.16"]
"nus-cvml/awesome-temporal-action-segmentation" ["l"="47.928,34.156"]
"colincsl/TemporalConvolutionalNetworks" ["l"="47.871,34.112"]
"ttlmh/Bridge-Prompt" ["l"="47.942,34.118"]
"ZheLi2020/TimestampActionSeg" ["l"="47.872,34.148"]
"alexanderrichard/action-sets" ["l"="47.83,34.122"]
"yiskw713/video_feature_extractor" ["l"="47.912,34.147"]
"asrafulashiq/wsad" ["l"="47.909,34.067"]
"bfshi/DGAM-Weakly-Supervised-Action-Localization" ["l"="47.922,34.054"]
"Pilhyeon/WTAL-Uncertainty-Modeling" ["l"="47.943,34.067"]
"MichiganCOG/A2CL-PT" ["l"="47.928,34.067"]
"PeisenZhao/Bottom-Up-TAL-with-MR" ["l"="47.949,34.056"]
"Keiku/Action-Recognition-CNN-LSTM" ["l"="47.852,33.767"]
"peachman05/action-recognition-tutorial" ["l"="47.852,33.746"]
"swathikirans/GSM" ["l"="47.825,33.829"]
"yaohungt/Gated-Spatio-Temporal-Energy-Graph" ["l"="47.556,32.147"]
"jiaozizhao/Two-in-One-ActionDetection" ["l"="47.719,33.991"]
"qinzhi-0110/pytorch-act-detector" ["l"="47.733,33.987"]
"ghammad/pyActigraphy" ["l"="48.519,34.213"]
"wadpac/GGIR" ["l"="48.506,34.227"]
"elyiorgos/sleeppy" ["l"="62.39,34.341"]
"OxWearables/actipy" ["l"="48.499,34.214"]
"OxWearables/biobankAccelerometerAnalysis" ["l"="48.486,34.201"]
"cbrnr/sleepecg" ["l"="62.289,34.366"]
"MRzzm/action-recognition-models-pytorch" ["l"="47.85,33.912"]
"V-Sense/ACTION-Net" ["l"="47.854,33.828"]
"Phoenix1327/tea-action-recognition" ["l"="47.834,33.84"]
"qijiezhao/Video-Classification-Action-Recognition" ["l"="47.885,33.971"]
"happyharrycn/actionformer_release" ["l"="48.024,33.979"]
"GowthamGottimukkala/I3D_Feature_Extraction_resnet" ["l"="53.081,14.613"]
"dingfengshi/TriDet" ["l"="48.018,34.014"]
"VividLe/A2Net" ["l"="47.962,34.056"]
"xlliu7/TadTR" ["l"="48.014,34.032"]
"HumamAlwassel/TSP" ["l"="47.979,34.033"]
"TencentYoutuResearch/ActionDetection-AFSD" ["l"="47.967,34.026"]
"sxzrt/Instructions-of-the-PersonX-dataset" ["l"="47.103,34.208"]
"sxzrt/Dissecting-Person-Re-ID-from-the-Viewpoint-of-Viewpoint" ["l"="47.139,34.193"]
"zhunzhong07/HHL" ["l"="55.99,32.763"]
"sxzrt/Proxy-Set" ["l"="47.059,34.216"]
"FlyHighest/UnrealPerson" ["l"="47.09,34.232"]
"vt-vl-lab/DRG" ["l"="47.407,33.804"]
"tfzhou/C-HOI" ["l"="47.407,33.827"]
"PingchuanMa/Temporal-Shift-Module" ["l"="47.644,33.979"]
"dspanah/Sensor-Based-Human-Activity-Recognition-LSTMsEnsemble-Pytorch" ["l"="48.255,34.171"]
"STRCWearlab/DeepConvLSTM_py3" ["l"="48.232,34.136"]
"AdelaideAuto-IDLab/Attend-And-Discriminate" ["l"="48.265,34.141"]
"feichtenhofer/st-resnet" ["l"="47.84,33.986"]
"bryanyzhu/Hidden-Two-Stream" ["l"="47.824,33.984"]
"laura-wang/video_repres_mas" ["l"="47.934,34.499"]
"kenshohara/3D-ResNets" ["l"="47.706,33.933"]
"Wuie/ST-NBNN-demo" ["l"="47.512,34.261"]
"kailiuXD/STWP" ["l"="47.526,34.245"]
"hbilen/dynamic-image-nets" ["l"="47.791,34.031"]
"Daisy-Zhang/Video-Classification-Pytorch" ["l"="47.898,33.69"]
"bartkowiaktomasz/har-wisdm-lstm-rnns" ["l"="48.349,34.046"]
"bartkowiaktomasz/har-wisdm-bidirectional-lstm-rnns" ["l"="48.364,34.028"]
"zhangzhao156/CNN-for-HAR-on-WISDM-Dataset" ["l"="48.38,34.033"]
"kracwarlock/action-recognition-visual-attention" ["l"="47.85,34.001"]
"gsig/temporal-fields" ["l"="47.782,34.005"]
"gsig/actions-for-actions" ["l"="47.773,34.021"]
"gudongfeng/3d-DenseNet" ["l"="47.906,33.985"]
"jingweio/Action_Recognition_using_Visual_Attention" ["l"="47.852,34.013"]
"hjjpku/Action_Detection_DQN" ["l"="47.811,34.116"]
"LeonHLJ/RSKP" ["l"="48.008,34.08"]
"kuangliu/pytorch-groupnorm" ["l"="47.585,34.431"]
"yangwangx/denseFlow_gpu" ["l"="47.939,33.974"]
"XiaoCode-er/Two-Stream-CNN" ["l"="47.065,34.602"]
"Yorwxue/Two-Stream-Convolutional-Networks" ["l"="47.943,34.081"]
"zhuzhuxia1994/CK-TensorFlow" ["l"="47.991,33.979"]
"chen0040/keras-video-classifier" ["l"="48.027,33.932"]
"nim94sha/keras-video-classification" ["l"="48.066,33.938"]
"zhang-can/PAN-PyTorch" ["l"="47.848,33.818"]
"kylemin/S3D" ["l"="47.901,34.793"]
"LossNAN/3D-Resnet-tensorflow" ["l"="48.025,33.95"]
"vdavid70619/TCN" ["l"="47.847,34.073"]
"JaywongWang/SST-Tensorflow" ["l"="47.824,34.067"]
"wizyoung/Optical-Flow-GPU-Docker" ["l"="47.956,33.994"]
"willprice/flowty" ["l"="47.997,34.018"]
"GuangmingZhu/Conv3D_CLSTM" ["l"="30.609,29.372"]
"leaderj1001/Action-Localization" ["l"="47.678,34.004"]
"ywchao/hico_benchmark" ["l"="47.4,33.865"]
"imatge-upc/Action-Tubelet-Detection-in-AVA" ["l"="47.715,33.999"]
"pengxj/action-faster-rcnn" ["l"="47.729,34.009"]
"THLfi/read.gt3x" ["l"="48.541,34.245"]
"paulhibbing/AGread" ["l"="48.563,34.256"]
"MTCloudVision/MTSVRC" ["l"="47.546,33.988"]
"sebgao/LIP" ["l"="47.716,34.161"]
"kevin-ssy/Optical-Flow-Guided-Feature" ["l"="47.773,33.962"]
"escorciav/deep-action-proposals" ["l"="47.794,34.078"]
"huajh/action_recognition" ["l"="47.583,34.188"]
"hueihan/Action_Recognition" ["l"="47.566,34.203"]
"jishnujayakumar/MV-Tractus" ["l"="47.596,34.1"]
"vadimkantorov/mpegflow" ["l"="47.623,34.105"]
"LukasBommes/mv-extractor" ["l"="47.616,34.074"]
"Prof-Lu-Cewu/Visual-Relationship-Detection" ["l"="47.531,32.003"]
"wadpac/oss-dev-webinar-series-pb-field" ["l"="48.578,34.264"]
"IDKiro/Xenoblade2Voice" ["l"="47.864,33.663"]
"IDKiro/ssmgr-install" ["l"="47.868,33.705"]
"zhujiagang/DTPP" ["l"="47.803,34.132"]
"saif-mahmud/self-attention-HAR" ["l"="48.29,34.121"]
"saif-mahmud/hierarchical-attention-HAR" ["l"="48.276,34.11"]
"HumamAlwassel/DETAD" ["l"="48.01,34.12"]
"xlliu7/MUSES" ["l"="47.997,34.061"]
"KevinDuarte/CapsuleVOS" ["l"="47.636,34.004"]
"salaniz/pytorch-gve-lrcn" ["l"="47.66,34.033"]
"JaggerYoung/LRCN-for-Activity-Recognition" ["l"="47.698,33.97"]
"garythung/torch-lrcn" ["l"="47.685,34.047"]
"alexanderrichard/NeuralNetwork-Viterbi" ["l"="47.824,34.156"]
"bpeck81/CNN_RNN_Human_Action_Recognition" ["l"="47.864,33.735"]
"yabufarha/anticipating-activities" ["l"="47.831,34.184"]
"ld-ing/tcfpn-isba" ["l"="47.838,34.154"]
"alexanderrichard/squirrel" ["l"="47.811,34.177"]
"bryanyzhu/awesome-action-recognition" ["l"="47.76,34.024"]
"bryanyzhu/GuidedNet" ["l"="47.742,34.032"]
"blacknwhite5/pytorch-two-stream-CNN" ["l"="47.959,34.11"]
"nsbb/SpatiotemporalNet" ["l"="47.966,34.123"]
"FingerRec/FingerveinRecognitionModel3" ["l"="47.613,33.727"]
"MohsenFayyaz89/FingerVein-SelfTaughtLearning" ["l"="47.638,33.746"]
"scwangdyd/zero_shot_hoi" ["l"="47.444,33.785"]
"YueLiao/PIC_HOIW" ["l"="47.404,33.847"]
"IBM/BigLittleNet" ["l"="47.74,33.721"]
"IBM/bLVNet-TAM" ["l"="47.762,33.747"]
"MichiganCOG/M-PACT" ["l"="47.74,33.667"]
"gurkirt/2D-kinectics" ["l"="47.762,33.712"]
"dazhang-cv/S3D" ["l"="47.857,34.1"]
"drbinliang/HMM-Action-Recognition" ["l"="47.549,34.223"]
"tploetz/LSTMEnsemble4HAR" ["l"="48.262,34.19"]
"holistic-video-understanding/Mini-HVU" ["l"="47.689,33.719"]
"yassersouri/fandak" ["l"="47.682,33.733"]
"bo-yang/stip_fisher" ["l"="47.996,34.242"]
"masoudpz/AVID-Adversarial-Visual-Irregularity-Detection" ["l"="47.627,33.721"]
"aimagelab/STAGE_action_detection" ["l"="47.704,33.957"]
"kennymckormick/pyskl" ["l"="46.945,34.648"]
"SwinTransformer/Video-Swin-Transformer" ["l"="48.024,33.802"]
"open-mmlab/mmpose" ["l"="31.794,28.089"]
"PaddlePaddle/PaddleVideo" ["l"="-54.543,-14.526"]
"yysijie/st-gcn" ["l"="46.97,34.655"]
"open-mmlab/mmpretrain" ["l"="50.465,29.896"]
"open-mmlab/mmcv" ["l"="50.53,29.877"]
"facebookresearch/detr" ["l"="50.658,29.739"]
"facebookresearch/moco" ["l"="53.011,29.578"]
"xingyizhou/CenterNet" ["l"="50.687,29.978"]
"kkahatapitiya/X3D-Multigrid" ["l"="47.808,33.687"]
"joaanna/something_else" ["l"="47.799,33.819"]
"liu-zhy/temporal-adaptive-module" ["l"="47.881,33.841"]
"StanfordVL/RubiksNet" ["l"="47.81,33.802"]
"Chuhanxx/Temporal_Query_Networks" ["l"="48.106,30.266"]
"mengyuest/AR-Net" ["l"="47.837,33.758"]
"mengyuest/AdaFuse" ["l"="47.825,33.728"]
"blackfeather-wang/AdaFocus" ["l"="49.279,32.899"]
"dophist/kaldi-lstm" ["l"="35.78,2.214"]
"yjxiong/caffe" ["l"="47.827,33.999"]
"tmbdev/clstm" ["l"="46.233,7.11"]
"purine/purine2" ["l"="57.737,23.498"]
"Russell91/apollocaffe" ["l"="45.814,29.612"]
"JonathanRaiman/theano_lstm" ["l"="44.858,27.636"]
"dmlc/cxxnet" ["l"="57.767,23.488"]
"xingwangsfu/caffe-yolo" ["l"="51.678,33.355"]
"daerduoCarey/SpatialTransformerLayer" ["l"="51.484,33.45"]
"christopher5106/last_caffe_with_stn" ["l"="51.453,33.464"]
"torrvision/crfasrnn" ["l"="53.499,30.75"]
"xiaolonw/caffe-video_triplet" ["l"="32.943,29.326"]
"alexandrosstergiou/SoftPool" ["l"="47.672,34.207"]
"MCG-NJU/FCOT" ["l"="54.556,33.691"]
"MCG-NJU/AdaMixer" ["l"="48.696,30.355"]
"MCG-NJU/CPD-Video" ["l"="47.757,33.993"]
"MCG-NJU/MMN" ["l"="48.058,33.063"]
"MCG-NJU/CRCNN-Action" ["l"="47.709,34.066"]
"hitachi-rd-cv/qpic" ["l"="47.375,33.807"]
"wanglimin/dense_flow" ["l"="47.86,33.98"]
"soeaver/caffe-model" ["l"="51.656,33.388"]
"wadhwasahil/Video-Classification-2-Stream-CNN" ["l"="47.891,33.981"]
"wanglimin/MRCNN-Scene-Recognition" ["l"="47.774,34.073"]
"yjxiong/dense_flow" ["l"="47.875,33.97"]
"sanghoon/pva-faster-rcnn" ["l"="51.664,33.429"]
"wanglimin/TDD" ["l"="47.81,34.029"]
"Flowerfan/SF-Net" ["l"="47.959,34.078"]
"zhenyingfang/Awesome-Temporal-Action-Detection-Temporal-Action-Proposal-Generation" ["l"="47.969,34.009"]
"Pilhyeon/Awesome-Weakly-Supervised-Temporal-Action-Localization" ["l"="47.979,34.055"]
"Alpha-Video/AlphaVideo" ["l"="47.56,33.893"]
"amazon-science/tubelet-transformer" ["l"="47.684,33.941"]
"joslefaure/HIT" ["l"="47.668,33.934"]
"ShoufaChen/WOO" ["l"="47.685,33.932"]
"MCG-NJU/MultiSports" ["l"="47.755,33.964"]
"yolish/har-with-imu-transformer" ["l"="48.373,34.109"]
"Chaolei98/Baseline-with-HAR-datasets" ["l"="48.346,34.13"]
"xushige/HAR-Dataset-Preprocess" ["l"="48.354,34.143"]
"crocodilegogogo/IF-ConvTransformer-UbiComp2022" ["l"="48.373,34.134"]
"OxWearables/ssl-wearables" ["l"="48.425,34.179"]
"getalp/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices" ["l"="48.357,34.116"]
"FIGLAB/IMUPoser" ["l"="30.47,28.861"]
"Hangwei12358/cross-person-HAR" ["l"="48.371,34.156"]
"yjh0410/YOWOv2" ["l"="47.643,33.924"]
"yjh0410/PyTorch_YOWO" ["l"="47.664,33.91"]
"boschresearch/UVAST" ["l"="47.942,34.157"]
"cotton-ahn/HASR_iccv2021" ["l"="47.872,34.18"]
"MCG-NJU/RTD-Action" ["l"="47.945,34.029"]
"chenxuluo/GST-video" ["l"="47.811,33.813"]
"xhl-video/SmallBigNet" ["l"="47.823,33.807"]
"Sense-X/X-Temporal" ["l"="50.642,2.719"]
"LeonHLJ/FAC-Net" ["l"="47.984,34.098"]
"msight-tech/research-v4d" ["l"="47.786,33.745"]
"flowerinheart/AttnSense" ["l"="48.266,34.157"]
"fredzzhang/upt" ["l"="47.358,33.807"]
"YueLiao/gen-vlkt" ["l"="47.359,33.794"]
"zhihou7/HOI-CL" ["l"="47.363,33.815"]
"YueLiao/CDN" ["l"="47.366,33.798"]
"kakaobrain/hotr" ["l"="47.379,33.795"]
"SherlockHolmes221/GGNet" ["l"="47.376,33.817"]
"yoyomimi/AS-Net" ["l"="47.389,33.804"]
"zhihou7/VCL" ["l"="47.4,33.786"]
"open-mmlab/denseflow" ["l"="47.932,34.007"]
"innerlee/setup" ["l"="49.449,29.462"]
"amazon-science/long-short-term-transformer" ["l"="50.622,30.951"]
"open-mmlab/pre-commit-hooks" ["l"="49.498,29.494"]
"demianzhang/thumos14-i3d" ["l"="47.96,34.042"]
"acherstyx/Compressed-Video-Reader" ["l"="47.553,34.121"]
"LukasBommes/rtsp-streamsync" ["l"="47.567,34.092"]
"openmovementproject/openmovement" ["l"="48.476,34.218"]
"OxWearables/stepcount" ["l"="48.519,34.193"]
"OxWearables/capture24" ["l"="48.478,34.177"]
"aschalkamp/UKBBprodromalPD" ["l"="48.503,34.187"]
"ahmedgamaleldin14/online-action-recognition" ["l"="47.844,33.712"]
"gorjanradevski/revisiting-spatial-temporal-layouts" ["l"="47.774,33.775"]
"JingweiJ/ActionGenome" ["l"="47.578,32.103"]
"eladb3/ORViT" ["l"="47.759,33.769"]
"coldmanck/VidHOI" ["l"="47.545,32.166"]
"pengzhansun/Counterfactual-Debiasing-Network" ["l"="47.762,33.786"]
"alibaba-mmai-research/TAdaConv" ["l"="48.03,33.829"]
"tobyperrett/trx" ["l"="48.125,33.895"]
"Anirudh257/strm" ["l"="48.106,33.886"]
"DigitalBiomarkerDiscoveryPipeline/Human-Activity-Recognition" ["l"="48.309,34.102"]
"BizhuWu/LRCN_PyTorch" ["l"="47.687,33.831"]
"DJAlexJ/LRCN-for-Video-Regression" ["l"="47.674,33.824"]
"artest08/LateTemporalModeling3DCNN" ["l"="47.825,33.791"]
"MVIG-SJTU/DIRV" ["l"="47.412,33.789"]
"SHI-Labs/Human-Object-Interaction-Detection" ["l"="47.417,33.772"]
"wanglimin/improved_trajectory" ["l"="47.824,34.033"]
"ChiYaoLa/TimeSeriesPredict" ["l"="47.971,34.386"]
"wangxiang1230/OHEM" ["l"="48.232,33.926"]
"alibaba-mmai-research/HyRSMPlusPlus" ["l"="48.198,33.925"]
"Pilhyeon/Learning-Action-Completeness-from-Points" ["l"="47.977,34.075"]
"ispc-lab/ACM-Net" ["l"="47.98,34.086"]
"UCSB-VRL/GTNet" ["l"="47.427,33.786"]
"flaviagiammarino/tcan-tensorflow" ["l"="47.925,34.376"]
"kkahatapitiya/Coarse-Fine-Networks" ["l"="47.791,33.614"]
"okankop/ASDNet" ["l"="47.446,34.066"]
"Overcautious/ADENet" ["l"="47.494,34.064"]
"tuanchien/asd" ["l"="47.507,34.072"]
"SRA2/SPELL" ["l"="47.468,34.06"]
"asrafulashiq/hamnet" ["l"="47.953,34.094"]
"mxbi/youtube8m-2019" ["l"="47.681,33.785"]
"jbrond/ActigraphCounts" ["l"="48.482,34.244"]
"hou-yz/MVDet" ["l"="47.207,34.166"]
"hou-yz/MultiviewX" ["l"="47.185,34.161"]
"hou-yz/MVDeTr" ["l"="47.188,34.176"]
"tteepe/EarlyBird" ["l"="47.302,34.126"]
"xjtlu-cvlab/3DROM" ["l"="47.202,34.184"]
"yeliudev/ConsNet" ["l"="47.39,33.771"]
"Alibaba-MIIL/STAM" ["l"="47.898,33.804"]
"srvCodes/continual-learning-benchmark" ["l"="48.324,34.141"]
"zhang-can/CoLA" ["l"="47.994,34.082"]
"harlanhong/MM2021-CO2-Net" ["l"="48.004,34.093"]
"MCG-NJU/SSD-LT" ["l"="47.716,34.028"]
"mcg2019/MOC-Detector" ["l"="47.699,34.024"]
"kaiqiangh/IDT_Fisher_Vector" ["l"="47.967,34.156"]
"jvgemert/apt" ["l"="47.867,34.127"]
"BoPang1996/TubeTK" ["l"="54.414,32.632"]
"BoPang1996/Semi-Coupled-Structure-for-visual-sequental-tasks" ["l"="47.511,33.904"]
"bbepoch/HoiTransformer" ["l"="47.39,33.789"]
"scwangdyd/large_vocabulary_hoi_detection" ["l"="47.349,33.773"]
"vadimkantorov/fastvideofeat" ["l"="47.643,34.129"]
"zbwglory/MV-release" ["l"="47.696,34.085"]
"gautamkumarjaiswal/videoClassification" ["l"="48.1,34.06"]
"birlrobotics/PMN" ["l"="47.385,33.852"]
"Andy1621/CT-Net" ["l"="47.892,33.789"]
"KimManjin/RSA" ["l"="62.551,36.375"]
"arunos728/SELFY" ["l"="47.841,33.785"]
"tianyuan168326/EAN-Pytorch" ["l"="47.855,33.785"]
"MCG-NJU/MGSampler" ["l"="47.937,33.782"]
"knmac/LCDC_release" ["l"="47.886,34.202"]
"khoiucd/LearningToCompare-Tensorflow" ["l"="48.475,33.838"]
"nguyentthong/CrossSummOptimalTransport" ["l"="48.455,33.82"]
"lehduong/poodle" ["l"="48.462,33.831"]
"silicx/GoldFromOres-BiLP" ["l"="34.451,31.882"]
"ceshine/yt8m-2019" ["l"="47.661,33.775"]
"lehduong/Knowledge-Distillation-by-Replacing-Cheap-Conv" ["l"="48.485,33.829"]
"rishikksh20/ViViT-pytorch" ["l"="48.031,33.779"]
"mx-mark/VideoTransformer-pytorch" ["l"="48.052,33.753"]
"drv-agwl/ViViT-pytorch" ["l"="48.058,33.736"]
"lucidrains/STAM-pytorch" ["l"="47.987,33.772"]
"lucidrains/TimeSformer-pytorch" ["l"="47.974,33.84"]
"google-research/scenic" ["l"="48.018,33.755"]
"haofanwang/video-swin-transformer-pytorch" ["l"="48.076,33.76"]
"noureldien/vivit_pytorch" ["l"="48.045,33.727"]
"cvdfoundation/kinetics-dataset" ["l"="48.004,33.839"]
"bomri/SlowFast" ["l"="48.059,33.71"]
"sallymmx/ActionCLIP" ["l"="48.051,33.881"]
"antoine77340/MIL-NCE_HowTo100M" ["l"="47.952,32.929"]
"RaivoKoot/Video-Dataset-Loading-Pytorch" ["l"="47.912,33.787"]
"YuxinZhaozyx/pytorch-VideoDataset" ["l"="47.922,33.716"]
"xiaobai1217/Awesome-Video-Datasets" ["l"="47.974,33.748"]
"microsoft/GLIP" ["l"="48.866,30.251"]
"google-research/big_vision" ["l"="48.957,30.256"]
"salesforce/LAVIS" ["l"="49.008,30.194"]
"google-research/vision_transformer" ["l"="50.739,29.626"]
"google/flax" ["l"="21.721,13.999"]
"mlfoundations/open_clip" ["l"="48.974,30.216"]
"facebookresearch/deit" ["l"="50.79,29.736"]
"Junhua-Liao/Light-ASD" ["l"="47.438,34.046"]
"Jiang-Yidi/TS-TalkNet" ["l"="47.419,34.044"]
"X-LANCE/MSDWILD" ["l"="47.427,34.058"]
"facebookresearch/VisualVoice" ["l"="40.398,5.068"]
"danmic/av-se" ["l"="40.398,5.038"]
"joonson/syncnet_python" ["l"="31.998,30.417"]
"EGO4D/audio-visual" ["l"="47.45,34.02"]
"afourast/avobjects" ["l"="39.657,5.634"]
"zcxu-eric/Ego4d_TalkNet_ASD" ["l"="47.467,34.021"]
"clovaai/lookwhostalking" ["l"="47.477,34.074"]
"TaoRuijie/Speaker-Recognition-Demo" ["l"="47.463,34.038"]
"chuckcho/video-caffe" ["l"="47.808,34.004"]
"cjw2021/QAHOI" ["l"="47.37,33.788"]
"zyong812/STIP" ["l"="47.355,33.782"]
"MuchHair/HQM" ["l"="47.346,33.791"]
"yunlong10/Awesome-LLMs-for-Video-Understanding" ["l"="47.523,30.059"]
"mbzuai-oryx/Video-ChatGPT" ["l"="47.532,30.076"]
"showlab/Awesome-Video-Diffusion" ["l"="33.558,31.208"]
"ChenHsing/Awesome-Video-Diffusion-Models" ["l"="33.506,31.243"]
"microsoft/XPretrain" ["l"="47.83,32.958"]
"iejMac/video2dataset" ["l"="33.639,31.396"]
"Vchitect/VBench" ["l"="33.547,31.318"]
"krantiparida/awesome-audio-visual" ["l"="39.614,5.621"]
"DAMO-NLP-SG/Video-LLaMA" ["l"="47.525,30.006"]
"ArrowLuo/CLIP4Clip" ["l"="47.909,32.98"]
"facebookresearch/vissl" ["l"="52.961,29.524"]
"MCG-NJU/STMixer" ["l"="47.675,33.955"]
"MCG-NJU/SportsMOT" ["l"="54.371,32.407"]
"MCG-NJU/BasicTAD" ["l"="47.983,34.008"]
"Sense-X/UniFormer" ["l"="48.062,33.826"]
"Martlgap/face-alignment-mtcnn" ["l"="47.356,34.129"]
"Blueblue4/IoU-AwareCalibration" ["l"="47.362,34.106"]
"tobyperrett/few-shot-action-recognition" ["l"="48.184,33.892"]
"alibaba-mmai-research/HyRSM" ["l"="48.16,33.925"]
"ffmpbgrnn/CMN" ["l"="48.163,33.907"]
"lovelyqian/AMeFu-Net" ["l"="48.184,33.913"]
"Blueblue4/Object-Detection-Confidence-Bias" ["l"="47.348,34.106"]
"microsoft/VideoX" ["l"="48.01,33.03"]
"OpenGVLab/VideoMAEv2" ["l"="48.083,33.864"]
"gtoderici/sports-1m-dataset" ["l"="47.738,33.824"]
"fredzzhang/hicodet" ["l"="47.368,33.828"]
"SherlockHolmes221/DOQ" ["l"="47.344,33.804"]
"Nmegha2601/activitygraph_transformer" ["l"="47.966,34.066"]
"davide-coccomini/TimeSformer-Video-Classification" ["l"="48.035,33.852"]
"The-AI-Summer/self-attention-cv" ["l"="61.927,36.939"]
"antoine77340/S3D_HowTo100M" ["l"="47.934,32.936"]
"boheumd/ASM-Loc" ["l"="47.998,34.101"]
"MengyuanChen21/CVPR2022-FTCL" ["l"="48.021,34.103"]
"MengyuanChen21/ECCV2022-DELU" ["l"="48.031,34.099"]
"zhou745/GauFuse_WSTAL" ["l"="48.015,34.087"]
"airsplay/vimpac" ["l"="47.758,32.995"]
"microsoft/SwinBERT" ["l"="47.951,32.895"]
"xyzforever/BEVT" ["l"="48.154,33.772"]
"alexandrosstergiou/adaPool" ["l"="47.658,34.232"]
"rentainhe/pytorch-pooling" ["l"="47.64,34.225"]
"wofmanaf/SA-Net" ["l"="53.587,31.124"]
"implus/GFocalV2" ["l"="50.768,30.262"]
"yifan123/IC-Conv" ["l"="-54.154,-12.312"]
"d-li14/PSConv" ["l"="53.672,31.095"]
"whwu95/MVFNet" ["l"="48.246,33.846"]
"whwu95/DSANet" ["l"="48.279,33.839"]
"whwu95/Text4Vis" ["l"="48.187,33.854"]
"alibaba-mmai-research/DiST" ["l"="48.154,33.837"]
"daniel-code/TubeViT" ["l"="48.12,33.82"]
"whwu95/BIKE" ["l"="48.139,33.845"]
"whwu95/ATM" ["l"="48.122,33.834"]
"vision4robotics/TCTrack" ["l"="54.433,33.68"]
"ptirupat/MLAD" ["l"="47.772,33.549"]
"dairui01/PDAN" ["l"="47.779,33.576"]
"qinzhi-0110/Temporal-Context-Aggregation-Network-Pytorch" ["l"="48.024,34.048"]
"sming256/ETAD" ["l"="48.051,34.078"]
"Tian0426/CL-HAR" ["l"="48.367,34.179"]
"bulatkh/csshar_tfa" ["l"="48.354,34.194"]
"harkash/contrastive-predictive-coding-for-har" ["l"="48.377,34.17"]
"Rxannro/SWL-Adapt" ["l"="48.4,34.161"]
"Rxannro/HMGAN" ["l"="48.4,34.148"]
"wangxiang1230/SSTAP" ["l"="48.074,34.033"]
"zhang-can/UP-TAL" ["l"="48.018,34.062"]
"dk-kim/DFWSGAR" ["l"="47.56,33.865"]
"mila-iqia/blocks-examples" ["l"="44.875,27.723"]
"Simon4Yan/Meta-set" ["l"="47.026,34.222"]
"Simon4Yan/Accuracy-estimation-with-self-supervision" ["l"="47.008,34.227"]
"jidhu-mohan/Activity-Detection-using-IMU-sensor" ["l"="48.414,34.109"]
"auduno/deepdraw" ["l"="47.81,33.97"]
"kylemcdonald/deepdream" ["l"="47.756,34.013"]
"xlliu7/E2E-TAD" ["l"="48.001,34.044"]
"dingfengshi/ReAct" ["l"="48.061,34.071"]
"MCG-NJU/PointTAD" ["l"="48.054,34.056"]
"klauscc/TALLFormer" ["l"="48.064,34.021"]
"HumamAlwassel/RefineLoc" ["l"="47.979,34.114"]
"JacobYuan7/RLIP" ["l"="47.332,33.785"]
"Jie-su/Awesome_Human_Activity_Recognition" ["l"="48.323,34.179"]
"Jie-su/BPD" ["l"="48.333,34.201"]
"lovelyqian/Embodied-One-Shot-Video-Recognition" ["l"="48.218,33.913"]
"elb3k/vtn" ["l"="48.064,33.683"]
"mariusbock/dl-for-har" ["l"="48.282,34.179"]
"leftthomas/ACRNet" ["l"="48.031,34.162"]
"lizhilin-ustc/AAAI2023-AICL" ["l"="48.045,34.159"]
"layer6ai-labs/ASL" ["l"="47.993,34.136"]
"skelemoa/tal-hmo" ["l"="48.031,34.078"]
"1jsingh/semantic-guidance" ["l"="47.047,34.232"]
"dairui01/MS-TCT" ["l"="47.791,33.585"]
"tagperson/tagperson-blender" ["l"="47.08,34.25"]
"Martlgap/FaceIDLight" ["l"="47.331,34.101"]
"ltttpku/ADA-CM" ["l"="47.301,33.808"]
"MCG-NJU/FSL-Video" ["l"="48.295,33.868"]
"HongguangZhang/arn-eccv20-master" ["l"="48.218,33.891"]
"BoPang1996/PGT" ["l"="47.475,33.914"]
"DirtyHarryLYL/sampling-argmax" ["l"="47.451,33.922"]
"papermsucode/mutual-modality-learning" ["l"="47.82,33.747"]
"nhammerla/deepHAR" ["l"="48.227,34.108"]
"yscacaca/DeepSense" ["l"="48.217,34.13"]
"drewanye/har-joint-model" ["l"="48.193,34.136"]
"wisdal/Deep-Learning-for-Sensor-based-Human-Activity-Recognition" ["l"="48.197,34.087"]
"youngminoh7/DeepConvLSTM_Python3" ["l"="48.193,34.122"]
"facebookresearch/mae_st" ["l"="48.116,33.8"]
"facebookresearch/mae" ["l"="50.721,29.589"]
"OpenGVLab/VideoMamba" ["l"="49.175,34.184"]
"fcakyon/video-transformers" ["l"="48.091,33.692"]
"SforAiDl/vformer" ["l"="-3.726,23.176"]
"facebookresearch/mvit" ["l"="48.075,33.779"]
"karttikeya/minREV" ["l"="48.122,33.731"]
"facebookresearch/hiera" ["l"="48.119,33.775"]
"microsoft/CSWin-Transformer" ["l"="49.133,33.077"]
"OpenGVLab/efficient-video-recognition" ["l"="48.127,33.857"]
"OliverRensu/Shunted-Transformer" ["l"="49.129,33.091"]
"Chenglin-Yang/LVT" ["l"="48.905,33.056"]
"raoyongming/HorNet" ["l"="49.048,33.055"]
"IBM/CrossViT" ["l"="49.187,33.09"]
"facebookresearch/Motionformer" ["l"="48.136,33.708"]
"Intelligent-Computing-Lab-Yale/SNN_HAR" ["l"="48.387,34.198"]
"sming256/OpenTAD" ["l"="48.068,33.977"]
"sauradip/STALE" ["l"="48.085,33.941"]
"ju-chen/Efficient-Prompt" ["l"="48.086,33.92"]
"lizhi1104/HAAN" ["l"="48.01,34.142"]
"fshp971/mcmc-unlearning" ["l"="48.098,34.091"]
"tmllab/2022_NeurIPS_PICMM" ["l"="48.085,34.1"]
"xianruizhong/SpHAM" ["l"="48.089,34.085"]
"fshp971/robust-unlearnable-examples" ["l"="48.081,34.091"]
"Raiden-Zhu/Generalization-of-DSGD" ["l"="48.098,34.101"]
"OpenGVLab/UniFormerV2" ["l"="48.1,33.834"]
"sail-sg/poolformer" ["l"="49.031,33.08"]
"facebookresearch/omnivore" ["l"="47.721,32.943"]
"Meituan-AutoML/Twins" ["l"="49.122,33.112"]
"czczup/ViT-Adapter" ["l"="48.832,30.241"]
"SHI-Labs/Neighborhood-Attention-Transformer" ["l"="49.082,33.057"]
"whai362/PVT" ["l"="50.757,29.87"]
"dingmyu/davit" ["l"="49.071,33.096"]
"fredzzhang/pvic" ["l"="47.312,33.787"]
"R00Kie-Liu/TA2N" ["l"="48.2,33.935"]
"NVlabs/RelViT" ["l"="47.351,33.72"]
"NVlabs/Bongard-HOI" ["l"="47.356,33.742"]
"4paradigm-CV/SE-STAD" ["l"="47.647,33.937"]
"Finspire13/DiffAct" ["l"="47.937,34.169"]
"LTContext/LTContext" ["l"="47.942,34.182"]
"ZijiaLewisLu/CVPR2024-FACT" ["l"="47.925,34.182"]
"Thinksky5124/SVTAS" ["l"="47.908,34.202"]
"muzairkhattak/ViFi-CLIP" ["l"="50.438,38.277"]
"MartinXM/GAP" ["l"="46.925,34.573"]
"CryhanFang/CLIP2Video" ["l"="47.918,32.996"]
"MengyuanChen21/CVPR2023-CMPAE" ["l"="48.048,34.112"]
"MengyuanChen21/CVPR2023-OWTAL" ["l"="48.045,34.121"]
"MengyuanChen21/ICLR2024-REDL" ["l"="48.057,34.127"]
"sibosutd/cnn-timeseries" ["l"="48.255,34.009"]
"jianboyang/CNNHAR" ["l"="48.235,34.025"]
"Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset" ["l"="47.593,33.939"]
"Whiffe/yolov5-slowfast-deepsort-PytorchVideo" ["l"="47.553,33.947"]
"wufan-tb/yolo_slowfast" ["l"="-54.932,-13.667"]
"yjh0410/AVA_Dataset" ["l"="47.594,33.922"]
"Huterox/Real-time-online-multi-target-behavior-detection-project-for-face-recognition" ["l"="47.549,33.932"]
"alibaba-mmai-research/CLIP-FSAR" ["l"="48.17,33.89"]
"OreoChocolate/MUREN" ["l"="47.335,33.775"]
"Cogito2012/OpenTAL" ["l"="48.042,34.134"]
"RenHuan1999/CVPR2023_P-MIL" ["l"="48.027,34.133"]
"oswaldoludwig/Human-Action-Recognition-with-Keras" ["l"="47.908,34.026"]
"imatge-upc/activitynet-2016-cvprw" ["l"="47.857,34.028"]
"TaeSoo-Kim/TCNActionRecognition" ["l"="47.072,34.628"]
"TalalWasim/Vita-CLIP" ["l"="48.146,33.888"]
"MediaBrain-SJTU/BCL" ["l"="63.315,12.692"]
"TengdaHan/TemporalAlignNet" ["l"="47.837,32.937"]
"buxiangzhiren/ContextLoc" ["l"="48.102,34.035"]
"mrwu-mac/EoID" ["l"="47.362,33.767"]
"enlighten0707/Body-Part-Map-for-Interactiveness" ["l"="47.322,33.762"]
"yorkeyao/Automated-Retail-Checkout" ["l"="47.043,34.205"]
"ruiwang2021/mvd" ["l"="48.146,33.811"]
"webber2933/iCLIP" ["l"="47.648,33.95"]
"nguyentthong/CLNTM" ["l"="48.473,33.818"]
"ShiYaya/emscore" ["l"="47.473,34.183"]
"lzp870/RSFD" ["l"="47.494,34.167"]
"neu-vi/Diag-HOI" ["l"="47.331,33.708"]
"lehduong/NPTM" ["l"="48.463,33.805"]
"xiaoxiaomiao323/MSA" ["l"="47.39,34.067"]
"VinAIResearch/fsvc-ata" ["l"="48.346,33.855"]
"terryum/TensorFlow_Exercises" ["l"="-4.653,-23.044"]
"sjchoi86/Tensorflow-101" ["l"="-4.73,-23.04"]
"guillaume-chevalier/Awesome-Deep-Learning-Resources" ["l"="47.675,28.639"]
"stuarteiffert/RNN-for-Human-Activity-Recognition-using-2D-Pose-Input" ["l"="47.094,34.693"]
"nicodjimenez/lstm" ["l"="-9.232,12.532"]
"ZheC/Realtime_Multi-Person_Pose_Estimation" ["l"="31.66,28.089"]
"guillaume-chevalier/seq2seq-signal-prediction" ["l"="44.846,24.197"]
"NLeSC/mcfly" ["l"="48.305,34.245"]
"NLeSC/mcfly-tutorial" ["l"="48.315,34.268"]
"FrancisArgnR/Time-series---deep-learning---state-of-the-art" ["l"="48.343,34.296"]
"dmbee/seglearn" ["l"="43.654,26.821"]
"sauradip/MUPPET" ["l"="48.123,33.945"]
"benedettaliberatori/T3AL" ["l"="50.618,38.227"]
"R00Kie-Liu/Sampler" ["l"="48.22,33.935"]
"MCG-NJU/EVAD" ["l"="47.638,33.907"]
"joslefaure/HIT_ava" ["l"="47.628,33.933"]
"Hope1337/YOWOv3" ["l"="47.609,33.925"]
"yjh0410/YOWOF" ["l"="47.621,33.907"]
"ZiweiXU/DTL-action-segmentation" ["l"="47.913,34.256"]
"derkbreeze/AwesomeActionSegmentation" ["l"="47.913,34.237"]
"jrbtaylor/ActivityNet" ["l"="47.791,34.091"]
"tcvrick/dynamic-images-for-action-recognition" ["l"="47.729,34.055"]
"yifita/action.sr_cnn" ["l"="47.759,34.085"]
"VinAIResearch/tise-toolbox" ["l"="48.419,33.837"]
"linziyi96/st-adapter" ["l"="48.167,33.858"]
"park-jungin/DualPath" ["l"="48.169,33.842"]
"EdisonLeeeee/Awesome-Masked-Autoencoders" ["l"="52.811,29.38"]
"Alpha-VL/ConvMAE" ["l"="52.866,29.376"]
"OpenGVLab/unmasked_teacher" ["l"="47.778,32.957"]
"facebookresearch/flip" ["l"="48.197,33.74"]
"dingfengshi/tridetplus" ["l"="48.083,34.017"]
"yingsen1/UniMD" ["l"="48.076,34.002"]
"TuanTNG/TemporalMaxer" ["l"="48.072,34.048"]
"whwu95/GPT4Vis" ["l"="48.19,33.833"]
"whwu95/Cap4Video" ["l"="47.871,33.007"]
"alibaba-mmai-research/Masked-Action-Recognition" ["l"="48.176,33.815"]
"wengzejia1/Open-VCLIP" ["l"="48.215,33.853"]
"farewellthree/STAN" ["l"="48.203,33.813"]
"syyeung/frameglimpses" ["l"="47.814,34.081"]
"cabaf/sparseprop" ["l"="47.804,34.067"]
"gurkirt/actNet-inAct" ["l"="47.862,34.086"]
"Yuhan-Shen/ProTAS" ["l"="47.935,34.191"]
"mingu6/action_seg_ot" ["l"="47.935,34.208"]
"walker1126/Latent_Action_Composition" ["l"="47.952,34.194"]
"taoyang1122/adapt-image-models" ["l"="48.142,33.868"]
"JieShibo/PETL-ViT" ["l"="50.222,38.231"]
"alibaba-mmai-research/MoLo" ["l"="48.195,33.906"]
"IntelLabs/GraVi-T" ["l"="47.451,34.085"]
"wangkang12/yolo_slowfast-master" ["l"="47.522,33.95"]
"LiangZai-Embedded/ThermalGesture_ESP32" ["l"="48.412,34.038"]
"Artanic30/HOICLIP" ["l"="47.295,33.779"]
"Jeeseung-Park/ViPLO" ["l"="47.315,33.772"]
"fubel/synthehicle" ["l"="47.336,34.12"]
"xiaomabufei/FGAHOI" ["l"="47.301,33.745"]
"XinyuSun/MME" ["l"="61.597,37.286"]
"MCG-NJU/VideoMAE-Action-Detection" ["l"="48.143,33.826"]
"MCG-NJU/AMD" ["l"="48.246,33.748"]
"xuyu0010/ATCoN" ["l"="47.774,34.196"]
"JacobYuan7/RLIPv2" ["l"="47.324,33.799"]
"derkbreeze/LPT" ["l"="47.874,34.321"]
"FedeSpu/HVQ" ["l"="47.888,34.311"]
"Yanyi-Zhang/Awesome-Compositional-Zero-Shot" ["l"="47.226,33.731"]
"soberguo/HOIGen" ["l"="47.254,33.749"]
"wdkhuans/DynamicWHAR" ["l"="48.431,34.148"]
"hildekuehne/HTK_actionRecognition" ["l"="47.803,34.21"]
"TaoRuijie/AVCleanse" ["l"="47.387,34.046"]
"olga-zats/GTDA" ["l"="47.898,34.285"]
"niais/Awesome-Skeleton-based-Action-Recognition" ["l"="47.002,34.649"]
"zcxu-eric/AVA-AVD" ["l"="47.402,34.034"]
"Tiago-Roxo/WASD" ["l"="47.414,34.071"]
"Junhua-Liao/LR-ASD" ["l"="47.42,34.028"]
"TaoRuijie/MFV-KSD" ["l"="47.406,34.056"]
"NVlabs/FasterViT" ["l"="49.017,32.999"]
"facebookresearch/ConvNeXt-V2" ["l"="48.976,33.022"]
"facebookresearch/ToMe" ["l"="48.99,33.157"]
"NVlabs/RADIO" ["l"="64.088,2.918"]
"facebookresearch/MetaCLIP" ["l"="48.898,30.287"]
"brjathu/LART" ["l"="30.351,28.824"]
"apple/ml-aim" ["l"="46.514,30.627"]
"mit-han-lab/efficientvit" ["l"="48.711,30.03"]
"Lupin1998/Awesome-MIM" ["l"="52.784,29.394"]
"apple/ml-fastvit" ["l"="48.949,32.981"]
"lgzlIlIlI/Boosting-WTAL" ["l"="48.028,34.118"]
"haonanwang0522/SRPose" ["l"="48.302,33.732"]
"haonanwang0522/GTPT" ["l"="48.276,33.734"]
"IDEA-Research/DiffHOI" ["l"="47.27,33.78"]
"ltttpku/CMMP" ["l"="47.244,33.789"]
"ChelsieLei/EZ-HOI" ["l"="47.264,33.8"]
"jindongwang/MachineLearning" ["l"="50.589,28.022"]
"TianzhongSong/Real-Time-Action-Recognition" ["l"="47.057,34.689"]
"lmb-freiburg/flownet2-docker" ["l"="47.911,33.973"]
"Charles-Xie/CQL" ["l"="47.322,33.749"]
"sming256/AdaTAD" ["l"="48.102,34.006"]
"wangzheallen/vsad" ["l"="47.752,34.103"]
"wanglimin/Places205-VGGNet" ["l"="47.738,34.123"]
"ltttpku/CMD-SE-release" ["l"="47.275,33.758"]
"scwangdyd/promting_hoi" ["l"="47.26,33.767"]
"acherstyx/CoCap" ["l"="47.524,34.144"]
"fomorians/distracted-drivers-keras" ["l"="33.267,30.053"]
"Visual-AI/FROSTER" ["l"="48.22,33.832"]
"pipixin321/HR-Pro" ["l"="48.044,34.173"]
"zohrehghaderi/VASTA" ["l"="47.508,34.157"]
"neu-vi/FleVRS" ["l"="47.319,33.693"]
"bryanyzhu/deepOF" ["l"="47.711,34.047"]
"BonnBytes/PyTorch-FWD" ["l"="47.892,34.326"]
"Jiang-Yidi/FlatTrajectoryDistillation_FTD" ["l"="47.365,34.073"]
"gulvarol/ltc" ["l"="47.779,34.116"]
"gudongfeng/C3D-tensorflow" ["l"="47.984,33.995"]
"openimages/dataset" ["l"="50.533,33.067"]
"ethereon/caffe-tensorflow" ["l"="50.47,33.139"]
"rh20624/Awesome-IMU-Sensing" ["l"="48.517,34.166"]
"kingdomrush2/CrossHAR" ["l"="48.544,34.163"]
"OpenGVLab/video-mamba-suite" ["l"="48.112,33.987"]
"hotfinda/VideoMambaPro" ["l"="48.16,33.987"]
"OpenGVLab/VideoChat-Flash" ["l"="47.552,30.13"]
"NVlabs/LITA" ["l"="47.614,30.079"]
"sudo-Boris/mr-Blip" ["l"="48.04,33.152"]
"sauradip/DiffusionTAD" ["l"="48.13,33.975"]
"yangle15/DyFADet-pytorch" ["l"="48.113,33.968"]
"MengyuanChen21/Awesome-Visual-Dialog" ["l"="48.083,34.141"]
"jiuntian/interactdiffusion" ["l"="47.207,33.769"]
"kamwoh/partcraft" ["l"="47.172,33.762"]
"tteepe/TrackTacular" ["l"="47.28,34.135"]
"cvlab-epfl/MVFlow" ["l"="47.258,34.143"]
"harvitronix/continuous-online-video-classification-blog" ["l"="48.003,33.927"]
"Ectsang/3D-CNN-Keras" ["l"="48.102,33.937"]
"melmikaty/3D_CNN" ["l"="48.143,33.941"]
"TianzhongSong/3D-ConvNets-for-Action-Recognition" ["l"="48.004,33.986"]
"pantheon5100/3D-CNN-resnet-keras" ["l"="48.083,33.96"]
"dhuy228/augmented-volumetric-image-generator" ["l"="48.091,33.97"]
"pipidog/DeepTimeSeries" ["l"="48.36,34.324"]
"jibikbam/CNN-3D-images-Tensorflow" ["l"="-33.072,16.512"]
"dipakkr/3d-cnn-action-recognition" ["l"="48.064,33.924"]
"bityangke/3DCNN" ["l"="48.02,33.921"]
"paolo626/3DCNN-with-keras" ["l"="48.049,33.938"]
"MCG-NJU/AWT" ["l"="48.295,33.712"]
"MCG-NJU/ProVP" ["l"="48.319,33.688"]
"MCG-NJU/MoG-VFI" ["l"="48.32,33.702"]
"forwchen/yt8m" ["l"="47.708,33.874"]
"lmb-freiburg/flownet2" ["l"="65.027,4.156"]
"sampepose/flownet2-tf" ["l"="65.055,4.176"]
"MCG-NJU/ZeroI2V" ["l"="48.334,33.675"]
"MengyuanChen21/Awesome-Evidential-Deep-Learning" ["l"="48.085,34.166"]
"MengyuanChen21/NeurIPS2024-CSP" ["l"="48.071,34.15"]
"farewellthree/PPLLaVA" ["l"="48.258,33.791"]
"robert80203/EgoPER_official" ["l"="47.619,32.893"]
"Feynman27/realtime-action-detection" ["l"="47.712,34.013"]
"antoine77340/Mixture-of-Embedding-Experts" ["l"="47.979,32.957"]
"Relja/netvlad" ["l"="59.334,9.445"]
"shamangary/LOUPE_Keras" ["l"="47.698,33.857"]
"HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis" ["l"="47.913,33.872"]
"antoine77340/video_feature_extractor" ["l"="47.978,32.914"]
"danieljf24/awesome-video-text-retrieval" ["l"="47.938,32.988"]
"facebookresearch/ClassyVision" ["l"="50.934,30.061"]
"SBoyNumber1/LSTM-video-classification" ["l"="48.014,33.894"]
"alxcnwy/Deep-Neural-Networks-for-Video-Classification" ["l"="48.031,33.899"]
"talhasaruhan/video-action-classification" ["l"="48.007,33.905"]
"vighneshvnkt/keras-deep-learning" ["l"="48.021,33.91"]
"yscacaca/HHAR-Data-Process" ["l"="48.221,34.168"]
"yscacaca/DeepIoT" ["l"="48.207,34.161"]
"LDenninger/CamContextI2V" ["l"="47.879,34.333"]
"STAIR-Lab-CIT/STAIR-actions" ["l"="47.75,34.171"]
"arpane4c5/ActivityNet" ["l"="47.791,34.103"]
"yyuanad/Pytorch_C3D_Feature_Extractor" ["l"="48.087,33"]
"ekosman/AnomalyDetectionCVPR2018-Pytorch" ["l"="53.078,14.655"]
"jx-zhong-for-academic-purpose/GCN-Anomaly-Detection" ["l"="53.061,14.653"]
"TwentyBN/GulpIO" ["l"="47.509,33.982"]
"willprice/GulpIO2" ["l"="47.481,33.982"]
"achalddave/predictive-corrective" ["l"="47.769,34.137"]
}